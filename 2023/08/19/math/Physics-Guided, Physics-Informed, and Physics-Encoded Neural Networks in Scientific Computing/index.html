<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"github.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="摘要最近计算能力的突破使得机器学习和深度学习可用于推进许多领域的科学计算，包括流体力学、固体力学、材料科学等，神经网络在这种混合科学中发挥着核心作用。 由于其固有的架构，传统神经网络在数据稀疏时无法成功训练以及确定范围，许多科学和工程领域就是这种情况。 尽管如此，神经网络给训练期间的物理驱动或基于知识的约束提供了坚实的基础。 一般来说，存在三种不同的神经网络框架来强化底层物理：(i) 物理引导神经">
<meta property="og:type" content="article">
<meta property="og:title" content="Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks in Scientific Computing">
<meta property="og:url" content="https://github.com/sophia-hxw/sophia-hxw.github.io/2023/08/19/math/Physics-Guided,%20Physics-Informed,%20and%20Physics-Encoded%20Neural%20Networks%20in%20Scientific%20Computing/index.html">
<meta property="og:site_name" content="橦言无忌">
<meta property="og:description" content="摘要最近计算能力的突破使得机器学习和深度学习可用于推进许多领域的科学计算，包括流体力学、固体力学、材料科学等，神经网络在这种混合科学中发挥着核心作用。 由于其固有的架构，传统神经网络在数据稀疏时无法成功训练以及确定范围，许多科学和工程领域就是这种情况。 尽管如此，神经网络给训练期间的物理驱动或基于知识的约束提供了坚实的基础。 一般来说，存在三种不同的神经网络框架来强化底层物理：(i) 物理引导神经">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-08-19T01:35:42.000Z">
<meta property="article:modified_time" content="2023-09-26T06:49:20.722Z">
<meta property="article:author" content="xinwen">
<meta property="article:tag" content="math">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://github.com/sophia-hxw/sophia-hxw.github.io/2023/08/19/math/Physics-Guided,%20Physics-Informed,%20and%20Physics-Encoded%20Neural%20Networks%20in%20Scientific%20Computing/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks in Scientific Computing | 橦言无忌</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">橦言无忌</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">一个不想改变世界的程序媛</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">87</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">19</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">101</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/sophia-hxw" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://github.com/sophia-hxw/sophia-hxw.github.io/2023/08/19/math/Physics-Guided,%20Physics-Informed,%20and%20Physics-Encoded%20Neural%20Networks%20in%20Scientific%20Computing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me.jpg">
      <meta itemprop="name" content="xinwen">
      <meta itemprop="description" content="想到哪儿记到哪儿的技术博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="橦言无忌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks in Scientific Computing
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-08-19 09:35:42" itemprop="dateCreated datePublished" datetime="2023-08-19T09:35:42+08:00">2023-08-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-09-26 14:49:20" itemprop="dateModified" datetime="2023-09-26T14:49:20+08:00">2023-09-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">math</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/08/19/math/Physics-Guided,%20Physics-Informed,%20and%20Physics-Encoded%20Neural%20Networks%20in%20Scientific%20Computing/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/08/19/math/Physics-Guided,%20Physics-Informed,%20and%20Physics-Encoded%20Neural%20Networks%20in%20Scientific%20Computing/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>36k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>33 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>最近计算能力的突破使得机器学习和深度学习可用于推进许多领域的科学计算，包括流体力学、固体力学、材料科学等，神经网络在这种混合科学中发挥着核心作用。 由于其固有的架构，传统神经网络在数据稀疏时无法成功训练以及确定范围，许多科学和工程领域就是这种情况。 尽管如此，神经网络给训练期间的物理驱动或基于知识的约束提供了坚实的基础。 一般来说，存在三种不同的神经网络框架来强化底层物理：(i) 物理引导神经网络 (PgNN)、(ii) 物理信息神经网络 (PiNN) 和 (iii) 物理编码神经网络 (PeNN)，这些方法为加速复杂多尺度多物理现象的数值建模提供了明显的优势。 此外，神经算子（NO）的最新发展为这些新的模拟范式增加了另一个维度，特别是当需要复杂的多物理系统的实时预测时。 所有这些模型也都有其独特的缺点和局限性，需要进一步的基础研究。 本研究旨在回顾科学计算研究中使用的四种神经网络框架（即 PgNN、PiNN、PeNN 和 NO），回顾了最先进的架构及其应用，讨论了局限性，并提出了在改进算法、考虑因果关系、扩展应用以及耦合科学和深度学习求解器方面的未来研究机会。 这篇批判性评论为研究人员和工程师提供了一个坚实的起点，帮助他们理解如何将不同的物理层集成到神经网络中。</p>
<p><strong>PGNN,PINN,PENN</strong><br><span id="more"></span></p>
<h1 id="一，介绍"><a href="#一，介绍" class="headerlink" title="一，介绍"></a>一，介绍</h1><p>机器学习（ML）和深度学习（DL）正在成为推动流体力学[1]、固体力学[2]、材料科学[3]等各个领域科学研究和计算的关键技术。 具有数千个处理器的多万亿次浮点运算机器出现并用于科学计算，并结合先进的基于感知的实验，预示着科学和工程领域结构化和非结构化异构数据的爆炸性增长。 ML 和 DL 方法首先被引入科学计算中，以解决缺乏有效数据进行建模的问题，该问题阻碍了科学家与异构且复杂的数据快速交互[4]。 这些方法显示出变革潜力，因为它们能够探索广阔的设计空间、识别多维联系以及管理不适定问题[5,6,7]。 然而，传统的机器学习和深度学习方法无法从复杂的多维数据中提取解释信息和专业知识，它们在映射观测或计算数据方面可能有效，但它们的预测可能在物理上不合理或可疑，导致概括性较差[8,9,10]。 因此，科学家们最初认为这些方法是一个神奇的黑匣子，缺乏坚实的数学基础，无法解释。 尽管如此，学习技术和理论构成了一种新的范式，可以比传统求解器更快地精确解决科学和实际问题。</p>
<p>深度学习（即模仿人脑的神经网络）和科学计算具有类似的历史和发展联系，例如可微性[8]。 图 1 显示了大量科学计算和深度学习方法的发展历史示意图（仅包括开创性的工作）。 在过去的十年中，深度学习和计算能力的突破使得深度学习能够在各种科学计算中使用，特别是在流体力学 [1, 10, 11]、固体力学 [2, 12, 13] 和材料科学领域 [14,15,16]，尽管牺牲了部分准确性和通用性[17]。 这些数据驱动的方法通常用于实现以下目标之一：（i）使用代理建模加速直接数值模拟[18]，（ii）加速伴随敏感性分析[8]，（iii）加速概率编程[19] ，以及（iv）加速逆问题[20]。 例如，在第一个目标中，系统的物理参数（例如尺寸、质量、动量、温度等）被用作输入来预测系统的下一个状态或其效果（即输出），并且在最后一个目标中，系统的输出（例如具有目标属性的材料）用作输入来推断满足要求的内在物理属性（即模型的输出）。 为了实现这些目标，可以构建轻量级深度学习模型来部分或完全取代科学计算过程中的瓶颈步骤 [17,21,22]。</p>
<p>由于传统深度学习方法的内在架构，其学习仅限于进行训练的数据集范围（例如特定边界条件、材料类型、时空离散化等），并且无法成功推理在任何未见的条件下（例如，新的几何形状、新的材料类型、新的边界条件等）。 由于大多数科学领域不是面向（大）数据的领域，并且无法提供涵盖所有可能条件的综合数据集，因此这些基于稀疏数据集训练的模型可以加速，但不能预测[22]。 因此，在对可用的稀疏数据点进行训练时，利用丰富的先验知识、基础物理学和领域专业知识来进一步约束这些模型是合乎逻辑的。 神经网络 (NN) 更适合在训练期间消化物理驱动或基于知识的约束。 根据基础物理学的整合方式，作者将科学计算中的神经网络应用分为三种不同的类型：(i) 物理引导神经网络 (PgNN)、(ii) 物理信息神经网络 (PiNN) 和 (iii) 物理编码神经网络（PeNN）。</p>
<p>在基于 PgNN 的模型中，现成的监督深度学习技术用于构建格式化输入和输出之间的代理映射，这些映射是在受控设置中使用实验和计算生成的，并通过广泛的过程进行管理，以确保符合物理原理和基本规则 [22]，此类模型需要丰富且足够的数据集才能可靠地进行训练和使用。 基于 PgNN 的模型使用具有未知参数 $w$ 的适当函数 $F$ 将一组输入 $x$ 映射到一组相关输出 $y$，使得 $y = F(x;w)$。 通过指定 $F$ 的特定结构，数据驱动方法通常会尝试微调参数 $w$，以使真实值 $\hat y$ 与模型预测值 $y$ 之间的总体误差最小化 [7]。 对于复杂的物理系统，由于数据获取成本高昂，数据可能很稀疏[41]。 绝大多数最先进的 PgNN 缺乏鲁棒性，无法实现任何泛化保证（即插值 [38, 42] 和外推 [43]）。 为了解决这个问题，引入了 PiNN 来执行监督学习任务，同时遵循一般非线性微分方程形式的给定物理定律 [44,10,45,46,6]。</p>
<p>基于 PiNN 的模型通过结合由物理方程残差和边界约束组成的弱损失函数来保证物理定律，他们利用自动微分[47]来求神经网络输出与其输入（即时空坐标和模型参数）的微分。 通过最小化损失函数，网络可以非常接近解[48, 49]。 因此，PiNN 为新的建模和计算范式奠定了基础，通过数学物理领域的长期成就丰富了深度学习 [38, 44]。 PiNN 模型面临着理论（例如，收敛性和稳定性 [50,6,51]）和实现（例如，神经网络设计、边界条件管理和优化方面）相关的许多限制 [40, 10 ]。 此外，在先验不完全了解的显式复杂动力学微分方程时，PiNN 会遇到严重的局限性 [52]。 对于这种情况，人们提出了另一类深度学习方法，称为物理编码神经网络（PeNN）[40]。</p>
<p>基于 PeNN 的模型利用先进的架构来解决 PgNN 和 PiNN 模型遇到的数据稀疏和缺乏泛化的问题。 基于 PeNN 的模型将已知物理强制编码到其核心架构中（例如 NeuralODE [53]），通过构建，基于 PeNN 的模型将神经网络的学习能力从实例学习（由 PgNN 和 PiNN 架构强加）扩展到连续学习 [53]。 PeNN 中底层物理的编码机制与 PiNN [54, 55] 中的编码机制根本不同，尽管它们可以集成以实现模型所需的非线性。 与 PgNN 和 PiNN 相比，PeNN 范式生成的神经网络在数据稀疏性和模型泛化性方面提供了更好的性能[40]。</p>
<p>还有另一类监督学习方法不太适合上面定义的 PgNN、PiNN 和 PeNN 类别，这些模型被称为神经算子，使用先进的架构（例如 DeepONet [39, 56]）学习底层的线性和非线性连续算子，例如积分和分数拉普拉斯算子。 神经算子的数据密集型学习过程可能类似于基于 PgNN 的模型学习，因为两者都使用标记的输入输出数据对来强化问题的物理原理。 然而，神经算子与基于 PgNN 的模型有很大不同，后者由于参数化不足而缺乏泛化属性。 神经算子可以与 PiNN 和 PeNN 方法相结合来训练模型，该模型可以以极高的泛化精度学习物理系统中的复杂非线性[43]。 对于需要实时推理的应用来说，神经算子的鲁棒性是一个显着特征[57]。</p>
<p>这篇综述论文主要面向对神经网络在计算流体和固体力学中的应用感兴趣的科学计算社区，它讨论了 PgNN、PiNN、PeNN 和神经算子的一般架构、优点和局限性，并回顾了这些方法在流体和固体力学中最突出的应用。 这项工作的其余部分结构如下：在第 2 节中，讨论了 PgNN 加速科学计算的潜力。 第 3 节概述了 PiNN，并讨论了它们推进 PgNN 的潜力。 在第 4 节中，讨论了几种领先的 PeNN 架构，以解决 PgNN 和 PiNN 的关键限制。 第 4 节回顾了神经算子的最新发展。 最后，第六节对未来研究方向进行了展望。</p>
<h1 id="二，PGNN"><a href="#二，PGNN" class="headerlink" title="二，PGNN"></a>二，PGNN</h1><p>PgNN 使用现成的监督深度学习模型，从良好控制的实验和计算获得训练数据集中提取特征或属性，来统计学习所需现象的已知物理现象[58]。 PgNN 由多层感知器（MLP，也称为人工神经网络，ANN 或深度神经网络，DNN，在与本综述相关的不同研究中）[58]、CNN [58]、RNN [58] 、GAN [59] 和图神经网络（GRNN）[60]中的一个或几个组成。 尽管 GAN 模型被归类为无监督学习，但在本文中，它们可以被归类为 PgNN，因为它们的底层训练被定义为监督学习问题 [59, 61]，示例 PgNN 架构的示意图如图 2 所示。任何物理问题都包括一组独立特征或输入特征，如 $x = [X_1, X_2, X_3,\cdots, X_n]$ 和一组因变量或期望的输出为 $y = [Y_1, Y_2, Y_3,\cdots , Y_n]$。 描述这种物理现象的数据可以通过实验（例如基于传感器的观测等）、闭合定律（例如傅立叶定律、达西定律、阻力等）或控制常微分方程（ODE）或偏微分方程（PDE），例如伯格方程、纳维-斯托克斯方程等的解来生成方程。因此，因变量和独立特征符合物理原理，并且训练后的神经网络在整个训练过程中本质上受到物理规律的指导。</p>
<p>在 PgNN 中，每层的神经元通过一组权重连接到下一层的神经元，每个节点的输出是通过将激活函数（例如，修正线性单元（ReLU）、Tanh、Sigmoid、线性等）应用于前一层神经元输出的加权和加上附加偏差来获得的 [62]。 该过程从输入开始，顺序获取每层神经元的输出，通常称为前向传播。 随后定义并计算损失函数（或者成本函数），以评估预测的准确性。 常用的回归损失函数是 L1 [63] 和均方误差 (MSE) [63]。 训练的下一步涉及误差反向传播，它计算损失函数相对于权重和偏差的偏导数/梯度（即如图 2 所示的 $\theta$）。 最后，使用梯度下降[64]、随机梯度下降[64]或小批量梯度下降[64]等优化技术来最小化损失函数，并使用计算出的梯度同时计算和更新 $\theta$ 反向传播过程。 迭代该过程，直到 PgNN 获得所需的精度水平。</p>
<p>近年来，PgNN 已广泛用于加速计算流体动力学（CFD）[65]、计算固体力学[66]和多功能材料设计[67]，它已被应用于科学计算的所有计算昂贵且耗时的组件中，例如（i）预处理[68,65,69]，网格生成； (ii)离散化和建模[70,71,72]，例如有限差分（FDM）、有限体积（FVM）、有限元（FEM）、离散元法（DEM）、分子动力学（MD）等； (iii) 后处理，例如输出同化和可视化[73,74,75]。 这些研究的目的是（i）在小数据集上训练浅层网络，以取代传统正向数值建模中的瓶颈（即计算成本高昂的步骤），例如集中复杂流体流动建模中的阻力系数计算[22,76,77,78,79]； 或者（ii）在针对特定问题生成的较大数据集上训练相对较深的网络，例如粗粒度聚合物基因组内的目标序列设计[80]。 这些网络承认生成训练数据的物理原理并加速模拟过程 [75,22]。</p>
<p>尽管 PgNN 的训练看起来很简单，但通过处理复杂物理问题的基础物理来生成数据可能需要大量的计算成本 [6,13]。 经过训练，PgNN 可以显着加快感兴趣现象的计算速度。 值得注意的是，虽然 PgNN 模型可以在训练集上取得良好的准确性，但它有可能记住训练集中的趋势、噪声和细节，而不是直观地理解数据集中的模式。 这是 PgNN 在训练数据集范围之外进行推断/测试时失去预测能力的原因之一。 PgNN 的过度拟合可以通过不同的方式缓解 [81,82,83]，以增强模型在训练数据范围内的可预测性。 在以下小节中，我们回顾了现有文献，并重点介绍了一些应用 PgNN 来加速流体和固体力学应用科学计算的不同步骤的最新研究。</p>
<h2 id="2-1-预处理"><a href="#2-1-预处理" class="headerlink" title="2.1 预处理"></a>2.1 预处理</h2><p>无论数值模型类型如何（例如 FEM、FDM、FVM 等），预处理通常是科学计算中工作量最大的部分。 该组件的主要步骤是将域分解为小而有限的部分（即网格生成、评估和优化），以及在隐式求解时放大/缩小网格属性以使用时空粗糙网格求解未解决的精细尺度物理问题。 这两个步骤非常耗时，并且需要专家级的知识，因此它们是被基于 PgNN 的加速模型取代的潜在候选者。</p>
<h3 id="2-1-1-网格生成"><a href="#2-1-1-网格生成" class="headerlink" title="2.1.1 网格生成"></a>2.1.1 网格生成</h3><p>网格生成是数值模拟的关键步骤。 张等[68]提出了基于整个域所需局部网格密度的预测来自动生成非结构化网格，为此训练了人工神经网络来指导标准网格生成算法。 他们还建议将研究扩展到其他架构，例如 CNN 或 GRNN，以用于未来的研究，包括更大的数据集或更高维度的问题。 黄等[65]采用深度学习方法来确定最佳网格密度。 他们使用经典 CFD 工具（例如 Simcenter STAR-CCM+ [84]）生成优化的网格，并提出训练 CNN 来预测任意几何形状的最佳网格密度。 自适应网格细化版本的添加加速了整个过程，而不会影响准确性和分辨率。 作者提出使用 ANN 学习最佳网格（由具有伴随功能的相应求解器生成），这可以用作其他模拟工具的起点，而不管具体的数值方法如何[65]。 吴等[69]还提出了一种将移动网格方法与深度学习相结合的网格优化方法，以解决网格优化问题。 通过进行实验，构建了一个高精度的神经网络来优化网格，同时保留初始给定网格的指定节点数和拓扑结构。 使用这种技术，他们还证明了移动网格算法独立于 CFD 计算 [69]。</p>
<p>在网格生成中，由于缺乏通用且有效的标准，一个关键问题是网格质量的评估。 陈等[85]提出了一个基准数据集（即 NACA-Market 参考数据集）来促进网格质量的评估。 他们提出了 GridNet，这是一种使用深度 CNN 对网格质量进行自动评估的技术，该方法接收网格作为输入并进行评估，使用在 NACA-Market 数据集上训练的深度 CNN 模型进行的网格质量评估被证明是可行的，准确率高达 92.5% [85]。</p>
<h3 id="2-1-2-多尺度技术"><a href="#2-1-2-多尺度技术" class="headerlink" title="2.1.2 多尺度技术"></a>2.1.2 多尺度技术</h3><p>人们总是希望在时空较粗糙的网格上数值求解多物理问题，以最大限度地减少计算成本。 因此，人们开发了不同的放大[86,87]、缩小[88]和交叉尺度[89]方法来确定在广泛的长度/时间尺度上非线性问题的精确数值解。 一种可行的选择是使用粗网格，它可以可靠地描述长波长动力学并解释未解决的小尺度物理问题。 另一方面，推导粗略表示的数学模型（例如边界条件）相对困难。 巴西奈等[87]提出了一种 PgNN 模型，用于基于已知基础方程的实际解来学习最佳 PDE 近似。 人工神经网络输出空间导数，然后对其进行优化，以便最好地满足低分辨率网格上的方程。 与典型的离散化方法（例如有限差分）相比，推荐的 ANN 方法在以粗糙 4 到 8 倍的分辨率对非线性方程组进行积分时要精确得多 [87]。 然而，这种方法的主要挑战是系统地导出这些自适应解的离散算子。 马杜等[86]开发了一个 PgNN，称为 STENCIL-NET，用于学习非线性 PDE 的特定于分辨率的局部离散化。 通过将规则笛卡尔网格上的空间和时间自适应参数池与离散时间积分知识相结合，STENCIL-NET 可以实现任意非线性 PDE 算子的数值稳定离散化。 STENCIL-NET 模型还可用于在比训练数据集更广泛的时空尺度上确定 PDE 求解方案。 在他们的论文中，作者使用 STENCIL-NET 对粗时空网格上的混沌 PDE 解进行长期预测，以检验他们的假设。 将 STENCIL-NET 模型与基线数值技术（例如，完全矢量化的 WENO [90]）进行比较，在保持相同精度的情况下，粗网格上的预测速度在 GPU 上快了 25 到 150 倍，在 CPU 上快了 2 到 14 倍[86]。</p>
<p>表 1 报告了近期利用 PgNN 加速科学计算预处理部分工作的非详尽列表。 这些研究共同得出的结论是，PgNN 可以成功集成，以在网格生成、网格评估和交叉缩放方面实现相当大的加速因子，这对于使用科学计算技术探索的许多复杂问题至关重要。 下一小节讨论 PgNN 合并到建模组件中的潜力，从而产生更高的加速因子或更高的精度。</p>
<h2 id="2-2-建模与后处理"><a href="#2-2-建模与后处理" class="headerlink" title="2.2 建模与后处理"></a>2.2 建模与后处理</h2><h3 id="2-2-1-应用到流体力学"><a href="#2-2-1-应用到流体力学" class="headerlink" title="2.2.1 应用到流体力学"></a>2.2.1 应用到流体力学</h3><p>PgNN 受到了流体力学界的广泛关注。 Lee 和 Chen [94] 关于使用 ANN 估计流体特性的研究是最早将 PgNN 应用到流体力学的研究之一。 从那时起，PgNN 在流体力学中的应用已扩展到广泛的应用领域，例如层流和湍流、非牛顿流体流动、空气动力学等，特别是加速了传统的计算流体动力学（CFD） 求解器。</p>
<p>对于不可压缩层流模拟，求解Navier-Stokes方程的数值过程被认为是主要瓶颈。 为了缓解这个问题，PgNN 已被用作解析过程的一部分。 例如，杨等[95]提出了一种使用人工神经网络的新型数据驱动投影方法，以避免基于网格的流体模拟中投影步骤的迭代计算，所提出的数据驱动投影方法的效率是显着的，特别是在大规模流体流动模拟中。 汤普森等[96] 使用 CNN 来预测流体流动无粘性欧拉方程的数值解，提出了一种结合多帧信息的无监督训练来提高长期稳定性。 与常用的雅可比方法 [97] 获得的速度场相比，CNN 模型产生了非常稳定的无散度速度场，并且精度更高。 陈等[98]后来开发了一种基于 U-net 的架构，这是 CNN 模型的一种特殊情况，用于预测层流中任意 2D 形状周围的速度和压力场图，CNN 模型使用由 Bezier 曲线构建的随机形状组成的数据集进行训练，然后使用 CFD 求解器求解Navier-Stokes方程。 CNN 模型的预测效率也使用临时误差函数对不可见的形状进行了评估，具体来说，这些预测的 MSE 水平与测试子集上获得的水平处于同一数量级，即压力和速度分别在数量级 $1.0\times 10^{-5}$ 和 $5.0\times 10^{-5}$ 之间。</p>
<p>从层流流态转向湍流流态，PgNN 已广泛用于湍流闭合模型的构建[99]。 Lings等[100]使用前馈 MLP 和专门的神经网络来预测Reynolds-averaged Navier-Stokes (RANS) 和 Large Eddy Simulation (LES) 湍流问题，他们的专门神经网络使用高阶乘法层嵌入伽利略不变性[101]，该模型的性能与 MLP 和真值数值模拟进行了比较，得出的结论是，专门的神经网络可以在不变张量的基础上预测各向异性张量，从而产生比 MLP 更准确的预测。 Maulik等[102] 提出了 Kraichnan 湍流亚网格建模的闭合框架 [103]，为了确定动态闭合强度，所提出的框架使用了隐式映射，其输入为网格解析变量和涡流粘度，使用从高保真直接数值模拟 (DNS) 获得的极度二次采样数据来训练 ANN，可以生成最佳地图。 人们发现，人工神经网络模型成功地将动态动能耗散融入衰减湍流问题，从而能够准确捕获相干结构和惯性范围保真度。 后来，Kim 和 Lee [104] 使用简单线性回归、SLinear、多元线性回归、MLinear 和 CNN，利用其他壁面信息（包括流向）来预测湍流传热（即壁面法线热通量，$q_w$） 壁面剪应力、展向壁面剪应力或流向涡度以及压力波动，通过通道流的 DNS 获得（见图 3（a））。 使用自适应矩估计（ADAM）[105, 106]对构建的网络进行训练，并执行网格搜索方法[107, 108]来优化CNN的深度和宽度。 他们的发现表明，PgNN 模型对输入分辨率不太敏感，表明其在湍流模拟中作为良好热通量模型的潜力。 Yousif等[109]还提出了一种基于PgNN生成湍流流入条件的有效方法，该PgNN是由多尺度卷积自动编码器与子像素卷积层（MSCSP-AE）[110,111]和长短期记忆相结合形成的 LSTM[112,113]模型，研究发现所提出的模型能够处理湍流场的空间映射。</p>
<p>PgNN 也已应用于空气动力学领域。 Kou和Zhang[114]提出了一篇关于典型数据驱动方法的综述论文，包括系统识别、特征提取和数据融合，这些方法已用于模拟非定常空气动力学，这些数据驱动方法的有效性通过气动弹性的几个基准案例进行描述。 Wang等[115]描述了ANN在燃烧室旋流流场建模中的应用（见图3（b））。 来自粒子图像测速 (PIV) 的旋流流场数据用于训练 ANN 模型，经过训练的 PgNN 模型已成功进行测试，可以预测未知入口条件下的旋流流场。 Chowdhary等[116] 研究了将 ANN 模型与基于投影 (PB) 的模型简化技术相结合的功效 [117,118]，为计算成本高昂的高保真物理模型（特别是复杂的高超音速湍流）开发 ANN 替代模型，替代模型用于对自由流条件和 SST（剪切应力传递）湍流模型的参数进行贝叶斯估计，然后使用激波隧道数据将替代模型嵌入到高保真（Reynolds平均Navier-Stokes）流动模拟器中。 Siddiqui等[119]为俯仰翼开发了一种非线性数据驱动模型，包括时滞神经网络（TDNN），俯仰角被视为模型的输入，而升力系数被视为输出。 结果表明，经过训练的模型能够比线性和半经验模型更准确地捕获非线性气动力，特别是在较高的偏移角下。 Wang等[120]还提出了一种基于多任务学习人工神经网络的多保真度降阶模型，以有效预测结冰翼型的非定常气动性能。 结果表明，与单保真度和单任务建模方法相比，所提出的模型具有更高的准确性和更好的泛化能力。</p>
<p>复杂流体流动的模拟，特别是使用表现出粘弹性和非线性流变行为的流体，是 PgNN 应用的另一个主题 [122,123]。 这些流体的动力学通常受非线性本构方程控制，导致刚性数值问题 [124,125]。 Faroughi等[22]开发了一个 PgNN 模型来预测球形颗粒在粘弹性流体中平移的阻力系数（见图 3（c））。 PgNN 考虑了一种堆叠技术（即集成Random Forrest [126]、Extreme Gradient Boosting [127] 和 ANN 模型）来消化输入（考虑 Oldroyd-B 和 Giesekus 流体的雷诺数、Weissenberg 数、粘度比和迁移率因子） ）并根据每个学习器的预测和 ANN 元回归器输出阻力预测，该模型的准确性已成功根据 DNS 生成的盲数据集进行了检查。 Lennon等[128]还开发了一种张量基神经网络（TBNN），允许流变学家构建可学习的本构模型，该模型包含基本的物理信息，同时对特定实验方案或流动运动学的细节保持不可知。 TBNN 模型在实质上客观的张量本构框架中结合了通用逼近器，该框架在构建时遵循连续介质力学所需的物理约束，例如框架不变性和张量对称性。 由于嵌入了 TBNN，开发的流变通用微分方程可以快速学习简单但准确且高度通用的模型来描述所提供的训练数据，从而可以快速发现本构方程。</p>
<p>最后，PgNN 还被广泛用于提高 CFD 求解器的精度和速度，Stevens 和 Colonius [121] 开发了一种深度学习模型（加权本质上非振荡神经网络，WENO-NN）来增强有限体积方法，用于离散化具有不连续解的偏微分方程，例如湍流-冲击波相互作用（见图 1）3(d)。 Kochkov等 [18] 使用混合离散化，将 CNN 和数值求解器的子组件相结合，以高精度将微分算子插值到粗网格上。 该模型的训练是在标准数值方法中进行的，用于将基础偏微分方程作为可微分程序求解，并且该方法允许对整个算法进行基于端对端梯度的优化。 该方法可学习对流通量和残差项的精确局部算子，并与以 8 至 10 倍精细分辨率运行的高级数值解算器的精度相匹配，同时执行计算速度提高 40 至 80 倍。 Cai等[129]实现了最小二乘ReLU神经网络（LSNN）来解决具有不连续解的线性平流反应问题，他们表明，所提出的方法在 DOF（自由度）数量方面优于基于网格的数值方法。 Haber等[130]建议使用自动编码器 CNN 来降低与 Navier-Stokes 方程耦合的标量传输方程的分辨率成本。 Lara 和 Ferrer [131] 提出使用神经网络加速高阶不连续 Galerkin 方法，检查了 1D Burgers 方程的各种网格、多项式阶数和粘度值的方法和界限。 List等[132] 使用 CNN 训练湍流模型，以改善模拟时不可压缩 Navier-Stokes 方程的欠解析、低分辨率解，所开发的方法在空间和时间维度上的分辨率始终优于模拟，分辨率提高了两倍。 对于混合层情况，混合模型平均类似于三倍参考模拟的性能，这相当于时间层加速 7.0 倍，空间混合层加速 3.7 倍。</p>
<p>表 2 报告了利用 PgNN 模拟流体流动问题的近期研究的非详尽列表。 这些研究共同得出的结论是，PgNN 可以成功地与 CFD 求解器集成，或用作独立的代理模型，为流体力学的科学计算开发准确且更快的建模组件。 下一节将讨论 PgNN 在计算固体力学中的潜在应用。</p>
<h3 id="2-2-2-应用到固体力学"><a href="#2-2-2-应用到固体力学" class="headerlink" title="2.2.2 应用到固体力学"></a>2.2.2 应用到固体力学</h3><p>物理引导神经网络（PgNN）也被计算固体力学界广泛采用。 Andersen等[35] 使用 ANN 进行焊接建模是最早将 PgNN 应用到固体力学的研究之一。 此后，PgNN 的应用已扩展到广泛的问题，例如结构分析、拓扑优化、逆向材料设计和建模、健康状况评估等，特别是在计算力学中加速了传统的正向和逆向建模方法。</p>
<p>在结构分析领域，Tadesse 等[137]提出了一种用于预测具有柔性剪力连接器的组合桥的中跨偏转的人工神经网络。 人工神经网络在六座不同的桥上进行了测试，产生的最大均方根误差 (RMSE) 为 3.79%，在实践中可以忽略不计，作者还开发了基于 ANN 的闭式解决方案，用于快速预测日常设计中的变形。 Guneyisi等[138]利用人工神经网络开发了钢梁抗弯超强系数的新公式，他们考虑了 141 个具有不同横截面类型的实验数据样本来训练模型。 结果显示，训练和测试准确率相当，达到 99%，这表明 ANN 模型提供了估计梁的超强强度的可靠工具。 Hung等[139]利用人工神经网络来预测非线性、非弹性钢桁架的极限载荷系数，他们考虑使用平面 39 杆钢桁架来证明所提出的 ANN 的效率，使用构件的横截面作为输入，将荷载系数作为输出，基于 ANN 的模型在预测非线性非弹性钢桁架的极限荷载系数方面具有很高的准确性，平均损失小于 0.02。 chen等[140]还使用ANN来解决弹塑性半球形金属壳与刚性冲击器之间碰撞的三维（3D）逆问题，目标是根据壳体的永久塑性变形来预测碰撞的位置、速度和持续时间，对于静态和动态加载，ANN 模型可以高精度预测位置、速度和碰撞持续时间。 Hosseinpour等[141]使用 PgNN 来评估承受横向扭曲屈曲的城堡形钢梁的屈曲能力，如图 4（a）所示，基于 ANN 的模型比众所周知的设计规范（例如 AS4100 [142]、AISC [143] 和 EC3 [144]）提供了更高的精度，用于建模和预测极限力矩能力 。</p>
<p>材料和超材料的拓扑优化是 PgNN 的另一个应用领域 [145,146]。 拓扑优化是一种识别放置在指定域内的最佳材料以实现最佳结构性能的技术[147]。 例如，Abueidda等[148]开发了一种CNN模型，可以在大变形和小变形下对线性和非线性弹性材料进行实时拓扑优化，训练后的模型可以非常准确地预测最优设计，无需迭代过程方案，并且推理计算时间非常短。 Yu等[149]提出了一种集成的两阶段技术，由基于 CNN 的编码器和解码器（作为第一阶段）和条件 GAN（作为第二阶段）组成，可以确定近乎最优的拓扑设计，这种集成产生了一个模型，该模型在像素值和合规性方面确定了近乎最佳的结构，并大大减少了计算时间。 Banga等[150]还提出了一种3D编码器-解码器CNN来加速3D拓扑优化并确定其部署的最佳计算策略，他们的研究结果表明，所提出的模型可以将总体计算时间减少 40%，同时实现 96% 的准确率。 Li等[151] 然后提出了一种基于 GAN 的非迭代近最优拓扑优化器，用于在黑白密度分布上训练的传导传热结构，用于低分辨率拓扑的 GAN 与超分辨率生成对抗网络 SRGAN [152,153] 相结合，且适用于两阶段分层预测细化管道中的高分辨率拓扑解决方案，与传统的拓扑优化技术相比，他们表明该策略在计算成本和效率方面具有明显的优势。</p>
<p>PgNN 还被应用于固体力学的逆向设计和建模。 Messner [156] 采用 CNN 开发替代模型来估计周期性复合材料的有效机械性能。 例如，基于 CNN 的模型被应用于解决寻找具有最佳机械性能结构的逆向设计问题，替代模型与完善的拓扑优化方法非常一致，例如带有惩罚的固体各向同性材料（SIMP）[157]，并且可以恢复拓扑优化的最佳解决方案。 Lininger等[158]使用CNN来解决由薄膜堆叠制成的超材料的逆设计问题，作者证明了 CNN 探索大型全局设计空间（多达 1012 个参数组合）并解决超材料结构与相关椭圆测量和反射/透射光谱之间所有关系的卓越能力 [159, 158]。 Kumar等[154]提出了一种两阶段的ANN模型，如图4（b）所示，用于超材料的逆向设计，该模型生成均匀且功能梯度的细胞机械超材料，具有针对旋曲线拓扑定制的各向异性刚度和密度。 本研究中使用的 ANN 模型是两级 ANN 的组合，第一个 ANN（即逆 PgNN）将查询刚度作为输入和输出设计参数，例如 $\Theta$，第二个 ANN（即前向 PgNN）将预测的设计参数作为输入并预测刚度以验证第一个 ANN 结果，刚度和设计参数的预测精度在两个网络中与真值数据进行验证，样本比较及其相应的 $R$ 平方值如图 4(b) 所示。Ni 和 Gau [155] 提出了代表性采样空间和条件 GAN、cGAN [160,161] 的组合。他们表明，其所提出的方法可以高精度部署，如图 4(c) 所示，同时避免使用传统方法中使用的昂贵迭代求解器，例如伴随加权方法 [162]，该模型特别适用于地质勘探、质量控制、复合材料评价等领域使用的实时弹性成像和高通量无损检测技术。</p>
<p>PgNN 模型还被用来克服固体力学中多尺度模拟的一些计算限制。 这是通过以下方式实现的：</p>
<ul>
<li>（i）绕过成本高昂的小规模计算，从而加速宏观尺度模拟[66]；</li>
<li>（ii）用替代模型替换步骤或完整模拟[66]。 </li>
</ul>
<p>例如，Liang等[163]开发了一种 ANN 模型，以基于有限元的主动脉几何形状作为输入，直接输出主动脉壁应力分布，绕过 FEM 计算。 FEM 计算的应力与 PgNN 模型估计的应力之间的差异实际上可以忽略不计，而 PgNN 模型只需要 FEM 计算时间的一小部分即可产生输出。 Mozaffar等[164]通过学习研究材料可塑性时发生的可逆、不可逆和历史相关现象，成功地将基于 RNN 的替代模型用于材料建模。 Mianroodi等[2] 使用基于 CNN 的求解器来预测具有高度非线性材料响应和机械对比特征的异质固体中的局部应力，与 FEM 等常见求解器相比，基于 CNN 的求解器为弹塑性材料提供了 8300 倍的加速因子。 Im等[5] 提出了一个 PgNN 框架，通过将 LSTM 网络与适当的正交分解 (POD) 方法集成来构建高维弹塑性 FEM 模型的代理模型 [165,166]，提出的 POD-LSTM 代理模型可以仅根据提供的训练数据集快速、精确且可靠地预测弹塑性结构。 Long 等[167]首次使用CNN来估计平面裂纹的应力强度因子，与 FEM 相比，所提出的基于 CNN 的轻量级裂纹评估方法的主要优点是它可以安装在无人机器上，以实时自动监测裂纹的严重程度。</p>
<p>表 3 报告了近期在固体力学和材料设计问题中利用 PgNN 的研究的非详尽列表。 这些研究共同得出的结论是，PgNN 可以成功地与传统求解器（例如 FEM 求解器）集成，或用作独立的代理模型，为固体力学中的科学计算开发准确且更快的建模组件。 尽管如此，PgNN 也有其自身的局限性和缺点，可能会影响不同条件下的解决方案，如下一节所述。</p>
<h2 id="2-3-PGNN的局限"><a href="#2-3-PGNN的局限" class="headerlink" title="2.3 PGNN的局限"></a>2.3 PGNN的局限</h2><p>尽管基于 PgNN 的模型显示出对加速输入输出相互依赖的非线性现象建模的巨大潜力，但它们仍存在一些关键的限制和缺点，而且当训练数据集稀疏时，其中一些限制变得更加明显。</p>
<ul>
<li>PgNN 的主要局限性源于其训练过程仅基于统计数据 [58]。 尽管训练数据集本质上受到物理学的限制（例如，通过直接数值模拟、闭合定律和去噪实验开发），PgNN 仍根据统计变化的相关性生成模型。 因此，输出（预测）自然是与物理无关的 [38,176]，并且可能违反基础物理 [6]。</li>
<li>PgNN 的另一个重要限制源于训练数据集通常稀疏的事实，尤其是在本文讨论的科学领域。 当训练数据稀疏并且不覆盖整个潜在理化属性范围时，基于 PgNN 的模型无法在训练范围之外的条件下进行盲测[43]，即它们不提供以下方面的外推能力： 时空变量和/或其他物理属性。</li>
<li>PgNN 的预测可能会受到严重影响，即使对于稀疏训练数据集范围内的输入也是如此 [22]。 在物理化学属性范围极其广泛（例如，从蠕动流到湍流的Reynolds范围）的复杂和非线性问题中，插值能力的缺乏更为明显。</li>
<li>PgNN 可能无法完全满足生成训练数据集所使用的初始条件和边界条件[38]。 每个问题的边界条件和计算域都不同，这使得数据生成和训练过程的成本极高。 此外，科学计算研究的很大一部分涉及逆问题，其中未知的感兴趣物理、化学属性仅通过与这些属性间接相关的测量或计算来估计[177,178,10,13]。 例如，在地下水流建模中，我们利用浸入含水层中的流体压力测量来估计含水层的几何形状和/或材料特征[179]，这些要求使开发在任何条件下都具有预测能力的简单神经网络的过程变得更加复杂。</li>
<li>基于PgNN 的模型在构造上不是分辨率不变的[180]，因此它们不能在较低分辨率上进行训练并直接在较高分辨率上进行推断，这个缺点是由于 PgNN 仅被设计用于学习单个实例（即输入-输出）物理现象的解决方案。</li>
<li>通过训练过程，基于PgNN 的网络可以学习整个数据集的输入输出相互依赖性，这样的过程可能会将不同输入和输出对之间的函数依赖性的微小变化视为噪声，并产生平均解决方案。 因此，虽然这些模型对于整个数据集来说是最优的，但在个别情况下它们可能会产生次优的结果。</li>
<li>当训练数据集多样化时，即当不同输入和输出对之间的相互依赖性截然不同时，PgNN 模型可能很难学习底层过程。 尽管可以通过增加模型大小来缓解这个问题，但需要更多数据来训练这样的网络，这使得训练成本高昂，并且在某些情况下不切实际。</li>
</ul>
<p>解决 PgNN 某些局限性的一种方法是生成更多训练数据，然而，由于数据采集成本高昂，这并不总是可行的解决方案，或者，PgNN 可以在没有任何先验假设的情况下通过控制物理定律来进一步约束，从而减少对大型数据集的需求。 后者是一个看似合理的解决方案，因为在大多数情况下，可以使用显式 ODE、PDE 和/或闭包定律来完整和部分地描述物理现象。 这种方法导致了基于物理的神经网络的发展 [38,44]，下一节将对此进行描述和回顾。</p>
<h1 id="三，PINN"><a href="#三，PINN" class="headerlink" title="三，PINN"></a>三，PINN</h1><p>在科学计算中，物理现象通常使用强大的数学形式来描述，其中包括控制微分方程以及初始条件和边界条件。 在域内的每个点，强形式指定解决方案必须满足的约束，控制方程通常是线性或非线性偏微分方程和/或常微分方程，众所周知，一些偏微分方程求解起来非常具有挑战性，例如，解释各种流体流动的 Navier-Stokes 方程[10]、描述固体中大变形的 Foppl–von Karman 方程 [181]，其他重要的还有热方程 [182]、波动方程 [183]、Burgers 方程 [184]、Laplace 方程 [185]、Poisson 方程 [186] 等。 可以在逻辑上利用大量经过充分测试的知识来进一步约束 PgNN，同时对可用数据点（如果有）进行训练 [38]。 为此，无网格物理信息神经网络（PiNN）得到了发展[38,44]，并迅速扩展[187,188]，并广泛部署在各种科学和应用领域[189,190,191,192,193,194]。 可参考 Karniadakis 等[6] 和 Cai [10] 对 PiNN 功能的基础回顾。 本节简要回顾了 PiNN 的核心架构及其在计算流体和固体力学中的最先进应用，并讨论了一些主要局限性。</p>
<p>图 5 展示了普通 PiNN 架构的示意图。在 PiNN 中，底层物理原理被纳入神经网络架构之外，以在训练时约束模型，从而确保输出遵循已知的物理定律。 模拟此过程的最常见方法是通过弱施加惩罚损失来惩罚不遵循物理约束的网络。 如图 5 所示，以时空特征（即 $x$ 和 $t$）作为输入参数、以 PDE 解元素作为输出参数（即 $u$）的神经网络可用于模拟任何 PDE。</p>
<p>然后，网络的输出被输入到下一层，即自动微分层。 在这种情况下，通过对输入参数（$x$ 和 $t$）的输出求微分来生成多个偏导数。 为了优化 PDE 解决方案，这些偏导数用于生成损失函数中所需的项。 PiNN 中的损失函数是由标记数据 ($\mathcal L_{Data}$)、控制偏微分方程 ($\mathcal L_{PDE}$)、应用初始条件 ($\mathcal L_{IC}$) 和应用边界条件 ($\mathcal L_{BC}$) 造成的损失的组合 [10]。 $\mathcal L_{BC}$ 确保 PiNN 的解决方案满足指定的边界约束，而 $\mathcal L_{Data}$ 确保 PiNN 遵循训练数据集（即历史数据，如果有）中的趋势。 此外，PDE 的结构通过 $\mathcal L_{PDE}$ 在 PiNN 中强制执行，$\mathcal L_{PDE}$ 指定 PDE 解成立的搭配点 [38]，由初始条件、边界条件、数据和 PDE 造成的损失的权重可以分别指定为 $w_i$、$w_b$、$w_d$ 和 $w_p$。 下一步是检查给定迭代的损失是否在可接受的容差 $\epsilon$ 内，如果不是，则通过误差反向传播来更新网络的可学习参数（$\theta$）和未知的偏微分方程参数（$\lambda$）。 对于给定的迭代次数，重复整个循环，直到 PiNN 模型产生损失函数小于 $\epsilon$ 的可学习参数。 请注意，与 PgNN 相比，PiNN 的训练更加复杂，因为 PiNN 由复杂的非凸和多目标损失函数组成，可能会导致优化过程中的不稳定[38,6,10]。</p>
<p>Dissanayake 和 Phan-Thien [195] 是第一个研究将先验知识纳入神经网络的人，随后，由于计算能力不断增强，Owhadi [196] 引入了物理信息学习模型的概念，使得能够使用具有更多可学习参数和层的日益复杂的网络。 PiNN 作为一种用于正向和逆向建模的新计算范式，由 Raissi 等在一系列论文中[38,197,44]提出。 Raissi等[38]在由不同边界条件、严格非线性和复值解（例如 Burgers、Schrodinger 和 Allen-Cahn 方程）组成的示例上部署了两个 PiNN 模型，一个是连续时间模型，一个是离散时间模型。 Burgers 方程的结果表明，给定足够数量的配置点（即作为连续模型的基础），可以获得准确且数据高效的学习过程[38]。</p>
<p>在连续PiNN模型中，当处理高维问题时，搭配点的数量指数级增长时，使得学习处理变得困难且计算成本昂贵[38,6]，Raissi等[38]提出了一种基于Runge-Kutta技术[198]的离散时间模型来解决计算成本问题，该模型仅将空间特征作为输入，随着几个时间迭代步，PiNN 会收敛到底层物理原理。 对于 Raissi 等人探索的所有例子[38]，连续和离散 PiNN 模型能够令人满意地构建基于物理的替代模型。 Nabian等[199]提出了一种管理搭配点的替代方法，他们研究了根据分布采样搭配点的效果，发现它与损失函数成正比，这个概念不需要额外的超参数，并且更容易在现有 PiNN 模型中部署，在他们的研究中，他们声称搭配点的采样方法增强了 PiNN 模型在训练期间的行为。 通过部署偏微分方程的假设来解决与弹性、扩散和平面应力物理相关的问题，结果得到了验证。</p>
<p>为了使用 PiNN 处理逆问题，深度神经网络的损失函数必须满足分布在整个问题域的一组配置点的测量值和未知值，Raissi等[44] 展示了连续和离散时间 PiNN 模型解决基准反演问题的潜力，例如非线性浅水波的传播（Korteweg-De Vries 方程）[200] 和不可压缩流体流动（Navier-Stokes 方程）[201]。</p>
<p>与 PgNN 相比，PiNN 模型为正向和逆向建模提供了更准确的预测，特别是在具有高非线性、有限数据或噪声数据的场景中 [202]，因此，它已在多个基础科学和应用领域得到应用。 除了正向和逆向问题之外，如果表示现象的基础物理的训练数据可用，PiNN 还可以用于开发未知现象的偏微分方程 [44]，Raissi等[44]利用连续时间和离散时间 PiNN 模型根据可用数据的类型和结构生成通用偏微分方程。 在本节的其余部分中，我们回顾了有关 PiNN 在计算流体和固体力学领域应用的最新文献。</p>
<h2 id="应用到流体力学"><a href="#应用到流体力学" class="headerlink" title="应用到流体力学"></a>应用到流体力学</h2><p>PiNN 在涉及流体流动的问题中的应用是一个活跃的、正在进行的研究领域[203,204]。 Raissi等[197] 在一项开创性的工作中，开发了一种 PiNN，即所谓的隐藏流体力学 (HFM)，来编码控制流体运动的物理定律，即 Navier-Stokes 方程。 他们利用基本守恒定律从被动标量浓度（例如在任意复杂域中传输的染料）的时空可视化中导出感兴趣的隐藏量，例如速度和压力场，他们解决数据同化问题的算法与边界和初始条件以及几何形状无关，他们的模型成功预测了受实际应用启发的基准问题中的 2D 和 3D 压力场和速度场。 图6，改编自 Raissi 等[197]，将 PiNN 预测与流经圆柱体的 2D 流经典问题的真值进行了比较。 该模型可用于提取有价值的定量信息，例如难以直接测量的壁剪应力以及升力和阻力。</p>
<p>Zhang等[205]还开发了一个PiNN框架，用于由 Navier-Stokes 方程控制的流过圆柱体的不可压缩流体，PiNN 学习模拟输出（即速度和压力）与基础几何形状、边界、初始条件和固有流体特性之间的关系。 他们证明，通过包含傅立叶特征[206]（例如频率和相位偏移参数），可以在时域和设计空间上增强泛化性能。 Cheng 和 Zhang [207] 开发了 Res-PiNN（即 Resnet 模块和 PiNN），用于模拟由 Burgers 和 Navier-Stokes 方程控制的空腔流动和经过圆柱体的流动。 他们的结果表明，Res-PiNN 比传统的 PgNN 和 vanilla PiNN 算法具有更好的预测能力。 Lou等[208]还证明了 PiNN 在解决逆多尺度流问题方面的潜力，他们用 PiNN 在以 Boltzmann-Bhatnagar-Gross-Krook (BGK) 碰撞模型为代表的连续介质和稀有场区域中进行逆向建模。 结果表明，PiNN-BGK 是一种统一的方法（即，它可以用于正向和逆向建模），易于实现，并且可以有效解决不适定逆问题[208]。</p>
<p>Wessels 等[209]采用PiNN开发了一种更新的拉格朗日方法，用于求解受无粘性欧拉方程约束的不可压缩自由表面流，即所谓的神经粒子方法（NPM），该方法不需要任何特定的算法处理，而这通常是准确求解不可压缩性约束所必需的。 在他们的工作中，证明 NPM 能够准确计算满足不可压缩条件的压力场，同时避免离散化过程的拓扑约束[209]，此外，PiNN 还被用来模拟复杂的非牛顿流体流动，涉及能够表征流体流变行为的非线性本构偏微分方程[210]。</p>
<p>Haghighat等[211]训练了一个 PiNN 模型来求解多孔介质中耦合多相流和变形控制方程的无量纲形式。 Almajid 和 Abu-Al-Saud [212] 将 PiNN 的预测与 PgNN（即传统的人工神经网络）的预测进行了比较，以解决充水多孔介质的瓦斯抽采问题，研究表明，PgNN 在某些条件下（即，当观测数据包含早期和晚期饱和状态时）表现良好，而 PiNN 模型即使在观测数据仅包含早期饱和状态时（需要外推）也表现稳健。 Depina等[213]应用PiNN来模拟由Richards PDE和van Genuchten本构模型控制的非饱和地下水流问题[214]，他们证明，PiNN 可以有效地估计 van Genuchten 模型参数，并以相对准确的 Richards 方程解的近似值来求解反问题。</p>
<p>流体力学中使用的 PiNN 模型的其他一些变体包括： nn-PiNN，其中 PiNN 用于结合非牛顿流体的质量和动量守恒来求解本构模型 [210]； ViscoelasticNet，其中 PiNN 用于应力发现和粘弹性流动模型选择[215]，例如 Oldroyd-B [124]、Giesekus 和 Linear PTT [216]； RhINN 是一种基于流变学的神经网络，用于求解一系列流动协议的触变弹粘塑性复杂流体的本构方程[189]； CAN-PiNN，这是一个耦合自动数值微分框架，结合了数值微分（ND）和自动微分（AD）的优点，可实现稳健且高效的 PiNN 训练[217]； ModalPiNN，它是 PiNN 与强制截断傅立叶分解 [218] 的组合，用于周期性流重建 [219]； GAPiNN，这是一种几何感知 PiNN，由变分自动编码器、PiNN 和边界约束网络组成，适用于具有不规则几何形状的实际应用，无需参数化 [220]； Spline-PiNN，它是 PiNN 和基于 CNN 的 Hermite 样条内核的组合，用于在没有任何预先计算的训练数据的情况下训练 PiNN，并提供快速、连续的解决方案，可推广到看不见的领域 [221]； cPiNN，这是一种保守的物理信息神经网络，由多个通过子域接口通量连续性进行通信的 PiNN 组成，用于求解守恒定律 [187]； SA-PiNN，这是一种自适应 PiNN，用于解决迫使 PiNN 准确拟合刚性偏微分方程解中的顽固点所需的自适应程序 [50]； XPiNN，它是一个扩展的 PiNN，用于增强 PiNN 的表示和并行化能力，并泛化到与 cPINN 相关的任何类型的偏微分方程 [188]。</p>
<p>表 4 报告了利用 PiNN 模拟流体流动问题的近期研究的非详尽列表。 此外，表 5 报告了近期研究的非详尽列表，这些研究开发了 PiNN 架构的其他变体，以提高流体流动问题的整体预测精度和计算成本。</p>
<h2 id="3-2-应用到固体力学"><a href="#3-2-应用到固体力学" class="headerlink" title="3.2 应用到固体力学"></a>3.2 应用到固体力学</h2><p>PiNN 在计算固体力学中的应用也是一个活跃的研究领域。 Haghighat 等人[234]关于使用 PiNN 进行线弹性建模是固体力学界最早引入 PiNN 的论文之一。 从那时起，该框架已扩展到其他固体力学问题（例如线性和非线性弹塑性等）。</p>
<p>Shukla等[235]使用PiNN对多晶镍的微观结构特性进行代理建模，在他们的研究中，除了采用 PiNN 模型之外，他们还应用了自适应激活函数来加速数值建模的收敛，由此产生的基于 PiNN 的替代模型展示了无损材料评估的可行策略。 Henkes等[236]使用 PiNN 对具有急剧相变的材料中的不均匀性引起的非线性应力和位移场进行了建模，为了克服 PiNN 在这个问题中的收敛问题，他们使用了自适应训练方法和域分解[209]。 根据他们的结果，域分解方法能够正确解析源自真实世界 μCT 扫描图像的异质微观结构中的非线性应力、位移和能量 [236]。 Zhang和Gu[237]基于最小能量标准训练了具有损失函数的PiNN模型来研究数字材料，与监督式深度学习方法（即 PgNN）相比，在一维拉伸、一维弯曲和二维拉伸问题上测试的模型表现出相同的性能。 通过为雅可比矩阵添加 hinge 损失，PiNN 方法能够正确逼近对数应变并纠正任何错误的变形梯度。</p>
<p>Rao等[238]提出了一种具有混合变量（位移和应力分量）输出的 PiNN 架构，用于在没有标记数据的情况下处理弹性动力学问题，与纯基于位移的 PiNN 模型相比，该方法可以提高网络的准确性和可训练性，图 7 将 FEM 生成的真实应力场与混合变量 PiNN 针对弹性动力学问题估计的应力场进行了比较 [238]，可以看出，混合变量 PiNN 可以准确估计应力分量。 Rao等[238]还提出了 PiNN 的复合方案，以硬方式强制执行初始和边界条件，而不是采用软初始和边界条件强制执行的传统（普通）PiNN，该模型针对一系列动力学问题（例如，循环单轴拉伸和弹性波传播下的缺陷板）进行了测试，并减少了 PiNN 遇到的边界附近的误差。</p>
<p>Fang和Zhan[239]提出了PiNN模型来设计各种实际应用中使用的电磁超材料，例如隐身、旋转器、集中器等，他们研究了PiNN对频域内高波数 Maxwell 方程[240]的推理问题，并改进激活函数来克服高波数问题，所提出的PiNN不仅恢复了连续函数，还恢复了分段函数，这是对PiNN在实际问题中应用的新贡献。 Zhang等[241]利用 PiNN 来识别弹性成像中的非均质材料，以应用于软组织，使用了两个 PiNN，一个用于前向问题的近似解，另一个用于近似未知材料参数的场，结果表明，使用 PiNN 可以准确地恢复机械性能的未知分布。 Abueidda等[242]采用 PiNN 来模拟 3D 超弹性问题，他们提出了一种增强型 PiNN 架构，由强形式的残差和势能组成 [243]，产生了几个有助于定义最小化的总损失函数的损失项，增强型 PiNN 的性能优于传统（普通）PiNN 和深能量方法，特别是当存在高解梯度区域时。</p>
<p>Haghighat等[13] 测试了 PiNN 的不同变体来处理固体力学中的逆问题和代理建模，他们在研究中没有采用单个神经网络，而是实现了具有多个神经网络的 PiNN，他们将该框架部署在线性弹静力和非线性弹塑性问题上，并表明改进的 PiNN 模型提供了更可靠的物理参数表示，此外，他们研究了 PiNN 中的迁移学习领域，发现使用迁移学习时训练阶段收敛得更快。 Yuan等[244]提出了一种辅助PiNN模型（称为A-PiNN）来解决非线性积分微分方程（IDE）的反问题。A-PiNN 通过在控制方程中建立辅助输出变量来表示积分，并用辅助输出的自动微分代替积分算子，从而规避了积分离散化的限制，因此，A-PiNN 及其多输出神经网络的构造使其确定主输出和辅助输出以逼近控制方程中的变量和积分，A-PiNN 用于解决非线性 IDE 的逆问题，包括 Volterra 方程 [245]，正如他们的研究结果所证明的那样，即使使用噪声数据，也可以令人满意地确定未知参数。</p>
<p>用于计算固体力学的 PiNN 的其他一些变体包括： PhySRNet，它是一种基于 PiNN 的超分辨率框架，用于从低分辨率对应项重建高分辨率输出场，而不需要高分辨率标记数据 [246]； PDDO-PiNN，它是近场动力学微分算子（PDDO）[247]和PiNN的组合，以克服PiNN在锐梯度下性能下降的问题[248]； PiELM，它是 PiNN 和极限学习机（ELM）[249]的组合，用于解决线性弹性的直接问题[250]； DPiNN，这是一种分布式 PiNN，利用分段神经网络表示来表示基础领域，而不是 FEM 中常用的分段多项式表示[51]； PiNN-FEM，它是基于 PiNN 和 FE 的混合公式，用于异构域中的计算力学 [251]。</p>
<p>表 6 报告了在计算固体力学中利用 PiNN 的近期研究的非详尽列表。 此外，表 7 报告了近期研究的非详尽列表，这些研究开发了 PiNN 架构的其他变体，以提高固体力学建模中的整体预测精度和计算成本。</p>
<h2 id="3-3-局限性"><a href="#3-3-局限性" class="headerlink" title="3.3 局限性"></a>3.3 局限性</h2><p>PiNN 在用 ODE 和/或 PDE 描述的动力系统建模方面显示出巨大的潜力，但是，它们具有一些必须考虑的限制和缺点：</p>
<ul>
<li>Vanilla PiNN 使用由一系列完全连接的层和梯度下降优化变体组成的深层网络。 学习过程和超参数调整是手动进行的，并且取决于样本大小和问题，因此，他们的训练可能面临梯度消失问题，并且对于实际的三维问题来说可能会非常慢[264]。 此外，由于使用全连接层，普通 PiNN 对低维时空参数化施加了限制 [40]。</li>
<li>对于线性、椭圆和抛物线偏微分方程，Shin 等[265]提供了第一个关于训练数据数量的收敛理论，他们还讨论了保证收敛的一系列条件。 然而，当 PiNN 应用于非线性 PDE 控制的问题时，没有“可靠”的收敛理论证明。 请注意，深度学习模型通常无法实现理论上建立的全局最小值； 因此，这一限制并不是 PiNN 特有的，并且适用于所有深度学习模型。[6]</li>
<li>PiNN 在损失函数中包含多项具有相对权重的项，这对预测的解决方案有很大影响，目前，还没有最佳选择权重的指南[51]。 损失函数中的不同项在训练过程中可能会相互竞争，这种竞争可能会降低训练过程的稳定性。 由于 PiNN 对软物理约束的依赖，因此在训练过程中遇到不适定优化问题时也会受到影响[40]。</li>
<li>PiNN 遭受低频引起的偏差，并且经常无法解决由高频或多尺度结构控制的问题的非线性偏微分方程[266]。 事实上，PiNN 可能会遇到将信息从初始条件或边界条件传播到领域中不可见的部分或未来时间的困难，特别是在大型计算领域（例如，不稳定的湍流）[43]。</li>
<li>PiNN 是解决方案学习算法，即它们学习单个实例的偏微分方程的解决方案，对于任何给定的函数参数或系数的新实例，PiNN 需要训练一个新的神经网络 [49]。 这是因为，通过构造知道，PiNN 无法学习给定现象的物理操作，这限制了它们的泛化（例如时空外推）。 因此，PiNN 方法遇到了与经典求解器相同的计算问题，尤其是在 3D 问题（例如 FEM、FVM 等）中，因为需要针对 PDE 参数、边界条件、 和初始条件求解优化问题[57]。</li>
<li>PiNN 在学习异构介质中逆问题的解决方案时遇到困难，例如由多种材料组成的复合板[264]。 在这种情况下，基础 PDE 的参数（例如电导率或渗透系数）会在整个域中发生变化，但 PiNN 由于其固有的设计而在整个域上输出唯一的参数值。</li>
</ul>
<p>尽管存在这些缺点，PiNN 仍然为难以网格化的复杂领域和数据采集成本高昂的实际问题提供了强有力的前景，为了规避普通 PiNN 的一些限制，人们提出了几种技术。 例如，为了解决上面列出的第一个限制，使用卷积滤波器的离散学习技术，例如 HybridNet [267]、密集卷积编码器解码器网络 [268]、自回归编码器解码器模型 [269]、TF-Net [ 270]、DiscretizationNet [271] 和 PhyGeoNet [272]（仅举几例）已在计算效率方面超过了普通 PiNN。 作为另一个例子，为了解决上面列出的最后一个限制，Dwivedi 等[264]提出了一种分布式PiNN（DPiNN），它比现有的PiNN具有潜在的优势，可以解决工程实践中最有可能遇到的异构介质中的逆问题，解决高维逆问题的其他一些解决方案是保守 PiNN (cPiNN) [187] 和自适应 PiNN [50]。 此外，XPiNN [188] 凭借其内在的并行化能力，可以在较小的子域中部署多个神经网络，可用于显着降低 PiNN 在大型（三维）域中的计算成本。 然而，这些修改和替代方案并不能解决 PiNN 的泛化问题，因为所得模型缺乏强化现有物理知识的能力。 为此，物理编码神经网络（PeNN）开始出现。 在下一节中，我们将回顾有关物理编码神经网络的最新文献。</p>
<h1 id="四，PeNN"><a href="#四，PeNN" class="headerlink" title="四，PeNN"></a>四，PeNN</h1><p>物理编码神经网络（PeNN）是科学计算中使用的另一类无网格算法，主要用于流体力学和固体力学领域，致力于将底层物理（即先验知识）硬编码到科学计算的核心架构中。 请注意，通过构建神经网络，基于 PeNN 的模型将神经网络的学习能力从实例学习（ PgNN 和 PiNN 架构特性）扩展到持续学习 [53,40,273]。 为了将物理定律（ODE、PDE、闭合定律等）硬编码到神经网络中，最近提出了不同的方法[40,53,180,8]。 PeNN 并不是一个全新的概念，因为长期以来的研究都提出了将物理约束建设性地构建到架构中的理念。 例如，可以参考使用Deterministic Annealing 神经网络 (DANN) 保留凸性 [274]、保留正性 [275]、使用拉格朗日神经网络 (LaNN) [276,277] 强制物理中的对称性、使用辛循环神经网络捕获轨迹（SRNN）[278,279]，在图上使用数据驱动的外部微积分（DDEC）[280]执行精确的物理和提取结构保持的代理模型等。在本节中，我们回顾了两种最重要的编码方法 神经网络架构中的物理学及其在计算流体和固体力学中的应用：(i) 物理编码循环卷积神经网络 (PeRCNN) [40,273]，以及 (ii) 微分编程 (DP) 或神经常微分方程 (NeuralODE) [53,8]。</p>
<h2 id="4-1-PERCNN"><a href="#4-1-PERCNN" class="headerlink" title="4.1 PERCNN"></a>4.1 PERCNN</h2><p>Rao等[40]引入了 PerRCNN 模型，它将控制非线性系统的先验知识硬编码到神经网络中， 图 8 所示的 PeRCNN 架构有助于以数据驱动的方式进行学习，同时对已知的物理知识进行强制编码， 该模型超出了 PgNN 和 PiNN 对于不存在显式偏微分方程公式且可用测量数据非常有限的现象的能力（例如，地球或气候系统建模 [52]），所提出的物理编码机制与基于惩罚的物理知情学习有本质上的不同，确保网络严格遵守给定的物理，他们没有使用非线性激活函数，而是提出了一种新颖的逐元素乘积运算来实现模型的非线性。 数值实验表明，与一些最先进的数据驱动建模模型相比，由此产生的物理编码学习范式对数据噪声、稀缺性和泛化性具有显着的鲁棒性。</p>
<p>如图 8 所示，PeRCNN 由以下部分组成： 输入层，由低分辨率噪声初始状态测量 $X=[X_1, X_2, X_3, …, X_n]$ 构成； 一个全卷积网络，作为初始状态生成器（ISG），它将低分辨率初始状态缩小、上采样为全分辨率初始状态，称为修改后的 $X_0$，用作进一步循环计算的输入。 为了进行循环计算，采用了一种非常规的卷积块，称为 $\pi$ [40]。 在 PeRCNN 的核心 $\pi$ 块中，修改后的 $X_0$ 经过多个并行卷积层，然后通过逐元素乘积层融合其特征图，此外，在乘积运算之后附加 1×1 卷积层[281]，以将多个通道聚合成所需通道数的输出。 假设1×1卷积层的输出逼近非线性函数，可以将其乘以时间间隔 $\delta t$，得到动力系统在时间 $t_k$ 的残差，即 $\delta U_k$。 最终，最后一层通过逐元素加法生成预测  $Y^′ =[Y_1^′,Y_2^′,Y_3^′,…,Y_n^′]$，这些操作如图 8 所示。</p>
<p>PeRCNN 架构在 2D Burgers 和 3D Gray-Scott 反应扩散方程的两个数据集上进行了测试 [40]。 在这两种情况下，PeRCNN 与卷积 LSTM [282]、深度残差网络 [283] 和深度隐藏物理模型 [176] 在准确性（均方根误差，RMSE）、数据噪声/稀缺性、 和泛化性进行了比较。 2D Burgers 数据集的比较如图 9(a) 所示，摘自[40]。 PerRCNN 的累积 RMSE 在训练区域中以较大值开始（由于数据中存在 10% 高斯噪声），并随着评估额外的时间步长而减小。 PerRCNN 的累积 RMSE 在外推阶段略有增加（作为模型泛化的衡量标准），但在长期外推方面明显超过所有其他算法。 Rao等[273]还使用PeRCNN从稀缺和噪声数据中发现时空偏微分方程，并证明了其与基线模型相比的有效性和优越性。</p>
<p>Ren等[284]提出了一种结合PeRCNN和PiNN的混合算法来解决PgNN和PiNN在低维时空参数化方面遇到的局限性，在由此产生的物理信息卷积循环网络（称为 PhyCRNet）中，提出了一种编码器-解码器卷积 LSTM 网络，用于低维空间特征提取和时间演化学习。 在 PhyCRNet 中，损失函数被指定为聚合离散 PDE 残差，边界条件通过指定的 padding 硬编码在网络中，初始条件定义为网络的第一个输入状态变量，使用明确模拟时间推进的自回归和残差连接来增强网络，该方法确保泛化到各种初始和边界条件场景，并在网络训练中产生适定的优化问题。 使用 PhyCRNet，还可以同时在网络中强制执行已知的守恒定律（例如，可以通过应用流函数作为流体动力学网络中的解变量来强制质量守恒）[284]。 Ren等[284] 使用几种非线性偏微分方程与最先进的基线算法（例如 PiNN 和自回归密集编码器-解码器模型 [269]）相比，评估和验证了 PhyCRNet 的性能，PhyCRNet 和 PiNN 求解 Burgers 方程的比较如图 9(b) [284] 所示，Ren 等获得的结果清楚地证明了 PhyCRNet 方法在解决方案准确性、外推性和普遍性方面的优越性。</p>
<h2 id="4-2-NeuralODE"><a href="#4-2-NeuralODE" class="headerlink" title="4.2 NeuralODE"></a>4.2 NeuralODE</h2><p>神经常微分方程 (NeuralODE) 方法是 PeNN 模型的另一个系列，其中通过使用可微函数参数化隐藏状态导数，将神经网络的隐藏状态从离散序列转换为连续非线性函数 [53]，然后使用传统的微分方程求解器计算网络的输出，在训练期间，误差通过网络以及 ODE 求解器反向传播，而无需访问其内部运算，这种架构是可行的，因为数值线性代数是科学计算和深度学习的共同底层基础设施，并通过自动微分（AD）[285]进行桥接。 由于微分方程和神经网络都是可微的，因此可以使用标准优化和误差反向传播技术来在训练期间优化网络的权重。 NeuralODE 中的模型不是直接从训练数据中学习非线性变换，而是学习非线性变换的结构，因此，由于神经网络优化方程是可微的，物理微分方程可以直接编码到层中，而不是添加更多层（例如更深的网络），这导致了一个更浅的网络模仿无限深的模型，可以以任何所需的精度连续推断，同时减少内存和计算成本[286]。</p>
<p>这些连续深度模型提供了 PiNN 和 PgNN 所缺乏的功能，例如：</p>
<ul>
<li>（i）减少监督学习的参数数量</li>
<li>（ii）作为深度函数的内存成本一定</li>
<li>（iii）连续时间学习（即使用以任意时间间隔获取的数据集进行训练）。</li>
</ul>
<p>然而，误差反向传播可能会在训练这种连续深度网络时造成技术困难。 Chen等[53] 使用伴随灵敏度方法 [287] 计算梯度，同时将 ODE 求解器视为黑匣子，他们证明，这种方法使用最少的内存，可以直接控制数值误差，而且最重要的是，可以随问题规模线性扩展。</p>
<p>Ma等[288]比较了离散和连续伴随敏感性分析的性能，他们指出，对于参数大约少于 100 个的问题，通过 AD 实现的正向模式离散局部灵敏度分析比反向模式和连续正向和/或伴随灵敏度分析更有效。 然而，就可扩展性而言，他们表明连续伴随方法比离散伴随方法和前向方法更有效。</p>
<p>为了促进 NeuralODE 的实际应用，已经实现了几个计算库，Poli等[289] 实现了 TorchDyn 库来训练 NeuralODE 模型，并且与常规的即插即用深度学习原语一样易于访问。 Innes等[8] 和 Rackauckas 等[286]在 Julia 编码生态系统中开发了 GPU 加速的 Zygote 和 DiffEqFlux 库，将可微分编程和通用微分方程求解器功能结合在一起。 例如，他们将常微分运动方程作为变换函数编码到神经网络中，以模拟投石机的逆动力学[8]，如图10所示，具有经典层的网络以目标位置和风速作为输入，估计弹丸击中目标的重量和角度，这些输出被输入 ODE 求解器来计算所达到的距离，模型将预测值与目标位置进行比较，并将误差反向传播到整个链，以调整网络的权重。 该 PeNN 模型在个人计算机上解决投石机逆动力学问题的速度比该逆问题的经典优化算法快 100 倍，一旦经过训练，这个网络就可以用来瞄准任何盲目标，而不仅仅是它所训练的目标，因此，该模型既是加速的可预测的。</p>
<p>NeuralODE 还与 PiNN 模型集成，称为 PiNODE，以便在训练期间使用已知的控制物理进一步约束网络，这种架构由一个神经网络组成，其隐藏状态由 ODE 参数化，其损失函数类似于 PiNN 的损失函数（见图 5），损失函数基于数据和控制 ODE 的强形式对算法进行惩罚，并通过应用伴随灵敏度方法 [288] 反向传播误差，以更新架构中的可学习参数。 可以部署 PiNODE 来克服高偏差（由于在科学建模中使用第一原理）和高方差（由于在科学建模中使用纯数据驱动模型）问题，换句话说，使用 PiNODE，可以在可用的情况下集成 ODE 方面的先验物理知识，在不可用的情况下使用函数逼近（例如神经网络）。 Lai等[290]使用PiNODE对结构动力学领域的控制方程进行建模（例如，具有三次非线性的4自由度动力系统的自由振动），他们表明，PiNODE 为结构健康监测（例如损坏检测）问题提供了一个适应性强的框架。 Roehrl等[291] 使用推车上的倒立摆正向模型测试了 PiNODE，并表明该方法可以学习具有很大不确定性的现实物理系统中的非保守力。</p>
<p>神经微分方程的应用也已扩展到学习偏微分方程描述系统的动力学。 Dulny等[292]通过使用多层卷积神经网络将线性方法（通过 ODE 系统表示任意复杂的 PDE）和 NeuralODE 相结合，提出了 NeuralPDE，他们在由平流扩散方程、Burgers 方程、波浪传播方程、气候建模等生成的几个时空数据集上测试了 NeuralPDE，他们发现 NeuralPDE 与其他基于深度学习的方法（例如 ResNet [293]）相比具有竞争力。 NeuralPDE 的局限性是由直线法的局限性决定的，例如，它不能用于求解椭圆二阶 PDE。 表 9 报告了利用 PeNN 模拟不同科学问题的领先研究的非详尽列表。</p>
<h2 id="4-3-局限性"><a href="#4-3-局限性" class="headerlink" title="4.3 局限性"></a>4.3 局限性</h2><p>尽管许多 PeNN 模型取得了进步，并且在复杂物理系统建模方面取得了成功，但这些新架构也面临着一些挑战，其中最重要的一点就是训练。 基于 PeNN 的模型利用连续深度网络的发展来促进持续学习，这使得 PeNN 比 PgNN 和 PiNN 更难训练，考虑到这一点，PgNN 和 PiNN 面临的大部分限制（例如收敛速度、稳定性、可扩展性、样本大小和问题依赖性）也同样适用于 PeNN，此外，PeNN 通常具有复杂的架构，其实现并不像 PiNN 或 PgNN 那么简单。尽管 PeNN 的实现复杂性，但它们在有限维设置中的高效算法、提供可转移解决方案的能力、对数据稀缺的鲁棒性以及与 PgNN 和 PiNN 相比的通用性，使它们具有显着加速的巨大潜力。 传统科学计算在计算流体和固体力学中的应用。</p>
<h1 id="五，NeralOperators"><a href="#五，NeralOperators" class="headerlink" title="五，NeralOperators"></a>五，NeralOperators</h1><p>迄今为止讨论的大多数科学深度学习方法，例如 PgNN、PiNN 和 PeNN，通常旨在映射单个实例的物理现象的解（例如，使用特定时空域和边界条件来求解偏微分方程，PiNN），因此必须重新训练或进一步训练（例如，迁移学习[294]）以映射不同时刻下的求解问题。 缓解这个问题的另一种方法是使用神经算子来学习函数空间之间的非线性映射[39,295,296]。 因此，神经算子形成了另一种模拟范式，它使用先进的架构来学习底层线性和非线性连续算子，这些模型与 PgNN 类似，使用标记的输入输出数据集对应问题的物理原理，但与 PgNN 以及 PiNN 和 PeNN 相比，提供增强的泛化性、可解释性、连续学习和计算效率 [180,53,43]。</p>
<p>这种新范式使用基于神经网络的网格不变、无限维算子，不需要事先了解偏微分方程，神经算子仅仅使用数据来学习感兴趣问题的分辨率不变的解决方案[43]。 换句话说，神经算子可以在一种时空分辨率上进行训练，并在任何其他分辨率上成功推断[296]。 这种分辨率不变的特征是利用神经算子通过在函数空间中参数化模型来学习连续函数而不是离散向量这一事实来实现的[43, 296]。 请注意，PgNN 和 PiNN（例如使用 MLP）也可以保证较小的泛化误差，但这只能通过足够大的网络来实现，神经算子的一个显着特征是它们对于需要实时推理的应用的鲁棒性[57]。 最近提出了三种主要的神经算子，即</p>
<ul>
<li>（i）深度算子网络（DeepONets）[56]</li>
<li>（ii）傅里叶神经算子（FNO）[180]</li>
<li>（iii）图神经算子（GNO）[296, 297 ]。 </li>
</ul>
<p>Goswami 等最近的综述[57]广泛比较了这些神经算子。 在本节中，我们将简要回顾 DeepONets 和 FNO 作为应用于计算流体和固体力学的两个重要神经算子。</p>
<h2 id="5-1-DeepONets"><a href="#5-1-DeepONets" class="headerlink" title="5.1 DeepONets"></a>5.1 DeepONets</h2><p>Lu等[39]基于算子的万能逼近定理开发了深度算子网络（DeepONets）[298]，可用于以非常小的泛化误差准确有效地学习算子。 Lu等[56]为 DeepONet 提出了两种架构，即堆叠式和非堆叠式。 堆叠式DeepONet架构如图11（a）所示，由一个主干网络和多个堆叠式分支网络组成，$k = 1,2\cdots,p$，选择主干网络作为宽度为 $p$ 的一层网络，每个分支网络作为宽度为 $n$ 的单隐层网络，形成堆叠式 DeepONet。 为了学习算子 $G:s \rightarrow G(s)$，堆叠式 DeepONet 架构将函数 $s$ 作为分支网络的输入，将 $y$（即 $G(s)$ 域中的点）作为主干网络的输入，这里，向量 $[(x_1), (x_2), \cdots, (x_m)]$ 表示数据的有限位置，或者称为传感器。 主干网络输出 $[t_1 , t_2 , \cdots, t_p ]^T \in  \mathbb R^p$ ，每个分支网络输出由 $b_k ∈ \mathbb R$ 表示的标量，其中 $k = 1,2,\cdots,p$，接下来，主干网络和分支网络生成的输出被集成在一起，为 $G(s)(y)\approx \sum^p_{k=1} b_k(s(x_1), s(x_2), \cdots,s(x_m))t_k(y)$。 非堆叠式 DeepONet 架构也如图 11（a）所示，它仅由一个分支网络（以深蓝色表示）和一个主干网络组成。 非堆叠式 DeepONet 可以被视为堆叠式 DeepONet，其中所有分支网络共享相同的参数集[56]，DeepONet 首先用于学习几个显式算子，包括积分和分数拉普拉斯算子，以及表示确定性和随机微分方程的隐式算子[56]。 Lu 等讨论的 DeepONet 的两个主要优点：</p>
<ul>
<li>（i）小的泛化误差；</li>
<li>（ii）训练和测试误差相对于训练数据量的快速收敛。</li>
</ul>
<p>Lin等[299]展示了 DeepONet 在数据密度和位置方面的有效性，当没有关于需要多少训练数据的先验知识或当数据获取有严格限制（例如，位置可访问性）时，这是有利的，为此，他们采用 DeepONet 和 LSTM（即 PgNN）对代表单个气泡形成的数据集进行建模，以响应环境液体压力随时间变化的变化。 为了生成数据集，他们使用Rayleigh-Plesset (R-P) 作为宏观模型，使用耗散粒子动力学 (DPD) 作为微观模型，他们使用 Gaussian 随机场来生成不同的压力场，作为该动力系统的输入信号。 比较结果如图11(b)所示，顶行显示当每个轨迹仅已知 20 个数据点（即稀疏训练数据）时液体压力轨迹的预测结果，底行显示相同但当每个轨迹已知 200 个数据点（即密集训练数据）时的预测结果，如图所示，无论训练数据多么稀疏，DeepONet 在预测液体压力轨迹方面都能够优于 LSTM。</p>
<p>此外，他们还检查了输入不包含在训练输入范围内的情况，即压力场的相关长度超出训练范围时。 在这种情况下，他们最初无法做出准确的预测，但通过将学习转移到预先训练的 DeepONet 主干网络并仅使用几个额外的数据点对其进行微调来缓解了该问题。 他们还证明，DeepONet 可以学习微观模型的噪声原始数据的平均成分，而无需任何额外的数据处理，并且计算时间可以从 48 个 CPU 小时减少到不到一秒。 这些结果证实 DeepONet 模型可以应用于气泡生长动力学的宏观和微观状态，为统一的神经网络模型奠定了基础，该模型可以无缝预测跨尺度相互作用的物理现象。</p>
<p>Oommen等[300]将卷积自动编码器架构与DeepONet（CA-DeepONet）相结合，以学习两相混合物的动态发展，并加快微结构演化预测的求解时间。 在低维潜在空间中，卷积自动编码器用于提供微观结构数据的紧凑表示，而 DeepONet 则用于从自动编码器的潜在空间中学习微观结构演化的介观动力学，然后，卷积自动编码器的解码器组件根据 DeepONet 的预测重建微观结构的演化，经过训练的 DeepOnet 架构可用于加速外推任务中的数值求解器，或替代插值问题中的高保真相场数值求解器。</p>
<p>通过从稀疏数据域的 PiNN 中汲取灵感，DeepONets 还可以使用非常稀疏的标记数据集进行训练，同时将已知的微分方程合并到损失函数中，这种方法产生了基于物理的 DeepONets (Pi-DeepONets) [301,302]。 Wang等[301]采用 Pi-DeepONets 来解决扩散反应、Burger 方程、平流方程和 eikonal 方程等基准问题，与普通 DeepONet 相比，结果表明预测准确性、泛化性能和数据效率都有显着提高。 此外，Pi-DeepONets 可以在没有任何成对输入输出训练数据的情况下学习解算子，从而使它们能够比传统求解器快三个数量级来模拟计算力学中的非线性和非平衡过程[301]。</p>
<p>Goswami等[302]使用 DeepONet (Pi-V-DeepONet) 的物理信息变分公式来研究脆性断裂力学，Pi-V-DeepONet 的训练是使用变分形式的控制方程和一些标记数据进行的。 他们使用 Pi-V-DeepONet 框架来确定准脆性材料脆性断裂的失效路径、失效区域和沿失效的损坏，他们训练模型将缺陷（例如裂纹）的初始配置映射到相关的感兴趣领域（例如损伤和位移，见图 12）。 他们表明，他们的模型可以快速预测任何初始裂纹配置和加载步骤的解决方案，在脆性断裂力学中，所提出的模型可用于增强设计、评估可靠性和量化不确定性。</p>
<p>由于评估积分算子的成本很高，DeepONets 可能难以开发出能够在无限维环境中替代卷积或循环神经网络的有效数值算法。 Li等[180]沿着这个思路做出了努力，通过参数化傅里叶空间中的积分核开发了算子回归，并将其称为傅里叶神经算子（FNO）。 在下一节中，我们将讨论 FNO 的核心架构以及围绕它的最新发展。</p>
<h2 id="5-2-Fourier-Neural-Operator-FNO"><a href="#5-2-Fourier-Neural-Operator-FNO" class="headerlink" title="5.2 Fourier Neural Operator (FNO)"></a>5.2 Fourier Neural Operator (FNO)</h2><p>为了受益于无限维空间中的神经算子，Li 等[180]在傅立叶空间中开发了一种神经算子，称为 FNO，其核心架构如图 13 所示。训练从输入 $X$ 开始，随后通过神经网络 $S$ 将其提升到更高维度的空间，第二阶段需要使用多个积分算子和激活函数的傅立叶层。 在每个傅里叶层中，使用 </p>
<ul>
<li>(i) 傅里叶变换 $F$ 对输入进行变换； </li>
<li>(ii) 对较低傅立叶模式进行线性变换 $T$，滤除较高模式； </li>
<li>(iii) 傅里叶逆变换，$F^{−1}$。 在应用激活函数 $\sigma$ 之前，还使用局部线性变换 $W$ 对输入进行变换。 </li>
</ul>
<p>傅里叶层被设计为离散化不变的，因为它们从任意离散化的函数中学习。 事实上，积分算子应用于卷积并表示为傅立叶域中的线性变换，从而允许 FNO 学习无限维空间上的映射。 在第三阶段，使用另一个神经网络 $M$ 将傅里叶层的结果投影回目标维度，最终输出所需的输出 $Y^’$[180]。 与其他深度学习方法不同，无论输入和输出分辨率如何，FNO 模型的误差都是一致的（例如，在 PgNN 方法中，误差随着分辨率的增加而增加）。</p>
<p>Li等[180]在三个不同的测试用例上使用了 FNO，包括 1D Burgers 方程、2D Darcy 流方程和 2D Navier-Stokes 方程。 对于每个测试用例，FNO 都与最先进的模型进行了比较，特别是，对于 Burgers 和 Darcy 的测试用例，用于比较的方法是传统的 ANN（即 PgNN）、减少偏差方法 [303]、全卷积网络 [304]、作为神经网络中编码器的主成分分析 [295]、图神经算子[296]和低秩分解神经算子（即非堆叠 DeepONet [39]）。 在所有测试案例中，FNO 产生的相对误差最低。 1D Burgers 和 2D Darcy 方程的模型误差比较如图 14 所示，改编自[180]。</p>
<p>如前所述，FNO 模型可以在特定分辨率上进行训练并在不同分辨率上进行测试。 Li等[180]通过在 2D 测试用例的 Navier-Stokes 方程上训练 FNO 证明了这一主张，分辨率为 $64\times 64\times 20 (n_x,n_y,n_t)$ 代表空间 $(x, y)$ 和时间分辨率，并且 然后以 $256\times 256\times 80$ 的分辨率对其进行评估，如图15（a）所示，与其他模型相比，FNO 是唯一能够在空间和时间上执行分辨率缩减的技术 [180]。 FNO 还可以实现比传统数值 PDE 求解器高几个数量级的加速因子，然而，由于输入数据的维数很大，这会显著增加网络权重的数量，因此它们仅用于 2D 或小型 3D 问题。 考虑到这个问题，Grady 等[305]提出了基于域分解的FNO并行版本来解决这个限制，利用这一扩展，他们能够在大规模建模中使用 FNO，例如，模拟地下非均质储层中二氧化碳羽流的瞬态演化，作为碳捕获和封存 (CCS) 技术的一部分 [306]，见图15(b)。 网络的输入（与 Li 等人[180]提出的结构类似）被设计为一个张量，其中包含每个 3D 空间位置的渗透率和地形场，使用 $60\times 60\times 64 (n_x ,n_y, n_z)$ 分辨率，输出为 $60\times 60\times 64\times n_t$。 对于 $n_t = 30 s$ 的时间分辨率，他们发现并行 FNO 模型比传统多孔介质求解器快 271 倍（甚至不利用 GPU），同时实现了相当的精度。 Wen等[307]还提出了U-FNO，FNO的扩展，用于模拟多孔介质中的多相流，特别是通过非均质介质的CO2-水多相流，具有广泛的储层条件、注入配置、流速和多相流特性。 他们将 U-FNO 与 FNO 和 CNN（即 PgNN）进行了比较，结果表明 U-FNO 架构为高度非均质地质构造中的气体饱和度和压力积聚预测提供了最佳性能。 他们还表明，U-FNO 架构提高了原始 FNO 的训练精度，但自然无法实现多个离散化训练和测试的灵活性。</p>
<p>You等[308]提出了一种隐式傅里叶神经算子（IFNO）来模拟材料由于其异质性和缺陷而产生的复杂响应，而无需使用传统的本构模型。 IFNO 模型捕获特征空间中的远程依赖性，并且随着网络变得更深，它变成一个定点方程，产生隐式神经算子（例如，它可以模拟位移/损伤场）。 You等[308]使用一系列测试用例（例如超弹性、各向异性和脆性材料）证明了 IFNO 的性能，图 16 描绘了 IFNO 和 FNO 对于玻璃陶瓷裂纹瞬态扩展的比较 [308]。 正如所证明的，在预测位移场方面，IFNO 优于 FNO（在精度方面）和传统本构模型（在计算成本方面）。</p>
<p>FNO 模型还与 PiNN 混合，创建了所谓的物理信息神经算子 (PiNO) [43]，PiNO 框架是操作学习（即 FNO）和功能优化（即 PiNN）框架的组合，可提高 PiNN 和 FNO 模型的收敛速度和准确性。 提出这种集成是为了来解决 PiNN 中的挑战（例如，泛化和优化，特别是对于多尺度动力系统）和 FNO 中的挑战（例如，需要昂贵且不切实际的大型训练数据集）[43]。 Li等[43]在几个基准问题（例如，Kolmogorov flow、lid-cavity flow等）上部署了 PiNO 模型，以表明 PiNO 可以优于 PiNN 和 FNO 模型，同时保持 FNO 相对于其他求解器的卓越加速因子。</p>
<h2 id="5-3-NOs局限性"><a href="#5-3-NOs局限性" class="headerlink" title="5.3 NOs局限性"></a>5.3 NOs局限性</h2><p>DeepONet [39]和FNO [180]作为迄今为止最常见的两种神经算子，具有一些共同点，但也存在显着差异。 DeepONet 架构受到 Chen 和 Chen [298] 的通用逼近定理的启发，而 FNO 是在傅立叶空间中参数化积分核的基础上建立的架构。 然而，连续形式的 FNO 可以被视为具有特定的主干网络（用三角基础表示）和分支网络架构的 DeepONet [309]。 与 DeepONet 不同，FNO 通过等间距网格中的逐点评估来离散化输入函数和输出函数，因此，网络训练后，FNO只能预测与输入函数相同网格中的解，而DeepONet可以在任意位置进行预测。 FNO 还需要全场观测数据进行训练，而 DeepONet 则更灵活，但 POD-DeepONet [310] 除外，它需要全场观测数据来计算适当的正交分解（POD）模式 [310] 。 DeepONet、FNO 及其各种变体仍然面临一些局限性，特别是在应用于大型多物理问题时，需要进一步研究。</p>
<ul>
<li>神经算子纯粹是数据驱动的，需要相对较大的训练数据集，因此当应用于数据获取复杂和/或昂贵的问题时，它们面临着限制[310]。 对于底层物理完全已知并且可以集成到损失函数中的问题，与 PiNN 的集成可以在一定程度上解决这个问题[301]。 此外，在实际应用中，仅根据损失函数中的控制方程来训练 NO 可能会产生不准确的预测，相反，建议采用混合物理数据训练[301]。</li>
<li>DeepONet 和 FNO 通常仅限于基本几何或结构化数据（例如 2D 或小型 3D 问题），因为它们的输入数据维度很大，这会显著增加网络权重的数量[310]。 随着可训练参数数量的增加，它们也容易出现过度拟合，从而使训练过程变得更加困难[311]。IFNO 在一定程度上解决了这一挑战[308]，在 IFNO 中，解算子首先被表述为隐式定义的映射，然后建模为不动点，后者旨在克服深层情况下网络训练的挑战，而前者最大限度地减少可训练参数的数量和内存成本。 尽管如此，由于神经网络架构的大小有限，对于大型数据集，NO（例如 DeepONet）误差相对于训练数据大小的收敛变得代数化，它希望是指数的[310]。</li>
<li>FNO 对于不连续函数可能不可靠，因为它依赖于傅立叶变换。 DeepONet 在一定程度上缓解了这一问题，因为它对于具有不连续性的函数（例如可压缩欧拉方程）表现良好[310]。</li>
</ul>
<p>尽管存在这些限制，神经算子仍然是各种实时推理应用中的领先算法，包括自主系统、设计问题中的代理和不确定性量化[57]。</p>
<h1 id="六，总结和未来研究方向"><a href="#六，总结和未来研究方向" class="headerlink" title="六，总结和未来研究方向"></a>六，总结和未来研究方向</h1><p>相当多的研究主题共同支持科学计算和深度学习方法相结合的功效，特别是，这种组合提高了高维问题的正向和逆向建模效率，这些问题的成本过高、包含噪声数据、需要复杂的网格，并且由非线性、不适定微分方程控制。 不断增强的计算机能力将通过允许使用更深层次的神经网络并考虑更高维度的相互依赖性和设计空间，继续进一步提供这种组合。</p>
<p>科学计算和深度学习方法的结合在实际工程中的许多常见场景中也超越了传统的计算力学求解器。 例如，通过实验获得的复杂（即难以获取数据）现象的稀疏数据集不能简单地与传统求解器集成。 而使用DL，可以执行以下任务：</p>
<ul>
<li>(i) 基于PgNN的模型可以应用于稀疏数据以提取潜在的相互依赖性并进行时空降尺度或升尺度（即插值数据）；</li>
<li>(ii) 基于 PiNN 的模型可以应用于插值数据，以推导出控制方程和现象的潜在未知边界或初始条件（即强数学形式）； </li>
<li>(iii) 基于PeNN的模型可用于结合插值数据和强数学形式来进行外推探索； </li>
<li>(iv) 基于 NO 的模型可用于对复杂动态进行实时预测。 </li>
</ul>
<p>因此，基于深度学习的方法与传统科学计算方法的结合为科学家提供了一个经济高效的工具箱，以探索不同尺度的问题，而这些问题在计算上被认为是牵强的。 为此，需要在深度学习方面取得其他几项突破，才能在大规模三维（或多维）问题中使用 PgNN、PiNN、PeNN 和 NO。 例如，复杂深度学习模型（例如 PiNN、PeNN 和 NO）的训练应该使用不同的并行化范例来加速。</p>
<p>表 10 比较了 PgNN、PiNN、PeNN 和 NO 的主要特征。 基于 PgNN 的模型主要受到其统计训练过程的影响，为此它们需要大量数据集。 他们仅根据统计变化的相关性来绘制精心策划的训练数据集，因此，他们的预测自然与物理无关。 基于 PiNN 的模型主要受到竞争损失项的存在的影响，这些损失项可能会破坏训练过程的稳定性。 PiNN 也是一种解决方案学习算法，由于无法学习特定现象的物理操作，因此泛化性有限。 另一方面，基于 PeNN 和 NO 的模型可能收敛速度较低，并且需要大量配对的结构化数据集，从而导致训练成本高昂。</p>
<p>考虑到科学计算和深度学习相结合这一新挑战的有效性，未来的研究可以分为三个不同的类别：</p>
<ul>
<li>(i)改进算法：开发 PgNN、PiNN、PeNN 和 NO 的高级变体，这些变体提供更简单的实现和增强的收敛性 速度; 更快地训练多维和多物理问题； 使用稀疏训练数据集时，对未见条件具有更高的准确性和泛化能力，在实时预测中使用更稳健； 更好地适应多时空分辨率，更灵活地编码各种类型的控制方程（例如，所有偏微分方程类型、闭包定律、数据驱动定律等），并与大量传统求解器提供更紧密的联系； </li>
<li>(ii)考虑因果关系：开发因果训练算法（例如因果Q学习[312]），通过重新加权控制方程（例如偏微分方程）来恢复PgNN、PiNN和PeNN模型训练期间的物理因果关系 ）每次迭代的残余损失。 这一系列研究将允许开发 PgNN、PiNN 和 PeNN 算法的符合因果关系的变体，这些变体可以为这些算法在不同领域的更广泛的复杂场景中的应用带来新的机会；</li>
<li>(iii) 扩展应用：利用 PgNN、PiNN、PeNN 和 NO 的潜力来解决复杂各向异性材料的问题（例如，高度异质多孔介质、金属和非金属颗粒复合材料中的流动等）； 多尺度多物理现象问题（例如磁流变流体、颗粒流体、干粉动力学、反应输运、非饱和土壤动力学等）； 多分辨率目标和广泛的时空降尺度或升尺度问题（例如，全球和区域气候建模、地球系统储层建模等）； 和结构健康监测（例如裂纹识别和扩展、氢气管道泄漏、二氧化碳羽流检测等）；</li>
<li>(iv) 耦合求解器：将 PgNN、PiNN 和 PeNN 以及 NO 与开源计算力学包（例如 OpenIFEM、OpenFOAM、Palabos、LAMMPS、LIGGGHTS、MOOSE 等）耦合。这一研究方向将允许更快的代理 建模，从而加快下一代求解器的开发。 它还加快了社区和行业对科学与深度学习相结合的计算范式的采用。</li>
</ul>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>xinwen
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://github.com/sophia-hxw/sophia-hxw.github.io/2023/08/19/math/Physics-Guided,%20Physics-Informed,%20and%20Physics-Encoded%20Neural%20Networks%20in%20Scientific%20Computing/" title="Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks in Scientific Computing">https://github.com/sophia-hxw/sophia-hxw.github.io/2023/08/19/math/Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks in Scientific Computing/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/math/" rel="tag"><i class="fa fa-tag"></i> math</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/07/10/other/chatGPT%E6%80%8E%E4%B9%88%E5%86%99%E6%8C%87%E4%BB%A4/" rel="prev" title="chatGPT怎么写指令？">
      <i class="fa fa-chevron-left"></i> chatGPT怎么写指令？
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/10/17/CNN/cnn_relate/" rel="next" title="cnn相关的面试题">
      cnn相关的面试题 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%EF%BC%8C%E4%BB%8B%E7%BB%8D"><span class="nav-text">一，介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C%EF%BC%8CPGNN"><span class="nav-text">二，PGNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">2.1 预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-%E7%BD%91%E6%A0%BC%E7%94%9F%E6%88%90"><span class="nav-text">2.1.1 网格生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E6%8A%80%E6%9C%AF"><span class="nav-text">2.1.2 多尺度技术</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E5%BB%BA%E6%A8%A1%E4%B8%8E%E5%90%8E%E5%A4%84%E7%90%86"><span class="nav-text">2.2 建模与后处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E5%BA%94%E7%94%A8%E5%88%B0%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6"><span class="nav-text">2.2.1 应用到流体力学</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E5%BA%94%E7%94%A8%E5%88%B0%E5%9B%BA%E4%BD%93%E5%8A%9B%E5%AD%A6"><span class="nav-text">2.2.2 应用到固体力学</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-PGNN%E7%9A%84%E5%B1%80%E9%99%90"><span class="nav-text">2.3 PGNN的局限</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%89%EF%BC%8CPINN"><span class="nav-text">三，PINN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E5%88%B0%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6"><span class="nav-text">应用到流体力学</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E5%BA%94%E7%94%A8%E5%88%B0%E5%9B%BA%E4%BD%93%E5%8A%9B%E5%AD%A6"><span class="nav-text">3.2 应用到固体力学</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-text">3.3 局限性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9B%EF%BC%8CPeNN"><span class="nav-text">四，PeNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-PERCNN"><span class="nav-text">4.1 PERCNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-NeuralODE"><span class="nav-text">4.2 NeuralODE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-text">4.3 局限性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%94%EF%BC%8CNeralOperators"><span class="nav-text">五，NeralOperators</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-DeepONets"><span class="nav-text">5.1 DeepONets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-Fourier-Neural-Operator-FNO"><span class="nav-text">5.2 Fourier Neural Operator (FNO)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-NOs%E5%B1%80%E9%99%90%E6%80%A7"><span class="nav-text">5.3 NOs局限性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AD%EF%BC%8C%E6%80%BB%E7%BB%93%E5%92%8C%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91"><span class="nav-text">六，总结和未来研究方向</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="xinwen"
      src="/images/me.jpg">
  <p class="site-author-name" itemprop="name">xinwen</p>
  <div class="site-description" itemprop="description">想到哪儿记到哪儿的技术博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">101</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">87</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/sophia-hxw" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;sophia-hxw"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/sophia_xw" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;sophia_xw" rel="noopener" target="_blank"><i class="crosshairs fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xinwen618@gmail.com" title="E-Mail → mailto:xinwen618@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2022 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xinwen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">429k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:30</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




// 代码折叠
<script src="/js/code-unfold.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'QS91rh0eXkhXnjzhcdHGIRzJ-gzGzoHsz',
      appKey     : 'DD7UTgTkdGwFia0JrJRcs7fs',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
