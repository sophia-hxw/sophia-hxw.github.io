<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ERNIE 3.0-LARGE-SCALE KNOWLEDGE ENHANCED PRE-TRAINING FOR LANGUAGE UNDERSTANDING AND GENERATION</title>
    <url>/2023/11/16/Multimodal/ERNIE%203.0-LARGE-SCALE%20KNOWLEDGE%20ENHANCED%20PRE-TRAINING%20FOR%20LANGUAGE%20UNDERSTANDING%20AND%20GENERATION/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><strong>本文思想</strong></p>
<p><a href="https://arxiv.org/abs/2107.02137">paper</a><br><a href="https://ai.baidu.com/ai-doc/ERNIE-Ultimate/pkyl7nxa6#:~:text=ERNIE%203.0%20%28Large-Scale%20Knowledge%20Enhanced%20Pre-Training%20for,Language%20Understanding%20And%20Generation%29%20%E6%98%AF%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E7%9A%84%E5%A4%9A%E8%8C%83%E5%BC%8F%E7%BB%9F%E4%B8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6%E3%80%82%20%E5%9C%A8ERNIE%203.0%E4%B8%AD%EF%BC%8C%E8%87%AA%E5%9B%9E%E5%BD%92%E5%92%8C%E8%87%AA%E7%BC%96%E7%A0%81%E7%BD%91%E7%BB%9C%E8%A2%AB%E5%88%9B%E6%96%B0%E5%9E%8B%E5%9C%B0%E8%9E%8D%E5%90%88%E5%9C%A8%E4%B8%80%E8%B5%B7%E8%BF%9B%E8%A1%8C%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%8C%E5%85%B6%E4%B8%AD%E8%87%AA%E7%BC%96%E7%A0%81%E7%BD%91%E7%BB%9C%E9%87%87%E7%94%A8ERNIE%202.0%E7%9A%84%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E5%A2%9E%E9%87%8F%E5%BC%8F%E6%9E%84%E5%BB%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1%EF%BC%8C%E6%8C%81%E7%BB%AD%E7%9A%84%E8%BF%9B%E8%A1%8C%E8%AF%AD%E4%B9%89%E7%90%86%E8%A7%A3%E5%AD%A6%E4%B9%A0%E3%80%82">doc</a></p>
<p><strong>基于知识增强的多范式统一预训练框架</strong><br><span id="more"></span></p>
]]></content>
      <categories>
        <category>Multimodal</category>
      </categories>
      <tags>
        <tag>ernie3.0</tag>
      </tags>
  </entry>
  <entry>
    <title>Megatron-LM-Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
    <url>/2023/11/16/Multimodal/Megatron-LM-Training%20Multi-Billion%20Parameter%20Language%20Models%20Using%20Model%20Parallelism/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><strong>本文思想</strong></p>
<p><a href="https://arxiv.org/pdf/1909.08053.pdf">paper</a><br><a href="https://github.com/NVIDIA/Megatron-LM">coding</a></p>
<p><strong>multi-node（多机） multi-gpu（多卡）的工作原理和细节</strong><br><span id="more"></span></p>
<h1 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h1><p>通过对Megatron的学习，期望掌握的是：</p>
<ul>
<li>Transformer如何通过multi-node, multi-GPU实现，例如其中的multi-head attention layer, point-wise feed-forward network；</li>
<li>如何实现三种并行：数据并行（mini-batch），Tensor并行（把一个张量切成若干部分），和Pipeline并行（把一个网络的多个层进行按层切割），以及这三种并行的综合使用；</li>
<li>训练，重训，fine-tuning,多语言扩展等方面的具体的应用。</li>
</ul>
<h1 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h1><p>intra-layer model parallel approach和inter-layer model parallel approach的区别。猛一看，这两个的中文翻译都是“层内模型并行方法”。其实它们是有区别的：</p>
<p><strong>inter-layer</strong> 并行，对应的是pipeline并行。例如6层CNN网络，前三层给一个GPU，后三层给另外一个GPU。</p>
<p>而另外一个是<strong>intra-layer</strong>并行，对应的是tensor并行。仍然以6层CNN网络为例，横向切一刀，即一个tensor张量，会被分配到不同的GPU上面。</p>
<p>其实竖向切了一刀vs.横向切了一刀，那么这两个切法，就是<strong>正交的</strong>（夹角90度）。而且这两个方法可以同时在一个代码中实现，具有<strong>互补</strong>的关系。</p>
<p><img src="inter_intra.jpeg" alt="inter_intra"></p>
<h2 id="row-parallel-linear-layer"><a href="#row-parallel-linear-layer" class="headerlink" title="row parallel linear layer"></a>row parallel linear layer</h2><p>也就是</p>
<p><img src="w_row_change.jpeg" alt="w_row"></p>
<p>我们就可以把X1和W1放到一个GPU上；把X2和W2放到另外一个GPU上。它们都计算完毕之后，再相加（一个同步点），然后把相加的结果，回传给这两个GPU（如果有必要的话）。</p>
<p><img src="row parallel linear layer.jpeg" alt="row_parallel"></p>
<h2 id="column-parallel-linear"><a href="#column-parallel-linear" class="headerlink" title="column parallel linear"></a>column parallel linear</h2><p>也就是</p>
<p><img src="w_column_change.jpeg" alt="w_column"></p>
<p>同样的，我们可以把X和W1的乘积运算放一个GPU上；把X和W2的乘积运算放另外一个GPU上。</p>
<p><img src="column parallel linear layer.jpeg" alt="column_parallel"></p>
<h1 id="MLP的多GPU实现"><a href="#MLP的多GPU实现" class="headerlink" title="MLP的多GPU实现"></a>MLP的多GPU实现</h1><p>在原始的Transformer中，MLP，即point-wise feed forward sublayer，是包括了两个线性层的，实现了hidden.size -&gt; 4*hidden.size -&gt; hidden.size的这样的变换的神经网络。</p>
<p>如果第一个线性层，使用横刀流，则需要在gelu之前加一个”同步点“（gpu停下手中的计算任务，把数据交换了再说，如果有结束的早的gpu，那就wait到其他的gpu的结果计算出来为止。。。这会导致一定程度的gpu浪费）。</p>
<p>反过来看第一个线性层y=XA，如果使用纵刀流，结果会怎样呢？</p>
<p>鉴于XA1和XA2是通过最后一个维度拼接的，那么我们当然可以先计算gelu(XA1)和gelu(XA2)，然后把gelu(XA1)和gelu(XA2)拼接，这不影响最终的结果。这个就有意思了，可以看到为了计算gelu这个非线性函数，我们不需要设置一个同步点了！</p>
<p><strong>所以，第一层线性层，我们优先选择纵刀流（纵向切割权重A）。</strong></p>
<p>第二个线性层呢？</p>
<p>现在看看，当第一个线性层选了纵刀流的时候，结果就是传递给第二个线性层的X’是已经纵向切割的了，我们当然可以对B继续使用纵刀流，分别对 $XA_1$ 和 $XA_2$ 进行处理，这个时候就需要对 $B$ 切三刀，得到四块了，有些琐碎。还有一个方法，就是对B使用横刀流，这样，可以继续在gpu1上运行 $(XA_1)<em>B_1$，以及在GPU2上运行 $(XA_2)</em>B_2$。只有到最后计算完毕之后，才需要一个同步点，把两个gpu上分别的计算结果加在一起。</p>
<p>所以，第二层线性层，我们优先选择横刀流（横向切割权重$B$）。</p>
<p>因此，总结MLP的多GPU计算如下图所示：</p>
<p><img src="mlp.jpeg" alt="mlp_multi_gpu"></p>
<p>上图中，</p>
<ul>
<li>$X$，输入张量，$f$，自定义的激活函数，forward就是identity 函数，即X传播到不同的GPU上面；</li>
<li>$XA_1$, $XA_2$，即通过对 $A$ 纵向一刀，得到的在两个gpu上运行的结果。当然，如果不是只切一刀，那当然可以使用更多的gpu来分别计算 $XA_1, XA_2, XA_3, XA_4\cdots$；</li>
<li>$GELU$，非线性激活函数，$Y_1=GeLU(XA_1)；Y_2=GeLU(XA_2)$；</li>
<li>$Y_1B_1$ 和 $Y_2B_2$，即通过对 $B$ 横向一刀，得到的在两个gpu上运行的结果。当然可以切很多刀，把更小的块，扔给一个个gpu。</li>
<li>$g$，自定义的激活函数，前向forward的时候，是需要按照最后一个维度把Z1和Z2相加起来，即 $Z=Z_1 + Z_2$，pytorch中的all_reduce函数可以实现这个“归约”操作；</li>
</ul>
<p>总结上面的切分权值矩阵的方式，如下图所示：<br><img src="final_mlp.jpeg" alt="final_mlp"></p>
<h1 id="Multi-head-self-attention-sublayer的实现"><a href="#Multi-head-self-attention-sublayer的实现" class="headerlink" title="Multi-head self-attention sublayer的实现"></a>Multi-head self-attention sublayer的实现</h1><p>分析清楚了MLP的多gpu的实现之后，multi-head self-attention sublayer就相对简单一些了，因为其中的四个线性层，都可以复用已有的column, row parallel linear layers。</p>
<p><img src="self_attention.jpeg" alt="self_attention"></p>
<p>上图的左边是通过 $f$，先把 $X$ 分发到多个GPU上（直接copy）。然后每个head相关的三个线性层转换 $Q, K, V$，这三个都是用的“纵刀流linear layer”。在代码中，可以是使用一个 $h$ 到 $3h$ 的linear layer，最后再按照最后一个维度三等分，得到 $Q, K, V$。</p>
<p>之后，计算  $Q^\top_1 K_1$，之后扔给softmax，再给dropout，之后就是 $V_1$ 和这个attention score进行乘积。得到 $Y_1$，再走一个“横刀流”的linear layer。</p>
<p>当然，实际在代码实现的时候，不一定是一个head占一个gpu，也可以是一个gpu上有若干head，然后每个head的hidden.size和gpu中head的数量，合在一起，进行linear layers和multi-head self-attention的计算。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://zhuanlan.zhihu.com/p/366906920">Megatron论文和代码详细分析(1)</a></p>
]]></content>
      <categories>
        <category>Multimodal</category>
      </categories>
      <tags>
        <tag>megatron</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning Transferable Visual Models From Natural Language Supervision</title>
    <url>/2023/11/16/Multimodal/Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><strong>本文思想</strong></p>
<p>[paper link]<br>[code]</p>
<hr>
<span id="more"></span>
<h1 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h1><p>本文是初步分析，旨在说明通用计算机视觉模型带来的一些挑战，并了解其偏差和影响。</p>
<p>我们希望这项工作能够激发未来对此类模型的能力、缺点和偏差的表征的研究，并且我们很高兴与研究界就这些问题进行合作。</p>
<p>我们相信，社区探索是向前迈出的一大步，以进一步表征 CLIP 等模型的功能，并且最重要的是，确定它们具有良好性能的应用领域以及它们可能会降低性能的领域。 这一表征过程可以帮助研究人员提高模型在以下方面的使用可能性：</p>
<p>• 在研究过程的早期识别模型潜在有益的下游用途，使其他研究人员能够考虑应用。<br>• 提出具有高度敏感性和大量社会利益相关者的任务，这可能需要政策制定者的干预。<br>• 更好地描述模型中的偏差，提醒其他研究人员关注的领域和需要干预的领域。<br>• 创建测试套件来评估CLIP 等系统，以便我们可以在开发周期的早期更好地表征模型功能。<br>• 确定潜在的故障模式和进一步工作的领域。</p>
<p>我们计划为这项工作做出贡献，并希望这一分析为后续研究提供一些激励性的例子。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>我们研究了是否有可能将 NLP 中与任务无关的网络规模预训练的成功转移到另一个领域。 我们发现采用这个公式会导致计算机视觉领域出现类似的行为，并讨论了这一研究领域的社会影响。 为了优化其训练目标，CLIP 模型在预训练期间学习执行各种任务。 然后可以通过自然语言提示来利用这种任务学习，以实现向许多现有数据集的零样本迁移。 在足够的规模下，这种方法的性能可以与特定任务的监督模型相媲美，尽管仍有很大的改进空间。</p>
]]></content>
      <categories>
        <category>Multimodal</category>
      </categories>
      <tags>
        <tag>clip</tag>
      </tags>
  </entry>
  <entry>
    <title>Oscar-Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
    <url>/2023/11/15/Multimodal/Oscar:%20Object-Semantics%20Aligned%20Pre-training%20for%20Vision-Language%20Tasks/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><strong>本文思想</strong><br>将相同语义下的物体（名词）作为图像和语言对齐的锚点（Anchor Point）从而简化图像和文本之间的语义对齐的学习任务。</p>
<p><a href="https://arxiv.org/abs/2004.06165">paper link</a><br><a href="https://github.com/microsoft/Oscar">code</a></p>
<p><strong>Oscar，微软家的VLP代表，曾经据说打败一众模型成为VLP多个任务的sota</strong><br><span id="more"></span></p>
<h1 id="一，本文贡献"><a href="#一，本文贡献" class="headerlink" title="一，本文贡献"></a>一，本文贡献</h1><ul>
<li>提出了 Oscar预训练方法，这是一种强大的 VLP 方法，用于学习用于 V+L 理解和生成任务的通用图像文本表示。</li>
<li>开发了一个Oscar模型，其可以在多个 V+L 基准上实现了新的 SoTA，显着优于现有方法</li>
<li>进行了广泛的实验和分析</li>
</ul>
<h1 id="二，Oscar对比现有VLP（vision-language-pretraining）模型"><a href="#二，Oscar对比现有VLP（vision-language-pretraining）模型" class="headerlink" title="二，Oscar对比现有VLP（vision language pretraining）模型"></a>二，Oscar对比现有VLP（vision language pretraining）模型</h1><p>通过将对象标签作为锚点引入，Oscar在两个方面与现有的VLP不同：</p>
<ul>
<li>1，输入表示。每个（图像-文本）样本定义为一个三元组（单词序列，物体标签，区域特征）。</li>
<li>2，预训练目标。根据三元组中三个项目的分组方式，作者从两个不同的角度查看输入：模态视角和。对于每一种视角作者均设计了的预训练目标：<ul>
<li>字典视角（Masked Token Loss）：字典视图的掩盖码恢复损失，它衡量模型根据上下文恢复丢失元素（单词或对象标签）的能力，类似Mask Language modeling，不过其同样mask掉了object tag（注意这里的object tag输入的embedding是word embedding）；</li>
<li>模态视角（Contrastive Loss）：模态视角的对比损失，它衡量模型区分原始三元组及其“污染”版本（即原始物体标签被随机采样的标签替换）的能力。</li>
<li>最终损失是这两个损失的加和</li>
</ul>
</li>
</ul>
<h1 id="三，结论"><a href="#三，结论" class="headerlink" title="三，结论"></a>三，结论</h1><p>Object tag的使用可以大大减少两个模态之间同一对象的距离。如Oscar中Person的图片和文本表示比基线方法中的视觉表示和文本表示更接近，这证明了物体标签在学习对齐语义中的重要性：物体被用做定位点链接和规范化了跨模式的特征学习。</p>
<h1 id="四，多模态学习初入门"><a href="#四，多模态学习初入门" class="headerlink" title="四，多模态学习初入门"></a>四，多模态学习初入门</h1><p>最近，视觉和语言预训练（Vision-Language Pretraining, 简称VLP）在解决多模态学习方面已显示出巨大的进步。这类方法最有代表性地通常包括如下两步：</p>
<p>预训练：是以自监督的方式在海量 “图像-文本”数据（Image-Text Pair，或者叫做“图文对”）上训练大型的基于Transformer的模型（例如根据上下文预测被掩盖掉的语言或者图像的元素）<br>微调：可以对预训练模型的交叉模式表示进行微调，以适应各种下游视觉和语言任务</p>
<p>大型的视觉-语言理解与生成任务：</p>
<ul>
<li><a href="https://visualqa.org/">VQA</a></li>
<li><a href="https://cs.stanford.edu/people/dorarad/gqa/index.html">GQA</a></li>
<li><a href="https://lil.nlp.cornell.edu/nlvr/">NLVR2</a></li>
<li><a href="https://github.com/kuanghuei/SCAN">Image-Text Retrieval</a></li>
<li><a href="https://cocodataset.org/#captions-2015">Image Captioning on COCO dataset</a></li>
<li><a href="https://nocaps.org/">NoCaps</a></li>
</ul>
]]></content>
      <categories>
        <category>Multimodal</category>
      </categories>
      <tags>
        <tag>oscar</tag>
      </tags>
  </entry>
  <entry>
    <title>WenLan-Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</title>
    <url>/2023/11/07/Multimodal/WenLan:%20Bridging%20Vision%20and%20Language%20by%20Large-Scale%20Multi-Modal%20Pre-Training/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><strong>针对的几个挑战性问题</strong></p>
<ul>
<li>无效的图文之间的强关联假设</li>
<li>预训练的效率低下</li>
<li>模型部署困难</li>
</ul>
<p><strong>解决问题</strong><br>提出了BriVL的双塔模型 + 借鉴MoCo的策略 + 一个给予队列的字典 + InfoNCE Loss</p>
<p><strong>模型适用场景</strong><br>图像检索文本、文本检索图像、图像标注、图像零样本分类、作为其他下游多模态任务的输入特征等。</p>
<p><a href="https://arxiv.org/abs/2103.06561">paper link</a><br><a href="https://openi.pcl.ac.cn/BAAI/WuDao-Model/src/branch/master/BriVL/BriVL-code-inference">code</a></p>
<p><strong>BriVL，首个中文通用图文多模态大规模预训练模型</strong><br><span id="more"></span></p>
<h1 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h1><ul>
<li>BriVL使用对比学习算法将图像和文本映射到了同一特征空间，可用于弥补图像特征和文本特征之间存在的隔阂。</li>
<li>基于视觉-语言弱相关的假设，除了能理解对图像的描述性文本外，也可以捕捉图像和文本之间存在的抽象联系。</li>
<li>图像编码器和文本编码器可分别独立运行，有利于实际生产环境中的部署。</li>
</ul>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>近年来，人们对多模态预训练模型进行了深入探索，以桥接视觉和语言。 然而，它们中的大多数通过假设文本和图像模态之间存在强语义相关性，明确地建模图像文本对之间的跨模态交互。 由于这种强有力的假设在现实场景中通常是无效的，因此我们选择对大规模多模态预训练的跨模态相关性进行隐式建模，这是我们领导的中国项目“wenlan”的重点 团队。 具体来说，利用图像-文本对的弱相关性假设，我们在跨模态对比学习框架内提出了一种称为 BriVL 的两塔预训练模型。 与OpenAI CLIP采用简单的对比学习方法不同，我们通过将最新方法MoCo适应跨模态场景，设计了更先进的算法。 通过构建一个基于队列的大型字典，我们的 BriVL 可以在有限的 GPU 资源中合并更多的负样本。 我们进一步构建了一个名为 RUC-CAS-WenLan 的大型中文多源图像文本数据集，用于预训练 BriVL 模型。 大量实验表明，预训练的 BriVL 模型在各种下游任务上均优于 UNITER 和 OpenAI CLIP。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文提出了中国第一个大规模多模态预训练模型 BriVL。 我们的 BriVL 模型的第一个版本有 10 亿个参数，在包含 3000 万个图像文本对的 RUC-CAS-WenLan 数据集上进行了预训练。 作为该项目的一部分，RUC-CAS-文澜是我们自己构建的用于多模态预训练的大型中文多源图文数据集。 值得注意的是，我们的 BriVL 模型在 RUC-CAS-WenLan 测试集和 AIC-ICC 验证集上显着优于 UNITER 和 OpenAI CLIP。 通过预先训练的 BriVL 模型，我们还开发了两个 Web 应用程序，称为 MatchSoul 和 Soul-Music。 在不久的将来，我们的 BriVL 模型将扩大到 100 亿个参数，并用 5 亿个图像文本对进行预训练。 此外，我们还将利用文本到图像生成借口任务进行多模态预训练。</p>
]]></content>
      <categories>
        <category>Multimodal</category>
      </categories>
      <tags>
        <tag>multi_modal</tag>
      </tags>
  </entry>
  <entry>
    <title>resources of machine learning</title>
    <url>/2023/10/31/tools/machine_learning_sources/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>记录一些机器学习的各种资源，书籍和视频都有，随时更新～</p>
<p><strong>come on~</strong><br><span id="more"></span></p>
<h1 id="一，机器学习的视频学习资源"><a href="#一，机器学习的视频学习资源" class="headerlink" title="一，机器学习的视频学习资源"></a>一，机器学习的视频学习资源</h1><p>作为初学者，以下是一些优秀的机器学习视频学习资源，它们可以帮助你入门机器学习：</p>
<h2 id="1-1-吴恩达的《机器学习》课程（Coursera）"><a href="#1-1-吴恩达的《机器学习》课程（Coursera）" class="headerlink" title="1.1 吴恩达的《机器学习》课程（Coursera）"></a>1.1 吴恩达的《机器学习》课程（Coursera）</h2><p>由斯坦福大学的吴恩达教授讲授，是机器学习领域的经典入门课程。它涵盖了广泛的主题，从监督学习到无监督学习，深度学习等。</p>
<h2 id="1-2-李宏毅的机器学习课程（YouTube）"><a href="#1-2-李宏毅的机器学习课程（YouTube）" class="headerlink" title="1.2 李宏毅的机器学习课程（YouTube）"></a>1.2 李宏毅的机器学习课程（YouTube）</h2><p>李宏毅是台湾大学的教授，他在YouTube上分享了一系列机器学习课程，包括深度学习和自然语言处理等。</p>
<h2 id="1-3-3Blue1Brown的机器学习视频（YouTube）"><a href="#1-3-3Blue1Brown的机器学习视频（YouTube）" class="headerlink" title="1.3 3Blue1Brown的机器学习视频（YouTube）"></a>1.3 3Blue1Brown的机器学习视频（YouTube）</h2><p>3Blue1Brown的视频以视觉方式解释机器学习和神经网络的核心概念，对初学者很友好。</p>
<h2 id="1-4-Google-的机器学习速成课程（Google-Developers）："><a href="#1-4-Google-的机器学习速成课程（Google-Developers）：" class="headerlink" title="1.4 Google 的机器学习速成课程（Google Developers）："></a>1.4 Google 的机器学习速成课程（Google Developers）：</h2><p>Google提供的免费机器学习速成课程，适合初学者。课程以实际应用为基础，涵盖了 TensorFlow 和机器学习的基本原理。</p>
<h2 id="1-5-Stanford大学公开课《CS229：机器学习》（YouTube）："><a href="#1-5-Stanford大学公开课《CS229：机器学习》（YouTube）：" class="headerlink" title="1.5 Stanford大学公开课《CS229：机器学习》（YouTube）："></a>1.5 Stanford大学公开课《CS229：机器学习》（YouTube）：</h2><p>斯坦福大学的机器学习课程视频，由斯坦福教授 Andrew Ng 讲授，内容深入浅出。</p>
<h2 id="1-6-DeepLizard的深度学习教程（YouTube）："><a href="#1-6-DeepLizard的深度学习教程（YouTube）：" class="headerlink" title="1.6 DeepLizard的深度学习教程（YouTube）："></a>1.6 DeepLizard的深度学习教程（YouTube）：</h2><p>DeepLizard的视频教程涵盖了深度学习和神经网络的基础知识，以及如何使用Python和深度学习框架构建模型。</p>
<h2 id="1-7-fast-ai的深度学习课程："><a href="#1-7-fast-ai的深度学习课程：" class="headerlink" title="1.7 fast.ai的深度学习课程："></a>1.7 fast.ai的深度学习课程：</h2><p>fast.ai提供了一系列深度学习课程，它们以实践和项目为导向，适合有一定编程经验的初学者。</p>
<h2 id="1-8-MIT开放课程：深度学习-for-Self-Driving-Cars："><a href="#1-8-MIT开放课程：深度学习-for-Self-Driving-Cars：" class="headerlink" title="1.8 MIT开放课程：深度学习 for Self-Driving Cars："></a>1.8 MIT开放课程：深度学习 for Self-Driving Cars：</h2><p>这个MIT的开放课程涵盖了深度学习在自动驾驶领域的应用，提供了大量实际示例和案例研究。</p>
<h1 id="二，周志华的《机器学习》资源"><a href="#二，周志华的《机器学习》资源" class="headerlink" title="二，周志华的《机器学习》资源"></a>二，周志华的《机器学习》资源</h1><p><a href="https://github.com/Sophia-11/Machine-Learning-Notes">机器学习手推笔记</a><br><a href="https://github.com/hanmq/MachineLearning_Zhouzhihua_ProblemSets">课后习题</a><br><a href="https://github.com/fengyang95/tiny_ml">numpy coding</a></p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>sources</tag>
      </tags>
  </entry>
  <entry>
    <title>Deeplab系列网络</title>
    <url>/2023/10/24/detection/deeplab-series/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>DeepLab系列论文一共有四篇，分别为：</p>
<ul>
<li><a href="http://arxiv.org/abs/1606.00915">DeepLab V1</a>，</li>
<li><a href="http://arxiv.org/abs/1606.00915">DeepLab V2</a>，</li>
<li><a href="https://arxiv.org/abs/1706.05587">DeepLab V3</a>，</li>
<li><a href="https://arxiv.org/abs/1802.02611">DeepLab V3+</a>。</li>
</ul>
<p><strong>一点一点加油~</strong><br><span id="more"></span></p>
<h3 id="DeepLab-V1"><a href="#DeepLab-V1" class="headerlink" title="DeepLab V1"></a>DeepLab V1</h3><p>DCNNs因为具有很好的平移不变性（空间信息细节已被高度抽象化）能够很好的处理图像分类问题，但是DCNNs的最后一层的输出不足以准确的定位物体进行像素级分类。DeepLab V1通过将最终DCNN层的输出与完全连接的条件随机场（CRF）结合起来，克服了DCNNs平移不变性的问题。</p>
<p>DCNNs有两个问题需要处理：</p>
<ol>
<li>池化和下采样多次，导致分辨率下降，空间位置信息难以恢复</li>
<li>高层特征具有空间不变性，且细节信息丢失严重</li>
</ol>
<p>解决方案：</p>
<ol>
<li>减少下采样次数，保证特征图的分辨率，同时使用空洞卷积，扩大感受野，获取更多的上下文信息</li>
<li>DeepLab采用完全连接的条件随机场(CRF)提高模型捕获细节的能力。</li>
</ol>
<p>DeepLab V1是基于VGG16网络改写的，一共做了三件事。</p>
<ol>
<li>将全连接层改为卷积层；</li>
<li>将最后最后两个池化的步长改为1（保证特征图的分辨率，只下采样8倍）</li>
<li>把VGG16中最后三个卷积层（conv5_1、conv5_2、conv5_3)的采样率设置为2，且第一个全连接层的dilate rate设置为4（保持感受野）</li>
</ol>
<p><img src="https://files.mdnice.com/user/15197/ae60401a-311e-4122-bb7f-af4c23ef1fc3.png" alt=""></p>
<p><strong>池化层作用：缩小特征图的尺寸；快速扩大感受野。</strong></p>
<p>为什么要扩大感受野呢？为了利用更多的上下文信息进行分析。既然pooling这么好用，为什么要去掉俩呢？这个问题需要从头捋。先说传统（早期）DCNN，主要用来解决图片的分类问题，举个栗子，对于分类任务，传统模型只需要指出图片是不是有小轿车，至于小轿车在哪儿，不care。这就需要网络网络具有平移不变性。我们都知道，卷积本身就具有平移不变性，而pooling可以进一步增强网络的这一特性，因为pooling本身就是一个模糊位置的过程。所以pooling对于传统DCNN可以说非常nice了。</p>
<p>再来说语义分割。语义分割是一个end-to-end的问题，需要对每个像素进行精确的分类，对像素的位置很敏感，是个精细活儿。这就很尴尬了，pooling是一个不断丢失位置信息的过程，而语义分割又需要这些信息，矛盾就产生了。没办法，只好去掉pooling喽。全去掉行不行，理论上是可行的，实际使用嘛，一来显卡没那么大的内存，二来费时间。所以只去掉了两层。<br>（PS：在DeepLab V1原文中，作者还指出一个问题，使用太多的pooling，特征层尺寸太小，包含的特征太稀疏了，不利于语义分割。）</p>
<p>去了两个pooling，感受野又不够了怎么办？没关系，把atrous convolution借来用一下，这也是对VGG16的最后一个修改。atrous convolution人称空洞卷积，相比于传统卷积，可以在不增加计算量的情况下扩大感受野，如下图：<br><img src="https://files.mdnice.com/user/15197/c95aca5c-f28a-4de3-ba3d-4e8b1750ca1b.png" alt="hole algorithm in 1-D, kernel=3, input_stride=2,output_stride=1"></p>
<p>空洞卷积与传统卷积的区别在于，传统卷积是三连抽，感受野是3，如果是input_stride=2的空洞卷积是隔着抽，感受野一下扩大到了5（rate=2），相当于两个传统卷积的感受野，通过调整rate可以自由选择感受野，感受野的问题就解决了。</p>
<p>另外，论文指出空洞卷积还可以增加特征的密度。<br><img src="https://files.mdnice.com/user/15197/9efef548-bb69-4077-b2b5-32707ca0a3f9.png" style="zoom:67%;" /></p>
<p>DeepLab V1的另一个贡献是使用条件随机场CRF提高分类精度。效果如下图，可以看到提升是非常明显的。</p>
<p><img src="https://files.mdnice.com/user/15197/19edd04a-424b-4479-9624-314531caea30.png" alt=""></p>
<p>之前的论文中提到：更深的CNN可以得到更加准确的分类结果，但是定位精度会更低。解决这个问题有2种主要的方法：</p>
<ol>
<li>将low level和high level的feature map进行融合，FCN就是这样做的。</li>
<li>引入super-pixel representation，用low level segmentation method来进行定位任务。<br>DeepLab v1中中使用全连接的条件随机场方法来对定位做finetune，这比当前的方法都要更好。</li>
</ol>
<p>DeepLab v1中也尝试使用了多尺度预测，来提高边界定位精度：将输入图像通过2层的感知机，与前四层的pooling layer输出进行concatenate，再输入到softmax激活函数中，相当于softmax的输入channel是640。但是这种方式增加了参数量和存储空间，而且性能比不上CRF。</p>
<h3 id="DeepLab-v2"><a href="#DeepLab-v2" class="headerlink" title="DeepLab v2"></a>DeepLab v2</h3><p>文章总结起来就是：<strong>空洞卷积+全连接CRF+ASPP模块</strong>， 主干网络从预训练的VGG变成了ResNet，是DeepLab v1的加强版本。</p>
<p>DCNNs中语义分割存在三个挑战：</p>
<ul>
<li>连续下采样和池化操作，导致最后特征图分辨率低。</li>
<li><strong>图像中存在多尺度的物体</strong>（相比V1而言提出的新的挑战）</li>
<li>空间不变性导致细节信息丢失</li>
</ul>
<p>应对策略：</p>
<ul>
<li>移除部分池化操作，使用空洞卷积。控制特征图的分辨率，保证较大感受野，得到较多的上下文信息，而不增加参数量</li>
<li>利用不同膨胀率的空洞卷积融合多尺度信息—atrous spatial pyramid pooling(ASPP)（新的创新点）。以多尺度的信息得到更精确的分割结果。ASPP并行的采用多个采样率的空洞卷积层来探测，以多个比例捕捉对象以及图像上下文。</li>
<li>全连接CRF。通过组合DCNN和概率图模型（CRF），改进分割边界结果。在DCNN中最大池化和下采样组合实现可平移不变性，但这对精度是有影响的。通过将最终的DCNN层响应与全连接的CRF结合来克服这个问题。</li>
</ul>
<p>提取密集特征的两种方式：</p>
<ol>
<li>上采样</li>
<li>空洞卷积</li>
</ol>
<p><img src="https://files.mdnice.com/user/15197/14c3e12f-533d-4519-97c0-4da75a298438.png" alt="上采样 vs. 空洞卷积"></p>
<ul>
<li>U型操作：首先下采样将分辨率降低2倍 → 做卷积 → 上采样得到结果。本质上这只是在原图片面积的1/4的内容上做卷积响应。</li>
<li>空洞卷积：如果我们将全分辨率图像做空洞卷积(采样率为2，核大小与上面卷积核相同)，直接得到结果。这样可以计算出整张图像的响应，效果更加平滑，特征更加密集。</li>
</ul>
<p><strong>ASPP</strong></p>
<p>多尺度主要是为了解决目标在图像中表现为不同大小时仍能够有很好的分割结果，比如同样的物体，在近处拍摄时物体显得大，远处拍摄时显得小。并行采用多个采样率的空洞卷积提取特征，再将特征进行融合，该结构称为空洞空间金字塔池化（atrous spatial pyramid pooling）。如下图所示<br><img src="https://files.mdnice.com/user/15197/dbe962ed-c680-4c27-9cf3-b0e349fc4262.png" alt="ASPP"></p>
<p>至于ASPP如何融合到VGG16中，将VGG16的conv6，换成不同rate的空洞卷积，再跟上conv7，8，最后做个大融合（对应相加或1*1卷积）就OK了。<br><img src="https://files.mdnice.com/user/15197/7ac3e675-4dad-4706-aede-6ab99776b72c.png" alt=""></p>
<p><strong>CRF</strong></p>
<p>crf同deeplab v1</p>
<h3 id="DeepLab-v3"><a href="#DeepLab-v3" class="headerlink" title="DeepLab v3"></a>DeepLab v3</h3><p>在文章开头提出几种常见的捕获multi-scale context的方法。</p>
<ol>
<li>图像金字塔。输入图像进行尺度变换得到不同分辨率input，然后将所有尺度的图像放入CNN中得到不同尺度的分割结果，最后将不同分辨率的分割结果融合得到原始分辨率的分割结果，类似的方法为DeepMedic；</li>
<li>编码-解码结构。FCN和UNet等结构；</li>
<li>本文提出的串联结构。</li>
<li>本文提出的并联结构，Deeplab v3结构。</li>
</ol>
<p><img src="https://files.mdnice.com/user/15197/1cbee597-014a-4aa3-b542-9af83945cac0.png" alt="几种常见的捕获multi-scale context的方法"></p>
<p><strong>空洞卷积的串行结构</strong></p>
<p>空洞卷积的串行结构会使网络加深，对应论文Sec 3.2。<br>使用multi-grid的方法，修改了resnet的block4~block7，使得他们的output_stride都是16，这样就可以保证空间位置信息不会损失太严重，而且论文中也发现如果一直进行下采样，将整体信息聚合到非常小的特征图上，会降低语义分割结果的准确度，如图（a）所示。加入atrous convolution的级联模块如下，主要是使用了不同rate的atrous convolution进行操作，增大filter的感受野。</p>
<p><img src="https://files.mdnice.com/user/15197/b3b852bd-b662-49c2-8ef0-d66cd5fa54ac.png" alt="使用串行结构加深网络"></p>
<p>受到了采用不同大小网格层次结构的多重网格方法的启发，我们提出的模型在block4和block7中采用了不同的空洞率。</p>
<p>特别的，我们定义 Multi_Grid =$[r_1,r_2, r_3]$ 为block4到block7内三个卷积层的unit rates。卷积层的最终空洞率等于unit rate和corresponding rate的乘积。例如，当output_stride = 16，Multi_Grid = (1, 2, 4)，block4中的三个卷积的rates = 2×(1, 2, 4) = (2, 4, 8) 。</p>
<p>但是实验表明，相比并行结构，这种串行的结构并不能得到一个很好的结果</p>
<p><strong>空洞卷积的并行结构</strong><br><img src="https://files.mdnice.com/user/15197/b3507801-0941-4485-baf0-ab5cdec7b344.png" alt=""></p>
<p>在并行结构中，改进内容包括：</p>
<ol>
<li>在ASPP中加入BN层。</li>
<li>当采样率变大，图像边界响应无法捕捉远距离信息，导致卷积核的有效权重变小。只有中心的权重是有效的，3×3退化为1×1卷积核。为了解决该问题需要整合全局上下文信息，对最后的feature map采用全局池化，并经过256个1×1的卷积核+BN，然后双线性插值到所需空间维度。</li>
</ol>
<p>最终并行结构包含</p>
<ul>
<li>ASPP：一个1×1的卷积和三个3×3、rates=(6,12,18)、output_stride=16的空洞卷积（256+BN）。</li>
<li>图像级特征。将特征做全局平均池化，后卷积，再上采样。</li>
</ul>
<p>ASPP中不同rates的空洞卷积通过控制不同的padding输出相同的尺寸，图像级特征中上采样后与ASPP尺寸一致。所有分支的结果被拼接起来并经过1×1的卷积（256+BN），最后经过1×1卷积生成分割结果。</p>
<h3 id="DeepLab-v3-1"><a href="#DeepLab-v3-1" class="headerlink" title="DeepLab v3+"></a>DeepLab v3+</h3><p>改进：</p>
<ol>
<li>BackBone：Xception</li>
<li>Encoder-Decoder Structure</li>
</ol>
<p>由于BackBone可以随意替换，Xception以后会讲，并且都2121年了，有很多更强更快的BackBone可供使用，EfficientNet0~7等等。</p>
<p>DeepLabV1、2、3都是backbone（or ASPP）输出的结果直接双线性上采样到原始分辨率，非常简单粗暴的方法，如下图中的(a)。DeepLab v3+吸取Encoder-Deconder的结构（图中的(b)为编码器-解码器结构），增加了一个浅层到输出的skip层，如下图中的(c)。</p>
<p><img src="https://files.mdnice.com/user/15197/438dbbd0-62b3-4e4f-ab63-345e00e23c2a.png" alt=""></p>
<p>完整的DeepLab v3+的网络结构：</p>
<p><img src="https://files.mdnice.com/user/15197/3b8bbd14-4dd1-4106-9f6f-3e6921f15ae0.png" alt=""></p>
<ul>
<li>encoder输出的feature的output_stride=16，经过双线性上采样4倍得到$F_A$，$F_A$ 的 $output \ stride=4$，</li>
<li>再取encoder中对应着相同分辨率（即$output \ stride=4$）的特征层，经过 $1×1$ 卷积降通道，此时输出的feature记为$F_B$。这里为什么要经过 $1×1$ 卷积降通道，是因为此分辨率的特征通道较多(256或512)，而 $F_A$ 输出只有256，故降通道值$48$，保持比 $F_A$ 通道数，利于模型学习。</li>
<li>将 $F_A$ 和 $F_B$ 做concat，再经过一个 $3 × 3$ 卷积细化feature，最终再双线性上采样4倍得到预测结果。</li>
</ul>
<p>在实验部分展示了 $output \ stride=16$ 是在速度和精度上最佳的平衡点，使用 $output \ stride=8$ 能够进一步提升精度，伴随着是更大的计算消耗。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>DeepLab v1：空洞卷积+CRF</strong></p>
<ul>
<li>减少下采样次数，尽可能的保留空间位置信息；</li>
<li>使用空洞卷积，扩大感受野，获取更多的上下文信息;</li>
<li>采用完全连接的条件随机场(CRF)这种后处理方式，提高模型捕获细节的能力。</li>
</ul>
<p><strong>DeepLab v2：空洞卷积+ASPP+CRF</strong></p>
<ul>
<li>在DeepLab v1的基础上提出了图像多尺度的问题，并提出ASPP模块来捕捉图像多个尺度的上下文信息。</li>
<li>仍然使用CRF后处理方式，处理边缘细节信息。</li>
</ul>
<p>DeepLab v3:</p>
<ul>
<li>改进了ASPP模块：加入了BN层，</li>
<li>探讨了ASPP模块的构建方式：并行的方式精度更好。</li>
<li>由于大采样率的空洞卷积的权重变小，只有中心权重起作用，退化成$1\times 1$卷积。所以将图像级特征和ASPP特征进行融合。</li>
</ul>
<p><strong>DeepLab v3+: deeplabv3 + encoder-decoder</strong></p>
<ul>
<li>使用了encoder-decoder（高层特征提供语义，decoder逐步回复边界信息）：提升了分割效果的同时，关注了边界的信息</li>
<li>encoder结构中：采用Xception做为 BackBone，并将深度可分离卷积（depthwise deparable conv）应用在了ASPP 和 encoder 模块中，使网络更快。</li>
</ul>
<h3 id="DeepLab-v3-代码"><a href="#DeepLab-v3-代码" class="headerlink" title="DeepLab v3+ 代码"></a>DeepLab v3+ 代码</h3><ol>
<li>主干网络采用ResNet<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.model_zoo <span class="keyword">as</span> model_zoo</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, dilation=<span class="number">1</span>, downsample=<span class="literal">None</span>, BatchNorm=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = BatchNorm(planes)</span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                               dilation=dilation, padding=dilation, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = BatchNorm(planes)</span><br><span class="line">        self.conv3 = nn.Conv2d(planes, planes * <span class="number">4</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = BatchNorm(planes * <span class="number">4</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.dilation = dilation</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block, layers, output_stride, BatchNorm, pretrained=<span class="literal">True</span></span>):</span><br><span class="line">        self.inplanes = <span class="number">64</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        blocks = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>]</span><br><span class="line">        <span class="keyword">if</span> output_stride == <span class="number">16</span>:</span><br><span class="line">            strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">            dilations = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">elif</span> output_stride == <span class="number">8</span>:</span><br><span class="line">            strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">            dilations = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Modules</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>,</span><br><span class="line">                                bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = BatchNorm(<span class="number">64</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, layers[<span class="number">0</span>], stride=strides[<span class="number">0</span>], dilation=dilations[<span class="number">0</span>], BatchNorm=BatchNorm)</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, layers[<span class="number">1</span>], stride=strides[<span class="number">1</span>], dilation=dilations[<span class="number">1</span>], BatchNorm=BatchNorm)</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, layers[<span class="number">2</span>], stride=strides[<span class="number">2</span>], dilation=dilations[<span class="number">2</span>], BatchNorm=BatchNorm)</span><br><span class="line">        self.layer4 = self._make_MG_unit(block, <span class="number">512</span>, blocks=blocks, stride=strides[<span class="number">3</span>], dilation=dilations[<span class="number">3</span>], BatchNorm=BatchNorm)</span><br><span class="line">        <span class="comment"># self.layer4 = self._make_layer(block, 512, layers[3], stride=strides[3], dilation=dilations[3], BatchNorm=BatchNorm)</span></span><br><span class="line">        self._init_weight()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pretrained:</span><br><span class="line">            self._load_pretrained_model()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, planes, blocks, stride=<span class="number">1</span>, dilation=<span class="number">1</span>, BatchNorm=<span class="literal">None</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.inplanes, planes * block.expansion,</span><br><span class="line">                          kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                BatchNorm(planes * block.expansion),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.inplanes, planes, stride, dilation, downsample, BatchNorm))</span><br><span class="line">        self.inplanes = planes * block.expansion</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, blocks):</span><br><span class="line">            layers.append(block(self.inplanes, planes, dilation=dilation, BatchNorm=BatchNorm))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_MG_unit</span>(<span class="params">self, block, planes, blocks, stride=<span class="number">1</span>, dilation=<span class="number">1</span>, BatchNorm=<span class="literal">None</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.inplanes, planes * block.expansion,</span><br><span class="line">                          kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                BatchNorm(planes * block.expansion),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.inplanes, planes, stride, dilation=blocks[<span class="number">0</span>]*dilation,</span><br><span class="line">                            downsample=downsample, BatchNorm=BatchNorm))</span><br><span class="line">        self.inplanes = planes * block.expansion</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(blocks)):</span><br><span class="line">            layers.append(block(self.inplanes, planes, stride=<span class="number">1</span>,</span><br><span class="line">                                dilation=blocks[i]*dilation, BatchNorm=BatchNorm))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x = self.conv1(<span class="built_in">input</span>)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        low_level_feat = x</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line">        <span class="keyword">return</span> x, low_level_feat</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weight</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br><span class="line"><span class="comment">#             elif isinstance(m, SynchronizedBatchNorm2d):</span></span><br><span class="line"><span class="comment">#                 m.weight.data.fill_(1)</span></span><br><span class="line"><span class="comment">#                 m.bias.data.zero_()</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_load_pretrained_model</span>(<span class="params">self</span>):</span><br><span class="line">        pretrain_dict = model_zoo.load_url(<span class="string">&#x27;https://download.pytorch.org/models/resnet101-5d3b4d8f.pth&#x27;</span>)</span><br><span class="line">        model_dict = &#123;&#125;</span><br><span class="line">        state_dict = self.state_dict()</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> pretrain_dict.items():</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">in</span> state_dict:</span><br><span class="line">                model_dict[k] = v</span><br><span class="line">        state_dict.update(model_dict)</span><br><span class="line">        self.load_state_dict(state_dict)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ResNet101</span>(<span class="params">output_stride=<span class="number">16</span>, BatchNorm=nn.BatchNorm2d, pretrained=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Constructs a ResNet-101 model.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], output_stride, BatchNorm, pretrained=pretrained)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">import</span> torch</span><br><span class="line">    model = ResNet101(BatchNorm=nn.BatchNorm2d, pretrained=<span class="literal">True</span>, output_stride=<span class="number">8</span>)</span><br><span class="line">    <span class="built_in">input</span> = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">512</span>, <span class="number">512</span>)</span><br><span class="line">    output, low_level_feat = model(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.size())</span><br><span class="line">    <span class="built_in">print</span>(low_level_feat.size())</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>DeepLab v3+<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> resnet <span class="keyword">import</span> ResNet101</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = ResNet101()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### ASPP</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_ASPPModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, kernel_size, padding, dilation</span>):</span><br><span class="line">        <span class="built_in">super</span>(_ASPPModule, self).__init__()</span><br><span class="line">        self.atrous_conv = nn.Conv2d(inplanes, planes, </span><br><span class="line">                                     kernel_size=kernel_size,</span><br><span class="line">                                     padding=padding, </span><br><span class="line">                                     dilation=dilation,</span><br><span class="line">                                     stride=<span class="number">1</span>, </span><br><span class="line">                                     bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn = nn.BatchNorm2d(planes)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.atrous_conv(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.relu(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ASPP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ASPP, self).__init__()</span><br><span class="line">        inplanes = <span class="number">2048</span>  <span class="comment"># resnet101 encoder</span></span><br><span class="line">        dilations = [<span class="number">1</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">18</span>]</span><br><span class="line"></span><br><span class="line">        self.aspp1 = _ASPPModule(inplanes, <span class="number">256</span>, <span class="number">1</span>, dilation=dilations[<span class="number">0</span>], padding=<span class="number">0</span>)  <span class="comment"># padding=dilation使得输出的4个特征图尺寸保持一致</span></span><br><span class="line">        self.aspp2 = _ASPPModule(inplanes, <span class="number">256</span>, <span class="number">3</span>, dilation=dilations[<span class="number">1</span>], padding=dilations[<span class="number">1</span>])</span><br><span class="line">        self.aspp3 = _ASPPModule(inplanes, <span class="number">256</span>, <span class="number">3</span>, dilation=dilations[<span class="number">2</span>], padding=dilations[<span class="number">2</span>])</span><br><span class="line">        self.aspp4 = _ASPPModule(inplanes, <span class="number">256</span>, <span class="number">3</span>, dilation=dilations[<span class="number">3</span>], padding=dilations[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                                             nn.Conv2d(inplanes, <span class="number">256</span>, <span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                             nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">                                             nn.ReLU())</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1280</span>, <span class="number">256</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">256</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = self.aspp1(x)</span><br><span class="line">        x2 = self.aspp2(x)</span><br><span class="line">        x3 = self.aspp3(x)</span><br><span class="line">        x4 = self.aspp4(x)</span><br><span class="line">        x5 = self.global_avg_pool(x)</span><br><span class="line">        x5 = F.interpolate(x5, size=x4.size()[<span class="number">2</span>:], mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        x = torch.cat((x1, x2, x3, x4, x5), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Decoder模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        </span><br><span class="line">        low_level_inplanes = <span class="number">256</span> <span class="comment">#for resnet101 backbone</span></span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(low_level_inplanes, <span class="number">48</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">48</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.last_conv = nn.Sequential(nn.Conv2d(<span class="number">304</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                       nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">                                       nn.ReLU(),</span><br><span class="line">                                       nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                                       nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">                                       nn.ReLU(),</span><br><span class="line">                                       nn.Dropout(<span class="number">0.1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">256</span>, num_classes, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, low_level_feat</span>):</span><br><span class="line">        low_level_feat = self.conv1(low_level_feat)</span><br><span class="line">        low_level_feat = self.bn1(low_level_feat)</span><br><span class="line">        low_level_feat = self.relu(low_level_feat)</span><br><span class="line"></span><br><span class="line">        x = F.interpolate(x, size=low_level_feat.size()[<span class="number">2</span>:], mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">        x = torch.cat((x, low_level_feat), dim=<span class="number">1</span>)</span><br><span class="line">        x = self.last_conv(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">### DeepLab v3+</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DeepLabv3p</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DeepLabv3p, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.backbone = ResNet101()</span><br><span class="line">        self.aspp = ASPP()</span><br><span class="line">        self.decoder = Decoder(num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        x, low_level_feat = self.backbone(<span class="built_in">input</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;backbone----x, low_level_feat: &#x27;</span>, x.size(), low_level_feat.size())</span><br><span class="line">        x = self.aspp(x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;ASPP output: &#x27;</span>, x.size())</span><br><span class="line">        x = self.decoder(x, low_level_feat)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;decoder output: &#x27;</span>, x.size())</span><br><span class="line">        x = F.interpolate(x, size=<span class="built_in">input</span>.size()[<span class="number">2</span>:], mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">### 测试代码</span></span><br><span class="line">deeplabv3p = DeepLabv3p()</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(k)</span><br><span class="line"></span><br><span class="line">image = torch.rand((<span class="number">4</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">mask = net(image)</span><br><span class="line">mask.size()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol>
<li><a href="https://arxiv.org/abs/1412.7062v2">https://arxiv.org/abs/1412.7062v2</a></li>
<li><a href="http://arxiv.org/abs/1606.00915">http://arxiv.org/abs/1606.00915</a></li>
<li><a href="https://arxiv.org/abs/1706.05587">https://arxiv.org/abs/1706.05587</a></li>
<li><a href="https://arxiv.org/abs/1802.02611">https://arxiv.org/abs/1802.02611</a></li>
<li><a href="https://blog.csdn.net/fanxuelian/article/details/85145558">https://blog.csdn.net/fanxuelian/article/details/85145558</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/38474698">https://zhuanlan.zhihu.com/p/38474698</a></li>
<li><a href="https://blog.csdn.net/Dlyldxwl/article/details/81148810">https://blog.csdn.net/Dlyldxwl/article/details/81148810</a></li>
<li><a href="https://www.dazhuanlan.com/2020/04/19/5e9bc2dbb7640/">https://www.dazhuanlan.com/2020/04/19/5e9bc2dbb7640/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/196491750">https://zhuanlan.zhihu.com/p/196491750</a></li>
<li><a href="https://www.cnblogs.com/vincent1997/p/10889430.html">https://www.cnblogs.com/vincent1997/p/10889430.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/75333140">https://zhuanlan.zhihu.com/p/75333140</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/139187977">https://zhuanlan.zhihu.com/p/139187977</a></li>
<li><a href="https://blog.csdn.net/fanxuelian/article/details/85145558">https://blog.csdn.net/fanxuelian/article/details/85145558</a></li>
<li><a href="https://blog.csdn.net/u011974639/article/details/79148719">https://blog.csdn.net/u011974639/article/details/79148719</a></li>
<li><a href="https://blog.csdn.net/u011974639/article/details/79144773">https://blog.csdn.net/u011974639/article/details/79144773</a></li>
<li><a href="https://blog.csdn.net/u011974639/article/details/79518175">https://blog.csdn.net/u011974639/article/details/79518175</a></li>
<li><a href="https://blog.csdn.net/u011974639/article/details/79134409">https://blog.csdn.net/u011974639/article/details/79134409</a></li>
<li><a href="https://www.jianshu.com/p/295dcc4008b4">https://www.jianshu.com/p/295dcc4008b4</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34929725">https://zhuanlan.zhihu.com/p/34929725</a></li>
<li><a href="https://blog.csdn.net/magic_ll/article/details/109731491">https://blog.csdn.net/magic_ll/article/details/109731491</a></li>
<li><a href="https://github.com/DarrenmondZhang/U_Net-DeepLabV3_Plus">https://github.com/DarrenmondZhang/U_Net-DeepLabV3_Plus</a></li>
</ol>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>deeplab</tag>
      </tags>
  </entry>
  <entry>
    <title>虚函数与纯虚函数</title>
    <url>/2023/10/19/cpp/%E8%99%9A%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>虚函数可以说是在涉及C++的面试问题中经久不衰的话题了，这里就介绍一下虚函数的概念以及相关的常见问题。</p>
<p><strong>加油~</strong><br><span id="more"></span></p>
<h1 id="初识虚函数"><a href="#初识虚函数" class="headerlink" title="初识虚函数"></a>初识虚函数</h1><ul>
<li><p>虚函数是指在<strong>基类内部</strong>声明的成员函数前添加<strong>关键字 virtual 指明的函数</strong></p>
</li>
<li><p>虚函数存在的意义是为了<strong>实现多态</strong>，<strong>让派生类能够重写(override)</strong>其基类的成员函数</p>
</li>
<li><p>派生类重写基类的虚函数时，可以添加 virtual 关键字，但不是必须这么做</p>
</li>
<li><p>虚函数<strong>是动态绑定的，在运行时才确定</strong>，而非虚函数的调用在编译时确定</p>
</li>
<li><p>虚函数<strong>必须是非静态成员函数</strong>，因为静态成员函数需要在编译时确定</p>
</li>
<li><p><strong>构造函数不能是</strong>虚函数，因为虚函数是动态绑定的，而构造函数创建时需要确定对象类型</p>
</li>
<li><p><strong>析构函数一般是</strong>虚函数</p>
</li>
<li><p>虚函数一旦声明，就一直是虚函数，派生类也无法改变这一事实</p>
</li>
</ul>
<p>下面举个例子来帮助理解：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span>  <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::f&quot;</span> &lt;&lt; endl; &#125;;	<span class="comment">//一般成员函数</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">f1</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::f1&quot;</span> &lt;&lt; endl; &#125;;  <span class="comment">//虚函数</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">	<span class="type">char</span> aa[<span class="number">3</span>];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="function"><span class="type">void</span> <span class="title">f1</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Derived::f1&quot;</span> &lt;&lt; endl; &#125;;	<span class="comment">//重写基类的虚函数f1()</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">	<span class="type">char</span> bb[<span class="number">3</span>];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base a;</span><br><span class="line">	Derived b;</span><br><span class="line">    Base *p = &amp;a;</span><br><span class="line">    Base *p0 = &amp;b;</span><br><span class="line">    Base *p1 = &amp;a;</span><br><span class="line">	Base *p2 = &amp;b;</span><br><span class="line">	Derived *p3 = &amp;b;</span><br><span class="line">    </span><br><span class="line">    p-&gt;<span class="built_in">f</span>();	   <span class="comment">//基类的指针调用自己的基类部分 Base::f(), 打印结果为 Base::f</span></span><br><span class="line">    p0-&gt;<span class="built_in">f</span>();   <span class="comment">//基类的指针调用派生类的基类部分 Base::f(), 打印结果为 Base::f</span></span><br><span class="line">    </span><br><span class="line">    p1-&gt;<span class="built_in">f1</span>();  <span class="comment">//基类的指针调用基类自身的函数 Base::f1(), 打印结果为 Base::f1</span></span><br><span class="line">	p2-&gt;<span class="built_in">f1</span>();  <span class="comment">//基类的指针调用派生类重写的函数 Derived::f1(), 打印结果为 Derived::f1</span></span><br><span class="line">	p3-&gt;<span class="built_in">f1</span>();  <span class="comment">//派生类调用自己重写的函数 Derived::f1()，打印结果为 Derived::f1</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="虚函数的工作机制"><a href="#虚函数的工作机制" class="headerlink" title="虚函数的工作机制"></a>虚函数的工作机制</h1><p><strong>主要思路</strong></p>
<p>虚函数表 + 虚表指针</p>
<p><strong>具体实现</strong> </p>
<ul>
<li><p>编译器在含有虚函数的类中创建一个虚函数表，称为vtable，这个vtable用来存放虚函数的地址。另外还隐式地设置了一个虚表指针，称为vptr，这个vptr指向了该类对象的虚函数表。</p>
</li>
<li><p>派生类在继承基类的同时，也会继承基类的虚函数表（暂且可以认为派生类此时包含了两个虚函数表所有的内容，具体接着看下面两种情况）。</p>
</li>
<li><p>当派生类重写（override）了基类的虚函数时，则会将重写后的虚函数的地址 <strong>替换掉</strong> 由基类继承而来的虚函数表中对应虚函数的地址（有点绕，多读一遍这一句就清楚了，这也是重写的含义所在吧，也就是覆盖掉基类部分虚函数的地址）。</p>
</li>
<li><p>若派生类没有重写，则由基类继承而来的虚函数的地址将直接保存在派生类的虚函数表中。</p>
</li>
</ul>
<ul>
<li>每个类都只有一个虚函数表，<strong>该类的所有对象共享这个虚函数表</strong>，而不是每个实例化对象都分别有一个虚函数表。</li>
</ul>
<h1 id="虚函数与运行时多态"><a href="#虚函数与运行时多态" class="headerlink" title="虚函数与运行时多态"></a>虚函数与运行时多态</h1><p>首先要理解什么是多态？</p>
<p>多态的实现主要分为静态多态和动态多态，静态多态主要是重载（overload），在编译时就已经确定；而动态多态是通过虚函数机制类实现，在运行时动态绑定。</p>
<p><strong>动态多态是指基类的指针指向其派生类的对象，通过基类的指针来调用派生类的成员函数。</strong>如何理解这句话？我们再来看看本文开头的一段代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base a;</span><br><span class="line">	Derived b;			<span class="comment">//派生类的对象</span></span><br><span class="line">    Base *p1 = &amp;a;</span><br><span class="line">	Base *p2 = &amp;b;		<span class="comment">//基类的指针，指向派生类的对象</span></span><br><span class="line">    p1-&gt;<span class="built_in">f1</span>();</span><br><span class="line">	p2-&gt;<span class="built_in">f1</span>();  <span class="comment">//基类的指针调用派生类重写的虚函数 Derived::f1(), 打印结果为 Derived::f1</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果基类通过引用或者指针调用的是非虚函数，无论实际的对象是什么类型，都执行基类所定义的函数。即：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base a;</span><br><span class="line">	Derived b;</span><br><span class="line">    Base *p = &amp;a;		<span class="comment">//基类的指针，指向基类的对象</span></span><br><span class="line">    Base *p0 = &amp;b;		<span class="comment">//基类的指针，指向派生类的对象</span></span><br><span class="line">    <span class="comment">//f()是非虚函数，所以无论指向是基类的对象a还是派生类的对象b，执行的都是基类的函数f()</span></span><br><span class="line">    p-&gt;<span class="built_in">f</span>();</span><br><span class="line">    p0-&gt;<span class="built_in">f</span>();</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在可以理解什么是运行时多态了。</p>
<p><strong>C++类的多态性是通过虚函数来实现的。如果基类通过引用或指针调用的是虚函数时，我们并不知道执行该函数的对象是什么类型的，只有在运行时才能确定调用的是基类的虚函数还是派生类中的虚函数，这就是运行时多态。</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base a;</span><br><span class="line">	Derived b;</span><br><span class="line">    Base *p1 = &amp;a;		<span class="comment">//基类的指针，指向基类的对象</span></span><br><span class="line">    Base *p2 = &amp;b;		<span class="comment">//基类的指针，指向派生类的对象</span></span><br><span class="line">    <span class="comment">//f1()是虚函数,只有运行时才知道真正调用的是基类的f1(),还是派生类的f1()</span></span><br><span class="line">    p1-&gt;<span class="built_in">f1</span>();	<span class="comment">//p1指向的是基类的对象，所以此时调用的是基类的f1()</span></span><br><span class="line">    p2-&gt;<span class="built_in">f1</span>();	<span class="comment">//p2指向的是派生类的对象，所以调用的是派生类重写后的f1()</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结一下：</p>
<p><strong>多态性其实就是想让基类的指针具有多种形态，能够在尽量少写代码的情况下让基类可以实现更多的功能</strong>。比如说，派生类重写了基类的虚函数f1()之后，基类的指针就不仅可以调用自身的虚函数f1()，还可以调用其派生类的虚函数f1()，这是不是就可以多实现一些操作了呀。</p>
<h1 id="虚函数与静态函数的区别"><a href="#虚函数与静态函数的区别" class="headerlink" title="虚函数与静态函数的区别"></a>虚函数与静态函数的区别</h1><p><strong>静态函数在编译时已经确定，而虚函数是在运行时动态绑定的</strong>。</p>
<p>虚函数因为用了虚函数表的机制，所以在调用的时候会<strong>增加一次内存开销</strong>。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://blog.csdn.net/haoel/article/details/1948051">C++虚函数表解析</a></p>
<p><a href="https://blog.csdn.net/qq_42039281/article/details/80596006">虚函数</a></p>
<h1 id="初识纯虚函数"><a href="#初识纯虚函数" class="headerlink" title="初识纯虚函数"></a>初识纯虚函数</h1><ul>
<li><p>纯虚函数<strong>只在基类中声明，但没有定义</strong>，因此没有函数体。</p>
</li>
<li><p>纯虚函数的声明只需在虚函数形参列表后面<strong>添加 =0</strong> 即可。</p>
</li>
<li><p><strong>含有纯虚函数的类都是抽象类</strong>。</p>
</li>
<li><p><strong>只含有纯虚函数的类称为接口类。</strong></p>
</li>
</ul>
<h1 id="纯虚函数声明"><a href="#纯虚函数声明" class="headerlink" title="纯虚函数声明"></a>纯虚函数声明</h1><p>纯虚函数的声明很简单，就是在虚函数的形参列表后面添加一个 <strong>=0</strong> 即可，如：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Cat</span> &#123;</span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">eat</span><span class="params">()</span></span>=<span class="number">0</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="纯虚函数与抽象类"><a href="#纯虚函数与抽象类" class="headerlink" title="纯虚函数与抽象类"></a>纯虚函数与抽象类</h1><p><strong>含有纯虚函数的类称为抽象类</strong>（注意！！只要含有就是）。</p>
<p>什么是抽象类？它有以下几个特点：</p>
<ul>
<li><p><strong>抽象类不能实例化对象。</strong></p>
</li>
<li><p><strong>抽象类的派生类也可以是抽象类（会继承）</strong>，<strong>也可以通过实现全部的纯虚函数使其变成非抽象类</strong>，从而可以实例化对象。</p>
</li>
<li><p><strong>抽象类的指针可以指向其派生类对象，并调用派生类对象的成员函数。</strong></p>
</li>
</ul>
<p>举个例子，在基类Cat中有两个纯虚函数eat()和sleap()，基类不能直接实例化一个对象来调用这两个函数，但在其派生类CatA和CatB中，可以通过实现这两个函数，当派生类不是抽象类时，便可以实例化对象了。具体请看下面代码示例：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Cat</span>&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">//含有纯虚函数，因此Cat为抽象类</span></span><br><span class="line">	<span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">eat</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">	<span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">sleap</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CatA</span> : <span class="keyword">public</span> Cat &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">eat</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;eat fish.&quot;</span> &lt;&lt; endl; &#125;;	<span class="comment">//实现了eat()函数</span></span><br><span class="line">	<span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">sleap</span><span class="params">()</span> </span>= <span class="number">0</span>;	<span class="comment">//仍为纯虚函数，因此CatA也是抽象类</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CatB</span> : <span class="keyword">public</span> CatA &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">//两个纯虚函数都被实现，都变成一般的虚函数，因此CatB不是抽象类</span></span><br><span class="line">	<span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">eat</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;eat fish.&quot;</span> &lt;&lt; endl; &#125;;</span><br><span class="line">	<span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">sleap</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;sleap for a long time.&quot;</span> &lt;&lt; endl; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	Cat a;			<span class="comment">//报错，Cat是抽象类，不能实例化对象</span></span><br><span class="line">	CatA A;			<span class="comment">//报错，CatA也是抽象类，不能实例化对象</span></span><br><span class="line">	CatB B;			<span class="comment">//正确，CatB不是抽象类</span></span><br><span class="line">	CatB *p1 = &amp;B;</span><br><span class="line">    CatA *p2 = &amp;B;	<span class="comment">//抽象类虽然不能实例化对象，但是可以声明其指针或引用</span></span><br><span class="line">	p1-&gt;<span class="built_in">eat</span>();		<span class="comment">//打印出 eat fish.</span></span><br><span class="line">	p1-&gt;<span class="built_in">sleap</span>();	<span class="comment">//打印出 sleap for a long time.</span></span><br><span class="line">    p2-&gt;<span class="built_in">eat</span>();		<span class="comment">//打印出 eat fish.</span></span><br><span class="line">    p2-&gt;<span class="built_in">sleap</span>();	<span class="comment">//打印出 sleap for a long time.</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>补充一个小疑惑：</p>
<p>刚了解抽象类概念的时候，有个疑惑：既然抽象类不能直接实例化对象，为何不直接使用带有虚函数的一般类就好，而是要定义一个不能实例化对象的抽象类？原因我们可以这样理解，比如我们定义一个类的时候，我们希望它应该具有某种功能（如Teachers类有一个teaching功能，但具体教什么科目还不知道），因此我们可以在定义了它的派生类之后（如MathTeacher），再具体实现teaching这个函数。</p>
<h1 id="纯虚函数与接口类"><a href="#纯虚函数与接口类" class="headerlink" title="纯虚函数与接口类"></a>纯虚函数与接口类</h1><ul>
<li><p><strong>只含有纯虚函数的类称为接口类。</strong>（注意！！是只含有）</p>
</li>
<li><p><strong>接口类没有任何数据成员，也没有构造函数和析构函数。</strong></p>
</li>
<li><p><strong>接口类的指针也可以指向其派生类对象</strong></p>
</li>
</ul>
]]></content>
      <categories>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>delete和delete[]的区别</title>
    <url>/2023/10/18/cpp/delete%E5%92%8Cdelete%5B%5D%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>C++中释放内存的方式有 delete 和 delete[] 两种，它们的区别是什么呢？</p>
<p><strong>加油~</strong><br><span id="more"></span></p>
<h1 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h1><p>在C++ Primer的书中提到，delete用来释放new申请的动态内存，delete[]用来释放由new[]申请的动态内存，也就是说：</p>
<ul>
<li><p>delete释放new分配的<strong>单个对象指针</strong>指向的内存</p>
</li>
<li><p>delete[]释放new分配的<strong>对象数组指针</strong>指向的内存</p>
</li>
</ul>
<p>直接这样理解对吗？先看看下面这段代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *a = <span class="keyword">new</span> <span class="type">int</span>[<span class="number">10</span>];</span><br><span class="line"><span class="keyword">delete</span> a;        <span class="comment">//方式1</span></span><br><span class="line"><span class="keyword">delete</span> [] a;     <span class="comment">//方式2</span></span><br></pre></td></tr></table></figure>
<p>事实上，上面两种释放内存的方式都是正确的，方式1并不会造成内存泄漏。这是为什么呢？因为在申请动态内存的时候，一般有两种情况：基本数据类型的分配和自定义数据类型的分配，两者在释放内存的时候略有不同。</p>
<h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><p>对于像 <strong>int/char/long</strong> 等等这些基本数据类型，使用new分配的不管是数组还是非数组形式的内存空间，delete和delete[]都可以正常释放，不会存在内存泄漏。<strong>原因是：分配这些基本数据类型时，其内存大小已经确定，系统可以记忆并且进行管理，在析构时不会调用析构函数，它通过指针可以直接获取实际分配的内存空间</strong>。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *a = <span class="keyword">new</span> <span class="type">int</span>[<span class="number">10</span>];</span><br><span class="line"><span class="keyword">delete</span> a;        <span class="comment">//正确</span></span><br><span class="line"><span class="keyword">delete</span> [] a;     <span class="comment">//正确</span></span><br></pre></td></tr></table></figure>
<h2 id="自定义数据类型"><a href="#自定义数据类型" class="headerlink" title="自定义数据类型"></a>自定义数据类型</h2><p>对于自定义的Class类，delete和delete[]会有一定的差异。先看一段代码示例：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">T</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	<span class="built_in">T</span>() &#123; cout &lt;&lt; <span class="string">&quot;constructor&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">	~<span class="built_in">T</span>() &#123; cout &lt;&lt; <span class="string">&quot;destructor&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	T* p1 = <span class="keyword">new</span> T[<span class="number">3</span>];				<span class="comment">//数组中包含了3个类对象</span></span><br><span class="line">	cout &lt;&lt; hex &lt;&lt; p1 &lt;&lt; endl;      <span class="comment">//输出P1的地址</span></span><br><span class="line">	<span class="keyword">delete</span>[] p1;</span><br><span class="line">    </span><br><span class="line">	cout &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">	T* p2 = <span class="keyword">new</span> T[<span class="number">3</span>];</span><br><span class="line">	cout &lt;&lt; p2 &lt;&lt; endl;             <span class="comment">//输出P2的地址</span></span><br><span class="line">	<span class="keyword">delete</span> p2;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>delete[]在释放内存的时候，调用了3次析构函数，也就是说，<strong>delete[]会调用析构函数对数组的所有对象进行析构</strong>；而<strong>delete只调用了一次析构函数</strong>，即数组中第一个对象的析构函数，而数组中剩下的对象的析构函数没有被调用，因此<strong>造成了内存泄漏</strong>。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><strong>对于基本数据类型：</strong></p>
<p>不管释放的是单个对象还是数组对象，使用delete和delete[]释放内存的效果相同。</p>
<p><strong>对于自定义数据类型：</strong></p>
<p>释放数组对象时，使用delete时只会调用第一个对象的析构函数，可能会造成内存泄漏；而使用delete[]时会逐个调用析构函数来释放数组的所有对象。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://www.cnblogs.com/wangjian8888/p/7905176.html">delete 和 delete[] 的真正区别</a></p>
<p><a href="https://blog.csdn.net/u012936940/article/details/80919880">C++中的delete和delete[ ]的区别</a></p>
]]></content>
      <categories>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>c++的四个智能指针</title>
    <url>/2023/10/18/cpp/c++%E4%B8%AD%E7%9A%84%E5%9B%9B%E4%B8%AA%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>智能指针是C++11中的一个很重要的新特性，理解智能指针的用法无论对面试还是平时编写代码都有很大的好处，</p>
<p>所以接下来就让我们了解一下C++中的4种智能指针是如何工作的吧。</p>
<p><strong>加油~</strong><br><span id="more"></span></p>
<h1 id="智能指针"><a href="#智能指针" class="headerlink" title="智能指针"></a>智能指针</h1><p>C++的标准模板库（STL）中提供了4种智能指针：<code>auto_ptr</code>、<code>unique_ptr</code>、<code>share_ptr</code>、<code>weak_ptr</code>，其中后面3种是C++11的新特性，而 <code>auto_ptr</code> 是C++98中提出的，已经被C++11弃用了，取而代之的是更加安全的 <code>unique_ptr</code>，后面会详细介绍。</p>
<h2 id="为什么要使用智能指针？"><a href="#为什么要使用智能指针？" class="headerlink" title="为什么要使用智能指针？"></a>为什么要使用智能指针？</h2><p>使用智能指针主要的目的是<strong>为了更安全且更加容易地管理动态内存</strong>。因为即使是一名经验丰富的程序员，也难免有时候会忘记释放之前申请的动态内存，造成内存泄漏。使用智能指针可以很大程度上的避免这个问题，因为智能指针就是一个类，当超出了类的作用域时，类会自动调用析构函数来会自动释放资源，不需要手动地释放内存。当然，这并不是说使用智能指针就不会发生内存泄漏，只是它在很大程度上可以防止由于程序员的疏忽造成的内存泄漏问题。</p>
<h2 id="1-auto-ptr"><a href="#1-auto-ptr" class="headerlink" title="1. auto_ptr"></a>1. auto_ptr</h2><p>智能指针 <code>auto_ptr</code> 由C++98引入，定义在头文件 <code>&lt;memory&gt;</code> 中，在C++11中已经被弃用了，因为它不够安全，而且可以被 <code>unique_ptr</code> 代替。那它为什么会被 <code>unique_ptr</code> 代替呢？先看下面这段代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="function">auto_ptr&lt;string&gt; <span class="title">p1</span><span class="params">(<span class="keyword">new</span> string(<span class="string">&quot;hello world.&quot;</span>))</span></span>;</span><br><span class="line">	auto_ptr&lt;string&gt; p2;</span><br><span class="line">	p2 = p1;				<span class="comment">//p2接管p1的所有权</span></span><br><span class="line">	cout &lt;&lt; *p2&lt;&lt; endl;		<span class="comment">//正确，输出: hello world.</span></span><br><span class="line">	<span class="comment">//cout &lt;&lt; *p1 &lt;&lt; endl;	//程序运行到这里会报错</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//system(&quot;pause&quot;);</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>由上面可以看到，当第9行代码执行赋值操作后，<code>p2</code> 接管了<code>p1</code>的所有权，但此时程序并不会报错，而且运行第10行代码时可以正确输出指向对象的内容。而第11行注释掉的代码则会报错，因为<code>p1</code>此时已经是空指针了，访问输出它会使程序崩溃。可见，<code>auto_ptr</code> 智能指针并不够安全，于是有了它的替代方案：即 <code>unique_ptr</code> 指针。</p>
<h2 id="2-unique-ptr"><a href="#2-unique-ptr" class="headerlink" title="2. unique_ptr"></a>2. unique_ptr</h2><p> <code>unique_ptr</code> 同 <code>auto_ptr</code> 一样也是采用所有权模式，即同一时间只能有一个智能指针可以指向某个对象 ，但之所以说<strong>使用 <code>unique_ptr</code> 智能指针更加安全</strong>，是因为它相比于 <code>auto_ptr</code> 而言<strong>禁止了拷贝操作</strong>（<code>auto_ptr</code>支持拷贝赋值，前面代码示例就是个例子）， <code>unique_ptr</code> 采用了移动赋值 <code>std::move()</code>函数来进行控制权的转移。例如：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="function">unique_ptr&lt;string&gt; <span class="title">p1</span><span class="params">(<span class="keyword">new</span> string(<span class="string">&quot;hello world&quot;</span>))</span></span>;</span><br><span class="line">	<span class="comment">//unique_ptr&lt;string&gt; p2(p1);	//编译不通过，禁止拷贝操作</span></span><br><span class="line">	<span class="comment">//unique_ptr&lt;string&gt; p2 = p1;	//编译不通过</span></span><br><span class="line">	unique_ptr&lt;string&gt; p2 = std::<span class="built_in">move</span>(p1);</span><br><span class="line">	cout &lt;&lt; *p2 &lt;&lt; endl;</span><br><span class="line">    <span class="comment">//cout &lt;&lt; *p1 &lt;&lt;endl;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//system(&quot;pause&quot;);</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>由上面代码示例可知，<code>unique_ptr</code>不会等到程序运行到访问<code>p1</code>的时候才崩溃掉，而是会在编译时就不给予通过，所以它相对而言更加安全。</p>
<p>当然，不管是 <code>auto_ptr</code> 还是 <code>unique_ptr</code>，都可以调用函数 <code>release</code> 或 <code>reset</code>将一个（非const） <code>unique_ptr</code>的控制权转移给另一个 <code>unique_ptr</code> ，如：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">unique_ptr&lt;string&gt; <span class="title">p1</span><span class="params">(<span class="keyword">new</span> string(<span class="string">&quot;hello world&quot;</span>))</span></span>;</span><br><span class="line"><span class="function">unique_ptr&lt;string&gt; <span class="title">p2</span><span class="params">(p1.release())</span></span>;	<span class="comment">//p2获得控制权，p1被置为空</span></span><br><span class="line">unique_ptr&lt;string&gt; p3;</span><br><span class="line">p3.<span class="built_in">reset</span>(p2.<span class="built_in">release</span>());	<span class="comment">//p3获得p2的控制权，reset释放了p2原来的内存</span></span><br></pre></td></tr></table></figure>
<p>此外，<strong><code>unique_ptr</code>使用也很灵活，如果<code>unique_ptr</code>是个临时右值，编译器允许拷贝操作</strong>。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">unique_ptr&lt;string&gt; p2;</span><br><span class="line">p2 = <span class="built_in">unique_ptr</span>&lt;string&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;hello world&quot;</span>)); <span class="comment">//正确，临时右值可以进行拷贝赋值操作</span></span><br></pre></td></tr></table></figure>
<p>上面这种方式进行拷贝操作不会留下悬挂指针，因为它调用了<code>unique_ptr</code>的构造函数，该构造函数创建的临时对象在其所有权让给 <code>p2</code> 后就会被销毁。</p>
<h2 id="3-share-ptr"><a href="#3-share-ptr" class="headerlink" title="3.share_ptr"></a>3.share_ptr</h2><p>共享指针 <code>share_ptr</code>是一种可以共享所有权的智能指针，定义在头文件<code>memory</code>中，它允许多个智能指针指向同一个对象，并使用引用计数的方式来管理指向对象的指针（成员函数use_count()可以获得引用计数），该对象和其相关资源会在“最后一个引用被销毁”时候释放。</p>
<p><code>share_ptr</code>是为了解决 <code>auto_ptr</code> 在对象所有权上的局限性(<code>auto_ptr</code> 是独占的)，在使用引用计数的机制上提供了可以共享所有权的智能指针。</p>
<blockquote>
<p>引用计数变化的几种情况：</p>
<p>①在创建智能指针类的新对象时，初始化指针，引用计数设置为1；</p>
<p>②当智能指针类的对象作为另一个对象的副本时（即进行了拷贝操作），引用计数加1；</p>
<p>③当使用赋值操作符对一个智能指针类对象进行赋值时（如<code>p2=p1</code>），左操作数（即<code>p2</code>）引用先减1，因为它已经指向了别的地方，如果减1后引用计数为0，则释放指针所指对象的内存；然后右操作数（即<code>p1</code>）引用加1，因为此时左操作数指向了右操作数的对象。</p>
<p>④调用析构函数时，析构函数先使引用计数减1，若减至0则delete对象。</p>
</blockquote>
<h3 id="share-ptr-与内存泄漏："><a href="#share-ptr-与内存泄漏：" class="headerlink" title="share_ptr 与内存泄漏："></a><code>share_ptr</code> 与内存泄漏：</h3><p>当两个对象分别使用一个共享指针<code>share_ptr</code> 指向对方时，会导致内存泄漏的问题。</p>
<p>可以这样来理解：智能指针是用来管理指针的，当它指向某个对象时，该对象上的引用计数会加1，当引用计数减到0时该对象会销毁，也就是说<strong>智能指针使用引用计数机制来管理着它所指对象的生命周期</strong>，因此若某个对象A的<code>share_ptr</code> 指向了对象B，那么对象A只能在对象B先销毁之后它才会销毁；同理，若对象B的<code>share_ptr</code> 也指向了对象A，则只有在对象A先销毁之后B才会被销毁。因此，当两个对象的<code>share_ptr</code> 相互指向对方时，两者的引用计数永远不会减至0，即两个对象都不会被销毁，就会造成内存泄漏的问题。</p>
<h2 id="4-weak-ptr"><a href="#4-weak-ptr" class="headerlink" title="4.weak_ptr"></a>4.weak_ptr</h2><p><code>weak_ptr</code> 弱指针是一种不控制对象生命周期的智能指针，它指向一个 <code>share_ptr</code> 管理的对象，进行该对象的内存管理的是那个强引用的 <code>share_ptr</code> ，也就是说 <code>weak_ptr</code> 不会修改引用计数，只是提供了一种访问其管理对象的手段，这也是它称为弱指针的原因所在。</p>
<p>此外，<code>weak_ptr</code> 和 <code>share_ptr</code> 之间可以相互转化，<code>share_ptr</code> 可以直接赋值给<code>weak_ptr</code> ，而<code>weak_ptr</code> 可以通过调用 <code>lock</code> 成员函数来获得<code>share_ptr</code> 。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//shared_ptr和weak_ptr代码示例</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span>;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">A</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	shared_ptr&lt;B&gt; _pb;</span><br><span class="line">    <span class="comment">//weak_ptr&lt;B&gt; _pb;</span></span><br><span class="line">	~<span class="built_in">A</span>() &#123; cout &lt;&lt; <span class="string">&quot;A delete&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">B</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	shared_ptr&lt;A&gt; _pa;</span><br><span class="line">	~<span class="built_in">B</span>() &#123; cout &lt;&lt; <span class="string">&quot;B delete&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="function">shared_ptr&lt;B&gt; <span class="title">pb</span><span class="params">(<span class="keyword">new</span> B())</span></span>;	<span class="comment">//创建一个B类对象的share_ptr</span></span><br><span class="line">	<span class="function">shared_ptr&lt;A&gt; <span class="title">pa</span><span class="params">(<span class="keyword">new</span> A())</span></span>;	<span class="comment">//创建一个A类对象的share_ptr</span></span><br><span class="line">	pb-&gt;_pa = pa;	<span class="comment">//B指向A</span></span><br><span class="line">	pa-&gt;_pb = pb;	<span class="comment">//A指向B</span></span><br><span class="line">	cout &lt;&lt; pb.<span class="built_in">use_count</span>() &lt;&lt; endl;	<span class="comment">//打印结果为：2</span></span><br><span class="line">    cout &lt;&lt; pa.<span class="built_in">use_count</span>() &lt;&lt; endl; <span class="comment">//打印结果为：2，可见两个指针相互引用了</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">//system(&quot;pause&quot;);</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//注：若将第9行代码注释掉，第10行取消注释，打印的结果将分别为：1 和 2</span></span><br><span class="line"><span class="comment">//当B析构函数时，其计数会变为0，B被释放，B释放的同时会使得A的计数减1，</span></span><br><span class="line"><span class="comment">//A再析构后A的计数也会变为0，得以释放</span></span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.nowcoder.com/tutorial/93/8f38bec08f974de192275e5366d8ae24">牛客网-C++工程师面试宝典</a></p>
<p><a href="https://blog.csdn.net/k346k346/article/details/81478223">C++ STL 四种智能指针</a></p>
]]></content>
      <categories>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>c++11的常用新特性</title>
    <url>/2023/10/18/cpp/c++11%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B0%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>C++11新增了很多新特性，这也成为了面试中非常常见的问题，这里介绍一些常用的新特性。C++11新特性有很多，这里就简单整理几个很常见的，应该足以应对面试中的问题了。</p>
<p><strong>加油~</strong><br><span id="more"></span></p>
<h1 id="C-11新特性"><a href="#C-11新特性" class="headerlink" title="C++11新特性"></a>C++11新特性</h1><h2 id="初始化列表"><a href="#初始化列表" class="headerlink" title="初始化列表"></a>初始化列表</h2><p>初始化列表，即用花括号来进行初始化。C++11中可以直接在变量名后面跟上初始化列表来进行对象的初始化，使用起来更加方便，例如：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; vec;			<span class="comment">//C++98/03给vector对象的初始化方式</span></span><br><span class="line">vec.<span class="built_in">push_back</span>(<span class="number">1</span>);</span><br><span class="line">vec.<span class="built_in">push_back</span>(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">vector&lt;<span class="type">int</span>&gt; vec&#123;<span class="number">1</span>,<span class="number">2</span>&#125;;		<span class="comment">//C++11给vector对象的初始化方式</span></span><br><span class="line">vector&lt;<span class="type">int</span>&gt; vec = &#123;<span class="number">1</span>,<span class="number">2</span>&#125;;	</span><br></pre></td></tr></table></figure>
<h2 id="auto关键字"><a href="#auto关键字" class="headerlink" title="auto关键字"></a>auto关键字</h2><p>C++11之前，<strong>在使用表达式给变量赋值的时候需要知道表达式的类型，如char、int等，然而有的时候要做到这一点并不容易</strong>，因此，为了解决这个问题，C++11引入了<code>auto</code>关键字，编译器可以分析表达式的结果来进行类型推导。当然，直接定义变量的时候也可以使用<code>auto</code>来推导类型，可以理解为<code>auto</code>相当于一个占位符，在编译期间会自动推导出变量的类型。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> a = <span class="number">2</span>;		<span class="comment">//推导出a为int类型</span></span><br><span class="line"><span class="keyword">auto</span> b = <span class="number">2.5</span>;	<span class="comment">//推导出b为double类型</span></span><br><span class="line"><span class="keyword">auto</span> c = &amp;a;	<span class="comment">//推导出c为int*类型</span></span><br><span class="line"></span><br><span class="line">vector&lt;<span class="type">int</span>&gt; vec = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>&#125;;</span><br><span class="line">vector&lt;<span class="type">int</span>&gt;::iterator it = vec.<span class="built_in">begin</span>();		<span class="comment">//初始化迭代器</span></span><br><span class="line"><span class="keyword">auto</span> it = vec.<span class="built_in">begin</span>();						<span class="comment">//使用auto后更加方便</span></span><br></pre></td></tr></table></figure>
<p>使用<code>auto</code>时<strong>必须对变量进行初始化</strong>；另外，也可以使用<code>auto</code>定义多个变量，但<strong>必须注意，多个变量推导的结果必须为相同类型</strong>，如：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> a;		<span class="comment">//错误，没有初始化</span></span><br><span class="line"><span class="type">int</span> a = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">auto</span> *p = &amp;a, b = <span class="number">4</span>;	<span class="comment">//正确，&amp;a为int*类型，因此auto推导的结果是int类型，b也是int类型</span></span><br><span class="line"><span class="keyword">auto</span> *p = &amp;a, b = <span class="number">4.5</span>;	<span class="comment">//错误，auto推导的结果为int类型，而b推导为double类型，存在二义性</span></span><br></pre></td></tr></table></figure>
<p><strong><code>auto</code>使用的限制：</strong></p>
<ul>
<li><p><code>auto</code>定义变量时必须初始化</p>
</li>
<li><p><code>auto</code>不能在函数的参数中使用</p>
</li>
<li><p><code>auto</code>不能定义数组，例如：<code>auto arr[] = &quot;abc&quot;</code>，（<code>auto arr = &quot;abc&quot;</code>这样是可以的，但arr不是数组，而是指针）</p>
</li>
<li><p><code>auto</code>不能用于类的非静态成员变量中</p>
</li>
</ul>
<h2 id="decltype关键字"><a href="#decltype关键字" class="headerlink" title="decltype关键字"></a>decltype关键字</h2><p>有时候会遇到这样的情况：希望从表达式的类型中推断出要定义的变量的类型，但是想用该表达式的值来初始化变量。C++11中引入了 <code>decltype</code>关键字来解决这个问题，编译器通过分析表达式的结果来返回相应的数据类型。</p>
<p>格式：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">decltype</span>(表达式) 变量名 [=初始值];	<span class="comment">//[]表示可选,下面用exp来表示表达式</span></span><br></pre></td></tr></table></figure>
<p><code>decltype</code> 的使用遵循以下3条规则：</p>
<ul>
<li><p>若exp是一个不被括号<code>()</code>包围的表达式，或者是单独的变量，其推导的类型将和表达式本身的类型一致</p>
</li>
<li><p>若exp是函数调用，则<code>decltype(exp)</code>的类型将和函数返回值类型一致</p>
</li>
<li><p>若exp是一个左值，或者是一个被括号<code>()</code>包围的值，那么 <code>decltype(exp)</code>的类型将是exp的引用</p>
</li>
</ul>
<p>具体示例：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span>&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> m;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">fun</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a+b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">decltype</span>(x) y = x;			<span class="comment">//y的类型为int，上述规则1</span></span><br><span class="line">    <span class="keyword">decltype</span>(<span class="built_in">fun</span>(x,y)) sum;		<span class="comment">//sum的类型为函数fun()的返回类型，上述规则2</span></span><br><span class="line">    </span><br><span class="line">    Base A;</span><br><span class="line">    <span class="keyword">decltype</span>(A.m) a = <span class="number">0</span>;		<span class="comment">//a的类型为int</span></span><br><span class="line">    <span class="keyword">decltype</span>((A.m)) b = a;		<span class="comment">//exp由括号包围，b的类型为int&amp;，符合上述规则3</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">decltype</span>(x+y) c = <span class="number">0</span>;		<span class="comment">//c的类型为int</span></span><br><span class="line">    <span class="keyword">decltype</span>(x=x+y) d = c;		<span class="comment">//exp为左值，则d的类型为int&amp;，符合上述规则3</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong><code>decltype</code>和<code>auto</code>的区别：</strong> （两者都可以推导出变量的类型）</p>
<ul>
<li><p><code>auto</code> 是根据等号右边的初始值推导出变量的类型，且变量必须初始化，<code>auto</code>的使用更加简洁</p>
</li>
<li><p><code>decltype</code> 是根据表达式推导出变量的类型，不要求初始化，<code>decltype</code>的使用更加灵活</p>
</li>
</ul>
<h2 id="范围for循环"><a href="#范围for循环" class="headerlink" title="范围for循环"></a>范围for循环</h2><p>类似于python中的for-in语句，使用格式及例子如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">vector&lt;<span class="type">int</span>&gt; nums = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>&#125;;</span><br><span class="line"><span class="comment">//使用冒号（:）来表示从属关系，前者是后者中的一个元素，for循环依次遍历每个元素，auto自动推导为int类型</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> num : nums)&#123;</span><br><span class="line">    cout &lt;&lt; num &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="nullptr关键字"><a href="#nullptr关键字" class="headerlink" title="nullptr关键字"></a>nullptr关键字</h2><p>C++11使用<code>nullptr</code>代替了<code>NULL</code>，原因是<code>NULL</code>有时存在二义性，有的编译器可能将<code>NULL</code>定义为<code>((void*)0)</code>，有的则直接定义为0。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">fun</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">	cout &lt;&lt; x &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">fun</span><span class="params">(<span class="type">int</span> *p)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (p != <span class="literal">NULL</span>) cout &lt;&lt; *p &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="built_in">fun</span>(<span class="number">0</span>);		<span class="comment">//在C++98中编译失败，存在二义性，在C++11中编译为fun(int)</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>nullptr</code>是一种特殊类型的字面值，可以被转换成任意其他的指针类型，也可以初始化一个空指针。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *p = <span class="literal">nullptr</span>;	<span class="comment">//等价于 int *p = 0;</span></span><br></pre></td></tr></table></figure>
<h2 id="lambda表达式"><a href="#lambda表达式" class="headerlink" title="lambda表达式"></a>lambda表达式</h2><p>lambda表达式定义了一个匿名函数，一个lambda具有一个返回类型、一个参数列表和一个函数体。与函数不同的是，lambda表达式可以定义在函数内部，其格式如下：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">[capture list] (parameter list) -&gt; <span class="keyword">return</span> type &#123; function body &#125;</span><br><span class="line"><span class="comment">//[捕获列表] (参数列表) -&gt; 返回类型 &#123; 函数体 &#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>capture list（捕获列表）：定义局部变量的列表（通常为空）</p>
</li>
<li><p>parameter list（参数列表）、return type（返回类型）、function body（函数体）和普通函数一样</p>
</li>
<li><p>可以忽略参数列表和返回类型，但<strong>必须包括捕获列表和函数体</strong></p>
</li>
</ul>
<p>示例：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> sum = [](<span class="type">int</span> a, <span class="type">int</span> b) -&gt; <span class="type">int</span> &#123; <span class="keyword">return</span> a+b; &#125;;		<span class="comment">//一个完整的lambda表达式</span></span><br><span class="line">cout &lt;&lt; <span class="built_in">sum</span>(<span class="number">1</span>, <span class="number">2</span>) &lt;&lt; endl;	<span class="comment">//输出3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> fun = [] &#123; <span class="keyword">return</span> <span class="number">4</span>; &#125;;	<span class="comment">//省略参数列表和返回类型</span></span><br><span class="line">cout &lt;&lt; <span class="built_in">fun</span>() &lt;&lt; endl;		<span class="comment">//打印结果为：4</span></span><br></pre></td></tr></table></figure>
<p>lambda表达式可以定义在函数内：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//使用lambda表达式和sort排序自定义一个降序排序算法</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">//bool cmp(const int a, const int b) &#123;</span></span><br><span class="line"><span class="comment">//	return a &gt; b;  // 前者大于后者返回true，因此为降序排序</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	vector&lt;<span class="type">int</span>&gt; nums&#123; <span class="number">13</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">43</span> &#125;;</span><br><span class="line">    <span class="comment">//sort(nums.begin(), nums.end(), cmp);	// 1.使用函数来定义，需要自定义一个cmp函数来调用</span></span><br><span class="line">    <span class="comment">//2.直接使用lambda表达式</span></span><br><span class="line">	<span class="built_in">sort</span>(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>(), [](<span class="type">int</span> a, <span class="type">int</span> b)-&gt; <span class="type">int</span> &#123; <span class="keyword">return</span> a &gt; b; &#125;); </span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">auto</span> i : nums) &#123;</span><br><span class="line">		cout &lt;&lt; i &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	cout &lt;&lt; endl;</span><br><span class="line">	<span class="built_in">system</span>(<span class="string">&quot;pause&quot;</span>);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用捕获列表：</p>
<ul>
<li><p>[] 不捕获任何变量</p>
</li>
<li><p>[&amp;] 捕获外部作用域中所有变量，并作为引用在函数体中使用（按引用捕获）。</p>
</li>
<li><p>[=] 捕获外部作用域中所有变量，并作为副本在函数体中使用（按值捕获）。</p>
</li>
<li><p>[=，&amp;x] 按值捕获外部作用域中所有变量，并按引用捕获 x 变量。</p>
</li>
<li><p>[x] 按值捕获 x 变量，同时不捕获其他变量。</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//下面使用lambda表达式编写一个函数，从数组中找到第一个大于给定长度的字符串</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	vector&lt;string&gt; str = &#123;<span class="string">&quot;abcd&quot;</span>, <span class="string">&quot;hello&quot;</span>, <span class="string">&quot;hi&quot;</span>, <span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello abcd&quot;</span>&#125;;</span><br><span class="line">	<span class="type">int</span> len = <span class="number">5</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//使用lambda表达式，len为按值捕获的变量</span></span><br><span class="line">	<span class="keyword">auto</span> iter = <span class="built_in">find_if</span>(str.<span class="built_in">begin</span>(), str.<span class="built_in">end</span>(), [len](<span class="type">const</span> string &amp;s) &#123;<span class="keyword">return</span> s.<span class="built_in">size</span>() &gt; len; &#125;);</span><br><span class="line">    </span><br><span class="line">	cout&lt;&lt;<span class="string">&quot;The length of first word longer than &quot;</span>&lt;&lt;len&lt;&lt;<span class="string">&quot; is :  &quot;</span>&lt;&lt;*iter&lt;&lt; endl;</span><br><span class="line">	<span class="comment">//system(&quot;pause&quot;);</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="智能指针"><a href="#智能指针" class="headerlink" title="智能指针"></a>智能指针</h2><p>C++提供了4中智能指针，<code>auto_ptr</code>、<code>unique_ptr</code>、<code>share_ptr</code>、<code>weak_ptr</code>，其中第一种为C++98中引入的，在C++11中已经被弃用，后三种是C++中引入的。</p>
<p>使用智能指针主要的目的是<strong>为了更安全且更加容易地管理动态内存</strong>。</p>
<p>关于智能指针的详细介绍，请参考 C++基础中的问题 <strong><em>05_请说一下你理解的 C++ 中的四个智能指针</em></strong> ，这里就不具体展开啦。</p>
<h2 id="右值引用"><a href="#右值引用" class="headerlink" title="右值引用"></a>右值引用</h2><p>右值引用的介绍，请参考 C++基础问题 <strong><em>31_c++中的左值引用与右值引用</em></strong> 。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://c.biancheng.net/cplus/11/">C++11教程：C++11新特性大汇总</a></p>
<p>《C++ Primer》第五版</p>
]]></content>
      <categories>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>cnn相关的面试题</title>
    <url>/2023/10/17/CNN/cnn_relate/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>cnn的常见问题，持续加更中～</p>
<p><strong>加油~</strong><br><span id="more"></span></p>
<h1 id="Chapter-1"><a href="#Chapter-1" class="headerlink" title="Chapter-1"></a>Chapter-1</h1><h2 id="问题1：Softmax-Cross-Entropy反向求导"><a href="#问题1：Softmax-Cross-Entropy反向求导" class="headerlink" title="问题1：Softmax+Cross Entropy反向求导"></a>问题1：Softmax+Cross Entropy反向求导</h2><h2 id="问题2：BatchNorm层的详细解读-具体可以参考之后出版的百面深度学习2333"><a href="#问题2：BatchNorm层的详细解读-具体可以参考之后出版的百面深度学习2333" class="headerlink" title="问题2：BatchNorm层的详细解读(具体可以参考之后出版的百面深度学习2333)"></a>问题2：BatchNorm层的详细解读(具体可以参考之后出版的百面深度学习2333)</h2><ul>
<li><img src="151566545899_.pic.jpg" alt="BatchNorm"></li>
<li>作用：<ul>
<li>使得每层的输入/输出分布更加稳定，避免参数更新和网络层次变深大幅度影响数据分布。从而使模型训练更稳定。</li>
</ul>
</li>
<li><p>参数 β 和 γ的作用</p>
<ul>
<li>保留网络各层在训练过程中的学习成果</li>
<li>保证激活单元的非线性表达能力 </li>
<li>使批归一化模块具有复原初始输出分布能力。</li>
</ul>
</li>
<li><p>BN放在激活层之前还是之后 </p>
</li>
<li>各种不同的Norm<img src="171566546190_.pic_hd.jpg" alt="各种不同的Norm"></li>
<li><a href="https://zhuanlan.zhihu.com/p/33173246">阅读材料</a></li>
</ul>
<h2 id="问题3：Conv-BN加速策略"><a href="#问题3：Conv-BN加速策略" class="headerlink" title="问题3：Conv+BN加速策略"></a>问题3：Conv+BN加速策略</h2><p>在inference阶段，可以将BN层的参数合并在之前的Linear或Conv层中，加速推断时间（因为二者都是线性变换）。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = module.weight.data</span><br><span class="line">b = module.bias.data      <span class="comment"># conv的bias可以用全0代替</span></span><br><span class="line">ws = [<span class="number">1</span>] * <span class="built_in">len</span>(w.size())</span><br><span class="line">ws[<span class="number">0</span>] = w.size()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">invstd = bn_module.running_var.clone().add_(bn_module.eps).pow_(-<span class="number">0.5</span>)</span><br><span class="line">w.mul_(invstd.view(*ws).expand_as(w))</span><br><span class="line">b.add_(-bn_module.running_mean).mul_(invstd)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> bn_module.affine:</span><br><span class="line">    w.mul_(bn_module.weight.data.view(*ws).expand_as(w))</span><br><span class="line">    b.mul_(bn_module.weight.data).add_(bn_module.bias.data)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="问题4：常见的模型加速方法"><a href="#问题4：常见的模型加速方法" class="headerlink" title="问题4：常见的模型加速方法"></a>问题4：常见的模型加速方法</h2><h2 id="问题5：目标检测里如何有效解决常见的前景少背景多的问题"><a href="#问题5：目标检测里如何有效解决常见的前景少背景多的问题" class="headerlink" title="问题5：目标检测里如何有效解决常见的前景少背景多的问题"></a>问题5：目标检测里如何有效解决常见的前景少背景多的问题</h2><ul>
<li>采用Focal Loss或OHEM进行负样本挖掘，加大Hard Example损失权重</li>
<li>训练时只利用Ground Truth周边的Prior Boxes进行训练，忽略其他背景区域，只考虑困难背景区域</li>
</ul>
<h2 id="问题6：目标检测里有什么情况是SSD、YOLOv3、Faster-R-CNN等所不能解决的，假设网络拟合能力无限强"><a href="#问题6：目标检测里有什么情况是SSD、YOLOv3、Faster-R-CNN等所不能解决的，假设网络拟合能力无限强" class="headerlink" title="问题6：目标检测里有什么情况是SSD、YOLOv3、Faster R-CNN等所不能解决的，假设网络拟合能力无限强"></a>问题6：目标检测里有什么情况是SSD、YOLOv3、Faster R-CNN等所不能解决的，假设网络拟合能力无限强</h2><h2 id="问题7：分类和检索两个问题可以怎么理解"><a href="#问题7：分类和检索两个问题可以怎么理解" class="headerlink" title="问题7：分类和检索两个问题可以怎么理解"></a>问题7：分类和检索两个问题可以怎么理解</h2><h2 id="问题8：ROIPool和ROIAlign的区别，以及ROIAlign的简单实现（不考虑并行，cpu串行即可）"><a href="#问题8：ROIPool和ROIAlign的区别，以及ROIAlign的简单实现（不考虑并行，cpu串行即可）" class="headerlink" title="问题8：ROIPool和ROIAlign的区别，以及ROIAlign的简单实现（不考虑并行，cpu串行即可）"></a>问题8：ROIPool和ROIAlign的区别，以及ROIAlign的简单实现（不考虑并行，cpu串行即可）</h2><ul>
<li>ROIPool存在两次量化误差，首先是将候选框边界量化为整数点坐标值，其次是将量化后的边界区域平均分割成 k x k 个单元，对每一个单元的边界进行量化。ROIAlign通过双线性插值避免了量化操作，保存了原始ROI的空间分布，有效避免了误差的产生；对于检测图片中大目标物体时，两种方案的差别不大，而如果是图片中有较多小目标物体需要检测，则优先选择ROIAlign，更精准一些</li>
</ul>
<h2 id="问题9：深度神经网络常见的参数初始化方式，如果全部初始化为0，会出现什么情况"><a href="#问题9：深度神经网络常见的参数初始化方式，如果全部初始化为0，会出现什么情况" class="headerlink" title="问题9：深度神经网络常见的参数初始化方式，如果全部初始化为0，会出现什么情况"></a>问题9：深度神经网络常见的参数初始化方式，如果全部初始化为0，会出现什么情况</h2><h2 id="问题10：多卡并行的时候怎么实现参数共享，通信梯度是指平均梯度，还是最大梯度，还是梯度总和"><a href="#问题10：多卡并行的时候怎么实现参数共享，通信梯度是指平均梯度，还是最大梯度，还是梯度总和" class="headerlink" title="问题10：多卡并行的时候怎么实现参数共享，通信梯度是指平均梯度，还是最大梯度，还是梯度总和"></a>问题10：多卡并行的时候怎么实现参数共享，通信梯度是指平均梯度，还是最大梯度，还是梯度总和</h2><h2 id="问题11：介绍常见的梯度下降优化方法"><a href="#问题11：介绍常见的梯度下降优化方法" class="headerlink" title="问题11：介绍常见的梯度下降优化方法"></a>问题11：介绍常见的梯度下降优化方法</h2><h2 id="问题12-神经网络（卷积-全连接）反向传播公式推导"><a href="#问题12-神经网络（卷积-全连接）反向传播公式推导" class="headerlink" title="问题12: 神经网络（卷积/全连接）反向传播公式推导"></a>问题12: 神经网络（卷积/全连接）反向传播公式推导</h2><h2 id="问题13-Focal-Loss解决了什么问题，如何解决的，与OHEM有什么不同"><a href="#问题13-Focal-Loss解决了什么问题，如何解决的，与OHEM有什么不同" class="headerlink" title="问题13: Focal Loss解决了什么问题，如何解决的，与OHEM有什么不同"></a>问题13: Focal Loss解决了什么问题，如何解决的，与OHEM有什么不同</h2><h2 id="问题14-斜着的矩形框如何求iou-两个多边形的框如何求iou"><a href="#问题14-斜着的矩形框如何求iou-两个多边形的框如何求iou" class="headerlink" title="问题14: 斜着的矩形框如何求iou, 两个多边形的框如何求iou"></a>问题14: 斜着的矩形框如何求iou, 两个多边形的框如何求iou</h2><p>首先要求解两个多边形的面积，方法见该<a href="https://www.shuxuele.com/geometry/area-irregular-polygons.html">链接</a></p>
<p>关键在于如何求出交集的面积</p>
<p><strong>思路一</strong></p>
<p>蒙特卡洛 + 采样，近似求解交集的面积，但是中间涉及判断点在不在多边形内，<a href="https://www.jianshu.com/p/ba03c600a557">判断点是否在多边形内</a></p>
<p><strong>思路二</strong></p>
<p>适合于两个凸多边形（非凸没想到好的思路），凸多边形可以看做是半平面的交集，因此两个凸多边形的交集，可以看作是（m+n）个半平面的交集（假设两个凸多边形分别有m个顶点和n个顶点），求出来半平面的交集（仍旧是一个凸多边形）之后，求解该多边形的面积即可。<a href="https://www.cnblogs.com/Harry-bh/p/9998850.html">求解半平面交集</a></p>
<h2 id="问题15-Detection你觉的还有哪些可做的点"><a href="#问题15-Detection你觉的还有哪些可做的点" class="headerlink" title="问题15: Detection你觉的还有哪些可做的点"></a>问题15: Detection你觉的还有哪些可做的点</h2><h2 id="问题16-卷积底层如何实现的"><a href="#问题16-卷积底层如何实现的" class="headerlink" title="问题16: 卷积底层如何实现的"></a>问题16: 卷积底层如何实现的</h2><h2 id="问题17-mini-Batch-SGD相对于GD有什么优点"><a href="#问题17-mini-Batch-SGD相对于GD有什么优点" class="headerlink" title="问题17: mini-Batch SGD相对于GD有什么优点"></a>问题17: mini-Batch SGD相对于GD有什么优点</h2><h2 id="问题18-DCN比普通卷积多了多少计算量"><a href="#问题18-DCN比普通卷积多了多少计算量" class="headerlink" title="问题18: DCN比普通卷积多了多少计算量"></a>问题18: DCN比普通卷积多了多少计算量</h2><h2 id="问题19-SyncBN如何实现的"><a href="#问题19-SyncBN如何实现的" class="headerlink" title="问题19: SyncBN如何实现的"></a>问题19: SyncBN如何实现的</h2><h2 id="问题20：当需要添加背景类时，怎样处理比较合理"><a href="#问题20：当需要添加背景类时，怎样处理比较合理" class="headerlink" title="问题20：当需要添加背景类时，怎样处理比较合理"></a>问题20：当需要添加背景类时，怎样处理比较合理</h2><h2 id="问题21：给出语义分割评估指标mIOU的计算公式和实现"><a href="#问题21：给出语义分割评估指标mIOU的计算公式和实现" class="headerlink" title="问题21：给出语义分割评估指标mIOU的计算公式和实现"></a>问题21：给出语义分割评估指标mIOU的计算公式和实现</h2><h2 id="问题22：人体姿态估计主流的两个做法是啥？简单介绍下"><a href="#问题22：人体姿态估计主流的两个做法是啥？简单介绍下" class="headerlink" title="问题22：人体姿态估计主流的两个做法是啥？简单介绍下"></a>问题22：人体姿态估计主流的两个做法是啥？简单介绍下</h2><h2 id="问题23：介绍带孔卷积以及其优势与劣势"><a href="#问题23：介绍带孔卷积以及其优势与劣势" class="headerlink" title="问题23：介绍带孔卷积以及其优势与劣势"></a>问题23：介绍带孔卷积以及其优势与劣势</h2><h2 id="问题24：Non-local模块与Self-attention的之间的关系与区别"><a href="#问题24：Non-local模块与Self-attention的之间的关系与区别" class="headerlink" title="问题24：Non-local模块与Self-attention的之间的关系与区别"></a>问题24：Non-local模块与Self-attention的之间的关系与区别</h2><h2 id="问题25：PyTorch和TensorFlow的运行原理"><a href="#问题25：PyTorch和TensorFlow的运行原理" class="headerlink" title="问题25：PyTorch和TensorFlow的运行原理"></a>问题25：PyTorch和TensorFlow的运行原理</h2><h2 id="问题26：卷积的实现原理以及如何快速高效实现局部weight-sharing的卷积操作方式"><a href="#问题26：卷积的实现原理以及如何快速高效实现局部weight-sharing的卷积操作方式" class="headerlink" title="问题26：卷积的实现原理以及如何快速高效实现局部weight sharing的卷积操作方式"></a>问题26：卷积的实现原理以及如何快速高效实现局部weight sharing的卷积操作方式</h2><h2 id="问题27：详解几种优化算法"><a href="#问题27：详解几种优化算法" class="headerlink" title="问题27：详解几种优化算法"></a>问题27：详解几种优化算法</h2><h2 id="问题28：BN在training和inference的时候有什么区别"><a href="#问题28：BN在training和inference的时候有什么区别" class="headerlink" title="问题28：BN在training和inference的时候有什么区别"></a>问题28：BN在training和inference的时候有什么区别</h2><ul>
<li>在训练时，我们可以计算出batch的均值和方差，迭代训练过程中，均值和方差一直在发生变化。但是在推理时，均值和方差是固定的，对于均值来说直接计算所有batch u值的平均值，对于标准偏差采用每个batch σB的无偏估计。</li>
</ul>
<h2 id="问题29：基于anchor匹配的目标检测和基于RNN集合匹配的目标检测有何区别，基于RNN集合匹配的损失定义有何缺陷"><a href="#问题29：基于anchor匹配的目标检测和基于RNN集合匹配的目标检测有何区别，基于RNN集合匹配的损失定义有何缺陷" class="headerlink" title="问题29：基于anchor匹配的目标检测和基于RNN集合匹配的目标检测有何区别，基于RNN集合匹配的损失定义有何缺陷"></a>问题29：基于anchor匹配的目标检测和基于RNN集合匹配的目标检测有何区别，基于RNN集合匹配的损失定义有何缺陷</h2><h2 id="问题30：CycleGAN的生成效果为啥一般都是位置不变纹理变化，为啥不能产生不同位置的生成效果"><a href="#问题30：CycleGAN的生成效果为啥一般都是位置不变纹理变化，为啥不能产生不同位置的生成效果" class="headerlink" title="问题30：CycleGAN的生成效果为啥一般都是位置不变纹理变化，为啥不能产生不同位置的生成效果"></a>问题30：CycleGAN的生成效果为啥一般都是位置不变纹理变化，为啥不能产生不同位置的生成效果</h2><h2 id="问题31：L2-regularization和Weight-decay"><a href="#问题31：L2-regularization和Weight-decay" class="headerlink" title="问题31：L2 regularization和Weight decay"></a>问题31：L2 regularization和Weight decay</h2><h2 id="问题32：深度学习中的batch的大小对学习效果有何影响？"><a href="#问题32：深度学习中的batch的大小对学习效果有何影响？" class="headerlink" title="问题32：深度学习中的batch的大小对学习效果有何影响？"></a>问题32：深度学习中的batch的大小对学习效果有何影响？</h2>]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Physics-Guided, Physics-Informed, and Physics-Encoded Neural Networks in Scientific Computing</title>
    <url>/2023/08/19/math/Physics-Guided,%20Physics-Informed,%20and%20Physics-Encoded%20Neural%20Networks%20in%20Scientific%20Computing/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>最近计算能力的突破使得机器学习和深度学习可用于推进许多领域的科学计算，包括流体力学、固体力学、材料科学等，神经网络在这种混合科学中发挥着核心作用。 由于其固有的架构，传统神经网络在数据稀疏时无法成功训练以及确定范围，许多科学和工程领域就是这种情况。 尽管如此，神经网络给训练期间的物理驱动或基于知识的约束提供了坚实的基础。 一般来说，存在三种不同的神经网络框架来强化底层物理：(i) 物理引导神经网络 (PgNN)、(ii) 物理信息神经网络 (PiNN) 和 (iii) 物理编码神经网络 (PeNN)，这些方法为加速复杂多尺度多物理现象的数值建模提供了明显的优势。 此外，神经算子（NO）的最新发展为这些新的模拟范式增加了另一个维度，特别是当需要复杂的多物理系统的实时预测时。 所有这些模型也都有其独特的缺点和局限性，需要进一步的基础研究。 本研究旨在回顾科学计算研究中使用的四种神经网络框架（即 PgNN、PiNN、PeNN 和 NO），回顾了最先进的架构及其应用，讨论了局限性，并提出了在改进算法、考虑因果关系、扩展应用以及耦合科学和深度学习求解器方面的未来研究机会。 这篇批判性评论为研究人员和工程师提供了一个坚实的起点，帮助他们理解如何将不同的物理层集成到神经网络中。</p>
<p><strong>PGNN,PINN,PENN</strong><br><span id="more"></span></p>
<h1 id="一，介绍"><a href="#一，介绍" class="headerlink" title="一，介绍"></a>一，介绍</h1><p>机器学习（ML）和深度学习（DL）正在成为推动流体力学[1]、固体力学[2]、材料科学[3]等各个领域科学研究和计算的关键技术。 具有数千个处理器的多万亿次浮点运算机器出现并用于科学计算，并结合先进的基于感知的实验，预示着科学和工程领域结构化和非结构化异构数据的爆炸性增长。 ML 和 DL 方法首先被引入科学计算中，以解决缺乏有效数据进行建模的问题，该问题阻碍了科学家与异构且复杂的数据快速交互[4]。 这些方法显示出变革潜力，因为它们能够探索广阔的设计空间、识别多维联系以及管理不适定问题[5,6,7]。 然而，传统的机器学习和深度学习方法无法从复杂的多维数据中提取解释信息和专业知识，它们在映射观测或计算数据方面可能有效，但它们的预测可能在物理上不合理或可疑，导致概括性较差[8,9,10]。 因此，科学家们最初认为这些方法是一个神奇的黑匣子，缺乏坚实的数学基础，无法解释。 尽管如此，学习技术和理论构成了一种新的范式，可以比传统求解器更快地精确解决科学和实际问题。</p>
<p>深度学习（即模仿人脑的神经网络）和科学计算具有类似的历史和发展联系，例如可微性[8]。 图 1 显示了大量科学计算和深度学习方法的发展历史示意图（仅包括开创性的工作）。 在过去的十年中，深度学习和计算能力的突破使得深度学习能够在各种科学计算中使用，特别是在流体力学 [1, 10, 11]、固体力学 [2, 12, 13] 和材料科学领域 [14,15,16]，尽管牺牲了部分准确性和通用性[17]。 这些数据驱动的方法通常用于实现以下目标之一：（i）使用代理建模加速直接数值模拟[18]，（ii）加速伴随敏感性分析[8]，（iii）加速概率编程[19] ，以及（iv）加速逆问题[20]。 例如，在第一个目标中，系统的物理参数（例如尺寸、质量、动量、温度等）被用作输入来预测系统的下一个状态或其效果（即输出），并且在最后一个目标中，系统的输出（例如具有目标属性的材料）用作输入来推断满足要求的内在物理属性（即模型的输出）。 为了实现这些目标，可以构建轻量级深度学习模型来部分或完全取代科学计算过程中的瓶颈步骤 [17,21,22]。</p>
<p>由于传统深度学习方法的内在架构，其学习仅限于进行训练的数据集范围（例如特定边界条件、材料类型、时空离散化等），并且无法成功推理在任何未见的条件下（例如，新的几何形状、新的材料类型、新的边界条件等）。 由于大多数科学领域不是面向（大）数据的领域，并且无法提供涵盖所有可能条件的综合数据集，因此这些基于稀疏数据集训练的模型可以加速，但不能预测[22]。 因此，在对可用的稀疏数据点进行训练时，利用丰富的先验知识、基础物理学和领域专业知识来进一步约束这些模型是合乎逻辑的。 神经网络 (NN) 更适合在训练期间消化物理驱动或基于知识的约束。 根据基础物理学的整合方式，作者将科学计算中的神经网络应用分为三种不同的类型：(i) 物理引导神经网络 (PgNN)、(ii) 物理信息神经网络 (PiNN) 和 (iii) 物理编码神经网络（PeNN）。</p>
<p>在基于 PgNN 的模型中，现成的监督深度学习技术用于构建格式化输入和输出之间的代理映射，这些映射是在受控设置中使用实验和计算生成的，并通过广泛的过程进行管理，以确保符合物理原理和基本规则 [22]，此类模型需要丰富且足够的数据集才能可靠地进行训练和使用。 基于 PgNN 的模型使用具有未知参数 $w$ 的适当函数 $F$ 将一组输入 $x$ 映射到一组相关输出 $y$，使得 $y = F(x;w)$。 通过指定 $F$ 的特定结构，数据驱动方法通常会尝试微调参数 $w$，以使真实值 $\hat y$ 与模型预测值 $y$ 之间的总体误差最小化 [7]。 对于复杂的物理系统，由于数据获取成本高昂，数据可能很稀疏[41]。 绝大多数最先进的 PgNN 缺乏鲁棒性，无法实现任何泛化保证（即插值 [38, 42] 和外推 [43]）。 为了解决这个问题，引入了 PiNN 来执行监督学习任务，同时遵循一般非线性微分方程形式的给定物理定律 [44,10,45,46,6]。</p>
<p>基于 PiNN 的模型通过结合由物理方程残差和边界约束组成的弱损失函数来保证物理定律，他们利用自动微分[47]来求神经网络输出与其输入（即时空坐标和模型参数）的微分。 通过最小化损失函数，网络可以非常接近解[48, 49]。 因此，PiNN 为新的建模和计算范式奠定了基础，通过数学物理领域的长期成就丰富了深度学习 [38, 44]。 PiNN 模型面临着理论（例如，收敛性和稳定性 [50,6,51]）和实现（例如，神经网络设计、边界条件管理和优化方面）相关的许多限制 [40, 10 ]。 此外，在先验不完全了解的显式复杂动力学微分方程时，PiNN 会遇到严重的局限性 [52]。 对于这种情况，人们提出了另一类深度学习方法，称为物理编码神经网络（PeNN）[40]。</p>
<p>基于 PeNN 的模型利用先进的架构来解决 PgNN 和 PiNN 模型遇到的数据稀疏和缺乏泛化的问题。 基于 PeNN 的模型将已知物理强制编码到其核心架构中（例如 NeuralODE [53]），通过构建，基于 PeNN 的模型将神经网络的学习能力从实例学习（由 PgNN 和 PiNN 架构强加）扩展到连续学习 [53]。 PeNN 中底层物理的编码机制与 PiNN [54, 55] 中的编码机制根本不同，尽管它们可以集成以实现模型所需的非线性。 与 PgNN 和 PiNN 相比，PeNN 范式生成的神经网络在数据稀疏性和模型泛化性方面提供了更好的性能[40]。</p>
<p>还有另一类监督学习方法不太适合上面定义的 PgNN、PiNN 和 PeNN 类别，这些模型被称为神经算子，使用先进的架构（例如 DeepONet [39, 56]）学习底层的线性和非线性连续算子，例如积分和分数拉普拉斯算子。 神经算子的数据密集型学习过程可能类似于基于 PgNN 的模型学习，因为两者都使用标记的输入输出数据对来强化问题的物理原理。 然而，神经算子与基于 PgNN 的模型有很大不同，后者由于参数化不足而缺乏泛化属性。 神经算子可以与 PiNN 和 PeNN 方法相结合来训练模型，该模型可以以极高的泛化精度学习物理系统中的复杂非线性[43]。 对于需要实时推理的应用来说，神经算子的鲁棒性是一个显着特征[57]。</p>
<p>这篇综述论文主要面向对神经网络在计算流体和固体力学中的应用感兴趣的科学计算社区，它讨论了 PgNN、PiNN、PeNN 和神经算子的一般架构、优点和局限性，并回顾了这些方法在流体和固体力学中最突出的应用。 这项工作的其余部分结构如下：在第 2 节中，讨论了 PgNN 加速科学计算的潜力。 第 3 节概述了 PiNN，并讨论了它们推进 PgNN 的潜力。 在第 4 节中，讨论了几种领先的 PeNN 架构，以解决 PgNN 和 PiNN 的关键限制。 第 4 节回顾了神经算子的最新发展。 最后，第六节对未来研究方向进行了展望。</p>
<h1 id="二，PGNN"><a href="#二，PGNN" class="headerlink" title="二，PGNN"></a>二，PGNN</h1><p>PgNN 使用现成的监督深度学习模型，从良好控制的实验和计算获得训练数据集中提取特征或属性，来统计学习所需现象的已知物理现象[58]。 PgNN 由多层感知器（MLP，也称为人工神经网络，ANN 或深度神经网络，DNN，在与本综述相关的不同研究中）[58]、CNN [58]、RNN [58] 、GAN [59] 和图神经网络（GRNN）[60]中的一个或几个组成。 尽管 GAN 模型被归类为无监督学习，但在本文中，它们可以被归类为 PgNN，因为它们的底层训练被定义为监督学习问题 [59, 61]，示例 PgNN 架构的示意图如图 2 所示。任何物理问题都包括一组独立特征或输入特征，如 $x = [X_1, X_2, X_3,\cdots, X_n]$ 和一组因变量或期望的输出为 $y = [Y_1, Y_2, Y_3,\cdots , Y_n]$。 描述这种物理现象的数据可以通过实验（例如基于传感器的观测等）、闭合定律（例如傅立叶定律、达西定律、阻力等）或控制常微分方程（ODE）或偏微分方程（PDE），例如伯格方程、纳维-斯托克斯方程等的解来生成方程。因此，因变量和独立特征符合物理原理，并且训练后的神经网络在整个训练过程中本质上受到物理规律的指导。</p>
<p>在 PgNN 中，每层的神经元通过一组权重连接到下一层的神经元，每个节点的输出是通过将激活函数（例如，修正线性单元（ReLU）、Tanh、Sigmoid、线性等）应用于前一层神经元输出的加权和加上附加偏差来获得的 [62]。 该过程从输入开始，顺序获取每层神经元的输出，通常称为前向传播。 随后定义并计算损失函数（或者成本函数），以评估预测的准确性。 常用的回归损失函数是 L1 [63] 和均方误差 (MSE) [63]。 训练的下一步涉及误差反向传播，它计算损失函数相对于权重和偏差的偏导数/梯度（即如图 2 所示的 $\theta$）。 最后，使用梯度下降[64]、随机梯度下降[64]或小批量梯度下降[64]等优化技术来最小化损失函数，并使用计算出的梯度同时计算和更新 $\theta$ 反向传播过程。 迭代该过程，直到 PgNN 获得所需的精度水平。</p>
<p>近年来，PgNN 已广泛用于加速计算流体动力学（CFD）[65]、计算固体力学[66]和多功能材料设计[67]，它已被应用于科学计算的所有计算昂贵且耗时的组件中，例如（i）预处理[68,65,69]，网格生成； (ii)离散化和建模[70,71,72]，例如有限差分（FDM）、有限体积（FVM）、有限元（FEM）、离散元法（DEM）、分子动力学（MD）等； (iii) 后处理，例如输出同化和可视化[73,74,75]。 这些研究的目的是（i）在小数据集上训练浅层网络，以取代传统正向数值建模中的瓶颈（即计算成本高昂的步骤），例如集中复杂流体流动建模中的阻力系数计算[22,76,77,78,79]； 或者（ii）在针对特定问题生成的较大数据集上训练相对较深的网络，例如粗粒度聚合物基因组内的目标序列设计[80]。 这些网络承认生成训练数据的物理原理并加速模拟过程 [75,22]。</p>
<p>尽管 PgNN 的训练看起来很简单，但通过处理复杂物理问题的基础物理来生成数据可能需要大量的计算成本 [6,13]。 经过训练，PgNN 可以显着加快感兴趣现象的计算速度。 值得注意的是，虽然 PgNN 模型可以在训练集上取得良好的准确性，但它有可能记住训练集中的趋势、噪声和细节，而不是直观地理解数据集中的模式。 这是 PgNN 在训练数据集范围之外进行推断/测试时失去预测能力的原因之一。 PgNN 的过度拟合可以通过不同的方式缓解 [81,82,83]，以增强模型在训练数据范围内的可预测性。 在以下小节中，我们回顾了现有文献，并重点介绍了一些应用 PgNN 来加速流体和固体力学应用科学计算的不同步骤的最新研究。</p>
<h2 id="2-1-预处理"><a href="#2-1-预处理" class="headerlink" title="2.1 预处理"></a>2.1 预处理</h2><p>无论数值模型类型如何（例如 FEM、FDM、FVM 等），预处理通常是科学计算中工作量最大的部分。 该组件的主要步骤是将域分解为小而有限的部分（即网格生成、评估和优化），以及在隐式求解时放大/缩小网格属性以使用时空粗糙网格求解未解决的精细尺度物理问题。 这两个步骤非常耗时，并且需要专家级的知识，因此它们是被基于 PgNN 的加速模型取代的潜在候选者。</p>
<h3 id="2-1-1-网格生成"><a href="#2-1-1-网格生成" class="headerlink" title="2.1.1 网格生成"></a>2.1.1 网格生成</h3><p>网格生成是数值模拟的关键步骤。 张等[68]提出了基于整个域所需局部网格密度的预测来自动生成非结构化网格，为此训练了人工神经网络来指导标准网格生成算法。 他们还建议将研究扩展到其他架构，例如 CNN 或 GRNN，以用于未来的研究，包括更大的数据集或更高维度的问题。 黄等[65]采用深度学习方法来确定最佳网格密度。 他们使用经典 CFD 工具（例如 Simcenter STAR-CCM+ [84]）生成优化的网格，并提出训练 CNN 来预测任意几何形状的最佳网格密度。 自适应网格细化版本的添加加速了整个过程，而不会影响准确性和分辨率。 作者提出使用 ANN 学习最佳网格（由具有伴随功能的相应求解器生成），这可以用作其他模拟工具的起点，而不管具体的数值方法如何[65]。 吴等[69]还提出了一种将移动网格方法与深度学习相结合的网格优化方法，以解决网格优化问题。 通过进行实验，构建了一个高精度的神经网络来优化网格，同时保留初始给定网格的指定节点数和拓扑结构。 使用这种技术，他们还证明了移动网格算法独立于 CFD 计算 [69]。</p>
<p>在网格生成中，由于缺乏通用且有效的标准，一个关键问题是网格质量的评估。 陈等[85]提出了一个基准数据集（即 NACA-Market 参考数据集）来促进网格质量的评估。 他们提出了 GridNet，这是一种使用深度 CNN 对网格质量进行自动评估的技术，该方法接收网格作为输入并进行评估，使用在 NACA-Market 数据集上训练的深度 CNN 模型进行的网格质量评估被证明是可行的，准确率高达 92.5% [85]。</p>
<h3 id="2-1-2-多尺度技术"><a href="#2-1-2-多尺度技术" class="headerlink" title="2.1.2 多尺度技术"></a>2.1.2 多尺度技术</h3><p>人们总是希望在时空较粗糙的网格上数值求解多物理问题，以最大限度地减少计算成本。 因此，人们开发了不同的放大[86,87]、缩小[88]和交叉尺度[89]方法来确定在广泛的长度/时间尺度上非线性问题的精确数值解。 一种可行的选择是使用粗网格，它可以可靠地描述长波长动力学并解释未解决的小尺度物理问题。 另一方面，推导粗略表示的数学模型（例如边界条件）相对困难。 巴西奈等[87]提出了一种 PgNN 模型，用于基于已知基础方程的实际解来学习最佳 PDE 近似。 人工神经网络输出空间导数，然后对其进行优化，以便最好地满足低分辨率网格上的方程。 与典型的离散化方法（例如有限差分）相比，推荐的 ANN 方法在以粗糙 4 到 8 倍的分辨率对非线性方程组进行积分时要精确得多 [87]。 然而，这种方法的主要挑战是系统地导出这些自适应解的离散算子。 马杜等[86]开发了一个 PgNN，称为 STENCIL-NET，用于学习非线性 PDE 的特定于分辨率的局部离散化。 通过将规则笛卡尔网格上的空间和时间自适应参数池与离散时间积分知识相结合，STENCIL-NET 可以实现任意非线性 PDE 算子的数值稳定离散化。 STENCIL-NET 模型还可用于在比训练数据集更广泛的时空尺度上确定 PDE 求解方案。 在他们的论文中，作者使用 STENCIL-NET 对粗时空网格上的混沌 PDE 解进行长期预测，以检验他们的假设。 将 STENCIL-NET 模型与基线数值技术（例如，完全矢量化的 WENO [90]）进行比较，在保持相同精度的情况下，粗网格上的预测速度在 GPU 上快了 25 到 150 倍，在 CPU 上快了 2 到 14 倍[86]。</p>
<p>表 1 报告了近期利用 PgNN 加速科学计算预处理部分工作的非详尽列表。 这些研究共同得出的结论是，PgNN 可以成功集成，以在网格生成、网格评估和交叉缩放方面实现相当大的加速因子，这对于使用科学计算技术探索的许多复杂问题至关重要。 下一小节讨论 PgNN 合并到建模组件中的潜力，从而产生更高的加速因子或更高的精度。</p>
<h2 id="2-2-建模与后处理"><a href="#2-2-建模与后处理" class="headerlink" title="2.2 建模与后处理"></a>2.2 建模与后处理</h2><h3 id="2-2-1-应用到流体力学"><a href="#2-2-1-应用到流体力学" class="headerlink" title="2.2.1 应用到流体力学"></a>2.2.1 应用到流体力学</h3><p>PgNN 受到了流体力学界的广泛关注。 Lee 和 Chen [94] 关于使用 ANN 估计流体特性的研究是最早将 PgNN 应用到流体力学的研究之一。 从那时起，PgNN 在流体力学中的应用已扩展到广泛的应用领域，例如层流和湍流、非牛顿流体流动、空气动力学等，特别是加速了传统的计算流体动力学（CFD） 求解器。</p>
<p>对于不可压缩层流模拟，求解Navier-Stokes方程的数值过程被认为是主要瓶颈。 为了缓解这个问题，PgNN 已被用作解析过程的一部分。 例如，杨等[95]提出了一种使用人工神经网络的新型数据驱动投影方法，以避免基于网格的流体模拟中投影步骤的迭代计算，所提出的数据驱动投影方法的效率是显着的，特别是在大规模流体流动模拟中。 汤普森等[96] 使用 CNN 来预测流体流动无粘性欧拉方程的数值解，提出了一种结合多帧信息的无监督训练来提高长期稳定性。 与常用的雅可比方法 [97] 获得的速度场相比，CNN 模型产生了非常稳定的无散度速度场，并且精度更高。 陈等[98]后来开发了一种基于 U-net 的架构，这是 CNN 模型的一种特殊情况，用于预测层流中任意 2D 形状周围的速度和压力场图，CNN 模型使用由 Bezier 曲线构建的随机形状组成的数据集进行训练，然后使用 CFD 求解器求解Navier-Stokes方程。 CNN 模型的预测效率也使用临时误差函数对不可见的形状进行了评估，具体来说，这些预测的 MSE 水平与测试子集上获得的水平处于同一数量级，即压力和速度分别在数量级 $1.0\times 10^{-5}$ 和 $5.0\times 10^{-5}$ 之间。</p>
<p>从层流流态转向湍流流态，PgNN 已广泛用于湍流闭合模型的构建[99]。 Lings等[100]使用前馈 MLP 和专门的神经网络来预测Reynolds-averaged Navier-Stokes (RANS) 和 Large Eddy Simulation (LES) 湍流问题，他们的专门神经网络使用高阶乘法层嵌入伽利略不变性[101]，该模型的性能与 MLP 和真值数值模拟进行了比较，得出的结论是，专门的神经网络可以在不变张量的基础上预测各向异性张量，从而产生比 MLP 更准确的预测。 Maulik等[102] 提出了 Kraichnan 湍流亚网格建模的闭合框架 [103]，为了确定动态闭合强度，所提出的框架使用了隐式映射，其输入为网格解析变量和涡流粘度，使用从高保真直接数值模拟 (DNS) 获得的极度二次采样数据来训练 ANN，可以生成最佳地图。 人们发现，人工神经网络模型成功地将动态动能耗散融入衰减湍流问题，从而能够准确捕获相干结构和惯性范围保真度。 后来，Kim 和 Lee [104] 使用简单线性回归、SLinear、多元线性回归、MLinear 和 CNN，利用其他壁面信息（包括流向）来预测湍流传热（即壁面法线热通量，$q_w$） 壁面剪应力、展向壁面剪应力或流向涡度以及压力波动，通过通道流的 DNS 获得（见图 3（a））。 使用自适应矩估计（ADAM）[105, 106]对构建的网络进行训练，并执行网格搜索方法[107, 108]来优化CNN的深度和宽度。 他们的发现表明，PgNN 模型对输入分辨率不太敏感，表明其在湍流模拟中作为良好热通量模型的潜力。 Yousif等[109]还提出了一种基于PgNN生成湍流流入条件的有效方法，该PgNN是由多尺度卷积自动编码器与子像素卷积层（MSCSP-AE）[110,111]和长短期记忆相结合形成的 LSTM[112,113]模型，研究发现所提出的模型能够处理湍流场的空间映射。</p>
<p>PgNN 也已应用于空气动力学领域。 Kou和Zhang[114]提出了一篇关于典型数据驱动方法的综述论文，包括系统识别、特征提取和数据融合，这些方法已用于模拟非定常空气动力学，这些数据驱动方法的有效性通过气动弹性的几个基准案例进行描述。 Wang等[115]描述了ANN在燃烧室旋流流场建模中的应用（见图3（b））。 来自粒子图像测速 (PIV) 的旋流流场数据用于训练 ANN 模型，经过训练的 PgNN 模型已成功进行测试，可以预测未知入口条件下的旋流流场。 Chowdhary等[116] 研究了将 ANN 模型与基于投影 (PB) 的模型简化技术相结合的功效 [117,118]，为计算成本高昂的高保真物理模型（特别是复杂的高超音速湍流）开发 ANN 替代模型，替代模型用于对自由流条件和 SST（剪切应力传递）湍流模型的参数进行贝叶斯估计，然后使用激波隧道数据将替代模型嵌入到高保真（Reynolds平均Navier-Stokes）流动模拟器中。 Siddiqui等[119]为俯仰翼开发了一种非线性数据驱动模型，包括时滞神经网络（TDNN），俯仰角被视为模型的输入，而升力系数被视为输出。 结果表明，经过训练的模型能够比线性和半经验模型更准确地捕获非线性气动力，特别是在较高的偏移角下。 Wang等[120]还提出了一种基于多任务学习人工神经网络的多保真度降阶模型，以有效预测结冰翼型的非定常气动性能。 结果表明，与单保真度和单任务建模方法相比，所提出的模型具有更高的准确性和更好的泛化能力。</p>
<p>复杂流体流动的模拟，特别是使用表现出粘弹性和非线性流变行为的流体，是 PgNN 应用的另一个主题 [122,123]。 这些流体的动力学通常受非线性本构方程控制，导致刚性数值问题 [124,125]。 Faroughi等[22]开发了一个 PgNN 模型来预测球形颗粒在粘弹性流体中平移的阻力系数（见图 3（c））。 PgNN 考虑了一种堆叠技术（即集成Random Forrest [126]、Extreme Gradient Boosting [127] 和 ANN 模型）来消化输入（考虑 Oldroyd-B 和 Giesekus 流体的雷诺数、Weissenberg 数、粘度比和迁移率因子） ）并根据每个学习器的预测和 ANN 元回归器输出阻力预测，该模型的准确性已成功根据 DNS 生成的盲数据集进行了检查。 Lennon等[128]还开发了一种张量基神经网络（TBNN），允许流变学家构建可学习的本构模型，该模型包含基本的物理信息，同时对特定实验方案或流动运动学的细节保持不可知。 TBNN 模型在实质上客观的张量本构框架中结合了通用逼近器，该框架在构建时遵循连续介质力学所需的物理约束，例如框架不变性和张量对称性。 由于嵌入了 TBNN，开发的流变通用微分方程可以快速学习简单但准确且高度通用的模型来描述所提供的训练数据，从而可以快速发现本构方程。</p>
<p>最后，PgNN 还被广泛用于提高 CFD 求解器的精度和速度，Stevens 和 Colonius [121] 开发了一种深度学习模型（加权本质上非振荡神经网络，WENO-NN）来增强有限体积方法，用于离散化具有不连续解的偏微分方程，例如湍流-冲击波相互作用（见图 1）3(d)。 Kochkov等 [18] 使用混合离散化，将 CNN 和数值求解器的子组件相结合，以高精度将微分算子插值到粗网格上。 该模型的训练是在标准数值方法中进行的，用于将基础偏微分方程作为可微分程序求解，并且该方法允许对整个算法进行基于端对端梯度的优化。 该方法可学习对流通量和残差项的精确局部算子，并与以 8 至 10 倍精细分辨率运行的高级数值解算器的精度相匹配，同时执行计算速度提高 40 至 80 倍。 Cai等[129]实现了最小二乘ReLU神经网络（LSNN）来解决具有不连续解的线性平流反应问题，他们表明，所提出的方法在 DOF（自由度）数量方面优于基于网格的数值方法。 Haber等[130]建议使用自动编码器 CNN 来降低与 Navier-Stokes 方程耦合的标量传输方程的分辨率成本。 Lara 和 Ferrer [131] 提出使用神经网络加速高阶不连续 Galerkin 方法，检查了 1D Burgers 方程的各种网格、多项式阶数和粘度值的方法和界限。 List等[132] 使用 CNN 训练湍流模型，以改善模拟时不可压缩 Navier-Stokes 方程的欠解析、低分辨率解，所开发的方法在空间和时间维度上的分辨率始终优于模拟，分辨率提高了两倍。 对于混合层情况，混合模型平均类似于三倍参考模拟的性能，这相当于时间层加速 7.0 倍，空间混合层加速 3.7 倍。</p>
<p>表 2 报告了利用 PgNN 模拟流体流动问题的近期研究的非详尽列表。 这些研究共同得出的结论是，PgNN 可以成功地与 CFD 求解器集成，或用作独立的代理模型，为流体力学的科学计算开发准确且更快的建模组件。 下一节将讨论 PgNN 在计算固体力学中的潜在应用。</p>
<h3 id="2-2-2-应用到固体力学"><a href="#2-2-2-应用到固体力学" class="headerlink" title="2.2.2 应用到固体力学"></a>2.2.2 应用到固体力学</h3><p>物理引导神经网络（PgNN）也被计算固体力学界广泛采用。 Andersen等[35] 使用 ANN 进行焊接建模是最早将 PgNN 应用到固体力学的研究之一。 此后，PgNN 的应用已扩展到广泛的问题，例如结构分析、拓扑优化、逆向材料设计和建模、健康状况评估等，特别是在计算力学中加速了传统的正向和逆向建模方法。</p>
<p>在结构分析领域，Tadesse 等[137]提出了一种用于预测具有柔性剪力连接器的组合桥的中跨偏转的人工神经网络。 人工神经网络在六座不同的桥上进行了测试，产生的最大均方根误差 (RMSE) 为 3.79%，在实践中可以忽略不计，作者还开发了基于 ANN 的闭式解决方案，用于快速预测日常设计中的变形。 Guneyisi等[138]利用人工神经网络开发了钢梁抗弯超强系数的新公式，他们考虑了 141 个具有不同横截面类型的实验数据样本来训练模型。 结果显示，训练和测试准确率相当，达到 99%，这表明 ANN 模型提供了估计梁的超强强度的可靠工具。 Hung等[139]利用人工神经网络来预测非线性、非弹性钢桁架的极限载荷系数，他们考虑使用平面 39 杆钢桁架来证明所提出的 ANN 的效率，使用构件的横截面作为输入，将荷载系数作为输出，基于 ANN 的模型在预测非线性非弹性钢桁架的极限荷载系数方面具有很高的准确性，平均损失小于 0.02。 chen等[140]还使用ANN来解决弹塑性半球形金属壳与刚性冲击器之间碰撞的三维（3D）逆问题，目标是根据壳体的永久塑性变形来预测碰撞的位置、速度和持续时间，对于静态和动态加载，ANN 模型可以高精度预测位置、速度和碰撞持续时间。 Hosseinpour等[141]使用 PgNN 来评估承受横向扭曲屈曲的城堡形钢梁的屈曲能力，如图 4（a）所示，基于 ANN 的模型比众所周知的设计规范（例如 AS4100 [142]、AISC [143] 和 EC3 [144]）提供了更高的精度，用于建模和预测极限力矩能力 。</p>
<p>材料和超材料的拓扑优化是 PgNN 的另一个应用领域 [145,146]。 拓扑优化是一种识别放置在指定域内的最佳材料以实现最佳结构性能的技术[147]。 例如，Abueidda等[148]开发了一种CNN模型，可以在大变形和小变形下对线性和非线性弹性材料进行实时拓扑优化，训练后的模型可以非常准确地预测最优设计，无需迭代过程方案，并且推理计算时间非常短。 Yu等[149]提出了一种集成的两阶段技术，由基于 CNN 的编码器和解码器（作为第一阶段）和条件 GAN（作为第二阶段）组成，可以确定近乎最优的拓扑设计，这种集成产生了一个模型，该模型在像素值和合规性方面确定了近乎最佳的结构，并大大减少了计算时间。 Banga等[150]还提出了一种3D编码器-解码器CNN来加速3D拓扑优化并确定其部署的最佳计算策略，他们的研究结果表明，所提出的模型可以将总体计算时间减少 40%，同时实现 96% 的准确率。 Li等[151] 然后提出了一种基于 GAN 的非迭代近最优拓扑优化器，用于在黑白密度分布上训练的传导传热结构，用于低分辨率拓扑的 GAN 与超分辨率生成对抗网络 SRGAN [152,153] 相结合，且适用于两阶段分层预测细化管道中的高分辨率拓扑解决方案，与传统的拓扑优化技术相比，他们表明该策略在计算成本和效率方面具有明显的优势。</p>
<p>PgNN 还被应用于固体力学的逆向设计和建模。 Messner [156] 采用 CNN 开发替代模型来估计周期性复合材料的有效机械性能。 例如，基于 CNN 的模型被应用于解决寻找具有最佳机械性能结构的逆向设计问题，替代模型与完善的拓扑优化方法非常一致，例如带有惩罚的固体各向同性材料（SIMP）[157]，并且可以恢复拓扑优化的最佳解决方案。 Lininger等[158]使用CNN来解决由薄膜堆叠制成的超材料的逆设计问题，作者证明了 CNN 探索大型全局设计空间（多达 1012 个参数组合）并解决超材料结构与相关椭圆测量和反射/透射光谱之间所有关系的卓越能力 [159, 158]。 Kumar等[154]提出了一种两阶段的ANN模型，如图4（b）所示，用于超材料的逆向设计，该模型生成均匀且功能梯度的细胞机械超材料，具有针对旋曲线拓扑定制的各向异性刚度和密度。 本研究中使用的 ANN 模型是两级 ANN 的组合，第一个 ANN（即逆 PgNN）将查询刚度作为输入和输出设计参数，例如 $\Theta$，第二个 ANN（即前向 PgNN）将预测的设计参数作为输入并预测刚度以验证第一个 ANN 结果，刚度和设计参数的预测精度在两个网络中与真值数据进行验证，样本比较及其相应的 $R$ 平方值如图 4(b) 所示。Ni 和 Gau [155] 提出了代表性采样空间和条件 GAN、cGAN [160,161] 的组合。他们表明，其所提出的方法可以高精度部署，如图 4(c) 所示，同时避免使用传统方法中使用的昂贵迭代求解器，例如伴随加权方法 [162]，该模型特别适用于地质勘探、质量控制、复合材料评价等领域使用的实时弹性成像和高通量无损检测技术。</p>
<p>PgNN 模型还被用来克服固体力学中多尺度模拟的一些计算限制。 这是通过以下方式实现的：</p>
<ul>
<li>（i）绕过成本高昂的小规模计算，从而加速宏观尺度模拟[66]；</li>
<li>（ii）用替代模型替换步骤或完整模拟[66]。 </li>
</ul>
<p>例如，Liang等[163]开发了一种 ANN 模型，以基于有限元的主动脉几何形状作为输入，直接输出主动脉壁应力分布，绕过 FEM 计算。 FEM 计算的应力与 PgNN 模型估计的应力之间的差异实际上可以忽略不计，而 PgNN 模型只需要 FEM 计算时间的一小部分即可产生输出。 Mozaffar等[164]通过学习研究材料可塑性时发生的可逆、不可逆和历史相关现象，成功地将基于 RNN 的替代模型用于材料建模。 Mianroodi等[2] 使用基于 CNN 的求解器来预测具有高度非线性材料响应和机械对比特征的异质固体中的局部应力，与 FEM 等常见求解器相比，基于 CNN 的求解器为弹塑性材料提供了 8300 倍的加速因子。 Im等[5] 提出了一个 PgNN 框架，通过将 LSTM 网络与适当的正交分解 (POD) 方法集成来构建高维弹塑性 FEM 模型的代理模型 [165,166]，提出的 POD-LSTM 代理模型可以仅根据提供的训练数据集快速、精确且可靠地预测弹塑性结构。 Long 等[167]首次使用CNN来估计平面裂纹的应力强度因子，与 FEM 相比，所提出的基于 CNN 的轻量级裂纹评估方法的主要优点是它可以安装在无人机器上，以实时自动监测裂纹的严重程度。</p>
<p>表 3 报告了近期在固体力学和材料设计问题中利用 PgNN 的研究的非详尽列表。 这些研究共同得出的结论是，PgNN 可以成功地与传统求解器（例如 FEM 求解器）集成，或用作独立的代理模型，为固体力学中的科学计算开发准确且更快的建模组件。 尽管如此，PgNN 也有其自身的局限性和缺点，可能会影响不同条件下的解决方案，如下一节所述。</p>
<h2 id="2-3-PGNN的局限"><a href="#2-3-PGNN的局限" class="headerlink" title="2.3 PGNN的局限"></a>2.3 PGNN的局限</h2><p>尽管基于 PgNN 的模型显示出对加速输入输出相互依赖的非线性现象建模的巨大潜力，但它们仍存在一些关键的限制和缺点，而且当训练数据集稀疏时，其中一些限制变得更加明显。</p>
<ul>
<li>PgNN 的主要局限性源于其训练过程仅基于统计数据 [58]。 尽管训练数据集本质上受到物理学的限制（例如，通过直接数值模拟、闭合定律和去噪实验开发），PgNN 仍根据统计变化的相关性生成模型。 因此，输出（预测）自然是与物理无关的 [38,176]，并且可能违反基础物理 [6]。</li>
<li>PgNN 的另一个重要限制源于训练数据集通常稀疏的事实，尤其是在本文讨论的科学领域。 当训练数据稀疏并且不覆盖整个潜在理化属性范围时，基于 PgNN 的模型无法在训练范围之外的条件下进行盲测[43]，即它们不提供以下方面的外推能力： 时空变量和/或其他物理属性。</li>
<li>PgNN 的预测可能会受到严重影响，即使对于稀疏训练数据集范围内的输入也是如此 [22]。 在物理化学属性范围极其广泛（例如，从蠕动流到湍流的Reynolds范围）的复杂和非线性问题中，插值能力的缺乏更为明显。</li>
<li>PgNN 可能无法完全满足生成训练数据集所使用的初始条件和边界条件[38]。 每个问题的边界条件和计算域都不同，这使得数据生成和训练过程的成本极高。 此外，科学计算研究的很大一部分涉及逆问题，其中未知的感兴趣物理、化学属性仅通过与这些属性间接相关的测量或计算来估计[177,178,10,13]。 例如，在地下水流建模中，我们利用浸入含水层中的流体压力测量来估计含水层的几何形状和/或材料特征[179]，这些要求使开发在任何条件下都具有预测能力的简单神经网络的过程变得更加复杂。</li>
<li>基于PgNN 的模型在构造上不是分辨率不变的[180]，因此它们不能在较低分辨率上进行训练并直接在较高分辨率上进行推断，这个缺点是由于 PgNN 仅被设计用于学习单个实例（即输入-输出）物理现象的解决方案。</li>
<li>通过训练过程，基于PgNN 的网络可以学习整个数据集的输入输出相互依赖性，这样的过程可能会将不同输入和输出对之间的函数依赖性的微小变化视为噪声，并产生平均解决方案。 因此，虽然这些模型对于整个数据集来说是最优的，但在个别情况下它们可能会产生次优的结果。</li>
<li>当训练数据集多样化时，即当不同输入和输出对之间的相互依赖性截然不同时，PgNN 模型可能很难学习底层过程。 尽管可以通过增加模型大小来缓解这个问题，但需要更多数据来训练这样的网络，这使得训练成本高昂，并且在某些情况下不切实际。</li>
</ul>
<p>解决 PgNN 某些局限性的一种方法是生成更多训练数据，然而，由于数据采集成本高昂，这并不总是可行的解决方案，或者，PgNN 可以在没有任何先验假设的情况下通过控制物理定律来进一步约束，从而减少对大型数据集的需求。 后者是一个看似合理的解决方案，因为在大多数情况下，可以使用显式 ODE、PDE 和/或闭包定律来完整和部分地描述物理现象。 这种方法导致了基于物理的神经网络的发展 [38,44]，下一节将对此进行描述和回顾。</p>
<h1 id="三，PINN"><a href="#三，PINN" class="headerlink" title="三，PINN"></a>三，PINN</h1><p>在科学计算中，物理现象通常使用强大的数学形式来描述，其中包括控制微分方程以及初始条件和边界条件。 在域内的每个点，强形式指定解决方案必须满足的约束，控制方程通常是线性或非线性偏微分方程和/或常微分方程，众所周知，一些偏微分方程求解起来非常具有挑战性，例如，解释各种流体流动的 Navier-Stokes 方程[10]、描述固体中大变形的 Foppl–von Karman 方程 [181]，其他重要的还有热方程 [182]、波动方程 [183]、Burgers 方程 [184]、Laplace 方程 [185]、Poisson 方程 [186] 等。 可以在逻辑上利用大量经过充分测试的知识来进一步约束 PgNN，同时对可用数据点（如果有）进行训练 [38]。 为此，无网格物理信息神经网络（PiNN）得到了发展[38,44]，并迅速扩展[187,188]，并广泛部署在各种科学和应用领域[189,190,191,192,193,194]。 可参考 Karniadakis 等[6] 和 Cai [10] 对 PiNN 功能的基础回顾。 本节简要回顾了 PiNN 的核心架构及其在计算流体和固体力学中的最先进应用，并讨论了一些主要局限性。</p>
<p>图 5 展示了普通 PiNN 架构的示意图。在 PiNN 中，底层物理原理被纳入神经网络架构之外，以在训练时约束模型，从而确保输出遵循已知的物理定律。 模拟此过程的最常见方法是通过弱施加惩罚损失来惩罚不遵循物理约束的网络。 如图 5 所示，以时空特征（即 $x$ 和 $t$）作为输入参数、以 PDE 解元素作为输出参数（即 $u$）的神经网络可用于模拟任何 PDE。</p>
<p>然后，网络的输出被输入到下一层，即自动微分层。 在这种情况下，通过对输入参数（$x$ 和 $t$）的输出求微分来生成多个偏导数。 为了优化 PDE 解决方案，这些偏导数用于生成损失函数中所需的项。 PiNN 中的损失函数是由标记数据 ($\mathcal L_{Data}$)、控制偏微分方程 ($\mathcal L_{PDE}$)、应用初始条件 ($\mathcal L_{IC}$) 和应用边界条件 ($\mathcal L_{BC}$) 造成的损失的组合 [10]。 $\mathcal L_{BC}$ 确保 PiNN 的解决方案满足指定的边界约束，而 $\mathcal L_{Data}$ 确保 PiNN 遵循训练数据集（即历史数据，如果有）中的趋势。 此外，PDE 的结构通过 $\mathcal L_{PDE}$ 在 PiNN 中强制执行，$\mathcal L_{PDE}$ 指定 PDE 解成立的搭配点 [38]，由初始条件、边界条件、数据和 PDE 造成的损失的权重可以分别指定为 $w_i$、$w_b$、$w_d$ 和 $w_p$。 下一步是检查给定迭代的损失是否在可接受的容差 $\epsilon$ 内，如果不是，则通过误差反向传播来更新网络的可学习参数（$\theta$）和未知的偏微分方程参数（$\lambda$）。 对于给定的迭代次数，重复整个循环，直到 PiNN 模型产生损失函数小于 $\epsilon$ 的可学习参数。 请注意，与 PgNN 相比，PiNN 的训练更加复杂，因为 PiNN 由复杂的非凸和多目标损失函数组成，可能会导致优化过程中的不稳定[38,6,10]。</p>
<p>Dissanayake 和 Phan-Thien [195] 是第一个研究将先验知识纳入神经网络的人，随后，由于计算能力不断增强，Owhadi [196] 引入了物理信息学习模型的概念，使得能够使用具有更多可学习参数和层的日益复杂的网络。 PiNN 作为一种用于正向和逆向建模的新计算范式，由 Raissi 等在一系列论文中[38,197,44]提出。 Raissi等[38]在由不同边界条件、严格非线性和复值解（例如 Burgers、Schrodinger 和 Allen-Cahn 方程）组成的示例上部署了两个 PiNN 模型，一个是连续时间模型，一个是离散时间模型。 Burgers 方程的结果表明，给定足够数量的配置点（即作为连续模型的基础），可以获得准确且数据高效的学习过程[38]。</p>
<p>在连续PiNN模型中，当处理高维问题时，搭配点的数量指数级增长时，使得学习处理变得困难且计算成本昂贵[38,6]，Raissi等[38]提出了一种基于Runge-Kutta技术[198]的离散时间模型来解决计算成本问题，该模型仅将空间特征作为输入，随着几个时间迭代步，PiNN 会收敛到底层物理原理。 对于 Raissi 等人探索的所有例子[38]，连续和离散 PiNN 模型能够令人满意地构建基于物理的替代模型。 Nabian等[199]提出了一种管理搭配点的替代方法，他们研究了根据分布采样搭配点的效果，发现它与损失函数成正比，这个概念不需要额外的超参数，并且更容易在现有 PiNN 模型中部署，在他们的研究中，他们声称搭配点的采样方法增强了 PiNN 模型在训练期间的行为。 通过部署偏微分方程的假设来解决与弹性、扩散和平面应力物理相关的问题，结果得到了验证。</p>
<p>为了使用 PiNN 处理逆问题，深度神经网络的损失函数必须满足分布在整个问题域的一组配置点的测量值和未知值，Raissi等[44] 展示了连续和离散时间 PiNN 模型解决基准反演问题的潜力，例如非线性浅水波的传播（Korteweg-De Vries 方程）[200] 和不可压缩流体流动（Navier-Stokes 方程）[201]。</p>
<p>与 PgNN 相比，PiNN 模型为正向和逆向建模提供了更准确的预测，特别是在具有高非线性、有限数据或噪声数据的场景中 [202]，因此，它已在多个基础科学和应用领域得到应用。 除了正向和逆向问题之外，如果表示现象的基础物理的训练数据可用，PiNN 还可以用于开发未知现象的偏微分方程 [44]，Raissi等[44]利用连续时间和离散时间 PiNN 模型根据可用数据的类型和结构生成通用偏微分方程。 在本节的其余部分中，我们回顾了有关 PiNN 在计算流体和固体力学领域应用的最新文献。</p>
<h2 id="应用到流体力学"><a href="#应用到流体力学" class="headerlink" title="应用到流体力学"></a>应用到流体力学</h2><p>PiNN 在涉及流体流动的问题中的应用是一个活跃的、正在进行的研究领域[203,204]。 Raissi等[197] 在一项开创性的工作中，开发了一种 PiNN，即所谓的隐藏流体力学 (HFM)，来编码控制流体运动的物理定律，即 Navier-Stokes 方程。 他们利用基本守恒定律从被动标量浓度（例如在任意复杂域中传输的染料）的时空可视化中导出感兴趣的隐藏量，例如速度和压力场，他们解决数据同化问题的算法与边界和初始条件以及几何形状无关，他们的模型成功预测了受实际应用启发的基准问题中的 2D 和 3D 压力场和速度场。 图6，改编自 Raissi 等[197]，将 PiNN 预测与流经圆柱体的 2D 流经典问题的真值进行了比较。 该模型可用于提取有价值的定量信息，例如难以直接测量的壁剪应力以及升力和阻力。</p>
<p>Zhang等[205]还开发了一个PiNN框架，用于由 Navier-Stokes 方程控制的流过圆柱体的不可压缩流体，PiNN 学习模拟输出（即速度和压力）与基础几何形状、边界、初始条件和固有流体特性之间的关系。 他们证明，通过包含傅立叶特征[206]（例如频率和相位偏移参数），可以在时域和设计空间上增强泛化性能。 Cheng 和 Zhang [207] 开发了 Res-PiNN（即 Resnet 模块和 PiNN），用于模拟由 Burgers 和 Navier-Stokes 方程控制的空腔流动和经过圆柱体的流动。 他们的结果表明，Res-PiNN 比传统的 PgNN 和 vanilla PiNN 算法具有更好的预测能力。 Lou等[208]还证明了 PiNN 在解决逆多尺度流问题方面的潜力，他们用 PiNN 在以 Boltzmann-Bhatnagar-Gross-Krook (BGK) 碰撞模型为代表的连续介质和稀有场区域中进行逆向建模。 结果表明，PiNN-BGK 是一种统一的方法（即，它可以用于正向和逆向建模），易于实现，并且可以有效解决不适定逆问题[208]。</p>
<p>Wessels 等[209]采用PiNN开发了一种更新的拉格朗日方法，用于求解受无粘性欧拉方程约束的不可压缩自由表面流，即所谓的神经粒子方法（NPM），该方法不需要任何特定的算法处理，而这通常是准确求解不可压缩性约束所必需的。 在他们的工作中，证明 NPM 能够准确计算满足不可压缩条件的压力场，同时避免离散化过程的拓扑约束[209]，此外，PiNN 还被用来模拟复杂的非牛顿流体流动，涉及能够表征流体流变行为的非线性本构偏微分方程[210]。</p>
<p>Haghighat等[211]训练了一个 PiNN 模型来求解多孔介质中耦合多相流和变形控制方程的无量纲形式。 Almajid 和 Abu-Al-Saud [212] 将 PiNN 的预测与 PgNN（即传统的人工神经网络）的预测进行了比较，以解决充水多孔介质的瓦斯抽采问题，研究表明，PgNN 在某些条件下（即，当观测数据包含早期和晚期饱和状态时）表现良好，而 PiNN 模型即使在观测数据仅包含早期饱和状态时（需要外推）也表现稳健。 Depina等[213]应用PiNN来模拟由Richards PDE和van Genuchten本构模型控制的非饱和地下水流问题[214]，他们证明，PiNN 可以有效地估计 van Genuchten 模型参数，并以相对准确的 Richards 方程解的近似值来求解反问题。</p>
<p>流体力学中使用的 PiNN 模型的其他一些变体包括： nn-PiNN，其中 PiNN 用于结合非牛顿流体的质量和动量守恒来求解本构模型 [210]； ViscoelasticNet，其中 PiNN 用于应力发现和粘弹性流动模型选择[215]，例如 Oldroyd-B [124]、Giesekus 和 Linear PTT [216]； RhINN 是一种基于流变学的神经网络，用于求解一系列流动协议的触变弹粘塑性复杂流体的本构方程[189]； CAN-PiNN，这是一个耦合自动数值微分框架，结合了数值微分（ND）和自动微分（AD）的优点，可实现稳健且高效的 PiNN 训练[217]； ModalPiNN，它是 PiNN 与强制截断傅立叶分解 [218] 的组合，用于周期性流重建 [219]； GAPiNN，这是一种几何感知 PiNN，由变分自动编码器、PiNN 和边界约束网络组成，适用于具有不规则几何形状的实际应用，无需参数化 [220]； Spline-PiNN，它是 PiNN 和基于 CNN 的 Hermite 样条内核的组合，用于在没有任何预先计算的训练数据的情况下训练 PiNN，并提供快速、连续的解决方案，可推广到看不见的领域 [221]； cPiNN，这是一种保守的物理信息神经网络，由多个通过子域接口通量连续性进行通信的 PiNN 组成，用于求解守恒定律 [187]； SA-PiNN，这是一种自适应 PiNN，用于解决迫使 PiNN 准确拟合刚性偏微分方程解中的顽固点所需的自适应程序 [50]； XPiNN，它是一个扩展的 PiNN，用于增强 PiNN 的表示和并行化能力，并泛化到与 cPINN 相关的任何类型的偏微分方程 [188]。</p>
<p>表 4 报告了利用 PiNN 模拟流体流动问题的近期研究的非详尽列表。 此外，表 5 报告了近期研究的非详尽列表，这些研究开发了 PiNN 架构的其他变体，以提高流体流动问题的整体预测精度和计算成本。</p>
<h2 id="3-2-应用到固体力学"><a href="#3-2-应用到固体力学" class="headerlink" title="3.2 应用到固体力学"></a>3.2 应用到固体力学</h2><p>PiNN 在计算固体力学中的应用也是一个活跃的研究领域。 Haghighat 等人[234]关于使用 PiNN 进行线弹性建模是固体力学界最早引入 PiNN 的论文之一。 从那时起，该框架已扩展到其他固体力学问题（例如线性和非线性弹塑性等）。</p>
<p>Shukla等[235]使用PiNN对多晶镍的微观结构特性进行代理建模，在他们的研究中，除了采用 PiNN 模型之外，他们还应用了自适应激活函数来加速数值建模的收敛，由此产生的基于 PiNN 的替代模型展示了无损材料评估的可行策略。 Henkes等[236]使用 PiNN 对具有急剧相变的材料中的不均匀性引起的非线性应力和位移场进行了建模，为了克服 PiNN 在这个问题中的收敛问题，他们使用了自适应训练方法和域分解[209]。 根据他们的结果，域分解方法能够正确解析源自真实世界 μCT 扫描图像的异质微观结构中的非线性应力、位移和能量 [236]。 Zhang和Gu[237]基于最小能量标准训练了具有损失函数的PiNN模型来研究数字材料，与监督式深度学习方法（即 PgNN）相比，在一维拉伸、一维弯曲和二维拉伸问题上测试的模型表现出相同的性能。 通过为雅可比矩阵添加 hinge 损失，PiNN 方法能够正确逼近对数应变并纠正任何错误的变形梯度。</p>
<p>Rao等[238]提出了一种具有混合变量（位移和应力分量）输出的 PiNN 架构，用于在没有标记数据的情况下处理弹性动力学问题，与纯基于位移的 PiNN 模型相比，该方法可以提高网络的准确性和可训练性，图 7 将 FEM 生成的真实应力场与混合变量 PiNN 针对弹性动力学问题估计的应力场进行了比较 [238]，可以看出，混合变量 PiNN 可以准确估计应力分量。 Rao等[238]还提出了 PiNN 的复合方案，以硬方式强制执行初始和边界条件，而不是采用软初始和边界条件强制执行的传统（普通）PiNN，该模型针对一系列动力学问题（例如，循环单轴拉伸和弹性波传播下的缺陷板）进行了测试，并减少了 PiNN 遇到的边界附近的误差。</p>
<p>Fang和Zhan[239]提出了PiNN模型来设计各种实际应用中使用的电磁超材料，例如隐身、旋转器、集中器等，他们研究了PiNN对频域内高波数 Maxwell 方程[240]的推理问题，并改进激活函数来克服高波数问题，所提出的PiNN不仅恢复了连续函数，还恢复了分段函数，这是对PiNN在实际问题中应用的新贡献。 Zhang等[241]利用 PiNN 来识别弹性成像中的非均质材料，以应用于软组织，使用了两个 PiNN，一个用于前向问题的近似解，另一个用于近似未知材料参数的场，结果表明，使用 PiNN 可以准确地恢复机械性能的未知分布。 Abueidda等[242]采用 PiNN 来模拟 3D 超弹性问题，他们提出了一种增强型 PiNN 架构，由强形式的残差和势能组成 [243]，产生了几个有助于定义最小化的总损失函数的损失项，增强型 PiNN 的性能优于传统（普通）PiNN 和深能量方法，特别是当存在高解梯度区域时。</p>
<p>Haghighat等[13] 测试了 PiNN 的不同变体来处理固体力学中的逆问题和代理建模，他们在研究中没有采用单个神经网络，而是实现了具有多个神经网络的 PiNN，他们将该框架部署在线性弹静力和非线性弹塑性问题上，并表明改进的 PiNN 模型提供了更可靠的物理参数表示，此外，他们研究了 PiNN 中的迁移学习领域，发现使用迁移学习时训练阶段收敛得更快。 Yuan等[244]提出了一种辅助PiNN模型（称为A-PiNN）来解决非线性积分微分方程（IDE）的反问题。A-PiNN 通过在控制方程中建立辅助输出变量来表示积分，并用辅助输出的自动微分代替积分算子，从而规避了积分离散化的限制，因此，A-PiNN 及其多输出神经网络的构造使其确定主输出和辅助输出以逼近控制方程中的变量和积分，A-PiNN 用于解决非线性 IDE 的逆问题，包括 Volterra 方程 [245]，正如他们的研究结果所证明的那样，即使使用噪声数据，也可以令人满意地确定未知参数。</p>
<p>用于计算固体力学的 PiNN 的其他一些变体包括： PhySRNet，它是一种基于 PiNN 的超分辨率框架，用于从低分辨率对应项重建高分辨率输出场，而不需要高分辨率标记数据 [246]； PDDO-PiNN，它是近场动力学微分算子（PDDO）[247]和PiNN的组合，以克服PiNN在锐梯度下性能下降的问题[248]； PiELM，它是 PiNN 和极限学习机（ELM）[249]的组合，用于解决线性弹性的直接问题[250]； DPiNN，这是一种分布式 PiNN，利用分段神经网络表示来表示基础领域，而不是 FEM 中常用的分段多项式表示[51]； PiNN-FEM，它是基于 PiNN 和 FE 的混合公式，用于异构域中的计算力学 [251]。</p>
<p>表 6 报告了在计算固体力学中利用 PiNN 的近期研究的非详尽列表。 此外，表 7 报告了近期研究的非详尽列表，这些研究开发了 PiNN 架构的其他变体，以提高固体力学建模中的整体预测精度和计算成本。</p>
<h2 id="3-3-局限性"><a href="#3-3-局限性" class="headerlink" title="3.3 局限性"></a>3.3 局限性</h2><p>PiNN 在用 ODE 和/或 PDE 描述的动力系统建模方面显示出巨大的潜力，但是，它们具有一些必须考虑的限制和缺点：</p>
<ul>
<li>Vanilla PiNN 使用由一系列完全连接的层和梯度下降优化变体组成的深层网络。 学习过程和超参数调整是手动进行的，并且取决于样本大小和问题，因此，他们的训练可能面临梯度消失问题，并且对于实际的三维问题来说可能会非常慢[264]。 此外，由于使用全连接层，普通 PiNN 对低维时空参数化施加了限制 [40]。</li>
<li>对于线性、椭圆和抛物线偏微分方程，Shin 等[265]提供了第一个关于训练数据数量的收敛理论，他们还讨论了保证收敛的一系列条件。 然而，当 PiNN 应用于非线性 PDE 控制的问题时，没有“可靠”的收敛理论证明。 请注意，深度学习模型通常无法实现理论上建立的全局最小值； 因此，这一限制并不是 PiNN 特有的，并且适用于所有深度学习模型。[6]</li>
<li>PiNN 在损失函数中包含多项具有相对权重的项，这对预测的解决方案有很大影响，目前，还没有最佳选择权重的指南[51]。 损失函数中的不同项在训练过程中可能会相互竞争，这种竞争可能会降低训练过程的稳定性。 由于 PiNN 对软物理约束的依赖，因此在训练过程中遇到不适定优化问题时也会受到影响[40]。</li>
<li>PiNN 遭受低频引起的偏差，并且经常无法解决由高频或多尺度结构控制的问题的非线性偏微分方程[266]。 事实上，PiNN 可能会遇到将信息从初始条件或边界条件传播到领域中不可见的部分或未来时间的困难，特别是在大型计算领域（例如，不稳定的湍流）[43]。</li>
<li>PiNN 是解决方案学习算法，即它们学习单个实例的偏微分方程的解决方案，对于任何给定的函数参数或系数的新实例，PiNN 需要训练一个新的神经网络 [49]。 这是因为，通过构造知道，PiNN 无法学习给定现象的物理操作，这限制了它们的泛化（例如时空外推）。 因此，PiNN 方法遇到了与经典求解器相同的计算问题，尤其是在 3D 问题（例如 FEM、FVM 等）中，因为需要针对 PDE 参数、边界条件、 和初始条件求解优化问题[57]。</li>
<li>PiNN 在学习异构介质中逆问题的解决方案时遇到困难，例如由多种材料组成的复合板[264]。 在这种情况下，基础 PDE 的参数（例如电导率或渗透系数）会在整个域中发生变化，但 PiNN 由于其固有的设计而在整个域上输出唯一的参数值。</li>
</ul>
<p>尽管存在这些缺点，PiNN 仍然为难以网格化的复杂领域和数据采集成本高昂的实际问题提供了强有力的前景，为了规避普通 PiNN 的一些限制，人们提出了几种技术。 例如，为了解决上面列出的第一个限制，使用卷积滤波器的离散学习技术，例如 HybridNet [267]、密集卷积编码器解码器网络 [268]、自回归编码器解码器模型 [269]、TF-Net [ 270]、DiscretizationNet [271] 和 PhyGeoNet [272]（仅举几例）已在计算效率方面超过了普通 PiNN。 作为另一个例子，为了解决上面列出的最后一个限制，Dwivedi 等[264]提出了一种分布式PiNN（DPiNN），它比现有的PiNN具有潜在的优势，可以解决工程实践中最有可能遇到的异构介质中的逆问题，解决高维逆问题的其他一些解决方案是保守 PiNN (cPiNN) [187] 和自适应 PiNN [50]。 此外，XPiNN [188] 凭借其内在的并行化能力，可以在较小的子域中部署多个神经网络，可用于显着降低 PiNN 在大型（三维）域中的计算成本。 然而，这些修改和替代方案并不能解决 PiNN 的泛化问题，因为所得模型缺乏强化现有物理知识的能力。 为此，物理编码神经网络（PeNN）开始出现。 在下一节中，我们将回顾有关物理编码神经网络的最新文献。</p>
<h1 id="四，PeNN"><a href="#四，PeNN" class="headerlink" title="四，PeNN"></a>四，PeNN</h1><p>物理编码神经网络（PeNN）是科学计算中使用的另一类无网格算法，主要用于流体力学和固体力学领域，致力于将底层物理（即先验知识）硬编码到科学计算的核心架构中。 请注意，通过构建神经网络，基于 PeNN 的模型将神经网络的学习能力从实例学习（ PgNN 和 PiNN 架构特性）扩展到持续学习 [53,40,273]。 为了将物理定律（ODE、PDE、闭合定律等）硬编码到神经网络中，最近提出了不同的方法[40,53,180,8]。 PeNN 并不是一个全新的概念，因为长期以来的研究都提出了将物理约束建设性地构建到架构中的理念。 例如，可以参考使用Deterministic Annealing 神经网络 (DANN) 保留凸性 [274]、保留正性 [275]、使用拉格朗日神经网络 (LaNN) [276,277] 强制物理中的对称性、使用辛循环神经网络捕获轨迹（SRNN）[278,279]，在图上使用数据驱动的外部微积分（DDEC）[280]执行精确的物理和提取结构保持的代理模型等。在本节中，我们回顾了两种最重要的编码方法 神经网络架构中的物理学及其在计算流体和固体力学中的应用：(i) 物理编码循环卷积神经网络 (PeRCNN) [40,273]，以及 (ii) 微分编程 (DP) 或神经常微分方程 (NeuralODE) [53,8]。</p>
<h2 id="4-1-PERCNN"><a href="#4-1-PERCNN" class="headerlink" title="4.1 PERCNN"></a>4.1 PERCNN</h2><p>Rao等[40]引入了 PerRCNN 模型，它将控制非线性系统的先验知识硬编码到神经网络中， 图 8 所示的 PeRCNN 架构有助于以数据驱动的方式进行学习，同时对已知的物理知识进行强制编码， 该模型超出了 PgNN 和 PiNN 对于不存在显式偏微分方程公式且可用测量数据非常有限的现象的能力（例如，地球或气候系统建模 [52]），所提出的物理编码机制与基于惩罚的物理知情学习有本质上的不同，确保网络严格遵守给定的物理，他们没有使用非线性激活函数，而是提出了一种新颖的逐元素乘积运算来实现模型的非线性。 数值实验表明，与一些最先进的数据驱动建模模型相比，由此产生的物理编码学习范式对数据噪声、稀缺性和泛化性具有显着的鲁棒性。</p>
<p>如图 8 所示，PeRCNN 由以下部分组成： 输入层，由低分辨率噪声初始状态测量 $X=[X_1, X_2, X_3, …, X_n]$ 构成； 一个全卷积网络，作为初始状态生成器（ISG），它将低分辨率初始状态缩小、上采样为全分辨率初始状态，称为修改后的 $X_0$，用作进一步循环计算的输入。 为了进行循环计算，采用了一种非常规的卷积块，称为 $\pi$ [40]。 在 PeRCNN 的核心 $\pi$ 块中，修改后的 $X_0$ 经过多个并行卷积层，然后通过逐元素乘积层融合其特征图，此外，在乘积运算之后附加 1×1 卷积层[281]，以将多个通道聚合成所需通道数的输出。 假设1×1卷积层的输出逼近非线性函数，可以将其乘以时间间隔 $\delta t$，得到动力系统在时间 $t_k$ 的残差，即 $\delta U_k$。 最终，最后一层通过逐元素加法生成预测  $Y^′ =[Y_1^′,Y_2^′,Y_3^′,…,Y_n^′]$，这些操作如图 8 所示。</p>
<p>PeRCNN 架构在 2D Burgers 和 3D Gray-Scott 反应扩散方程的两个数据集上进行了测试 [40]。 在这两种情况下，PeRCNN 与卷积 LSTM [282]、深度残差网络 [283] 和深度隐藏物理模型 [176] 在准确性（均方根误差，RMSE）、数据噪声/稀缺性、 和泛化性进行了比较。 2D Burgers 数据集的比较如图 9(a) 所示，摘自[40]。 PerRCNN 的累积 RMSE 在训练区域中以较大值开始（由于数据中存在 10% 高斯噪声），并随着评估额外的时间步长而减小。 PerRCNN 的累积 RMSE 在外推阶段略有增加（作为模型泛化的衡量标准），但在长期外推方面明显超过所有其他算法。 Rao等[273]还使用PeRCNN从稀缺和噪声数据中发现时空偏微分方程，并证明了其与基线模型相比的有效性和优越性。</p>
<p>Ren等[284]提出了一种结合PeRCNN和PiNN的混合算法来解决PgNN和PiNN在低维时空参数化方面遇到的局限性，在由此产生的物理信息卷积循环网络（称为 PhyCRNet）中，提出了一种编码器-解码器卷积 LSTM 网络，用于低维空间特征提取和时间演化学习。 在 PhyCRNet 中，损失函数被指定为聚合离散 PDE 残差，边界条件通过指定的 padding 硬编码在网络中，初始条件定义为网络的第一个输入状态变量，使用明确模拟时间推进的自回归和残差连接来增强网络，该方法确保泛化到各种初始和边界条件场景，并在网络训练中产生适定的优化问题。 使用 PhyCRNet，还可以同时在网络中强制执行已知的守恒定律（例如，可以通过应用流函数作为流体动力学网络中的解变量来强制质量守恒）[284]。 Ren等[284] 使用几种非线性偏微分方程与最先进的基线算法（例如 PiNN 和自回归密集编码器-解码器模型 [269]）相比，评估和验证了 PhyCRNet 的性能，PhyCRNet 和 PiNN 求解 Burgers 方程的比较如图 9(b) [284] 所示，Ren 等获得的结果清楚地证明了 PhyCRNet 方法在解决方案准确性、外推性和普遍性方面的优越性。</p>
<h2 id="4-2-NeuralODE"><a href="#4-2-NeuralODE" class="headerlink" title="4.2 NeuralODE"></a>4.2 NeuralODE</h2><p>神经常微分方程 (NeuralODE) 方法是 PeNN 模型的另一个系列，其中通过使用可微函数参数化隐藏状态导数，将神经网络的隐藏状态从离散序列转换为连续非线性函数 [53]，然后使用传统的微分方程求解器计算网络的输出，在训练期间，误差通过网络以及 ODE 求解器反向传播，而无需访问其内部运算，这种架构是可行的，因为数值线性代数是科学计算和深度学习的共同底层基础设施，并通过自动微分（AD）[285]进行桥接。 由于微分方程和神经网络都是可微的，因此可以使用标准优化和误差反向传播技术来在训练期间优化网络的权重。 NeuralODE 中的模型不是直接从训练数据中学习非线性变换，而是学习非线性变换的结构，因此，由于神经网络优化方程是可微的，物理微分方程可以直接编码到层中，而不是添加更多层（例如更深的网络），这导致了一个更浅的网络模仿无限深的模型，可以以任何所需的精度连续推断，同时减少内存和计算成本[286]。</p>
<p>这些连续深度模型提供了 PiNN 和 PgNN 所缺乏的功能，例如：</p>
<ul>
<li>（i）减少监督学习的参数数量</li>
<li>（ii）作为深度函数的内存成本一定</li>
<li>（iii）连续时间学习（即使用以任意时间间隔获取的数据集进行训练）。</li>
</ul>
<p>然而，误差反向传播可能会在训练这种连续深度网络时造成技术困难。 Chen等[53] 使用伴随灵敏度方法 [287] 计算梯度，同时将 ODE 求解器视为黑匣子，他们证明，这种方法使用最少的内存，可以直接控制数值误差，而且最重要的是，可以随问题规模线性扩展。</p>
<p>Ma等[288]比较了离散和连续伴随敏感性分析的性能，他们指出，对于参数大约少于 100 个的问题，通过 AD 实现的正向模式离散局部灵敏度分析比反向模式和连续正向和/或伴随灵敏度分析更有效。 然而，就可扩展性而言，他们表明连续伴随方法比离散伴随方法和前向方法更有效。</p>
<p>为了促进 NeuralODE 的实际应用，已经实现了几个计算库，Poli等[289] 实现了 TorchDyn 库来训练 NeuralODE 模型，并且与常规的即插即用深度学习原语一样易于访问。 Innes等[8] 和 Rackauckas 等[286]在 Julia 编码生态系统中开发了 GPU 加速的 Zygote 和 DiffEqFlux 库，将可微分编程和通用微分方程求解器功能结合在一起。 例如，他们将常微分运动方程作为变换函数编码到神经网络中，以模拟投石机的逆动力学[8]，如图10所示，具有经典层的网络以目标位置和风速作为输入，估计弹丸击中目标的重量和角度，这些输出被输入 ODE 求解器来计算所达到的距离，模型将预测值与目标位置进行比较，并将误差反向传播到整个链，以调整网络的权重。 该 PeNN 模型在个人计算机上解决投石机逆动力学问题的速度比该逆问题的经典优化算法快 100 倍，一旦经过训练，这个网络就可以用来瞄准任何盲目标，而不仅仅是它所训练的目标，因此，该模型既是加速的可预测的。</p>
<p>NeuralODE 还与 PiNN 模型集成，称为 PiNODE，以便在训练期间使用已知的控制物理进一步约束网络，这种架构由一个神经网络组成，其隐藏状态由 ODE 参数化，其损失函数类似于 PiNN 的损失函数（见图 5），损失函数基于数据和控制 ODE 的强形式对算法进行惩罚，并通过应用伴随灵敏度方法 [288] 反向传播误差，以更新架构中的可学习参数。 可以部署 PiNODE 来克服高偏差（由于在科学建模中使用第一原理）和高方差（由于在科学建模中使用纯数据驱动模型）问题，换句话说，使用 PiNODE，可以在可用的情况下集成 ODE 方面的先验物理知识，在不可用的情况下使用函数逼近（例如神经网络）。 Lai等[290]使用PiNODE对结构动力学领域的控制方程进行建模（例如，具有三次非线性的4自由度动力系统的自由振动），他们表明，PiNODE 为结构健康监测（例如损坏检测）问题提供了一个适应性强的框架。 Roehrl等[291] 使用推车上的倒立摆正向模型测试了 PiNODE，并表明该方法可以学习具有很大不确定性的现实物理系统中的非保守力。</p>
<p>神经微分方程的应用也已扩展到学习偏微分方程描述系统的动力学。 Dulny等[292]通过使用多层卷积神经网络将线性方法（通过 ODE 系统表示任意复杂的 PDE）和 NeuralODE 相结合，提出了 NeuralPDE，他们在由平流扩散方程、Burgers 方程、波浪传播方程、气候建模等生成的几个时空数据集上测试了 NeuralPDE，他们发现 NeuralPDE 与其他基于深度学习的方法（例如 ResNet [293]）相比具有竞争力。 NeuralPDE 的局限性是由直线法的局限性决定的，例如，它不能用于求解椭圆二阶 PDE。 表 9 报告了利用 PeNN 模拟不同科学问题的领先研究的非详尽列表。</p>
<h2 id="4-3-局限性"><a href="#4-3-局限性" class="headerlink" title="4.3 局限性"></a>4.3 局限性</h2><p>尽管许多 PeNN 模型取得了进步，并且在复杂物理系统建模方面取得了成功，但这些新架构也面临着一些挑战，其中最重要的一点就是训练。 基于 PeNN 的模型利用连续深度网络的发展来促进持续学习，这使得 PeNN 比 PgNN 和 PiNN 更难训练，考虑到这一点，PgNN 和 PiNN 面临的大部分限制（例如收敛速度、稳定性、可扩展性、样本大小和问题依赖性）也同样适用于 PeNN，此外，PeNN 通常具有复杂的架构，其实现并不像 PiNN 或 PgNN 那么简单。尽管 PeNN 的实现复杂性，但它们在有限维设置中的高效算法、提供可转移解决方案的能力、对数据稀缺的鲁棒性以及与 PgNN 和 PiNN 相比的通用性，使它们具有显着加速的巨大潜力。 传统科学计算在计算流体和固体力学中的应用。</p>
<h1 id="五，NeralOperators"><a href="#五，NeralOperators" class="headerlink" title="五，NeralOperators"></a>五，NeralOperators</h1><p>迄今为止讨论的大多数科学深度学习方法，例如 PgNN、PiNN 和 PeNN，通常旨在映射单个实例的物理现象的解（例如，使用特定时空域和边界条件来求解偏微分方程，PiNN），因此必须重新训练或进一步训练（例如，迁移学习[294]）以映射不同时刻下的求解问题。 缓解这个问题的另一种方法是使用神经算子来学习函数空间之间的非线性映射[39,295,296]。 因此，神经算子形成了另一种模拟范式，它使用先进的架构来学习底层线性和非线性连续算子，这些模型与 PgNN 类似，使用标记的输入输出数据集对应问题的物理原理，但与 PgNN 以及 PiNN 和 PeNN 相比，提供增强的泛化性、可解释性、连续学习和计算效率 [180,53,43]。</p>
<p>这种新范式使用基于神经网络的网格不变、无限维算子，不需要事先了解偏微分方程，神经算子仅仅使用数据来学习感兴趣问题的分辨率不变的解决方案[43]。 换句话说，神经算子可以在一种时空分辨率上进行训练，并在任何其他分辨率上成功推断[296]。 这种分辨率不变的特征是利用神经算子通过在函数空间中参数化模型来学习连续函数而不是离散向量这一事实来实现的[43, 296]。 请注意，PgNN 和 PiNN（例如使用 MLP）也可以保证较小的泛化误差，但这只能通过足够大的网络来实现，神经算子的一个显着特征是它们对于需要实时推理的应用的鲁棒性[57]。 最近提出了三种主要的神经算子，即</p>
<ul>
<li>（i）深度算子网络（DeepONets）[56]</li>
<li>（ii）傅里叶神经算子（FNO）[180]</li>
<li>（iii）图神经算子（GNO）[296, 297 ]。 </li>
</ul>
<p>Goswami 等最近的综述[57]广泛比较了这些神经算子。 在本节中，我们将简要回顾 DeepONets 和 FNO 作为应用于计算流体和固体力学的两个重要神经算子。</p>
<h2 id="5-1-DeepONets"><a href="#5-1-DeepONets" class="headerlink" title="5.1 DeepONets"></a>5.1 DeepONets</h2><p>Lu等[39]基于算子的万能逼近定理开发了深度算子网络（DeepONets）[298]，可用于以非常小的泛化误差准确有效地学习算子。 Lu等[56]为 DeepONet 提出了两种架构，即堆叠式和非堆叠式。 堆叠式DeepONet架构如图11（a）所示，由一个主干网络和多个堆叠式分支网络组成，$k = 1,2\cdots,p$，选择主干网络作为宽度为 $p$ 的一层网络，每个分支网络作为宽度为 $n$ 的单隐层网络，形成堆叠式 DeepONet。 为了学习算子 $G:s \rightarrow G(s)$，堆叠式 DeepONet 架构将函数 $s$ 作为分支网络的输入，将 $y$（即 $G(s)$ 域中的点）作为主干网络的输入，这里，向量 $[(x_1), (x_2), \cdots, (x_m)]$ 表示数据的有限位置，或者称为传感器。 主干网络输出 $[t_1 , t_2 , \cdots, t_p ]^T \in  \mathbb R^p$ ，每个分支网络输出由 $b_k ∈ \mathbb R$ 表示的标量，其中 $k = 1,2,\cdots,p$，接下来，主干网络和分支网络生成的输出被集成在一起，为 $G(s)(y)\approx \sum^p_{k=1} b_k(s(x_1), s(x_2), \cdots,s(x_m))t_k(y)$。 非堆叠式 DeepONet 架构也如图 11（a）所示，它仅由一个分支网络（以深蓝色表示）和一个主干网络组成。 非堆叠式 DeepONet 可以被视为堆叠式 DeepONet，其中所有分支网络共享相同的参数集[56]，DeepONet 首先用于学习几个显式算子，包括积分和分数拉普拉斯算子，以及表示确定性和随机微分方程的隐式算子[56]。 Lu 等讨论的 DeepONet 的两个主要优点：</p>
<ul>
<li>（i）小的泛化误差；</li>
<li>（ii）训练和测试误差相对于训练数据量的快速收敛。</li>
</ul>
<p>Lin等[299]展示了 DeepONet 在数据密度和位置方面的有效性，当没有关于需要多少训练数据的先验知识或当数据获取有严格限制（例如，位置可访问性）时，这是有利的，为此，他们采用 DeepONet 和 LSTM（即 PgNN）对代表单个气泡形成的数据集进行建模，以响应环境液体压力随时间变化的变化。 为了生成数据集，他们使用Rayleigh-Plesset (R-P) 作为宏观模型，使用耗散粒子动力学 (DPD) 作为微观模型，他们使用 Gaussian 随机场来生成不同的压力场，作为该动力系统的输入信号。 比较结果如图11(b)所示，顶行显示当每个轨迹仅已知 20 个数据点（即稀疏训练数据）时液体压力轨迹的预测结果，底行显示相同但当每个轨迹已知 200 个数据点（即密集训练数据）时的预测结果，如图所示，无论训练数据多么稀疏，DeepONet 在预测液体压力轨迹方面都能够优于 LSTM。</p>
<p>此外，他们还检查了输入不包含在训练输入范围内的情况，即压力场的相关长度超出训练范围时。 在这种情况下，他们最初无法做出准确的预测，但通过将学习转移到预先训练的 DeepONet 主干网络并仅使用几个额外的数据点对其进行微调来缓解了该问题。 他们还证明，DeepONet 可以学习微观模型的噪声原始数据的平均成分，而无需任何额外的数据处理，并且计算时间可以从 48 个 CPU 小时减少到不到一秒。 这些结果证实 DeepONet 模型可以应用于气泡生长动力学的宏观和微观状态，为统一的神经网络模型奠定了基础，该模型可以无缝预测跨尺度相互作用的物理现象。</p>
<p>Oommen等[300]将卷积自动编码器架构与DeepONet（CA-DeepONet）相结合，以学习两相混合物的动态发展，并加快微结构演化预测的求解时间。 在低维潜在空间中，卷积自动编码器用于提供微观结构数据的紧凑表示，而 DeepONet 则用于从自动编码器的潜在空间中学习微观结构演化的介观动力学，然后，卷积自动编码器的解码器组件根据 DeepONet 的预测重建微观结构的演化，经过训练的 DeepOnet 架构可用于加速外推任务中的数值求解器，或替代插值问题中的高保真相场数值求解器。</p>
<p>通过从稀疏数据域的 PiNN 中汲取灵感，DeepONets 还可以使用非常稀疏的标记数据集进行训练，同时将已知的微分方程合并到损失函数中，这种方法产生了基于物理的 DeepONets (Pi-DeepONets) [301,302]。 Wang等[301]采用 Pi-DeepONets 来解决扩散反应、Burger 方程、平流方程和 eikonal 方程等基准问题，与普通 DeepONet 相比，结果表明预测准确性、泛化性能和数据效率都有显着提高。 此外，Pi-DeepONets 可以在没有任何成对输入输出训练数据的情况下学习解算子，从而使它们能够比传统求解器快三个数量级来模拟计算力学中的非线性和非平衡过程[301]。</p>
<p>Goswami等[302]使用 DeepONet (Pi-V-DeepONet) 的物理信息变分公式来研究脆性断裂力学，Pi-V-DeepONet 的训练是使用变分形式的控制方程和一些标记数据进行的。 他们使用 Pi-V-DeepONet 框架来确定准脆性材料脆性断裂的失效路径、失效区域和沿失效的损坏，他们训练模型将缺陷（例如裂纹）的初始配置映射到相关的感兴趣领域（例如损伤和位移，见图 12）。 他们表明，他们的模型可以快速预测任何初始裂纹配置和加载步骤的解决方案，在脆性断裂力学中，所提出的模型可用于增强设计、评估可靠性和量化不确定性。</p>
<p>由于评估积分算子的成本很高，DeepONets 可能难以开发出能够在无限维环境中替代卷积或循环神经网络的有效数值算法。 Li等[180]沿着这个思路做出了努力，通过参数化傅里叶空间中的积分核开发了算子回归，并将其称为傅里叶神经算子（FNO）。 在下一节中，我们将讨论 FNO 的核心架构以及围绕它的最新发展。</p>
<h2 id="5-2-Fourier-Neural-Operator-FNO"><a href="#5-2-Fourier-Neural-Operator-FNO" class="headerlink" title="5.2 Fourier Neural Operator (FNO)"></a>5.2 Fourier Neural Operator (FNO)</h2><p>为了受益于无限维空间中的神经算子，Li 等[180]在傅立叶空间中开发了一种神经算子，称为 FNO，其核心架构如图 13 所示。训练从输入 $X$ 开始，随后通过神经网络 $S$ 将其提升到更高维度的空间，第二阶段需要使用多个积分算子和激活函数的傅立叶层。 在每个傅里叶层中，使用 </p>
<ul>
<li>(i) 傅里叶变换 $F$ 对输入进行变换； </li>
<li>(ii) 对较低傅立叶模式进行线性变换 $T$，滤除较高模式； </li>
<li>(iii) 傅里叶逆变换，$F^{−1}$。 在应用激活函数 $\sigma$ 之前，还使用局部线性变换 $W$ 对输入进行变换。 </li>
</ul>
<p>傅里叶层被设计为离散化不变的，因为它们从任意离散化的函数中学习。 事实上，积分算子应用于卷积并表示为傅立叶域中的线性变换，从而允许 FNO 学习无限维空间上的映射。 在第三阶段，使用另一个神经网络 $M$ 将傅里叶层的结果投影回目标维度，最终输出所需的输出 $Y^’$[180]。 与其他深度学习方法不同，无论输入和输出分辨率如何，FNO 模型的误差都是一致的（例如，在 PgNN 方法中，误差随着分辨率的增加而增加）。</p>
<p>Li等[180]在三个不同的测试用例上使用了 FNO，包括 1D Burgers 方程、2D Darcy 流方程和 2D Navier-Stokes 方程。 对于每个测试用例，FNO 都与最先进的模型进行了比较，特别是，对于 Burgers 和 Darcy 的测试用例，用于比较的方法是传统的 ANN（即 PgNN）、减少偏差方法 [303]、全卷积网络 [304]、作为神经网络中编码器的主成分分析 [295]、图神经算子[296]和低秩分解神经算子（即非堆叠 DeepONet [39]）。 在所有测试案例中，FNO 产生的相对误差最低。 1D Burgers 和 2D Darcy 方程的模型误差比较如图 14 所示，改编自[180]。</p>
<p>如前所述，FNO 模型可以在特定分辨率上进行训练并在不同分辨率上进行测试。 Li等[180]通过在 2D 测试用例的 Navier-Stokes 方程上训练 FNO 证明了这一主张，分辨率为 $64\times 64\times 20 (n_x,n_y,n_t)$ 代表空间 $(x, y)$ 和时间分辨率，并且 然后以 $256\times 256\times 80$ 的分辨率对其进行评估，如图15（a）所示，与其他模型相比，FNO 是唯一能够在空间和时间上执行分辨率缩减的技术 [180]。 FNO 还可以实现比传统数值 PDE 求解器高几个数量级的加速因子，然而，由于输入数据的维数很大，这会显著增加网络权重的数量，因此它们仅用于 2D 或小型 3D 问题。 考虑到这个问题，Grady 等[305]提出了基于域分解的FNO并行版本来解决这个限制，利用这一扩展，他们能够在大规模建模中使用 FNO，例如，模拟地下非均质储层中二氧化碳羽流的瞬态演化，作为碳捕获和封存 (CCS) 技术的一部分 [306]，见图15(b)。 网络的输入（与 Li 等人[180]提出的结构类似）被设计为一个张量，其中包含每个 3D 空间位置的渗透率和地形场，使用 $60\times 60\times 64 (n_x ,n_y, n_z)$ 分辨率，输出为 $60\times 60\times 64\times n_t$。 对于 $n_t = 30 s$ 的时间分辨率，他们发现并行 FNO 模型比传统多孔介质求解器快 271 倍（甚至不利用 GPU），同时实现了相当的精度。 Wen等[307]还提出了U-FNO，FNO的扩展，用于模拟多孔介质中的多相流，特别是通过非均质介质的CO2-水多相流，具有广泛的储层条件、注入配置、流速和多相流特性。 他们将 U-FNO 与 FNO 和 CNN（即 PgNN）进行了比较，结果表明 U-FNO 架构为高度非均质地质构造中的气体饱和度和压力积聚预测提供了最佳性能。 他们还表明，U-FNO 架构提高了原始 FNO 的训练精度，但自然无法实现多个离散化训练和测试的灵活性。</p>
<p>You等[308]提出了一种隐式傅里叶神经算子（IFNO）来模拟材料由于其异质性和缺陷而产生的复杂响应，而无需使用传统的本构模型。 IFNO 模型捕获特征空间中的远程依赖性，并且随着网络变得更深，它变成一个定点方程，产生隐式神经算子（例如，它可以模拟位移/损伤场）。 You等[308]使用一系列测试用例（例如超弹性、各向异性和脆性材料）证明了 IFNO 的性能，图 16 描绘了 IFNO 和 FNO 对于玻璃陶瓷裂纹瞬态扩展的比较 [308]。 正如所证明的，在预测位移场方面，IFNO 优于 FNO（在精度方面）和传统本构模型（在计算成本方面）。</p>
<p>FNO 模型还与 PiNN 混合，创建了所谓的物理信息神经算子 (PiNO) [43]，PiNO 框架是操作学习（即 FNO）和功能优化（即 PiNN）框架的组合，可提高 PiNN 和 FNO 模型的收敛速度和准确性。 提出这种集成是为了来解决 PiNN 中的挑战（例如，泛化和优化，特别是对于多尺度动力系统）和 FNO 中的挑战（例如，需要昂贵且不切实际的大型训练数据集）[43]。 Li等[43]在几个基准问题（例如，Kolmogorov flow、lid-cavity flow等）上部署了 PiNO 模型，以表明 PiNO 可以优于 PiNN 和 FNO 模型，同时保持 FNO 相对于其他求解器的卓越加速因子。</p>
<h2 id="5-3-NOs局限性"><a href="#5-3-NOs局限性" class="headerlink" title="5.3 NOs局限性"></a>5.3 NOs局限性</h2><p>DeepONet [39]和FNO [180]作为迄今为止最常见的两种神经算子，具有一些共同点，但也存在显着差异。 DeepONet 架构受到 Chen 和 Chen [298] 的通用逼近定理的启发，而 FNO 是在傅立叶空间中参数化积分核的基础上建立的架构。 然而，连续形式的 FNO 可以被视为具有特定的主干网络（用三角基础表示）和分支网络架构的 DeepONet [309]。 与 DeepONet 不同，FNO 通过等间距网格中的逐点评估来离散化输入函数和输出函数，因此，网络训练后，FNO只能预测与输入函数相同网格中的解，而DeepONet可以在任意位置进行预测。 FNO 还需要全场观测数据进行训练，而 DeepONet 则更灵活，但 POD-DeepONet [310] 除外，它需要全场观测数据来计算适当的正交分解（POD）模式 [310] 。 DeepONet、FNO 及其各种变体仍然面临一些局限性，特别是在应用于大型多物理问题时，需要进一步研究。</p>
<ul>
<li>神经算子纯粹是数据驱动的，需要相对较大的训练数据集，因此当应用于数据获取复杂和/或昂贵的问题时，它们面临着限制[310]。 对于底层物理完全已知并且可以集成到损失函数中的问题，与 PiNN 的集成可以在一定程度上解决这个问题[301]。 此外，在实际应用中，仅根据损失函数中的控制方程来训练 NO 可能会产生不准确的预测，相反，建议采用混合物理数据训练[301]。</li>
<li>DeepONet 和 FNO 通常仅限于基本几何或结构化数据（例如 2D 或小型 3D 问题），因为它们的输入数据维度很大，这会显著增加网络权重的数量[310]。 随着可训练参数数量的增加，它们也容易出现过度拟合，从而使训练过程变得更加困难[311]。IFNO 在一定程度上解决了这一挑战[308]，在 IFNO 中，解算子首先被表述为隐式定义的映射，然后建模为不动点，后者旨在克服深层情况下网络训练的挑战，而前者最大限度地减少可训练参数的数量和内存成本。 尽管如此，由于神经网络架构的大小有限，对于大型数据集，NO（例如 DeepONet）误差相对于训练数据大小的收敛变得代数化，它希望是指数的[310]。</li>
<li>FNO 对于不连续函数可能不可靠，因为它依赖于傅立叶变换。 DeepONet 在一定程度上缓解了这一问题，因为它对于具有不连续性的函数（例如可压缩欧拉方程）表现良好[310]。</li>
</ul>
<p>尽管存在这些限制，神经算子仍然是各种实时推理应用中的领先算法，包括自主系统、设计问题中的代理和不确定性量化[57]。</p>
<h1 id="六，总结和未来研究方向"><a href="#六，总结和未来研究方向" class="headerlink" title="六，总结和未来研究方向"></a>六，总结和未来研究方向</h1><p>相当多的研究主题共同支持科学计算和深度学习方法相结合的功效，特别是，这种组合提高了高维问题的正向和逆向建模效率，这些问题的成本过高、包含噪声数据、需要复杂的网格，并且由非线性、不适定微分方程控制。 不断增强的计算机能力将通过允许使用更深层次的神经网络并考虑更高维度的相互依赖性和设计空间，继续进一步提供这种组合。</p>
<p>科学计算和深度学习方法的结合在实际工程中的许多常见场景中也超越了传统的计算力学求解器。 例如，通过实验获得的复杂（即难以获取数据）现象的稀疏数据集不能简单地与传统求解器集成。 而使用DL，可以执行以下任务：</p>
<ul>
<li>(i) 基于PgNN的模型可以应用于稀疏数据以提取潜在的相互依赖性并进行时空降尺度或升尺度（即插值数据）；</li>
<li>(ii) 基于 PiNN 的模型可以应用于插值数据，以推导出控制方程和现象的潜在未知边界或初始条件（即强数学形式）； </li>
<li>(iii) 基于PeNN的模型可用于结合插值数据和强数学形式来进行外推探索； </li>
<li>(iv) 基于 NO 的模型可用于对复杂动态进行实时预测。 </li>
</ul>
<p>因此，基于深度学习的方法与传统科学计算方法的结合为科学家提供了一个经济高效的工具箱，以探索不同尺度的问题，而这些问题在计算上被认为是牵强的。 为此，需要在深度学习方面取得其他几项突破，才能在大规模三维（或多维）问题中使用 PgNN、PiNN、PeNN 和 NO。 例如，复杂深度学习模型（例如 PiNN、PeNN 和 NO）的训练应该使用不同的并行化范例来加速。</p>
<p>表 10 比较了 PgNN、PiNN、PeNN 和 NO 的主要特征。 基于 PgNN 的模型主要受到其统计训练过程的影响，为此它们需要大量数据集。 他们仅根据统计变化的相关性来绘制精心策划的训练数据集，因此，他们的预测自然与物理无关。 基于 PiNN 的模型主要受到竞争损失项的存在的影响，这些损失项可能会破坏训练过程的稳定性。 PiNN 也是一种解决方案学习算法，由于无法学习特定现象的物理操作，因此泛化性有限。 另一方面，基于 PeNN 和 NO 的模型可能收敛速度较低，并且需要大量配对的结构化数据集，从而导致训练成本高昂。</p>
<p>考虑到科学计算和深度学习相结合这一新挑战的有效性，未来的研究可以分为三个不同的类别：</p>
<ul>
<li>(i)改进算法：开发 PgNN、PiNN、PeNN 和 NO 的高级变体，这些变体提供更简单的实现和增强的收敛性 速度; 更快地训练多维和多物理问题； 使用稀疏训练数据集时，对未见条件具有更高的准确性和泛化能力，在实时预测中使用更稳健； 更好地适应多时空分辨率，更灵活地编码各种类型的控制方程（例如，所有偏微分方程类型、闭包定律、数据驱动定律等），并与大量传统求解器提供更紧密的联系； </li>
<li>(ii)考虑因果关系：开发因果训练算法（例如因果Q学习[312]），通过重新加权控制方程（例如偏微分方程）来恢复PgNN、PiNN和PeNN模型训练期间的物理因果关系 ）每次迭代的残余损失。 这一系列研究将允许开发 PgNN、PiNN 和 PeNN 算法的符合因果关系的变体，这些变体可以为这些算法在不同领域的更广泛的复杂场景中的应用带来新的机会；</li>
<li>(iii) 扩展应用：利用 PgNN、PiNN、PeNN 和 NO 的潜力来解决复杂各向异性材料的问题（例如，高度异质多孔介质、金属和非金属颗粒复合材料中的流动等）； 多尺度多物理现象问题（例如磁流变流体、颗粒流体、干粉动力学、反应输运、非饱和土壤动力学等）； 多分辨率目标和广泛的时空降尺度或升尺度问题（例如，全球和区域气候建模、地球系统储层建模等）； 和结构健康监测（例如裂纹识别和扩展、氢气管道泄漏、二氧化碳羽流检测等）；</li>
<li>(iv) 耦合求解器：将 PgNN、PiNN 和 PeNN 以及 NO 与开源计算力学包（例如 OpenIFEM、OpenFOAM、Palabos、LAMMPS、LIGGGHTS、MOOSE 等）耦合。这一研究方向将允许更快的代理 建模，从而加快下一代求解器的开发。 它还加快了社区和行业对科学与深度学习相结合的计算范式的采用。</li>
</ul>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>chatGPT怎么写指令？</title>
    <url>/2023/07/10/tools/chatGPT%E6%80%8E%E4%B9%88%E5%86%99%E6%8C%87%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>chatGPT该怎么用？怎么写指令会得到更符合自己预期的反馈呢？</p>
<p><strong>研究研究～</strong><br><span id="more"></span></p>
<h1 id="一，写清楚指令"><a href="#一，写清楚指令" class="headerlink" title="一，写清楚指令"></a>一，写清楚指令</h1><h2 id="1，有细节才能得到更相关的答案"><a href="#1，有细节才能得到更相关的答案" class="headerlink" title="1，有细节才能得到更相关的答案"></a>1，有细节才能得到更相关的答案</h2><p>错误示范：写一篇全国甲卷文章</p>
<ul>
<li>[x] 写一篇全国甲卷文章，要求：选准角度，确定立意，明确文体，自拟标题；不要套作，不得抄袭；不得泄漏个人信息；不少于800字；</li>
</ul>
<h2 id="2，要求模型扮演特定角色"><a href="#2，要求模型扮演特定角色" class="headerlink" title="2，要求模型扮演特定角色"></a>2，要求模型扮演特定角色</h2><p>可以指定模型在回复中使用的角色，它会表现得更专业。</p>
<ul>
<li>[x] 比如可以让它用鲁迅的文风写一段文字</li>
</ul>
<h2 id="3，用分隔符清晰标示输入的不同部分"><a href="#3，用分隔符清晰标示输入的不同部分" class="headerlink" title="3，用分隔符清晰标示输入的不同部分"></a>3，用分隔符清晰标示输入的不同部分</h2><p>“””三重引号”””,<xml标记>, 章节标题等有助于划分需要区别对待的文本部分</p>
<ul>
<li>[x] 将以下由三引导包含的文本总结为一句话，”””在这里插入文本”””</li>
</ul>
<h2 id="4，明确指定完成任务所需的步骤"><a href="#4，明确指定完成任务所需的步骤" class="headerlink" title="4，明确指定完成任务所需的步骤"></a>4，明确指定完成任务所需的步骤</h2><p>明确地写出这些步骤可以使模型更容易执行</p>
<ul>
<li><p>[x] 按照以下步骤说明来响应用户输入</p>
</li>
<li><p>[x] 第1步-用户将用三引号提供文本。以”总结：”作为前缀，用一句话总结这段文字；</p>
</li>
<li><p>[x] 第2步-将第1步的摘要翻译成中文，前缀为”翻译：”。”””在这里插入文本”””</p>
</li>
</ul>
<h2 id="5，提供示例"><a href="#5，提供示例" class="headerlink" title="5，提供示例"></a>5，提供示例</h2><ul>
<li>[x] 比如你想让它用鲁迅的文风给你写一段文字，那你先给它提供一些鲁迅的文章</li>
</ul>
<h2 id="6，指定所需输出长度"><a href="#6，指定所需输出长度" class="headerlink" title="6，指定所需输出长度"></a>6，指定所需输出长度</h2><p>目标输出长度可以用单词数、句子数、段落数、项目符号等来表示</p>
<ul>
<li>[x] 将以下由三引号分隔的文本总结为约50个单词。”””在这里插入文本”””</li>
</ul>
<h1 id="二，提供参考文本"><a href="#二，提供参考文本" class="headerlink" title="二，提供参考文本"></a>二，提供参考文本</h1><h2 id="1，让模型参照参考资料进行回答"><a href="#1，让模型参照参考资料进行回答" class="headerlink" title="1，让模型参照参考资料进行回答"></a>1，让模型参照参考资料进行回答</h2><p>使用由”””三重引号”””分割的提供的文章来回答问题。如果答案不能在文章中找到，请回答”我无法找到答案”。</p>
<ul>
<li>[x] &lt;插入多个以三重引号分隔的文章&gt;</li>
<li>[x] 我的问题：&lt;在此处插入问题&gt;</li>
</ul>
<h2 id="2，让模型引用参考资料进行回答"><a href="#2，让模型引用参考资料进行回答" class="headerlink" title="2，让模型引用参考资料进行回答"></a>2，让模型引用参考资料进行回答</h2><p>区别于上面的参照</p>
<h1 id="三，拆分复杂任务"><a href="#三，拆分复杂任务" class="headerlink" title="三，拆分复杂任务"></a>三，拆分复杂任务</h1><h2 id="1，进行意图分类"><a href="#1，进行意图分类" class="headerlink" title="1，进行意图分类"></a>1，进行意图分类</h2><p>对于需要处理不同情况的大量具有独立性的任务，可以先对这些任务进行分类。然后，根据分类来确定所需的指令。</p>
<h2 id="2，对先前对话进行概括或筛选"><a href="#2，对先前对话进行概括或筛选" class="headerlink" title="2，对先前对话进行概括或筛选"></a>2，对先前对话进行概括或筛选</h2><p>由于GPT-4对话窗口是有限制的，上下文不能太长，不能在一个对话窗口中无限进行下去，解决方法之一是对先前的对话进行概括。一旦输入的文本长度达到预定的阈值，就可以触发一个查询，概括对话的一部分，被概括出来的这部分内容可以变成系统消息的一部分。</p>
<h2 id="3，逐段概括长文档，并递归构建完整概述"><a href="#3，逐段概括长文档，并递归构建完整概述" class="headerlink" title="3，逐段概括长文档，并递归构建完整概述"></a>3，逐段概括长文档，并递归构建完整概述</h2><p>解决的也是文本太长的问题，比如你要让GPT-4概括一本书，就可以使用一系列查询来概括这本书的每个部分。然后将每部分概述连接起来进行总结，汇成一个总的答案。</p>
<h1 id="四，给GPT时间”思考”"><a href="#四，给GPT时间”思考”" class="headerlink" title="四，给GPT时间”思考”"></a>四，给GPT时间”思考”</h1><h2 id="1，让模型制定解决方案"><a href="#1，让模型制定解决方案" class="headerlink" title="1，让模型制定解决方案"></a>1，让模型制定解决方案</h2><p>有时候，在得出结论之前明确地指导模型从基本原理推理会得到更好的结果。</p>
<h2 id="2，隐藏推理过程"><a href="#2，隐藏推理过程" class="headerlink" title="2，隐藏推理过程"></a>2，隐藏推理过程</h2><p>这个是和前面相反，比如你想做个循循善诱的导师，就可以让模型使用自己的分析构建一个以有益导师为人格的回复，一步步给出指导。</p>
<h2 id="3，询问模型是否遗漏了内容"><a href="#3，询问模型是否遗漏了内容" class="headerlink" title="3，询问模型是否遗漏了内容"></a>3，询问模型是否遗漏了内容</h2><p>解决的也是文本太长的问题，如果源文档很大，那么模型经常会过早停止并未列出所有相关的摘录，因此可以采用这个策略，询问模型在之前是否遗漏了任何内容。</p>
<h1 id="五，其他工具加持"><a href="#五，其他工具加持" class="headerlink" title="五，其他工具加持"></a>五，其他工具加持</h1><h2 id="1，使用基于嵌入的搜索实现高效的知识检索"><a href="#1，使用基于嵌入的搜索实现高效的知识检索" class="headerlink" title="1，使用基于嵌入的搜索实现高效的知识检索"></a>1，使用基于嵌入的搜索实现高效的知识检索</h2><p>略</p>
<h2 id="2，使用代码执行进行更准确的计算或调用外部API"><a href="#2，使用代码执行进行更准确的计算或调用外部API" class="headerlink" title="2，使用代码执行进行更准确的计算或调用外部API"></a>2，使用代码执行进行更准确的计算或调用外部API</h2><p>略</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>chatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么写国内外研究现状</title>
    <url>/2023/07/10/tools/%E6%80%8E%E4%B9%88%E5%86%99%E5%9B%BD%E5%86%85%E5%A4%96%E7%A0%94%E7%A9%B6%E7%8E%B0%E7%8A%B6/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>科研论文写作中，研究现状是不可避免的一个环节，能整理我们对这个知识领域的了解程度，形成相应的知识框架。后面准备一篇相应的英文研究现状模板。。。</p>
<p><strong>科研论文之—国内外研究现状</strong><br><span id="more"></span></p>
<h1 id="一，国外研究现状"><a href="#一，国外研究现状" class="headerlink" title="一，国外研究现状"></a>一，国外研究现状</h1><ul>
<li><p>国外关于……的研究……，<strong>许多学者</strong>对……进行了深入的研究，<strong>内容扩展</strong>到……的领域，学者A和学者B<strong>从……的角度</strong>对……的差异进行研究，<strong>得出了</strong>……实施……的必要性。</p>
</li>
<li><p>学者C在……年<strong>对……问题</strong>进行了研究，他<strong>认为……可以实现……提高</strong>，学者D<strong>建议……</strong>引入到……中，**实现……以化解因为……造成的……</p>
</li>
<li><p>学者F在……年从……角度对……进行了研究。他在研究中阐述了目前国际……对……两个方面的争论，一是……，二是……，在他看来，这些争论表明了现在……发展的一个趋势。</p>
</li>
<li><p>G学者认为……对……是必要的，可以……。在此基础上他从……的角度对……等方面进行了研究，另外还对……中存在的……风险等一些问题进行了研究，并给出了自己的意见。</p>
</li>
<li><p>学者H在……年，从……的角度对……的作用作了阐述。他认为……是非常有必要的，但是他不赞同……，而是提倡……，认为只有这样才能……</p>
</li>
<li><p>学者I在……年认为现行……应该……，只有在……和……实施相应的……才能实现……</p>
</li>
<li><p>从国外的发展趋势和研究现状来看，……国家的……相对比较……，一般都有多个……案例在……实施，并且在……方面积累了大量经验，形成了适合自己……的运作模式，这些经验值得学习和借鉴。</p>
</li>
</ul>
<h1 id="二，国内研究现状及发展趋势"><a href="#二，国内研究现状及发展趋势" class="headerlink" title="二，国内研究现状及发展趋势"></a>二，国内研究现状及发展趋势</h1><ul>
<li><p>国内学者对于……进行了多方面的研究，在……的……方面，学者A，B，C等人都对……进行了……</p>
</li>
<li><p>对于……的合理性，学者D和学者E在……年从……方面进行了阐述。学者F和学者G等人对……与……的区别于联系进行了阐述。</p>
</li>
<li><p>在……方面……主要有两种，其一是由……，其二是由……，目前国内……普遍主张采用……模式，以……为例，学者H在……年认为，采用……的方式有诸多好处，其一是……其二是……第三是……</p>
</li>
<li><p>从我国目前的发展来看，……发展……，……理论研究主要在……方面，对于……和……主题的研究仍在探索中。</p>
</li>
</ul>
<h1 id="三，文献评述"><a href="#三，文献评述" class="headerlink" title="三，文献评述"></a>三，文献评述</h1><ul>
<li>近……年来，我国学者就……方向和……角度展开了多维度、多角度的探讨与研究，取得……成果。但由于……的原因，……仍有争议、或存在……缺口。主要原因有下：一是……二是……故本文……（引出本文选题意义）</li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>writing</tag>
      </tags>
  </entry>
  <entry>
    <title>On Interpretability of Artificial Neural Networks-A Survey</title>
    <url>/2023/06/28/CNN/On%20Interpretability%20of%20Artificial%20Neural%20Networks%20A%20Survey/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>title: On Interpretability of Artificial Neural Networks A Survey</p>
<p>链接: <a href="https://pan.baidu.com/s/1mmXJSe-n63EYjxNztRrCig">https://pan.baidu.com/s/1mmXJSe-n63EYjxNztRrCig</a><br>提取码: 8q5v<br>—来自百度网盘超级会员v6的分享</p>
<p><strong>ANN 模型的解释综述文章～</strong><br><span id="more"></span></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>由人工深度神经网络（DNN）实现的深度学习最近在处理文本、图像、视频、图形等的许多重要领域取得了巨大成功。 然而，DNN 的黑盒性质，成为了其在医疗诊断、治疗等关键应用中被广泛采用的主要障碍之一。 由于深度学习的巨大潜力，DNN 的可解释性最近引起了很多研究关注。 在本文中，我们提出了一种简单但全面的可解释性分类法，系统地回顾了神经网络可解释性的最新研究，描述了可解释性在医学中的应用，并讨论了未来的研究方向，例如与模糊逻辑和大脑科学相关的研究方向。</p>
<h1 id="一，介绍"><a href="#一，介绍" class="headerlink" title="一，介绍"></a>一，介绍</h1><p>深度学习[71]已成为许多常见重要领域的主流方法，例如文本[40]、图像[181]、视频[132]和图形[88]。 然而，深度学习作为一种黑盒模型，虽然在实践中表现得很好，但很难解释其底层机制和行为。 人们经常会问这样的问题，深度学习如何做出这样的预测？为什么模型中某些特征比其他特征更受青睐？需要进行哪些更改来提高模型性能？不幸的是，目前能严谨回答这些问题的研究很有限。</p>
<p>深度神经网络（DNN）的可解释性对于许多领域至关重要，尤其是医疗保健领域[67]、[68]、[173]，首先，模型稳健性是医疗应用中至关重要的问题。 最近的研究表明模型的可解释性和稳健性密切相关[131]，一方面，模型稳健性的提高促进了模型的可解释性。 例如，通过对抗性训练（一种用对抗性示例增强训练数据的训练方法）训练的深度模型，比没有对抗性示例训练的相同模型表现出更好的可解释性（具有更准确的显著图）[131]。 另一方面，当我们深入理解一个模型时，我们可以彻底检查它的弱点，因为可解释性可以帮助识别复杂模型的潜在漏洞，从而提高其准确性和可靠性。 此外，可解释性在深度学习技术的道德使用中发挥着重要作用[57]。 为了建立患者对深度学习的信任，需要可解释性来让深度学习系统负责[57]。 如果模型构建者可以解释为什么模型在某些条件下做出特定决策，用户就会知道这样的模型是否会导致不良事件。 然后就可以建立标准和协议来更好地使用深度学习系统。</p>
<p>然而，缺乏可解释性已成为深度学习在关键任务应用中的主要障碍。 例如，欧盟于2016年提出规定，受算法影响的个人有权获得解释[61]。 尽管在深度学习的可解释性方面做出了巨大的努力，并且已经有了关于该主题的几篇综述，但我们认为仍然需要最新的综述，特别是考虑到该领域的快速发展。 Zhang和Zhu[201]的综述主要是视觉可解释性。 他们的综述中的代表性出版物，在我们综述中是特征分析、显著性和代理分类法。 Chakraborty 等人[28]采纳了Lipton [112]关于可解释性的意见，并相应地构建了他们的综述以提供深入的观点，但范围有限，例如只引用了49 篇参考文献。 杜等人的综述[43]也有类似的弱点，只涵盖了40篇论文，分为事后解释和临时解释，以及全局解释和局部解释。 他们的分类比较粗粒度，忽略了一些重要的出版物，例如关于文本解释、案例解释等的出版物。相比之下，我们的综述非常详细和全面，包括最新的研究结果。 [58]中的出版物将解释性分类为理解神经网络的工作流程、理解神经网络的表示和解释生成，我们涵盖了所有这些方面，并讨论了如何构建可解释神经网络原型的研究。 Guidotti 等人的综述[65] 以及 Adadi 和 Berrada [2] 涵盖了现有的黑盒机器学习模型，而不是专注于神经网络。 因此，他们的调查中缺少几篇解释神经网络的标志性论文，例如从数学和物理角度的解释。</p>
<p>Arrieta等人[10]对可解释的人工智能（XAI）进行了广泛的回顾，其中澄清了概念和分类法，并确定了挑战。 虽然该综述涵盖了 AI/ML 的一般可解释性，但我们的综述是针对 DNN 的，并提供了独特的视角和见解。 具体来说，我们的综述在以下意义上是新颖的：</p>
<ul>
<li>1）我们分别讨论事后和临时可解释性，因为前者解释现有模型，而后者构建可解释的模型； </li>
<li>2）我们的方向包括广泛研究的生成模型、总结深度学习理论进展、回顾先进的数学/物理方法以及可解释性在医学中的应用； </li>
<li>3）通过定制示例和通过GitHub公开的代码来说明重要方法； </li>
<li>4）可解释性研究是一个快速发展的领域，每年都会发表许多研究文章。<br>因此，我们的综述应该是对文献的有价值且最新的补充。</li>
</ul>
<p>在开始调查之前，让我们首先陈述有关可解释性的三个基本问题：</p>
<ul>
<li>1）可解释性意味着什么？ </li>
<li>2）为什么解释性很困难？ </li>
<li>3）如何建立良好的解释方法？ </li>
</ul>
<p>第一个问题在[112]中得到了很好的解决，为了完整性，我们将他们的陈述放在这里。 第二个问题在[112]和[145]中得到了部分涉及，我们将这些综述纳入其中并用我们自己的观点进行补充。 对于第三个问题，我们提出自己的看法。</p>
<h2 id="A，什么是可解释性？"><a href="#A，什么是可解释性？" class="headerlink" title="A，什么是可解释性？"></a>A，什么是可解释性？</h2><p>尽管“可解释性”一词被频繁使用，但人们对可解释性的确切含义并没有达成共识，这在一定程度上解释了当前解释方法如此多样化的原因。 例如，一些研究人员探索模型的事后解释，而另一些研究人员则关注模型机器之间的相互作用机制。 一般来说，可解释性是指人类理解和推理模型的能力程度。 基于Lipton[112]的分类，我们总结了不同层面的可解释性的含义。</p>
<p>可仿真性被认为是对整个模型的理解，从某种意义上说，我们可以在统一的理论框架中理解顶层模型的机制。一个例子是[140]中报道的：一类径向基函数（RBF）网络可以表示为带有正则化项的插值问题的解决方案，其中 RBF 网络是一种以 RBF 作为激活函数的人工神经网络。 从可仿真性来看，模型越简单，模型的可仿真性就越高。 例如，线性分类器或回归器是完全可以理解的。 为了增强可仿真性，我们可以改变模型的一些性质或使用精心设计的正则化项。</p>
<p>可分解性是指根据模型的组件（例如神经元、层、块等）来理解模型，这种模块化分析在工程领域非常流行。 例如，复杂系统的内部工作被分解为功能化模块的组合。 大量的工程实例，例如软件开发和光学系统设计，已经证明模块化分析是有效的。 在机器学习中，决策树是一种模块化方法，其中每个节点都有一个明确的效用来判断判别条件是否满足，每个分支输出一个判断结果，每个叶节点代表最终的结果，计算所有属性后做出决定。 模块化神经网络有利于网络设计的优化，因为我们知道整个模型的每个组件的作用。</p>
<p>算法透明度是为了理解模型的训练过程和动态，神经网络的目标函数的情况是高度非凸的。 深度模型没有唯一的解决方案这一事实损害了模型的透明度。 然而，有趣的是，当前基于随机梯度下降（SGD）的学习算法仍然高效且有效地执行。 如果我们能够理解学习算法为何起作用，深度学习的研究和应用将会加速。</p>
<h2 id="B，为什么可解释性很难？"><a href="#B，为什么可解释性很难？" class="headerlink" title="B，为什么可解释性很难？"></a>B，为什么可解释性很难？</h2><p>在理解了可解释性的含义之后，一个自然的问题是，是什么阻碍了实践者研究可解释性？这个问题在[145]中从商业障碍和数据的自然特性方面得到了部分解决。 在这里，我们用关于人类局限性和算法复杂性的其他方面来补充他们的观点，我们认为可解释神经网络的障碍来自以下四个方面。</p>
<h3 id="人为限制"><a href="#人为限制" class="headerlink" title="人为限制"></a>人为限制</h3><p>在许多应用中，专业知识对模型而言往往是不足的。 如今，深度学习已被广泛应用于解决复杂的问题，即使是专业人士也无法充分理解。 更糟糕的是，这些问题并不少见。 例如，在最近的一项研究[46]中，我们提出使用人工神经网络来预测伪随机事件。 具体来说，我们将 100000 个二进制连续数字输入到网络中，以预测序列中的第 100001 个数字。 在我们的预测中，高度复杂的隐藏关系被学会以 $3\sigma$ 的准确率击败纯粹的随机猜测。 此外，据推测，神经网络的高灵敏度和高效率可能有助于区分伪随机性和真实量子随机性之间的根本差异。 在这种情况下，神经网络的可解释性将会缺失也就不足为奇了，因为即使是最有才华的物理学家也对这个问题的本质知之甚少，更不用说完全理解神经网络的预测了。</p>
<h3 id="商业壁垒"><a href="#商业壁垒" class="headerlink" title="商业壁垒"></a>商业壁垒</h3><p>在商业世界中，企业有强烈的动机隐藏其模型。 首先也是最重要的是，公司从黑盒模型中获利。 公司通过完全透明的模型获取资本的做法并不常见[145]。 其次，模型不透明有助于保护辛苦工作免遭逆向工程。 这样的黑匣子才是有效而理想的，因为被服务的客户可以获得满意的结果，而竞争对手无法轻易窃取他们的知识产权[145]。 第三，建立可解释模型的原型可能会在财务、计算和其他资源方面花费太多。 现有的开源高级模型可以轻松地为特定任务构建性能良好的算法。 然而，对结果模型的行为产生可靠且一致的理解需要付出更多的努力。</p>
<h3 id="数据原生性"><a href="#数据原生性" class="headerlink" title="数据原生性"></a>数据原生性</h3><p>一方面，虽然现在是大数据时代，但很多领域往往无法获取高质量的数据。 例如，在预测电网故障的项目中[145]，数据库涉及文本文档、追溯到1890年代的电力会计数据以及新沙井检查的数据。 高度异构和不一致的数据不仅阻碍了深度学习模型的准确性，而且阻碍了可解释性的构建。 另一方面，现实世界的数据具有高维度的特征，这抑制了推理。 例如，给定 MNIST 图像分类问题，输入图像的大小为 28 × 28 = 784，因此，解决此问题的深度学习模型必须学习 784 个变量到 10 个数字之一的有效映射。 如果我们考虑 ImageNet 数据集，输入变量的数量将达到 512×512×3 = 768 432。</p>
<h3 id="算法复杂性"><a href="#算法复杂性" class="headerlink" title="算法复杂性"></a>算法复杂性</h3><p>深度学习是一种大规模、高度非线性的算法。 卷积、池化、非线性激活、捷径等都有助于神经网络的可变性。 深度模型的可训练参数数量可以达到数亿甚至更多。 尽管非线性不一定会导致不透明（例如，决策树模型不是线性的，但可以解释），但深度学习的一系列非线性操作确实阻止了我们理解其内部工作原理。 此外，递归性是另一个困难来源。 一个典型的例子是非线性递归导致的混沌行为。 众所周知，即使是简单的递归数学模型也可能导致棘手的动力学[107]。 在[174]中，证明即使在简单的神经网络中也存在诸如分叉之类的混沌行为。 在混沌系统中，初始输入的微小变化可能会导致巨大的结果差异，从而增加解释方法的复杂性。</p>
<h2 id="C，怎么构建好的可解释性方法？"><a href="#C，怎么构建好的可解释性方法？" class="headerlink" title="C，怎么构建好的可解释性方法？"></a>C，怎么构建好的可解释性方法？</h2><p>第三个主要问题是评估所提出的解释方法质量的标准。 由于现有的评估方法还为时过早，我们提出了五个通用且明确的经验法则：</p>
<ul>
<li>1）准确性； </li>
<li>2）一致性； </li>
<li>3）完整性； </li>
<li>4）普遍性； </li>
<li>5）奖励。 </li>
</ul>
<p>与[42]中描述的方法相比，我们的经验法则是细粒度的，并且侧重于解释方法的特征：基于应用程序、基于人性化和基于功能。</p>
<h3 id="准确度"><a href="#准确度" class="headerlink" title="准确度"></a>准确度</h3><p>准确度是指解释方法的准确程度。 仅限于定性描述还是定量分析？ 一般来说，定量解释方法比定性解释方法更可取。</p>
<h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h3><p>一致性表明解释中不存在矛盾。 对于多个相似的样本，公平的解释应该产生一致的答案。 此外，解释方法应符合真实模型的预测。 例如，基于代理的方法是根据它们复制原始黄金模型的程度来评估的。</p>
<h3 id="完整性"><a href="#完整性" class="headerlink" title="完整性"></a>完整性</h3><p>从数学上讲，神经网络是学习最适合数据的映射。 一个好的解释方法应该显示出支持最大数量的数据实例和数据类型的有效性。</p>
<h3 id="通用性"><a href="#通用性" class="headerlink" title="通用性"></a>通用性</h3><p>随着深度学习的快速发展，深度学习的宝库得到了极大的丰富。 这种多样化的深度学习模型在广泛的应用中发挥着重要作用。 一个重要的问题是我们是否可以开发一种通用解释器来解释尽可能多的模型以节省劳动力和时间。 但是，由于模型之间的高度可变性，这在技术上具有挑战性。</p>
<h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>加深对神经网络的理解有哪些收获？ 除了从业者和用户的信任之外，可解释性的成果还可以是对网络设计、训练等的深入了解。由于其黑盒性质，使用神经网络在很大程度上是一个试错过程，有时会产生相互矛盾的直觉。 对深度学习的透彻理解将有助于神经网络的研究和应用。</p>
<p>简而言之，我们在这篇综述中的贡献有三个：</p>
<ul>
<li>1）我们提出了一种用于神经网络可解释性的全面分类法，并用我们的见解描述了关键方法； </li>
<li>2）我们系统地说明了可解释性方法作为教育辅助工具，如图3、5、6、7、9、10、16和17所示；</li>
<li>3）我们在神经网络和规则的融合方面阐明了可解释性研究的未来方向，系统、神经网络和脑科学之间的协同作用以及医学的可解释性。</li>
</ul>
<h1 id="二，解释方法综述"><a href="#二，解释方法综述" class="headerlink" title="二，解释方法综述"></a>二，解释方法综述</h1><p>在本节中，我们首先介绍我们的分类法，然后回顾分类法每个类别下的可解释性结果。 我们于2020年9月22日在Web of Science中输入搜索词“深度学习可解释性”、“神经网络可解释性”、“可解释神经网络”和“可解释深度学习”，时间范围为2000年至2019年，图1绘制了相对于年份的文章数量，它清楚地显示了该领域的指数趋势。 通过这项调查，我们的动机是涵盖尽可能多的重要论文。 因此，我们并不将自己限制在 Web of Science 内，还使用 Google Scholar、PubMed、IEEE Xplore 等搜索相关文章。</p>
<h2 id="A，分类定义"><a href="#A，分类定义" class="headerlink" title="A，分类定义"></a>A，分类定义</h2><p>如图2 所示，我们的分类法基于我们调查的论文和现有的分类法。 我们首先将调查的论文分为事后可解释性分析和即席可解释建模。 事后可解释性分析来解释现有模型，可以进一步分为特征分析、模型检查、显著性、代理、高级数学/物理分析、案例解释和文本解释。 即席可解释建模构建可解释模型，可以进一步分类为可解释表示和模型更新。 在我们提出的分类法中，高级数学/物理分析类是新颖的，但不幸的是在之前的评论中缺失了。 我们认为这门课相当重要，因为数学/物理的结合对于为深度学习奠定坚实的基础至关重要。 下面，我们澄清分类学的定义及其说明。 我们想强调的是，一种方法可能属于不同的类别，具体取决于人们如何看待它。</p>
<h3 id="事后可解释性分析"><a href="#事后可解释性分析" class="headerlink" title="事后可解释性分析"></a>事后可解释性分析</h3><p>事后可解释性是在充分学习模型后进行的。 事后方法的一个主要优点是不需要牺牲可解释性和预测性能，因为预测和解释是两个独立的过程，互不干扰。 然而，事后解释通常并不完全忠实于原始模型。 如果与原始模型相比，解释是 100% 准确的，那么它就成为原始模型。 因此，任何此类解释方法或多或少都是不准确的。 更糟糕的是我们常常不知道其中的细微差别[145]。 这样的细微差别使得从业者很难对一种解释方法有充分的信任，因为解释方法的正确性得不到保证。</p>
<p><strong>特征分析技术</strong>的重点是比较、分析和可视化神经元和层的特征。 通过特征分析，识别敏感特征和处理它们的方法，从而在一定程度上解释模型的基本原理。<br>特征分析技术可以应用于任何神经网络，并提供有关网络学习哪些特征的定性见解。 然而，这些技术缺乏深入、严格和统一的理解，因此不能用于修正模型以获得更高的可解释性。</p>
<p><strong>模型检查</strong>利用外部算法深入研究神经网络，系统地提取神经网络内部工作机制的重要结构和参数信息。<br>此类方法比特征分析中的方法在技术上更可靠，因为统计等分析工具直接参与性能分析。 因此，通过模型检查方法获得的信息更值得信赖和有价值。 在一项示例性研究[183]中，寻找重要的数据路由路径被用作理解模型的一种方式。 通过这样的数据路由路径，模型可以被忠实地压缩为紧凑的模型。 换句话说，可解释性提高了模型压缩的可信度。</p>
<p><strong>显著性方法</strong>识别输入数据的哪些属性与模型的预测或潜在表示最相关。 在这一类别中，涉及人工检查来确定显著性图是否合理。 显著性图很有用，即，如果北极熊总是出现在与雪或冰一起出现的图片中，则模型可能误用了雪或冰的信息来检测北极熊，而不是利用北极熊的真实特征进行检测。 通过显著图，可以发现并避免这个问题。<br>显著性方法在可解释性研究中很流行，然而，广泛的随机测试表明，一些显著性方法可能独立于模型和数据[3]。即，某些方法提供的显著性图与边缘检测器产生的结果高度相似。 这是有问题的，因为这意味着这些显著性方法无法找到解释模型预测的输入的真实属性。 因此，在这些情况下应开发与模型相关和数据相关的显著性方法。</p>
<p><strong>代理方法</strong>构建一个更简单、更可解释的代理，它非常类似于经过训练的大型复杂黑盒模型。 代理方法可以是局部空间中的局部方法，也可以是整个解决方案空间中的全局方法。 示例性代理模型包括决策树、规则系统等。 代理方法的弱点是构建代理模型需要额外的成本。</p>
<p><strong>先进的数学/物理分析方法</strong>将神经网络置于理论数学/物理框架中，通过先进的数学/物理工具来理解神经网络的机制。 该类别中涵盖深度学习的理论进展，包括非凸优化、表征能力和泛化能力。<br>本类的一个问题是，为了建立合理的解释，有时会做出不切实际的假设来促进理论分析，这可能会损害解释的实际有效性。</p>
<p><strong>案例解释方法</strong>遵循基于案例的推理[90]。 人们喜欢例子。 人们可能不会被产品的无聊统计数字所吸引，但在聆听其他用户使用此类产品的体验时可能会感到惊讶。 这一理念赢得了许多从业者的心，并激发了基于案例的深度学习解释。 案例解释方法提供了抓住模型本质的代表性示例。<br>本类中的方法很有趣且鼓舞人心。 然而，这种做法更像是健全性检查而不是一般解释，因为从选定的查询案例中无法理解有关神经网络内部工作的太多信息。</p>
<p><strong>文本解释方法</strong>在图像语言联合任务中生成文本描述，有利于理解模型的行为。 此类还可以包含生成用于解释的符号的方法。<br>此类中的方法在图像语言联合任务中特别有用，例如从 X 射线照片生成诊断报告。 然而，文本解释并不是任何深度学习模型的通用技术，因为它只有在模型中存在语言模块时才能起作用。</p>
<h3 id="即席可解释建模"><a href="#即席可解释建模" class="headerlink" title="即席可解释建模"></a>即席可解释建模</h3><p>即席可解释建模消除了事后可解释性分析中或多或少存在的偏差。 尽管人们普遍认为可解释性和模型可表达性之间存在权衡[123]，但仍然有可能找到一个既强大又可解释的模型。 一个值得注意的例子是[30]中报告的工作，其中可解释的两层额外风险模型赢得了 FICO 认可竞赛的第一名。</p>
<p>可解释的表示方法采用<strong>正则化技术</strong>来引导神经网络的优化朝着更可解释的表示方向发展。 可分解性、稀疏性和单调性等属性可以增强可解释性。 因此，正则化特征成为允许更多可解释模型的一种方式。 相应地，出于可解释性的目的，损失函数必须包含正则化项，这限制了原始模型执行其完整的学习任务。</p>
<p><strong>模型更新方法</strong>通过设计更多可解释的机器并将其部署到网络中来寻求可解释性。 这些机器包括具有专门设计的激活功能的神经元、具有特殊功能的插入层、模块化架构等等。 未来的方向是使用越来越多的可解释组件，这些组件可以同时为不同的任务实现类似的最先进的性能。</p>
<h2 id="B，事后可解释性分析"><a href="#B，事后可解释性分析" class="headerlink" title="B，事后可解释性分析"></a>B，事后可解释性分析</h2><h3 id="特征分析"><a href="#特征分析" class="headerlink" title="特征分析"></a>特征分析</h3><p>基于反转的方法[41]、[117]、[163]、[200]通过将特征图反转为合成图像来破解神经网络的表示。 例如，Mahendran 和 Vedaldi [117] 假设输入为图像 $x_0$ 的神经网络 $\Omega_0$ 的表示被建模为 $\Omega_0 = \Omega (x_0)$，其中 $\Omega$ 是神经网络映射，通常不可逆。 然后，反演问题被公式化为找到一个图像 $x^\ast$，其神经网络表示最匹配 $\Omega_0$； 即 $\mathop {argmin}_x |\Omega (x) − \Omega_0 |_2 + \lambda R(x)$，其中 $R(x)$ 是表示有关输入图像的先验知识的正则化项。 目标是通过比较倒置图像和原始图像之间的差异来揭示丢失的信息。 Dosovitskiy 和 Brox [41] 直接训练一个新网络，以感兴趣模型生成的特征作为输入，图像作为标签，将中间层的特征反转为图像。 研究发现，即使从更深层次的特征中，仍然可以重建轮廓和颜色。 Zeiler 和 Fergus [200] 设计了一个由反池化、校正、反卷积操作组成的反卷积网络，与原始卷积网络配对，以便无需训练即可反转特征。 在反卷积网络中，通过使用最大值的位置来实现反池化层； 通过将负值设置为0来实现校正； 反卷积层使用转置卷积滤波器。</p>
<p>激活最大化方法[45]、[128]、[129]、[168]致力于合成图像，最大化感兴趣的神经网络或神经元的输出。 由此产生的图像被称为“深度梦”，因为它们可以被视为神经网络或神经元的梦境图像。</p>
<p>在[16]、[85]、[108]、[196]和[210]中，有人指出可以从每个神经元中提取有关深度模型的信息。 Yosinski等[196]直接检查了每层神经元针对不同图像或视频的激活值。 他们发现不同输入的实时激活值变化有助于理解模型的工作原理。 Li等[108]对比不同初始化生成的特征，以研究神经网络在随机初始化时是否学习相似的表示。 感受野（RF）是神经元与输入体积连接的空间范围[111]，为了研究神经元给定输入的 RF 大小和形状，Zhou 等[210]提出了一种网络剖析方法，首先选择感兴趣神经元的高激活值的K张图像，并为每张图像构建5000张遮挡图像，然后将它们的给定单位输入神经网络来观察感兴趣神经元激活值的变化，较大的差异表明存在重要补丁。 最后，对差异较大的被遮挡图像进行居中和平均以生成 RF，这种网络剖析方法已扩展到生成网络[17]。 此外，Bao等[16]将给定层的低分辨率激活图放大到与输入相同的大小，将图阈值化为二元激活图，然后计算二元激活图和真值之间的重叠区域的二进制分割图作为可解释性度量。 Karpathy等[85] 将 LSTM [73] 中的门定义为左饱和或右饱和，具体取决于其激活值小于 0.1 或大于 0.9。 在这方面，经常处于饱和状态的神经元很有趣，因为这意味着这些神经元可以长期记住它们的值。 Zhang等[202]剖析了网络中的特征关系，前提是每层中滤波器的特征图可以由较早层中的部分模式激活。 他们逐层挖掘零件模式，从每一层的特征图中发现零件模式的激活峰值，并构建一个解释图来描述层次特征的关系，每个节点代表一个零件模式，相邻层之间的边代表一个共激活关系。</p>
<h3 id="模型检验"><a href="#模型检验" class="headerlink" title="模型检验"></a>模型检验</h3><p>经验影响函数是衡量估计量对样本的依赖性[99]。 Koh和Liang[89]应用影响函数的概念来解决以下问题：给定一个样本的预测，数据集中的其他样本对该预测有正面影响还是负面影响？ 该分析还可以帮助识别数据中存在的错误注释标签和异常值。 如图 3 所示，给定一个类似 LeNet-5 的网络，通过影响函数可以识别给定图像的两个有害图像。</p>
<p>Bansal[12]，Lakkaraju[97]和张等[203]致力于检测神经网络中的故障或偏差。 例如，Bansal[12] 开发了一种与模型无关的算法来识别神经网络可能无法提供任何预测的实例。 在这种情况下，模型会发出“不要相信这些预测”之类的警告作为警报。 具体来说，他们用二进制属性集合标记所有失败的图像，并将这些图像聚集在属性空间中。 因此，每个集群都指示一种故障模式。 为了有效地识别数据集中那些具有高预测分数的错误标记实例，Lakkaraju 等[97]引入了两个基本推测：第一个是错误标记高置信度的实例是由于系统偏差而不是随机扰动，而第二个是每个失败的示例都具有足够的代表性和信息性。 然后，他们将图像聚类成几个组，并设计了一种多分支搜索策略，将每个组作为一个分支，计划在每个步骤中应该查询和采样哪个组。 为了发现代表性偏差，Zhang 等[203]根据人类的常识（火热与冰冷）利用属性之间的真值关系来检查神经网络挖掘的属性关系是否很好地符合真值。</p>
<p>Wang等[183]通过识别关键数据路由揭开了网络的神秘面纱。 具体来说，将门控制二元向量 $\lambda_k \in \{0, 1\}^{n_k}$ （其中 $n_k$ 是第 $k$ 层中的神经元数量）乘以第 $k$ 层的输出，并制定了寻找控制门值的问题为：搜索 $\lambda_1, \cdots,\lambda_k$ 满足下面公式公式</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathop{argmin}_{\lambda_1, \cdots,\lambda_k}d(f_{\theta}(x),f_{\theta}(x;\lambda_1, \cdots,\lambda_k))+\gamma\sum_k\|\lambda_k\|_1
\end{align*}</script><p>其中 $f_\theta$ 是由 $\theta$ 参数化的神经网络表示的映射，$f_\theta (x;\lambda_1,…,\lambda_k)$ 是控制门 $\lambda_1,…\lambda_k$ 固定时的映射。$d(·,·)$ 是距离度量，$\gamma$ 是控制损失和正则化之间权衡的常数，$|\cdot|_1$ 是 $l_1$ 范数，使得 $\lambda_k$ 是稀疏的，学习到的控制门可以暴露模型的重要数据处理路径。 Kim[86]开发了概念激活向量（CAV），它可以定量测量概念 $C$ 对于模型任何层的敏感性。 首先，训练二元线性分类器 $h$ 来区分由两组样本刺激的层激活：$\{f_l(x) : x \in P_C\}$ 和 $\{f_l(x) : x \notin P_C\}$，其中 $f_l(x)$ 是第 $l$ 层的层激活，$P_C$ 表示体现概念 $C$ 的数据。然后，CAV 被定义为线性分类器超平面的法向单位向量 $v^l_C$，该超平面分离具有/不具有定义的样本。 最后，$v^l_C$ 用于计算第 $l$ 层中概念 $C$ 作为方向导数的敏感性</p>
<script type="math/tex; mode=display">
\begin{align*}
S_{C,k,l}=\lim_{\epsilon\rightarrow 0}\frac{h_{l,k}(f_l(x)+\epsilon v^l_C)-(h_{l,k}f_l(x))}{\epsilon}=\nabla h_{l,k}(f_l(x))v^l_C
\end{align*}</script><p>其中 $h_{l,k}$ 表示输出类 $k$ 的训练二元线性分类器的 logits。 You等[195]将神经网络映射为关系图，然后通过大量实验（将图转录为网络并在数据集上实现网络）研究神经网络的图结构及其预测性能之间的关系。 他们发现网络的预测性能与两个图形度量相关：1）聚类系数，2）平均路径长度。</p>
<h3 id="显著性"><a href="#显著性" class="headerlink" title="显著性"></a>显著性</h3><p>有很多方法可以获取显著性图。 部分依赖图（PDP）和个体条件期望（ICE）[53]、[59]、[74]是与模型无关的统计工具，用于可视化因变量和预测变量之间的依赖性。 为了计算 PDP，假设输入维度为 $p$，并令 $S,C \subseteq {1,2,…,p}$ 为两个互补集，其中 $S$ 是固定的集合，$C$ 是可变的集合。 然后，$x_S$ 的 PDP 定义为 $f_S =\int f(x_S,x_C)dx_C$，其中 $f$ 是模型。ICE 的定义很简单，$x_S$ 处的 ICE 曲线是通过固定 $x_C$ 并改变 $x_S$ 获得的。 图 4 显示了一个简单的示例，说明如何分别计算 PDP 和 ICE。</p>
<p>一种简单的方法是研究删除一个特征后预测的变化，也称为留一归因[4]、[83]、[105]、[143]、[211]。 例如，Kádár[83]利用这个想法定义了一个消去分数：$1−\mathop{cosine}(h(S), h(S_{\setminus i}))$，其中 $\mathop{cosine}(\cdot,\cdot)$ 是余弦距离，$h$ 是句子的表示 ，$S$ 是完整的句子，$S_{\setminus i}$ 是没有第 $i$ 个单词的句子，并分析每个单词的重要性。 Adler[4]建议衡量相关输入的间接影响，例如，在房屋贷款决策系统中，种族不应该成为决策的因素。 然而，仅仅去除种族因素并不足以排除种族的影响，因为一些剩余因素，例如“邮政编码”，与种族高度相关。</p>
<p>此外，合作博弈论中的Shapley值被用于[6]、[27]、[39]、[113]和[115]中，在数学上，集合函数 $\hat f$ 相对于特征 $i$ 的 Shapley 值定义为</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathop{Shapley}_i(\hat f)=\sum_{S\subseteq P\setminus \{i\}}\frac{(N-|S|-1)!|S|!}{N!}\bigg(\hat f (S\cup\{i\}-\hat f(S) )\bigg)
\end{align*}</script><p>其中，$|\cdot|$ 是集合的大小，$P$ 是 $N$ 个层的总层集合，并且集合函数 $\hat f$ 将每个子集 $S \subseteq P$ 映射到一个实数。 此外，通过用零值替换输入中不属于 $S$ 集合中的特征，可以将 Shapley 值的定义扭曲为神经网络函数 $f$。 为了减少组合爆炸带来的令人望而却步的计算成本，Ancona 等[6]提出了一种新颖的 Shapley 值多项式时间近似，它基本上计算随机组合的期望，而不是枚举每个组合。 图 5 显示了如何为在加州住房数据集上训练的全连接层网络计算 Shapley 值的简单示例，其中包括八个属性，例如作为输入的房屋年龄和房间号以及作为标签的房价。</p>
<p>研究人员并没有删除一个或多个特征，而是采用了梯度。 Simonyan[156]，Smilkov[160]，Sundararajan [167]和Singla[159]利用梯度的思想来探测输入的显著性。 Simonyan[156]计算了类别分数相对于图像像素的一阶泰勒展开，通过该一阶系数生成类别的显著性图。 Smilkov[160]证明梯度作为显著性显示了属性和标签之间的相关性，但是，通常梯度相当嘈杂。 为了消除噪声，他们提出了用SmoothGrad将噪声多次添加到输入图像中并对结果进行平均，$\widehat{M_c}(x)=(1/N)\sum^N_{n=1}M_c^{(n)}(x+N(0,\sigma^2)) $，其中 $M_c^{(n)}$ 是以 $\sigma$ 为标准方差的高斯噪声。 基本上，$M(x)$ 是显著图的平滑版本。 Sundararajan[167]为显著性方法设定了两个基本要求：1）（敏感性）如果输入和基线之间只有一个特征不同，输入和基线的输出不同，那么这个特征应该被赋予非零值; 2）（实现不变性）两个功能等效的网络中相同特征的属性应该是相同的。 注意到早期基于梯度的显著性方法不满足上面两个要求。他们提出了积分梯度，公式是：$(x_i-x_i’)\int^1_0[\partial F (x’+\alpha (x-x’))/\partial x_i]d\alpha $</p>
<p>其中 $F(\cdot)$ 是神经网络映射，$x = (x_1, x_2, \cdots, x_N )$ 是输入，$x’ = (x’_1 ,x’_2 ,\cdots,x’_N )$ 是基线且满足 $(\partial/\partial x)F(x)|_{x=x’} = 0$。实际中，积分可以转化为离散求和 $[(x_i − x_i’)/M] \times \sum^M_{m=1}[\partial F (x’ + (m/M)(x − x’))/\partial x_i]$，其中 $M$ 是积分近似的步数。 Singla[159]提出使用泰勒展开的二阶近似来生成显著图，以考虑特征依赖性。</p>
<p>Bach[11]提出了分层相关性传播（LRP），通过假设模型表示 $f(x)$ 可以表示为像素相关性 $R^l_p$ 之和，来计算一个属性与预测的相关性，其中 $x$ 是输入图像，$l$ 是层的索引，$p$ 是 $x$ 的像素的索引。 因此，$f(x) = \sum_p R^L_p$，其中 $L$ 是最后一层，$RL = [(w_p x^{L−1}_p )/(\sum_p w_p x^{L−1}_p )]f (x)$，其中 $w_p$ 是连接第 (L − 1) 层和最后一层的像素 $p$ 的权重。 给定前馈神经网络，输入的像素相关性得分 $R^1_p$ 是通过向后计算 $R^l_p =\sum_j[z_{pj} /(\sum_{p’}z_{p’j} )]R^{l+1}_j$ 得出的，其中 $z_{pj} = x^l_p w^{(l,l+1)}_{pj}$，其中 $w^{(l,l+1)}_{pj}$ 是第 $l$ 层的像素 $p$ 和第 $(l + 1)$ 层的像素 $j$ 之间的权重。 此外Arras[9] 将 LRP 扩展到循环神经网络（RNN）以进行情感分析。 蒙塔冯等人。 [125]采用深度泰勒分解的整个一阶项来产生显著图，而不仅仅是梯度。 假设 x 是模型 f(x) 精心选择的函数根：f(x) = 0，因为 f(x) 可以分解为 f(x) = f(x)+([∂f/ ∂x]|x=x)T · (x−x)+ε = 0+ i(∂f/∂xi)|x=x(xi−xˆi)+ε，其中 ε 是高阶项， 像素 i 的像素相关性表示为 Ri = [∂f /∂xi]|x=x(xi − xˆi)。 Shrikumar 等人受到这样一个事实的启发：即使神经元没有被激发，它仍然有可能揭示有用的信息。 [155]提出 DeepLIFT 来计算每个神经元的激活与其参考之间的差异，其中参考是当网络提供参考输入时该神经元的激活，然后将差异反向传播到图像空间层 像LRP一样层。 辛格等人。 [158]引入了上下文分解，其层传播公式为 βi = Wβi−1 + [|Wβi−1|/(|Wβi−1| + |Wγi−1|)] · b 且 γi = Wγi−1 + [| Wγi−1|/(|Wβi−1| + |Wγi−1|)]·b，其中W是第i层和第(i−1)层之间的权重矩阵，b是偏置向量。 限制条件为gi(x)=βi(x)+γi(x)，其中gi(x)是第i层的输出。 βi(x) 被认为是输入的上下文贡献，γi(x) 意味着输入对 gi(x) 的贡献不包含在 βi(x) 中。</p>
<p>图 6 展示了使用类似 LeNet-5 的网络对原始梯度、SmoothGrad、IntegratedGrad 和 Deep Taylor 方法的评估。 其中，IntegratedGrad 和 Deep Taylor 方法在五位数上表现出色。<br>用于量化深度模型的输入和潜在表示之间的关联的互信息度量也可以类似地起到显著性的作用[63]、[148]、[193]。 此外，还有其他方法来获取显著性图。 罗斯等人。 [144]定义了一个新的损失项<br>i (Ai(∂/∂xi) Kk=1 log(yˆk))2 用于训练，其中 i 是像素的索引，Ai 是要优化的二进制掩码，yˆk 是标签的第 k 位，K 是班级数量。 这种损失是为了惩罚梯度的锐度，以获得更清晰的解释边界。 Fong 和 Vedaldi [52] 探索学习要删除的最小区域，即找到最优的 m*</p>
<script type="math/tex; mode=display">
\begin{align*}
m^\ast = \mathop{argmin}
\end{align*}</script>]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>interpretability</tag>
      </tags>
  </entry>
  <entry>
    <title>交叉熵和KL散度</title>
    <url>/2023/06/28/math/cross_entropy_and_the_KL_divergence/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>KL散度和交叉熵都可以用来作为模型的loss函数，但二者的使用场景不一样。在这里引申一下模型loss的含义：“通过样本来计算模型分布与目标分布间的差异。”，这就是KL散度的作用。但有时候我们的目标分布会是常数，也就是这个分布是已知且不变的，例如分类任务，这个时候我们就会使用交叉熵来衡量模型的预测分布与实际分布之间的差异。</p>
<p>CNN模型中常用的交叉熵对比数学中的KL散度概念～</p>
<p><strong>调参的背后是无尽的数学理论啊～</strong><br><span id="more"></span></p>
<h1 id="概率和惊喜度"><a href="#概率和惊喜度" class="headerlink" title="概率和惊喜度"></a>概率和惊喜度</h1><p>我们用 $p(x)$ 表示事件 $x$ 发生的概率。这里我们先不讨论概率的内涵, 只需要遵循直觉：$\frac{1}{p(x)}$ 可以衡量事件 $x$ 发生时会造成的惊喜（行文需要，请按照中性理解）程度：<br>概率越低的事件发生所造成的惊喜程度高；概率越高的事件发生所造成的惊喜程度低。</p>
<p>但是概率倒数这一运算的性质不是很好，所以在不改变单调性的情况下，可以将惊喜度（surprisal）定义为：</p>
<script type="math/tex; mode=display">\log\frac{1}{p(x)}=-\log p(x)</script><p>这样定义后产生了另外两个好处： </p>
<ol>
<li>确定性事件的惊喜度 = 0； </li>
<li>如果有多个独立事件同时发生，他们产生的惊喜度可以直接相加。是的，一个事件发生概率的倒数再取对数就是惊喜。</li>
</ol>
<h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>信息熵，不过只是惊喜的期望</p>
<p>惊喜度，在大部分文章里，都叫做信息量，但这个命名只是香农根据他研究对象的需要而做的，对于很多其它的场景，要生搬硬套就会变得非常不好理解了。 </p>
<p>信息量 = 惊喜度，那么信息熵呢？看看公式不言自明：</p>
<script type="math/tex; mode=display">H_p(X)=-\sum_xp(x)\log_b(p(x))</script><p>或是连续形式：</p>
<script type="math/tex; mode=display">H_p(X)=\int p(x)\cdot\log\frac{1}{p(x)}dx</script><p>这不就是惊喜度的期望吗？</p>
<p>换句话说，信息熵描述的是整个事件空间会产生的平均惊喜。</p>
<p>什么情况下，平均惊喜最低呢？确定事件。以某个离散随机分布为例，整个分布在特定值  为 1，其它处均为 0，此时的信息熵/平均惊喜也为 0。<br>什么情况下产生的平均惊喜最高呢？自然是不确定越高平均惊喜越高。对于给定均值和方差的连续分布，正态分布（高斯分布）具有最大的信息熵（也就是平均惊喜）。所以再想想为什么大量生活中会看到的随机事件分布都服从正态分布呢？说明大自然有着创造最大惊喜的倾向，或者说，就是要让你猜不透。这也是理解热力学中的熵增定律的另一个角度。</p>
<h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>交叉熵，交叉的是古典和贝叶斯学派。</p>
<p>对于概率，比较经典的理解是看做是重复试验无限次后事件频率会逼近的值，<strong>是一个客观存在的值</strong>；但是贝叶斯学派提出了另一种理解方式：即将概率理解为<strong>我们主观上对事件发生的确信程度</strong>。</p>
<p>针对同一个随机变量空间有两个分布，分别记作 $p_o$ 和 $p_s$；<br> $p_s(x)$ 是我们主观认为 $x$ 会发生的概率，下标 $s$ 代表 subjective；<br> $p_o(x)$ 是客观上 $x$ 会发生的概率，下标 $o$ 代表 objective。<br>这种情况下，客观上这个随机事件  会给我们造成惊喜的期望应该是：</p>
<script type="math/tex; mode=display">H_{p_o,p_s}(X)=\int p_o(x)\log\frac{1}{p_s(x)}dx</script><p>这个量就是交叉熵</p>
<p>再翻译一下，交叉熵是什么？可以理解为：我们带着某个主观认知去接触某个客观随机现象的时候，会产生的平均惊喜度。</p>
<p>那什么时候交叉熵（也就是我们会获得的平均惊喜度）会大？<strong>就是当我们主观上认为一个事情发生的概率很低 $\frac{1}{p_s(x)}$ 很大)，但是客观上发生概率很高 ($p_o(x)$ 很大) 的时候，也就是主观认知和客观现实非常不匹配的时候</strong>。机器学习当中为啥用交叉熵来当作损失函数应该也就不言自明了。</p>
<h1 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h1><p>KL 散度通常用来度量两个分布之间的差异。KL 散度全称叫kullback leibler 散度，也叫做相对熵（relative entropy）。在机器学习中常用到，譬如近似推断中，有变分推断和期望传播，都是通过 Minimize KL散度来实现推断实现逼近目标分布。除了KL散度还有 $\alpha$ 散度，$\aplha-\beta$ 散度。</p>
<p>交叉熵可以衡量我们基于某种主观认识去感受客观世界时，会产生的平均惊喜。但是根据上面的分析，即使主观和客观完全匹配，这时交叉熵等于信息熵，只要事件仍然随机而非确定，就一定会给我们造成一定程度的惊喜。那我们要怎么度量主观认识和客观之间差异呢？可以用应该用以当前对“世界观”产生的惊喜期望和完全正确认识事件时产生的惊喜期望的差值来衡量，这个就是相对熵（常称作 KL-散度），通常写作：</p>
<script type="math/tex; mode=display">
\begin{align*}
    D_{KL}(p_o||p_s)&=H_{p_o,p_s}(X)-H_{p_o}(X)\\
    &=\int p_o(x)\log\frac{1}{p_s(x)}dx-\int p_o(x)\log\frac{1}{p_o(x)}dx \\
    &=\int p_o(x)\log\frac{p_o(x)}{p_s(x)}dx
\end{align*}</script><p>当我们的主观认知完全匹配客观现实的时候，KL-散度应该等于 0，其它任何时候都会大于 0。由于存在恒为正这一性质，KL-散度经常用于描述两个分布是否接近，也就是作为两个分布之间“距离”的度量；不过由于运算不满足交换律，所以又不能完全等同于“距离”来理解。<br>机器学习中通常用交叉熵作为损失函数的原因在与，客观分布并不随参数变化，所以即使是优化 KL-散度，对参数求导的时候也只有交叉熵的导数了。</p>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>A Survey of Transformers</title>
    <url>/2023/06/28/Transformer/A%20Survey%20of%20Transformers/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Transformer模型的发展综述～</p>
<p>论文百度云链接: <a href="https://pan.baidu.com/s/1fwGSq4SysfLSOzSPKRQzjg">https://pan.baidu.com/s/1fwGSq4SysfLSOzSPKRQzjg</a><br>提取码: r2ii<br>—来自百度网盘超级会员v6的分享</p>
<p><strong>继续加油读文章</strong><br><span id="more"></span></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Transformers 在自然语言处理、计算机视觉、音频处理等许多人工智能领域取得了巨大的成功。 因此，自然会引起学术界和工业界研究人员的极大兴趣。 到目前为止，已经提出了各种各样的 Transformer 变体（又名 X-formers），但是，仍然缺乏对这些 Transformer 变体的系统和全面的文献综述。 在本次调查中，我们对各种 X-formers 进行了全面审查。 我们首先简要介绍 vanilla Transformer，然后提出一个新的 X-formers 分类法。 接下来，我们从架构修改、预训练、应用三个角度介绍各种X-former。 最后，我们概述了未来研究的一些潜在方向。</p>
<h1 id=""><a href="#" class="headerlink" title=" "></a> </h1>]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>A Survey on Vision Transformer</title>
    <url>/2023/06/28/Transformer/A%20Survey%20on%20Vision%20Transformer/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>title: A Survey on Vision Transformer</p>
<p><a href="https://arxiv.org/abs/2012.12556">paper link</a></p>
<p><strong>继续加油读文章</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Transformer最早应用于自然语言处理领域，是一类主要基于self-attention机制的深度神经网络。由于其强大的表示能力，研究人员正在寻找将 Transformer 应用于计算机视觉任务的方法。在各种视觉基准测试中，基于 Transformer 的模型性能与其他类型的网络（例如卷积神经网络和递归神经网络）相似或更好。鉴于其高性能和对<strong>特定视觉归纳偏差</strong>【？】的需求较少，Transformer 越来越受到计算机视觉社区的关注。在本文中，我们通过将它们分类为不同的任务并分析它们的优缺点来回顾这些视觉Transformer模型。我们探索的主要模型类别包括骨干网络、高级/中级视觉、低级视觉和视频处理，还包括有效的将Transformer推入实际的基于设备的应用程序。此外，我们还简要介绍了计算机视觉中的自注意力机制，因为它是 Transformer 中的基本组件。在本文的最后，我们讨论了未来的挑战并为视觉Transformer提供了几个可以进一步的研究方向。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>深度神经网络 (DNN) 已成为当今人工智能 (AI) 系统的基础设施，不同类型的任务通常涉及不同类型的网络。例如，多层感知器（MLP）或全连接（FC）网络是神经网络的经典类型，它由多个线性层和非线性激活叠加在一起组成，卷积神经网络 (CNN) 引入卷积层和池化层来处理图像等移位不变数据，循环神经网络 (RNN) 利用循环单元来处理顺序数据或时间序列数据，Transformer 是一种新型的神经网络。它主要利用self-attention机制来提取内在特征，显示出在AI应用中广泛使用的巨大潜力。</p>
<p>Transformer 首先应用于自然语言处理 (NLP) 任务，并取得了显着改进。例如 vaswani2017attention 首先提出了基于注意力机制的Transformer用于机器翻译和英语选区解析任务。 bert 引入了一种新的语言表示模型，称为 BERT（Bidirectional Encoder Representations from Transformers 的缩写），它在未标记文本上预训练一个Transformer，因为它是双向的，会同时考虑到每个单词的上下文，当 BERT 发布时，它在 11 个 NLP 任务上获得了最优的性能。 gpt3 使用 1750 亿个参数在 45 TB 的压缩明文数据上预训练了一个名为 GPT-3（Generative Pre-trained Transformer 3 的缩写）的大型基于 Transformer 的模型，它在不同类型的下游自然语言任务上实现了强大的性能，而无需任何微调。这些基于 Transformer 的模型以其强大的表示能力，在 NLP 领域取得了重大突破。</p>
<p>受 Transformer 架构在 NLP 领域取得的重大成功的启发，研究人员最近将 Transformer 应用于计算机视觉 (CV) 任务。在视觉应用中，CNN 被认为是基本组件，但如今 Transformer 表明它是 CNN 的潜在替代品。igpt 训练了一个序列Transformer来自动回归预测像素，在图像分类任务上取得了与 CNN 相当的结果。另一个视觉Transformer模型是 ViT，它将纯Transformer直接应用于图像块序列以对整个图像进行分类。最近由 vit 提出，它在多个图像识别基准上取得了最先进的性能。除了图像分类之外，Transformer 还被用于解决其他各种视觉问题，包括目标检测 、语义分割 、图像处理 ，以及视频理解。由于其卓越的性能，越来越多的研究人员提出基于 Transformer 的模型来改进各种视觉任务。</p>
<p>由于基于 Transformer 的视觉模型数量迅速增加，跟上新进展的速度变得越来越困难，因此，对现有工程进行调研是当务之急，并且对社区有益。在本文中，我们着重于全面概述视觉 Transformer 的最新进展，并讨论潜在可研究方向。为了便于未来对不同主题的研究，我们根据应用场景对 Transformer 模型进行了分类，如表1所列，主要类别包括骨干网、高/中级视觉、低级视觉和视频处理。高级视觉处理对图像中看到的内容进行解释和使用，而中级视觉处理如何将这些信息组织成我们所体验的物体和表面，鉴于在基于 DNN 的视觉系统中，高级和中级视觉之间的差距变得越来越模糊，我们在这里将它们视为一个类别。解决这些高级/中级视觉任务的 Transformer 模型的一些示例包括用于对象检测的 DETR、可变形的 DETR 和用于分割的 Max-DeepLab。低级图像处理主要处理从图像中提取描述（这种描述通常表示为图像本身），典型应用包括超分辨率、图像去噪和风格转换。目前，只有少数作品在低级视觉中使用了 Transformers，因此需要进一步研究。另一类是视频处理，它是计算机视觉和基于图像的任务的重要组成部分。由于视频的顺序属性，Transformer 天生就非常适合用于视频任务，它的表现开始与传统的 CNN 和 RNN 相提并论。在这里，我们调查了与基于 Transformer 的视觉模型相关的工作，以跟踪该领域的进展。图1显示了 vision Transformer 的开发时间表——毫无疑问，未来会有更多的里程碑。</p>
<p>本文的其余部分安排如下，第 2 节讨论了标准 Transformer 的重构和自注意力机制。第 4 节是本文的主要部分，其中我们总结了主干、高级/中级视觉、低级视觉和视频任务的视觉Transformer模型，还简要描述了高效的Transformer方法，因为它们与我们的主题密切相关。在最后一节中，我们给出了结论并讨论了几个研究方向和挑战。由于篇幅限制，我们在补充材料中描述了 NLP 中 Transformer 的方法，因为研究经验可能对视觉任务有益。在补充材料中，我们还回顾了 CV 的自我注意机制作为视觉Transformer模型的补充。在本次调研中，我们主要包括代表性作品（早期的、开创性的、新颖的或启发性的作品），因为 arXiv 上有很多预印本作品，我们无法在有限的页面中将它们全部包括在内。</p>
<h2 id="2，Transformer提出"><a href="#2，Transformer提出" class="headerlink" title="2，Transformer提出"></a>2，Transformer提出</h2><p>Transformer 最早用于自然语言处理（NLP）领域的机器翻译任务， 如图2所示，它由一个编码器和一个解码器以及几个相同架构的Transformer 块组成。 编码器生成输入的编码，而解码器采用所有编码并使用它们合并的上下文信息来生成输出序列。 每个 Transformer 块由多头注意力层、前馈神经网络、跳跃连接和层归一化组成。 下面，我们将详细描述 Transformer 的每个组件。</p>
<h2 id="2-1-自注意力"><a href="#2-1-自注意力" class="headerlink" title="2.1 自注意力"></a>2.1 自注意力</h2><p>在self-attention层，首先将输入向量转化为三个不同的向量：查询向量 $\mathbf q$，关键向量 $\mathbf k$ 和值向量 $\mathbf v$，维度为 $d_q=d_k =d_v= d_{model}=512$。 然后将来自不同输入的向量打包成三个不同的矩阵，即 $\mathbf Q$、$\mathbf K$ 和 $\mathbf V$。 随后，计算不同输入向量之间的注意力函数如下（如图3左所示）：</p>
<ul>
<li><strong>step 1</strong>：计算不同输入向量之间的分数，其中 $\mathbf S=\mathbf Q\cdot \mathbf K^\top$;</li>
<li><strong>step 2</strong>：使用 $\mathbf S_n=\mathbf{S}/{\sqrt{d_k}}$ 归一化梯度稳定性的分数；</li>
<li><strong>step 3</strong>：使用 softmax 函数将分数转换为概率 $\mathbf P=\mathrm{softmax}(\mathbf S_n)$;</li>
<li><strong>step 4</strong>：求得加权值矩阵，其中$\mathbf Z=\mathbf V\cdot \mathbf P$。</li>
</ul>
<p>该过程可以统一为一个函数：</p>
<script type="math/tex; mode=display">\mathrm{Attention}(\mathbf Q,\mathbf K,\mathbf V)=\mathrm{softmax}(\frac{\mathbf Q\cdot\mathbf K^\top}{\sqrt{d_k}})\cdot\mathbf V \tag{1}\label{eq1}</script><p>Eq.\ref{eq1} 背后的逻辑很简单。步骤 1 计算每对不同向量之间的分数，这些分数决定了我们在对当前位置的单词进行编码时给予其他单词的关注程度。第 2 步对分数进行归一化以增强梯度稳定性以改进训练，第 3 步将分数转化为概率。最后，每个值向量乘以概率之和。具有较大概率的向量会受到后面层的额外关注。</p>
<p>解码器模块中的编码器-解码器注意层类似于编码器模块中的自注意层，但有以下不同之处：键矩阵 $K$ 和值矩阵 $V$ 来自编码器模块，查询矩阵 $Q$ 是从上一层推导出来的。</p>
<p>请注意，前面的过程对于每个单词的位置是不变的，这意味着自注意力层缺乏捕获句子中单词位置信息的能力。然而，语言中句子的顺序性质要求我们将位置信息合并到我们的编码中。为了解决这个问题并允许获得单词的最终输入向量，在原始输入嵌入中添加了一个维度为 $d_{model}$ 的位置编码。具体来说，该位置使用以下等式进行编码：</p>
<p>$\mathrm{PE}(pos,2i)=\sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \tag{2}$$</p>
<script type="math/tex; mode=display">\mathrm{PE}(pos,2i+1)=\cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \tag{3}</script><p>其中 $pos$ 表示单词在句子中的位置，$i$ 表示位置编码的当前维度。 这样，位置编码的每个元素都对应一个正弦曲线，它允许 Transformer 模型学习关注相对位置，并在推理过程中外推到更长的序列长度。 除了 vanilla Transformer 中的固定位置编码外，学习位置编码和相对位置编码也被用于各种模型。</p>
<p><strong>多头机制</strong><br>多头注意力是一种可用于提升普通自注意力层性能的机制。请注意，对于给定的参考词，我们通常希望在浏览句子时关注其他几个词，单头自注意力层限制了我们专注于一个或多个特定位置而不会同时影响对其他同等重要位置注意力的能力。这是通过给予注意力层不同的表示子空间来实现的。具体来说，不同的head使用不同的query、key和value矩阵，这些矩阵由于随机初始化可以在训练后将输入向量投影到不同的表示子空间。</p>
<p>为了更详细地阐述这一点，给定一个输入向量和头的数量 $h$，输入向量首先被转换为三个不同的向量组：查询组、键组和值组。在每组中，有 $h$ 个维度为 $d_{q’}=d_{k’}=d_{v’}=d_{model}/h=64$ 的向量。然后将来自不同输入的向量打包成三组不同的矩阵：$\{\mathbf Q_i\}_{i=1}^h$, $\{\mathbf K_i\}_{i=1}^ h$ 和 $\{\mathbf V_i\}_{i=1}^h$。 多头注意力机制的流程如下图所示：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathrm{MultiHead}(\mathbf Q^{'},\mathbf K^{'},\mathbf V^{'})=\mathrm{Concat}(head_1,\cdots ,head_h)\mathbf W^o,\\
where head_i=\mathrm{Attention}(\mathbf Q_i,\mathbf K_i,\mathbf V_i)
\end{align*}
\tag{4}</script><p>这里，$\mathbf Q’$（以及类似的 $\mathbf K’$ 和 $\mathbf V’$）是 $\{\mathbf Q_i\}_{i=1}^h$ 和 $\mathbf W^o\in\mathbb R^{d_{model}\times d_{model}}$ 是投影权重。</p>
<h3 id="2-2-Transformer其他重要内容"><a href="#2-2-Transformer其他重要内容" class="headerlink" title="2.2 Transformer其他重要内容"></a>2.2 Transformer其他重要内容</h3><p><strong>前馈网络</strong><br>在每个编码器和解码器的自注意层之后应用前馈网络 (FFN),它由两个线性变换层和其中的一个非线性激活函数组成，可以表示为以下函数：</p>
<script type="math/tex; mode=display">\mathrm{FFN}(\mathbf X)=\mathbf W_2\sigma(\mathbf W_1 \mathbf X) \tag{5}</script><p>其中$\mathbf W_1$和$\mathbf W_2$是两个线性变换层的两个参数矩阵，$\sigma$表示非线性激活函数，如GELU，隐藏层的维度是$d_h=2048$。</p>
<p><strong>编解码中的残差连接</strong><br>如图2所示，在编码器和解码器中的每个子层都添加了一个残差连接，这加强了信息流，以实现更高的性能。 在残差连接之后进行层归一化，这些操作的输出可以描述为：</p>
<script type="math/tex; mode=display">\mathrm{LayerNorm}(\mathbf X+\mathrm{Attention}(\mathbf X)) \tag{6}</script><p>这里 $\mathbf X$ 作为self-attention层的输入，query、key和value矩阵 $\mathbf Q、\mathbf K$ 和 $\mathbf V$ 都是从同一个输入矩阵 $\mathbf X$ 推导出来的。 一种预归一化层的变体（Pre-LN）也被广泛使用， 在残差连接内部和多头注意力或 FFN 之前插入层归一化。 对于归一化层，有几种选择，例如批量归一化 bn。 当特征值急剧变化时，批量归一化在应用于 Transformer 时通常表现较差。 其他一些归一化算法 xu2019understanding 等已经被提出来改进Transformer的训练。</p>
<p><strong>解码器的最后一层</strong><br>解码器的最后堆栈层用于将向量转换回单词，这是通过一个线性层和一个 softmax 层来实现的。线性层将向量投影为具有 $d_{word}$ 维度的 logits 向量，其中 $d_{word}$ 是词汇表中的单词数。然后使用 softmax 层将 logits 向量转换为概率。</p>
<p>用于CV任务时，Transformer大多采用原装 Transformer 的encoder模块，这样的Transformer可以被视为一种新型的特征提取器。与仅关注局部特征的 CNN 相比，Transformer 可以捕获远距离特征，这意味着它可以轻松推导出全局信息。与隐藏状态必须按顺序计算的 RNN 相比，Transformer 的效率更高，因为自注意力层和全连接层的输出可以并行计算并且易于加速。由此，我们可以得出结论，进一步研究在计算机视觉和 NLP 中使用 Transformer 将产生有益的结果。</p>
<h2 id="3，视觉Transformer"><a href="#3，视觉Transformer" class="headerlink" title="3，视觉Transformer"></a>3，视觉Transformer</h2><p>在本节中，我们回顾了基于 Transformer 的模型在计算机视觉中的应用，包括图像分类、高级/中级视觉、低级视觉和视频处理。 我们还简要总结了自注意力机制和模型压缩方法在高效 Transformer 中的应用。</p>
<h3 id="3-1-表征学习的backbone"><a href="#3-1-表征学习的backbone" class="headerlink" title="3.1 表征学习的backbone"></a>3.1 表征学习的backbone</h3><p>受 Transformer 在 NLP 领域取得成功的启发，一些研究人员探索了类似的模型是否可以学习图像的有效表示。鉴于与文本相比，图像涉及更多维度、噪声和冗余模态，因此它们被认为更难用于生成建模。</p>
<p>除了 CNN，Transformer 还可以用作图像分类的骨干网络。 Wu 采用 ResNet 作为常用的基线，并使用视觉Transformer替换最后阶段的卷积。具体来说，他们应用卷积层来提取低级特征，然后将这些特征输入视觉Transformer。对于视觉Transformer，他们使用 <strong>tokenizer</strong> 将像素分组为少量 <strong>visual tokens</strong>，每个代表图像中的语义概念。这些 <strong>visual tokens</strong> 直接用于图像分类，而 Transformers 用于对 token 之间的关系建模。如图4所示，工作可以分为单纯使用Transformer做视觉和结合CNN和Transformer。我们在表2和图6中总结了这些模型的结果，以展示主干的发展。除了监督学习，vision Transformer也探索了自监督学习。</p>
<h4 id="3-1-1-纯Transformer"><a href="#3-1-1-纯Transformer" class="headerlink" title="3.1.1 纯Transformer"></a>3.1.1 纯Transformer</h4><p><strong>ViT-Vision Transformer</strong><br>Vision Transformer (ViT)是一种直接应用于图像块序列的纯Transformer，用于图像分类任务，它尽可能遵循 Transformer 的原始设计，图5展示了 ViT 的框架。</p>
<p>为了处理 2D 图像，图像 $X\in \mathbb{R}^{h\times w \times c}$ 被重塑为一系列扁平的 2D 块 $X_p\in \mathbb{R}^{n\times (p^2 \cdot c)}$，其中 $c$ 是通道数。 $(h,w)$ 是原始图像的分辨率，而 $(p,p)$ 是每个图像块的分辨率。因此，Transformer的有效序列长度为 $n=hw/p^2$。由于Transformer在其所有层中使用恒定宽度，因此可训练的线性投影将每个矢量化路径映射到模型维度 $d$，其输出称为块嵌入。</p>
<p>类似于 BERT 的 $[class]$  token ，可学习的嵌入应用于嵌入块序列，该嵌入的状态用作图像表示。在预训练和微调阶段，分类头都附加在相同的尺寸上。此外，将一维位置嵌入添加到块嵌入中以保留位置信息。值得注意的是，ViT 仅使用标准Transformer的编码器（层归一化位置除外），其输出位于 MLP 头之前。在大多数情况下，ViT 在大型数据集上进行预训练，然后针对较小数据的下游任务进行微调。</p>
<p>当在 ImageNet 等中型数据集上训练时，ViT 产生适度的结果，其准确度比同等规模的 ResNet 低几个百分点。因为 Transformer 缺乏 CNN 固有的一些归纳偏差——例如平移等方差和局部性——它们在训练数据量不足时不能很好地泛化。然而，作者发现在大型数据集（1400 万到 3 亿张图像）上训练模型的效果优于归纳偏差。当以足够的规模进行预训练时，Transformers 在数据点较少的任务上取得了优异的成绩。例如，当在 JFT-300M 数据集上进行预训练时，ViT 在多个图像识别基准上的性能接近甚至超过了最先进的性能。具体来说，它在 ImageNet 上达到了 88.36% 的准确率，在包含 19 个任务的 VTAB 套件上达到了 77.16% 的准确率。</p>
<p>DeiT 通过仅在 ImageNet 数据库上进行训练，提出了一种具有竞争力的无卷积Transformer，称为 Data-efficient image Transformer (DeiT)。视觉Transformer DeiT-B 具有与 ViT-B 相同的架构，并使用 8600 万个参数。通过强大的数据增强，DeiT-B 在没有外部数据的情况下在 ImageNet 上实现了 83.1%（单裁剪评估）的 top-1 准确率。此外，作者观察到使用 CNN teacher 比使用 Transformer 具有更好的性能。具体来说，DeiT-B 可以在基于 token 的蒸馏下达到 top-1 准确率 84.40%。</p>
<p><strong>ViT的变体</strong><br>遵循 ViT 的范例，已经提出了一系列 ViT 变体来提高视觉任务的性能，主要方法包括增强局部性、自注意力提升和架构设计。</p>
<p>原始视觉Transformer擅长捕获块之间的远程依赖关系，但忽略了局部特征提取，因为 2D 块被简单线性投影成向量。最近，研究人员开始关注提高对局部信息的建模能力。 TNT 进一步将块划分为多个子块，并引入了一种新颖的 Transformer-in-Transformer 架构，该架构利用内部 Transformer 块来模拟子块和外部 Transformer 块之间的关系，以进行块级信息交换。 Twins 和 CAT 逐层交替执行局部和全局注意力。 Swin Transformers 在一个窗口内进行局部关注，并为跨窗口连接引入了一种移动窗口分区方法。 Shuffle Transformer 进一步利用空间洗牌操作而不是移位窗口分区来允许跨窗口连接。 RegionViT 从图像中生成区域token 和局部token，局部token通过区域token的注意力接收全局信息。除了本地注意力之外，其他一些工作还提出通过本地特征聚合来提升本地信息，例如 T2T。这些工作展示了视觉Transformer中本地信息交换和全局信息交换的好处。</p>
<p>作为Transformer的关键组成部分，self-attention层提供了图像块之间全局交互的能力。改进self-attention层的计算吸引了很多研究者，DeepViT 提出建立跨头通信来重新生成注意力图，以增加不同层的多样性。KVT 引入了 $k$-NN 注意力以利用图像块的局部性并通过仅计算具有前 $k$ 相似 token 的注意力来忽略噪声 token 。Refiner 探索高维空间中的注意力扩展，并应用卷积来增强注意力图的局部模式。<br>XCiT 跨特征通道而不是 token 执行自注意力计算，这允许高效处理高分辨率图像。<strong>自注意力机制的计算复杂度和注意力精度是未来优化的两个关键点。</strong></p>
<p>正如 CNN 领域所证明的那样，网络架构是一个重要因素。 ViT 的原始架构是相同形状 Transformer 块的简单堆叠。<br>视觉Transformer的新架构设计一直是一个有趣的话题。类似金字塔的架构被许多视觉Transformer模型使用，如 wang2021pyramid 等，包括PVT~, HVT, Swin Transformer 和 PiT。还有其他类型的架构，比如two-stream 和U-net，还研究了神经架构搜索 (NAS) 以搜索更好的Transformer架构。目前vision Transformer的网络设计和NAS主要借鉴了CNN的经验。<strong>未来，我们期待在视觉Transformer领域出现特定的、新颖的架构。</strong></p>
<p>除了上述方法外，还有一些其他方向可以进一步改进视觉Transformer，比如，位置编码，归一化策略，跳跃连接和去除注意力等。</p>
<h4 id="3-1-2-带卷积的Transformer"><a href="#3-1-2-带卷积的Transformer" class="headerlink" title="3.1.2 带卷积的Transformer"></a>3.1.2 带卷积的Transformer</h4><p>尽管视觉Transformer能够捕获输入中的远程依赖性，因此成功应用于各种视觉任务，但Transformer与现有 CNN 之间的性能差距仍然存在，一个主要原因可能是缺乏提取本地信息的能力。除了上面提到的增强局部性的 ViT 变体之外，将 Transformer 与卷积相结合可能是将局部性引入传统 Transformer 的更直接的方法。</p>
<p>有很多工作试图通过卷积来增强传统Transformer块或自注意力层，例如，CPVT 提出了一种条件位置编码 (CPE) 方案，该方案以输入 token 的局部邻域为条件，适用于任意输入大小，以利用卷积进行精细级特征编码。 CvT、CeiT、LocalViT 和 CMT 分析了直接从 NLP 借用 Transformer 架构并将卷积与 Transformer 结合在一起时的潜在缺点，具体来说，每个Transformer块中的前馈网络 (FFN) 与促进相邻 token 之间相关性的卷积层相结合。 LeViT 重新审视了大量有关 CNN 文献中的原理，并将其应用于 Transformer，提出了一种用于快速推理图像分类的混合神经网络。 BoTNet 在 ResNet 的最后三个瓶颈块中用全局自注意力替换了空间卷积，并在实例分割和对象检测任务上显着改进了基线，延迟开销最小。</p>
<p>此外，一些研究人员已经证明，基于 Transformer 的模型可能更难以享受良好的数据拟合能力，换句话说，它们对优化器、超参数的选择很敏感，和训练策略。 Visformer 揭示了具有两种不同训练参数下的 Transformer 和 CNN 之间的差距。第一个是 CNN 的标准设置，即训练时间更短，数据增强仅包含随机裁剪和水平翻转。另一个是 DeiT 中使用的训练设置，即训练时间更长，数据增强更强。xiao2021early 改变了 ViT 的早期视觉处理，将其嵌入stem替换为标准卷积stem，并发现这种变化使 ViT 收敛得更快，并且可以使用 AdamW 或 SGD 而不会显着降低精度。除了这两个作品，graham2021levit 和 guo2021cmt还选择在Transformer的顶部加入卷积stem。</p>
<h4 id="3-1-3-自监督表征学习"><a href="#3-1-3-自监督表征学习" class="headerlink" title="3.1.3 自监督表征学习"></a>3.1.3 自监督表征学习</h4><p><strong>基于生成的方法</strong><br>图像的生成式预训练方法已经存在很长时间了，igpt 重新审视了这一类方法，并将其与自监督方法相结合。之后，几项工作 li2021mst 等被提出来扩展视觉 Transformer 的基于生成的自监督学习。</p>
<p>我们简单介绍一下 iGPT 的机制，这种方法包括预训练阶段和微调阶段。在预训练阶段，探索了自回归和 BERT 目标。为了实现像素预测，采用了序列 Transformer 架构而不是语言 token（如 NLP 中使用的）。当与提前停止结合使用时，预训练可以被认为是一种有利的初始化或正则化。在微调阶段，他们向模型添加了一个小的分类头。这有助于优化分类目标并调整所有权重。</p>
<p>通过 $k$ 均值聚类将图像像素转换为序列数据。给定一个由高维数据 $\mathbf x=(x_1,\cdots,x_n)$ 组成的未标记数据集 ${X}$，他们通过最小化负对数似然来训练模型的数据：</p>
<script type="math/tex; mode=display">\mathrm{L}_{AR}=\mathop{\Bbb{E}}_{\bf x\sim\bf X}[-\log p(\bf x)] \tag{7}</script><p>其中 $p(\mathbf x)$ 是图像数据的概率密度，可以建模为：</p>
<script type="math/tex; mode=display">p(\bf x)=\prod^n_{i=1}p(x_{\pi_i}|x_{\pi_1},\cdots,x_{\pi_{i-1}},\theta) \tag{8}</script><p>这里对 $1\leqslant i \leqslant n$ 采用恒等排列$\pi_i=i$，也称为光栅顺序。 Chen 还考虑了 BERT 目标，它对子序列 $M\subset[1,n]$ 进行采样，使得每个索引 $i$ 独立地具有 0.15 的概率出现在 $M$ 中。 $M$ 称为 BERT mask，模型通过最小化以“unmasked”元素 $x_{[1,n]\backslash M}$ 为条件的“masked”元素 $x_M$ 的负对数似然来训练：</p>
<script type="math/tex; mode=display">L_{BERT}=\mathop{\Bbb E}_{\bf x\sim \bf X}\mathop{\Bbb E}_{M}\sum\limits_{i\in M}[-\log p(x_i|x_{[1,n]\backslash M})] \tag{9}</script><p>在预训练阶段，他们选择 $L_{AR}$ 或 $L_{BERT}$，并最小化预训练数据集的损失。</p>
<p>使用了 GPT-2 公式的 Transformer 解码器块，为了确保在训练 AR 目标时进行适当的调节，Chen 等人将标准上三角mask应用于注意对数的 $n\times n$ 矩阵。 使用 BERT 目标时不需要注意逻辑mask，Chen 在将内容嵌入应用于输入序列后将位置归零。 在 Transformer 最后的层之后，他们应用层范数并从输出中学习投影以对每个序列元素的条件分布进行参数化。 在训练 BERT 时，他们只是忽略 unmasked 位置的逻辑。</p>
<p>在微调阶段，他们对整个序列维度的最后归一化层的输出进行平均池化，以提取每个示例的 $d$ 维特征向量。他们学习从c池化特征到类 logits 的投影，并使用该投影来最小化交叉熵损失。实际应用提供了经验证据，表明交叉熵损失和预训练损失（$L_{AR}$ 或 $L_{BERT}$）的联合目标效果更好。</p>
<p>iGPT 和 ViT 是将 Transformer 应用于视觉任务的两项开创性工作。 iGPT 和 ViT-like 模型的区别主要在于 3 个方面：1）iGPT 的输入是通过聚类像素的一系列调色板，而 ViT 将图像统一划分为多个局部块； 2）iGPT的架构是encoder-decode r框架，而ViT只有 Transformer encoder； 3) iGPT 利用自回归自监督损失进行训练，而 ViT 通过监督图像分类任务进行训练。</p>
<p><strong>基于对比学习的方法</strong><br>目前，对比学习是计算机视觉最流行的自我监督学习方式，对比学习已应用于无监督预训练的视觉Transformer。</p>
<p>mocov3 研究了几个基本组件对训练自监督 ViT 的影响，作者观察到，不稳定性是降低准确性的一个主要问题，这些结果确实是部分失败，当训练变得更稳定时，它们可以得到改善。</p>
<p>他们引入了“MoCo v3”框架，这是对 MoCo 的增量改进。具体来说，作者在随机数据增强下对每张图像进行两次裁剪，它们由两个编码器 $f_q$ 和 $f_k$ 编码，输出向量 $\mathbf q$ 和 $\mathbf k$。直觉上，$\mathbf q$ 的行为类似于“查询”，学习的目标是检索相应的“键”。这被表述为最小化对比损失函数，可以写成：</p>
<script type="math/tex; mode=display">\cal L_q=-\log\frac{\exp(\bf q \cdot\bf k^\top/\tau)}{\exp(\bf q\cdot\bf k^\top/\tau)+\sum_{\bf k^-}\exp(\bf q\cdot\bf k^\top/\tau)} \tag{10}</script><p>这里 $\mathbf k^+$ 是 $f_k$ 在与 $\mathbf q$ 相同图像上的输出，称为 $\mathbf q$ 的正样本，集合 {$\mathbf k^-$} 由 $f_k$ 来自其他图像的输出组成，称为 $\mathbf q$ 的负样本，$\tau$ 是 $l_2$ 归一化 $\mathbf q$, $\mathbf k$ 的温度超参数。 MoCo v3 使用在同一批次中自然共存的键并放弃内存队列，他们发现如果批次足够大（例如 4096），内存队列的收益会递减。通过这种简化，对比损失可以以一种简单的方式实现。编码器 $f_q$ 由主干（例如ViT）、投影头和额外的预测头组成，而编码器 $f_k$ 有主干和投影头，但没有预测头。 $f_k$ 由 $f_q$ 的移动平均值更新，不包括预测头。</p>
<p>MoCo v3 表明不稳定性是训练自监督 ViT 的主要问题，因此他们描述了一个简单的技巧，可以提高各种实验情况下的稳定性。他们观察到没有必要训练块投影层，对于标准的 ViT 块大小，块投影矩阵是完备的或超完备的。在这种情况下，随机投影应该足以保留原始块的信息。但是，该技巧缓解了问题，但没有解决问题。如果学习率太大，模型仍然可能不稳定，并且第一层不太可能是不稳定的根本原因。</p>
<h4 id="3-1-4-讨论"><a href="#3-1-4-讨论" class="headerlink" title="3.1.4 讨论"></a>3.1.4 讨论</h4><p>视觉Transformer的所有组件，包括多头自注意力、多层感知器、跳跃连接、层归一化、位置编码和网络拓扑，在视觉识别中起着关键作用。 如上所述，已经提出了许多工作来提高视觉Transformer的有效性和效率。 从图6的结果可以看出，结合CNN和Transformer可以获得更好的性能，表明它们通过局部连接和全局连接相互补充。 对骨干网络的进一步调研可以带来整个视觉社区的改进，至于vision Transformer的自监督表示学习，我们还需要努力追求NLP领域大规模预训练的成功。</p>
<h3 id="3-2-高-中层的视觉"><a href="#3-2-高-中层的视觉" class="headerlink" title="3.2 高/中层的视觉"></a>3.2 高/中层的视觉</h3><p>最近越来越有兴趣使用Transformer来做高/中级的计算机视觉任务，比如物体检测，车道检测，分割和姿态估计，我们将在本节中回顾这些方法。</p>
<h4 id="3-2-1-生成目标检测"><a href="#3-2-1-生成目标检测" class="headerlink" title="3.2.1 生成目标检测"></a>3.2.1 生成目标检测</h4><p>传统的目标检测器主要建立在 CNN 之上，但基于 Transformer 的目标检测由于其优势能力最近获得了极大的关注。</p>
<p>一些目标检测方法尝试使用Transformer的self-attention机制，然后增强现代检测器的特定模块，例如特征融合模块和预测头。 我们在补充材料中对此进行了讨论。基于Transformer的目标检测方法大致分为两类：基于Transformer的批量预测方法和基于Transformer的backbone方法，如图7中所示。 与基于 CNN 的检测器相比，基于 Transformer 的方法在准确性和运行速度方面都表现出了强大的性能。 表3显示了前面提到的各种基于 Transformer 的对象检测器在 COCO 2012 val 集上的检测结果。</p>
<p><strong>基于Transformer的批量预测方法</strong><br>作为基于 Transformer 的检测方法的先驱，Carion 提出的检测 Transformer（DETR）重新设计了目标检测的框架。 DETR 是一种简单且完全端到端的目标检测器，将目标检测任务视为一个直观的批量预测问题，消除了锚点生成和非最大抑制 (NMS) 后处理等传统人工设计组件。如图8所示，DETR 从 CNN 主干开始，从输入图像中提取特征。为了用位置信息补充图像特征，在将特征送到 Transformer 的编码器-解码器之前，将固定位置编码添加到扁平化特征中。解码器使用来自编码器的嵌入以及 $N$ 个学习的位置编码（对象查询），并产生 $N$ 个输出嵌入。这里 $N$ 是预定义参数，通常大于图像中的对象数。简单的前馈网络 (FFN) 用于计算最终预测，其中包括边界框坐标和类标签以指示特定对象类（或指示不存在对象）。与按顺序计算预测的原始Transformer不同，DETR 并行解码 $N$ 个对象。 DETR 采用二分匹配算法来分配预测对象和真实对象。如等式\ref{eq11} 所示，利用 Hungarian 损失来计算所有匹配对象对的损失函数。</p>
<script type="math/tex; mode=display">\cal L_{Hungarian}(y,\hat y)=\sum\limits^N_{i=1}[-\log\hat p_{\sigma(i)}(c_i)+\mathbb 1_{c_i\neq\emptyset}\cal L_{box}(b_i,\hat b_{\hat\sigma}(i)) ] \tag{11}</script><p>其中 $\hat{\sigma}$ 是最优分配，$c_i$ 和 $\hat{p}_{\hat{\sigma}(i)}(c_{i})$ 是目标类标签和预测标签，$b_i$ 和 $\hat{b}_{\hat{\sigma}}(i)$ 分别是真实框和预测框，$y=\{(c_i, b_i)\}$ 和 $\hat y$ 分别是对象的真值和预测值。DETR 在对象检测方面表现出令人印象深刻的性能，在 COCO 基准测试中提供与流行且成熟的 Faster R-CNN 基线相当的准确性和速度。</p>
<p>DETR 是基于 Transformer 的目标检测框架的全新设计，使社区能够开发完全端到端的检测器，然而，vanilla DETR 带来了一些挑战，特别是更长的训练时间和小物体检测性能差。为了应对这些挑战，ddetr 提出了 Deformable DETR，它已成为一种显着提高检测性能的流行方法。可变形注意模块关注参考点周围的一小组关键位置，而不是像 Transformer 中的原始多头注意机制那样查看图像特征图上的所有空间位置。这种方法显着降低了计算复杂度，并在快速收敛方面带来了好处，更重要的是，可变形注意力模块可以很容易地应用于融合多尺度特征。可变形 DETR 的性能优于 DETR，训练成本降低 10 倍，推理速度提高 1.6 倍，并且通过使用迭代边框细化方法和两阶段方案，Deformable DETR 可以进一步提高检测性能。</p>
<p>也有几种方法来处理原始 DETR 收敛慢的问题。例如， sun2020rethinking 调查了为什么DETR模型收敛慢，发现这主要是 Transformer decoder 中的 cross-attention 模块造成的。为了解决这个问题，提出了 DETR 的仅编码器版本，在检测精度和训练收敛性方面取得了相当大的改进。此外，设计了一种新的二分匹配方案以提高训练稳定性和加快收敛速度​​，并提出了两种基于 Transformer 的批量预测模型，即 TSP-FCOS 和 TSP-RCNN，以改进具有特征金字塔的编码器版本 DETR。与原始 DETR 模型相比，这些新模型实现了更好的性能，gao2021fast 提出了空间调制共同注意 (SMCA) 机制，通过将共同注意响应限制在最初估计的边界框附近的位置来加速收敛。通过将所提出的 SMCA 模块集成到 DETR 中，在可比的推理成本下，可以用大约 10$\times$ 更少的训练周期获得类似的 mAP。</p>
<p>鉴于与 DETR 相关的高计算复杂性，zheng2020end 提出了一种自适应聚类Transformer (ACT) 来降低预训练 DETR 的计算成本。 ACT 使用局部敏感性哈希 (LSH) 方法自适应地对查询特征进行聚类，并将注意力输出广播到由所选原型表示的查询。 ACT 用于替换预训练 DETR 模型的自注意力模块，无需任何重新训练。这种方法显着降低了计算成本，同时精度略有下降，通过使用多任务知识蒸馏 (MTKD) 方法可以进一步减少性能下降，该方法利用原始 Transformer 通过几个 epoch 的微调来提取 ACT 模块。yao2021efficient 指出 DETR 中的随机初始化是导致解码器层数多、收敛速度慢的主要原因。为此，他们提出了 Efficient DETR，通过一个额外的区域提议网络将密集先验合并到检测管道中。更好的初始化使他们能够仅使用一个解码器层而不是六层，以通过更紧凑的网络实现具有竞争力的性能。</p>
<p><strong>基于Transformer的检测backbone</strong><br>与 DETR 通过 Transformer 将目标检测重新设计为一组预测任务不同，beal2020toward 提出利用 Transformer 作为常见检测框架的主干，例如 Faster R-CNN。 输入图像被分成几个块并送入视觉 Transformer，其输出嵌入特征根据空间信息重新组织，然后通过检测头获得最终结果。 一个巨大的预训练 Transformer 主干可以为 ViT-FRCNN 带来好处。 还有很多方法可以探索通用的视觉 Transformer 主干设计并将这些主干转移到传统的检测框架，如 RetinaNet 和 Cascade R-CNN。 例如，Swin Transformer 在 ResNet-50 主干上获得了大约 4 box AP 增益，各种检测框架具有相似的 FLOP。</p>
<p><strong>基于Transformer的目标检测预训练</strong><br>受 NLP 中预训练 Transformer 方案的启发，人们提出了几种方法来探索基于 Transformer 的目标检测模型的不同预训练方案。 dai2020detr 提出了目标检测的无监督预训练（UP-DETR），具体来说，提出了一种名为随机查询块检测的新型无监督任务来预训练 DETR 模型。通过这种无监督的预训练方案，UP-DETR 在相对较小的数据集（PASCAL VOC）上显着提高了检测精度，在具有足够训练数据的 COCO 基准上，UP-DETR 仍然优于 DETR，证明了无监督预训练方案的有效性。</p>
<p>fang2021you 探索了如何将在 ImageNet 上预训练的纯 ViT 结构迁移到更具挑战性的目标检测任务，并提出了 YOLOS 检测器。为了应对目标检测任务，所提出的 YOLOS 首先删除 ViT 中的分类 token 并附加可学习的检测 token。此外，二分匹配损失用于对对象进行批量预测。通过这种在 ImageNet 数据集上的简单预训练方案，所提出的 YOLOS 在 COCO 基准测试中显示出具有竞争力的目标检测性能。</p>
<h4 id="3-2-2-分割"><a href="#3-2-2-分割" class="headerlink" title="3.2.2 分割"></a>3.2.2 分割</h4><p>分割是计算机视觉界的一个重要课题，广义上包括全景分割、实例分割和语义分割等。 Vision Transformer 在分割领域也展现出了令人瞩目的潜力。</p>
<p><strong>全景分割 Transformer</strong><br>DETR 可以自然地扩展到全景分割任务，并通过在解码器上附加一个mask头来获得有竞争力的结果。 wang2020max 提出 Max-DeepLab，直接用 mask Transformer 预测全景分割结果，不涉及框检测等子任务。 与 DETR 类似，Max-DeepLab 以端到端的方式简化了全景分割任务，并直接预测一组不重叠的蒙版和相应的标签。 模型训练是使用全景质量 (PQ) 样式损失执行的，但与之前将Transformer堆叠在 CNN 主干之上的方法不同，Max-DeepLab 采用双路径框架，将 CNN 和Transformer结合起来。</p>
<p><strong>实例分割 Transformer</strong><br>VisTR 是一种基于 Transformer 的视频实例分割模型，由 wang2020end 提出，用于根据一系列输入图像生成实例预测结果，提出了一种匹配实例序列的策略，将预测结果对齐到真值。为了获得每个实例的mask序列，VisTR 利用实例序列分割模块从多个帧中累积mask特征，并使用 3D CNN 分割mask序列。hu2021istr 提出了一个实例分割 Transformer (ISTR) 来预测低维mask嵌入，并将它们与 ground truth 相匹配以获得批量损失。 不同于现有自上而下和自下而上的框架逻辑，ISTR 使用循环细化策略进行检测和分割。yang2021association 研究了如何实现更好、更高效的嵌入学习，以解决具有挑战性的多对象场景下的半监督视频对象分割问题。wu2021fully 等论文也讨论了使用Transformer处理分割任务。</p>
<p><strong>语义分割 Transformer</strong><br>zheng2021 提出了一种基于Transformer的语义分割网络（SETR）, SETR 使用类似于 ViT 的编码器作为编码器从输入图像中提取特征，采用多级特征聚合模块来执行逐像素分割。strudel2021segmenter 引入了 Segmenter，它依赖于图像块对应的输出嵌入，并通过逐点线性解码器或带掩码的 Transformer 解码器获得类标签。xie2021segformer 提出了一个简单、高效但功能强大的语义分割框架，它将 Transformer 与轻量级多层感知 (MLP) 解码器统一起来，输出多尺度特征并避免复杂的解码器。</p>
<p><strong>用于医学图像分割的 Transformer</strong><br>cao2021swin 提出了一种用于医学图像分割的类 Unet 纯 Transformer 网络，通过将标记化的图像块送到基于 Transformer 的 U 形编码-解码器架构中，该架构具有局部-全局语义特征的跳跃连接学习。 valanarasu2021medical 探索了基于 Transformer 的解决方案，研究了使用基于 Transformer 的网络架构进行医学图像分割任务的可行性，并提出了一种门坐标轴注意力模型，该模型通过在自注意力模块中引入额外的控制机制来扩展现有架构。Cell-DETR，基于DETR全景分割模型，尝试使用Transformer进行细胞实例分割，它添加了跳跃连接，在分割头中的主干 CNN 和 CNN 解码器之间桥接特征，以增强特征融合，Cell-DETR 实现了从显微镜图像进行细胞实例分割的最先进性能。</p>
<h4 id="3-2-3-姿态估计"><a href="#3-2-3-姿态估计" class="headerlink" title="3.2.3 姿态估计"></a>3.2.3 姿态估计</h4><p>人体姿势和手部姿势估计是引起研究界极大兴趣的基础主题，关节姿态估计类似于结构化预测任务，旨在从输入的 RGB/D 图像中预测关节坐标或网格顶点。 在这里，我们讨论一些方法如huang2020hand等，探索如何利用 Transformer 对人体姿势和手部姿势的全局结构信息进行建模。</p>
<p><strong>用于手势估计的 Transformer</strong><br>huang2020hand 提出了一种基于Transformer的网络，用于从点集进行 3D 手姿势估计。编码器首先利用 PointNet 从输入点云中提取逐点特征，然后采用标准的多头自注意力模块来生成嵌入。为了向解码器公开更多与全局姿势相关的信息，使用 PointNet++ 等特征提取器来提取手部关节特征，然后将其作为位置编码输入解码器。同样，huang2020hot 提出了 HOT-Net（手对象变换网络的简称）用于 3D 手对象姿态估计。与前面使用 Transformer 直接从输入点云预测 3D 手部姿势的方法不同，HOT-Net 使用 ResNet 生成初始 2D 手部对象姿势，然后将其输入 Transformer 以预测 3D 手部对象姿势，因此，谱图卷积网络用于为编码器提取输入嵌入。hampali2021handsformer 提出在给定单色图像的情况下估计两只手的 3D 姿势。具体来说，双手关节的一组潜在 2D 位置的外观和空间编码被输入到一个Transformer中，注意力机制被用来挑选出关节的正确配置并输出双手的 3D 姿势。</p>
<p><strong>用于人体姿势估计的 Transformer</strong><br>lin2020end 提出了一种网格Transformer (METRO)，用于从单个 RGB 图像预测 3D 人体姿势和网格。 METRO 通过 CNN 提取图像特征，然后通过将模板人体网格连接到图像特征来执行位置编码，提出了一种具有渐进降维功能的多层Transformer编码器，以逐渐降低嵌入维数，最终生成人体关节和网格顶点的 3D 坐标。为了鼓励学习人体关节之间的非局部关系，METRO 在训练期间随机屏蔽了一些输入查询。transpose 等基于 Transformer 架构和低级卷积块构建了一个名为 TransPose 的可解释模型，Transformer 中内置的注意力层可以捕获关键点之间的远程空间关系，并解释预测的关键点位置高度依赖的依赖关系。li2021tokenpose 提出了一种基于 Token 表示的人体姿态估计新方法（TokenPose）。每个关键点都被明确地嵌入为一个标记，以同时从图像中学习约束关系和外观线索。mao2021tfpose 提出了一种人体姿势估计框架，以基于回归的方式解决了该任务，他们将姿态估计任务转化为一个序列预测问题，并通过 Transformer 求解，从而绕过了基于热图的姿态估计器的缺点。jiang2021skeletor 提出了一种新颖的基于 Transformer 的网络，它可以以无监督的方式学习姿势和运动的分布，而不是跟踪身体部位并尝试暂时平滑它们。该方法克服了检测中的不准确性并纠正了部分或整个骨架损坏。mazzia2021action 引入了 Action Transformer (AcT) 来利用小时间窗口上的 2D 姿势表示，并提供低延迟解决方案以实现准确有效的实时性能。hao2021test 提议在给定一组人的测试图像的情况下个性化人体姿势估计器，而不使用任何手动注释。该方法在测试期间调整姿势估计器以利用特定于人的信息，并使用 Transformer 模型在自监督关键点和监督关键点之间建立转换。</p>
<h4 id="3-2-4-其他任务"><a href="#3-2-4-其他任务" class="headerlink" title="3.2.4 其他任务"></a>3.2.4 其他任务</h4><p>还有很多不同的高级/中级视觉任务探索了视觉Transformer的使用以获得更好的性能，我们简要回顾以下几项任务。</p>
<p><strong>行人检测</strong><br>由于在遮挡和人群场景中物体分布非常密集，当普通检测网络应用于行人检测任务时，通常需要额外的分析和适应。当直接将 DETR 或 Deformable DETR 应用于行人检测任务时，lin2020detr 揭示了解码器中稀疏统一查询和弱注意力场导致性能下降。为了减轻这些缺点，作者提出了行人端到端检测器（PED），它采用了一种称为密集查询和校正注意域 (DQRF) 的新解码器来支持密集查询并减轻查询的嘈杂或狭窄注意域。他们还提出了 V-Match，它通过充分利用可见注释实现了额外的性能改进。</p>
<p><strong>车道线检测</strong><br>基于PolyLaneNet，liu2020end 提出了一种叫做LSTR的方法，它通过使用 Transformer 网络学习全局上下文来提高曲线车道检测的性能。与 PolyLaneNet 类似，LSTR 将车道检测视为用多项式拟合车道的任务，并使用神经网络来预测多项式的参数，为了捕捉车道和全局上下文的细长结构，LSTR 在架构中引入了一个Transformer网络，这使得能够处理由 CNN 提取的低级特征，此外，LSTR 使用 Hungarian 损失来优化网络参数。正如论文中所展示的，LSTR 优于 PolyLaneNet，使用少 5 倍的参数，精度高 2.82%，FPS 高 3.65 倍。Transformer 网络、CNN 和 Hungarian Loss 的结合最终形成了一个精确、快速和微小的车道检测框架。考虑到整个球道线一般呈拉长形且射程远，liu2021condlanenenet 利用Transformer编码器结构来更有效地提取上下文特征，这种 Transformer 编码器结构大大提高了建议点的检测，它依赖于上下文特征和全局信息，尤其是在骨干网络是一个小模型的情况下。</p>
<p><strong>场景图</strong><br>场景图是一种场景的结构化表示，可以清楚地表达场景中的对象、属性以及对象之间的关系。 为了生成场景图，大多数现有方法首先提取基于图像的对象表示，然后在它们之间进行消息传播。 Graph R-CNN 利用自注意力来整合来自图中相邻节点的上下文信息。 最近，Sharifzadeh2020 在提取的对象嵌入上使用了Transformer，Sharifzadeh2021 提出了一个名为 Texema 的新管道，并采用预训练的 Text-to-Text 传输Transformer （T5）从文本输入中构建场景图，利用它们来改进关系推理模块，T5 模型使 Texema 能够利用文本中的知识。</p>
<p><strong>追踪</strong><br>一些研究人员还探索在基于模板的判别跟踪器中使用 Transformer 编码器-解码器架构，例如 TMT、TrTr 和 TransT。所有这些工作都使用类似 Siamese 的跟踪管道来进行视频对象跟踪，并利用编码器-解码器网络为全局和丰富的上下文相互依赖性替换显式互相关操作。具体来说，Transformer编码器和解码器分别分配给模板分支和搜索分支，此外，Sun 提出了 TransTrack，这是一个在线联合检测和跟踪管道。 它利用查询键机制来跟踪预先存在的对象，并将一组学习对象查询引入管道以检测新出现的对象。</p>
<p><strong>重识别</strong><br>He 提出 TransReID 以研究纯 Transformer 在对象重识别 (ReID) 领域的应用。在将 Transformer 网络引入对象 ReID 时，TransReID 重叠的切片来保留块周围的局部相邻结构，并引入 2D 双线性插值以帮助处理任何给定的输入分辨率。通过 Transformer 模块和损失函数，提出了一个强大的基线来实现与基于 CNN 的框架相当的性能。此外，拼图块模块 (JPM) 旨在促进对象的扰动不变和鲁棒特征表示，并引入辅助信息嵌入 (SIE) 来编码辅助信息。最终框架 TransReID 在人员和车辆 ReID 基准测试中均实现了最先进的性能。Liu2021reid3view 和 Zhang2021stt 都提供了将 Transformer 网络引入基于视频的行人 Re-ID 的解决方案。同样，他们都使用分离的Transformer网络来细化空间和时间特征，然后使用交叉视图Transformer来聚合多视图特征。</p>
<p><strong>点云学习</strong><br>最近还出现了许多探索用于点云学习的Transformer架构的其他作品，如 engel2020point 等。 例如，guo2020pct 提出了一个新颖的框架，用更合适的 offset-attention 模块替换原来的 self-attention 模块，其中包括隐式 Laplace 算子和归一化细化。 此外，zhao2020point 设计了一种名为 Point Transformer 的新型 Transformer 架构。 所提出的自注意力层对点集的排列具有不变性，使其适用于点集处理任务，Point Transformer 对来自 3D 点云的语义分割任务表现出强大的性能。</p>
<h4 id="3-2-5-讨论"><a href="#3-2-5-讨论" class="headerlink" title="3.2.5 讨论"></a>3.2.5 讨论</h4><p>正如前面部分所讨论的，Transformer 在几个高级任务上表现出了强大的性能，包括检测、分割和姿态估计。<br>在将 Transformer 用于高级任务之前需要解决的关键问题涉及<strong>输入嵌入</strong>、<strong>位置编码</strong>和<strong>预测损失</strong>。一些方法提出从不同角度改进 self-attention 模块，例如deformable attention、adaptive clustering 和point Transformer。尽管如此，将 Transformer 用于高级视觉任务的探索仍处于初级阶段，因此进一步的研究可能会证明是有益的。例如，是否有必要在 Transformer 之前使用 CNN 和 PointNet 等特征提取模块以获得更好的性能？ vision Transformer如何像BERT、GPT-3那样在NLP领域充分利用大规模预训练数据集？是否可以预训练单个 Transformer 模型并针对不同的下游任务对其进行微调，而只需几个 epoch 的微调？如何通过结合特定任务的先验知识来设计更强大的架构？之前的几项工作已经对上述主题进行了初步讨论，我们希望进行更多的进一步研究，以探索更强大的高级视觉Transformer。</p>
<h3 id="3-3-低层视觉"><a href="#3-3-低层视觉" class="headerlink" title="3.3 低层视觉"></a>3.3 低层视觉</h3><p>很少有工作将Transformer应用于低级视觉领域，例如图像超分辨率和生成。 这些任务通常将图像作为输出（例如高分辨率或去噪图像），这比分类、分割和检测等输出为标签或框的高级视觉任务更具挑战性。</p>
<h4 id="3-3-1-图像生成"><a href="#3-3-1-图像生成" class="headerlink" title="3.3.1 图像生成"></a>3.3.1 图像生成</h4><p>将 Transformer 模型应用于图像生成任务的一种简单而有效的方法是直接将架构从 CNN 更改为 Transformer，如图9 (a) 所示。jiang2021transgan 提出了TransGAN，它使用Transformer架构构建GAN。由于难以逐像素生成高分辨率图像，因此通过在不同阶段逐渐增加特征图分辨率来利用内存友好型生成器。相应地，设计了一个多尺度鉴别器来处理不同阶段不同大小的输入。引入了各种训练方法，包括网格自注意力、数据增强、相对位置编码和改进的归一化，以稳定训练并提高其性能。在各种基准数据集上的实验证明了基于 Transformer 的 GAN 模型在图像生成任务中的有效性和潜力。 lee2021vitgan 提出了 ViTGAN，它在生成器和鉴别器中引入了几种技术来稳定训练过程和收敛，为自我注意模块引入欧几里德距离，以加强 Transformer 鉴别器的 Lipschitzness，提出了自调制层范数和隐式神经表示来增强生成器的训练。因此，ViTGAN 是第一个证明基于 Transformer 的 GAN 可以达到与最先进的基于 CNN 的 GAN 相当的性能的作品。</p>
<p>parmar2018image 提出了 Image Transformer，迈出了泛化 Transformer 模型的第一步，以自动回归的方式制定图像翻译和生成任务。 Image Transformer 由两部分组成：用于提取图像表示的编码器和用于生成像素的解码器。对于值为 $0-255$ 的每个像素，学习 $256 \times d$ 维嵌入，用于将每个值编码为 $d$ 维向量，该向量作为输入馈入编码器，编码器和解码器采用与 vaswani2017attention 中相同的架构。每个输出像素 $q’$ 是通过计算输入像素 $q$ 和先前生成的像素 $m_1,m_2,…$ 之间的自注意力生成的，位置嵌入为 $p_1,p_2,…$。对于图像条件生成，例如超分辨率和修复，使用编码器-解码器架构，其中编码器的输入是低分辨率或损坏的图像。对于无条件和类条件生成，即图像噪声，只有解码器用于输入噪声向量，由于解码器的输入是先前生成的像素（在生成高分辨率图像时涉及高计算成本），因此提出了局部自注意方案。该方案仅使用最接近的生成像素作为解码器的输入，使 Image Transformer 能够在图像生成和翻译任务上实现与基于 CNN 的模型相当的性能，证明了基于 Transformer 的模型在低级视觉任务上的有效性。</p>
<p>由于Transformer模型难以直接生成高分辨率图像，esser2021taming 提出了Taming Transformer，由两部分组成：VQGAN 和 Transformer。 VQGAN 是 oord2017neural 的变体，它使用鉴别器和感知损失来提高视觉质量。通过 VQGAN，图像可以用一系列上下文丰富的离散向量表示，因此这些向量可以很容易地通过自回归方式由 Transformer 模型预测。 Transformer 模型可以学习远程交互以生成高分辨率图像。因此，所提出的 Taming Transformer 在各种图像合成任务上取得了最先进的结果。</p>
<p>除了图像生成，dalle 还提出了用于文本到图像生成的Transformer 模型，它根据给定的说明合成图像。整个框架包括两个阶段。在第一阶段，使用离散 VAE 来学习视觉码本，在第二阶段，文本通过 BPE-encode 解码，相应的图像通过第一阶段学习的 dVAE 解码。然后使用自回归Transformer来学习编码文本和图像之间的先验，在推理过程中，图像的标记由Transformer预测并由学习的解码器解码。引入 CLIP 模型对生成的样本进行排序。文本到图像生成任务的实验证明了所提出模型的强大能力。请注意，我们的调查主要集中在纯视觉任务上，我们不包括图9中的 DALL$\cdot$E 框架。</p>
<h4 id="3-3-2-图像处理"><a href="#3-3-2-图像处理" class="headerlink" title="3.3.2 图像处理"></a>3.3.2 图像处理</h4><p>最近的一些工作避免使用每个像素作为 Transformer 模型的输入，而是使用块（像素集）作为输入。 例如，yang2020learning 提出了 TTSR，在基于参考的图像超分辨率问题中使用Transformer架构，它旨在将相关纹理从参考图像转移到低分辨率图像，以一张低分辨率图像和参考图像分别作为query $\mathbf Q$和key $\mathbf K$，用下式计算相对量：</p>
<script type="math/tex; mode=display">r_{i,j}=\bigg\langle \frac{\mathbf q_i}{\|\mathbf q_i\|},\frac{\mathbf k_i}{\|\mathbf k_i\|} \bigg\rangle \tag{12}</script><p>提出了一种硬注意模块，根据参考图像选择高分辨率特征$\mathbf V$，从而可以利用相关性来匹配低分辨率图像。 硬注意模块计算如下：</p>
<script type="math/tex; mode=display">h_i=\arg\max_j r_{i,j} \tag{13}</script><p>最相关的参考块是 $\mathbf t_i = \mathbf v_{h_i}$，其中 $\mathbf T$ 中的 $\mathbf t_i$ 是转移的特征， 然后使用软注意模块将 $\mathbf V$ 转移到低分辨率特征，来自高分辨率纹理图像的传输特征和低分辨率特征用于生成低分辨率图像的输出特征，通过利用基于Transformer的架构，TTSR 可以在超分辨率任务中成功地将纹理信息从高分辨率参考图像传输到低分辨率图像。</p>
<p>不同于以往在单一任务上使用Transformer模型的方法，chen2020pre 提出了Image Processing Transformer (IPT)，它通过使用大型预训练数据集充分利用了Transformer的优势，在多个图像处理任务中实现了最先进的性能，包括超分辨率、去噪和去雨。如图10所示，IPT由多个头、一个编码器、一个解码器和多个尾组成。针对不同的图像处理任务引入了多头、多尾结构和任务嵌入，这些特征被分成块，这些块被馈送到编码器-解码器架构中，在此之后，输出被重塑为具有相同大小的特征。鉴于在大数据集上预训练Transformer模型的优势，IPT使用ImageNet数据集进行预训练。具体来说，来自该数据集的图像通过手动添加噪声、雨纹或下采样来降级，以生成损坏的图像，退化图像用作 IPT 的输入，而原始图像用作输出的优化目标，还引入了一种自我监督的方法来增强 IPT 模型的泛化能力。训练模型后，通过使用相应的头、尾和任务嵌入对每个任务进行微调。 IPT 很大程度上实现了图像处理任务的性能提升（例如在图像去噪任务中提升了 2 dB），展示了将基于 Transformer 的模型应用于低级视觉领域的巨大潜力。</p>
<p>除了单一图像生成，wang2020sceneformer 还提出了 SceneFormer 在 3D 室内场景生成中利用 Transformer。通过将场景视为一系列对象，Transformer 解码器可用于预测一系列对象及其位置、类别和大小。这使得 SceneFormer 在用户研究中优于传统的基于 CNN 的方法。</p>
<p>应该注意的是，iGPT 是在类似修复的任务上预训练的，由于 iGPT 主要关注图像分类任务的微调性能，因此我们将这项工作更像是对使用 Transformer 的图像分类任务的尝试，而不是低级视觉任务。</p>
<p>总之，与分类和检测任务不同，图像生成和处理的输出是图像，图11 说明了在低级视觉中使用 Transformer 。在图像处理任务中，图像首先被编码成一系列标记或块，然后 Transformer 编码器使用该序列作为输入，从而使 Transformer 解码器成功生成所需的图像。在图像生成任务中，基于 GAN 的模型直接学习解码器生成块以通过线性投影输出图像，而基于Transformer的模型训练自动编码器学习图像的码本并使用自回归Transformer模型来预测编码的 token 。未来研究的一个有意义的方向是为不同的图像处理任务设计合适的架构。</p>
<h3 id="3-4-视频处理"><a href="#3-4-视频处理" class="headerlink" title="3.4 视频处理"></a>3.4 视频处理</h3><p>Transformer 在基于序列的任务上表现出色，尤其是在 NLP 任务上。 在计算机视觉~（特别是视频任务）中，时空维度信息受到青睐，催生了Transformer在很多视频任务中的应用，比如帧合成 liu2020convTransformer，动作识别 girdhar2019video，以及视频检索 liu2017two。</p>
<h4 id="3-4-1-高层视频处理"><a href="#3-4-1-高层视频处理" class="headerlink" title="3.4.1 高层视频处理"></a>3.4.1 高层视频处理</h4><p><strong>视频动作识别</strong><br>顾名思义，视频人类动作任务涉及识别和定位视频中的人类动作，上下文（例如其他人和物体）在识别人类行为方面起着至关重要的作用。 Rohit 提出了动作Transformer来模拟感兴趣的人与周围环境之间的潜在关系，具体来说，I3D 被用作提取高级特征图的主干，从中间特征图中提取（使用 RoI 池化）的特征被视为查询（Q），而键（K）和值（V）是从中间特征计算的，自我注意机制应用于三个组件，并输出分类和回归预测。 lohit2019temporal 提出了一个可解释的可微分模块，称为时间变换网络，以减少类内方差并增加类间方差。此外，Fayyaz 和 Gall 提出了一种时间Transformer，在弱监督设置下执行动作识别任务。除了人类动作识别，Transformer 还被用于群体活动识别。 Gavrilyuk 提出了一种 actor-Transformer 架构来进行学习和表示，使用 2D 和 3D 网络生成的静态和动态表示作为输入，Transformer的输出是预测的活动。</p>
<p><strong>视频检索</strong><br>基于内容的视频检索的关键是找到视频之间的相似性，shaotemporal 仅利用视频级别的特征图像来克服相关挑战，建议使用Transformer对远程语义依赖性进行建模，他们还引入了有监督的对比学习策略来执行硬负样本挖掘，在基准数据集上使用这种方法的结果证明了它的性能和速度优势。此外，gabeur2020multi 提出了一个多模态Transformer来学习不同的跨模态线索以表示视频。</p>
<p><strong>视频对象检测</strong><br>要检测视频中的对象，需要全局和局部信息，Chen 引入了内存增强的全局-局部聚合~(MEGA) 以捕获更多内容，代表性特征增强了整体性能并解决了无效和不足的问题。此外，yin2020lidar 提出了一种时空Transformer来聚合空间和时间信息，与另一个空间特征编码组件一起，这两个组件在 3D 视频对象检测任务上表现良好。</p>
<p><strong>多任务学习</strong><br>未修剪的视频通常包含许多与目标任务无关的帧，因此，挖掘相关信息并丢弃冗余信息至关重要。为了提取此类信息，Seong 提出了视频多任务Transformer网络，它处理未修剪视频的多任务学习，对于 CoVieW 数据集，任务是场景识别、动作识别和重要性分数预测。 ImageNet 和 Places365 上的两个预训练网络提取场景特征和对象特征。堆叠多任务Transformer以实现特征融合，利用类转换矩阵。</p>
<h4 id="3-4-2-低层视频处理"><a href="#3-4-2-低层视频处理" class="headerlink" title="3.4.2 低层视频处理"></a>3.4.2 低层视频处理</h4><p><strong>帧/视频合成</strong><br>帧合成任务涉及合成两个连续帧之间或帧序列之后的帧，而视频合成任务涉及合成视频。Liu 提出了 ConvTransformer，它由五个部分组成：特征嵌入、位置编码、编码器、查询解码器和合成前馈网络。与基于 LSTM 的作品相比，ConvTransformer 以更可并行化的架构取得了更好的结果。 schatz2020a 提出了另一种基于 Transformer 的方法，它使用循环 Transformer 网络从新颖的视角合成人类行为。</p>
<p><strong>视频修复</strong><br>视频修复任务涉及完成帧内任何缺失的区域，这是具有挑战性的，因为它需要合并空间和时间维度的信息。 Zeng 提出了一个时空Transformer网络，它使用所有输入帧作为输入并并行填充它们，时空对抗损失用于优化 Transformer 网络。</p>
<h4 id="3-4-3-讨论"><a href="#3-4-3-讨论" class="headerlink" title="3.4.3 讨论"></a>3.4.3 讨论</h4><p>与图像相比，视频有一个额外的维度来编码时间信息，利用空间和时间信息有助于更好地理解视频，得益于 Transformer 的关系建模能力，视频处理任务通过同时挖掘空间和时间信息得到了改进。 然而，由于视频数据的高度复杂性和冗余性，如何高效准确地对空间和时间关系进行建模仍然是一个悬而未决的问题。</p>
<h3 id="3-5-多模式任务"><a href="#3-5-多模式任务" class="headerlink" title="3.5 多模式任务"></a>3.5 多模式任务</h3><p>由于 Transformer 在基于文本的 NLP 任务中的成功，许多工作研究热衷于利用其处理多模态任务（例如，视频-文本、图像-文本和音频-文本）的潜力。这方面的一个例子是 VideoBERT，它使用基于 CNN 的模块来预处理视频以获得表示 token 。然后，Transformer编码器在这些标记上进行训练，以学习下游任务（例如视频字幕）的视频文本表示。其他一些示例包括 VisualBERT 和 VL-BERT，它们采用单流统一Transformer来捕获视觉元素和图像文本关系，用于视觉问答 (VQA) 等下游任务和视觉常识推理（VCR）。此外，SpeechBERT 等几项研究探索了使用Transformer编码器对音频和文本对进行编码以处理语音问答 (SQA) 等自动文本任务的可能性。</p>
<p>除了上述开创性的多模态Transformer之外，对比语言-图像预训练 (CLIP) 以自然语言作为监督来学习更有效的图像表示。 CLIP 联合训练文本编码器和图像编码器来预测相应的训练文本-图像对，其文本编码器是一个标准的Transformer，带有掩码的自注意力，用于保留预训练语言模型的初始化能力。对于图像编码器，CLIP 考虑了两种类型的架构，ResNet 和 Vision Transformer，在一个包含从互联网收集的 4 亿对（图像、文本）对的新数据集上进行训练，更具体地，给定一批 $N$（图像，文本）个数据对，CLIP 联合学习文本和图像嵌入，以最大化 $N$ 对匹配嵌入的余弦相似度，同时最小化 $N^{2} - N$ 不正确匹配嵌入。在零样本传输中，CLIP 展示了惊人的零样本分类性能，在不使用任何 ImageNet 训练标签的情况下，在 ImageNet-1K 数据集上实现了 $76.2\%$ top-1 精度。具体来说，在推理时，CLIP 的文本编码器首先计算所有 ImageNet 标签的特征嵌入，然后图像编码器计算所有图像的嵌入。通过计算文本和图像嵌入的余弦相似度，得分最高的文本-图像对应该是图像及其对应的标签。在 30 个各种 CV 基准上的进一步实验表明了 CLIP 的零样本迁移能力和 CLIP 学习到的特征多样性。</p>
<p>CLIP 根据文本描述映射图像，而另一项工作 DALL-E 合成输入文本中描述的类别的新图像。与 GPT-3 类似，DALL-E 是一种多模式Transformer，具有 120 亿个模型参数，在 330 万个文本图像对的数据集上进行自回归训练。更具体地说，为了训练 DALL-E，使用了两阶段训练程序，其中在第 1 阶段，使用离散变分自动编码器将 256$\times$ 256 个 RGB 图像压缩为 32$\times$32 个图像标记，然后在第 2 阶段，一个自回归Transformer被训练来模拟图像和文本标记的联合分布。实验结果表明，DALL-E 可以从头开始生成各种风格的图像，包括逼真图像、卡通和表情符号，或者扩展现有图像，同时仍然匹配文本中的描述。随后，Ding 提出了 CogView，这是一个类似 DALL-E 的带有 VQ-VAE tokenizer 的 Transformer，但支持中文文本输入。他们声称 CogView 优于 DALL-E 和以前的基于 GAN 的方法，而且与 DALL-E 不同的是，CogView 不需要额外的 CLIP 模型来重新排列从 Transformer $i.e.$ DALL-E 中提取的样本。</p>
<p>最近，提出了一种 Unified Transformer (UniT)模型来应对多模态多任务学习，它可以同时处理跨不同领域的多个任务，包括目标检测、自然语言理解和视觉语言推理。具体来说，UniT 有两个 Transformer 编码器分别处理图像和文本输入，然后 Transformer 解码器根据任务模式获取单个或连接的编码器输出，最后，特定于任务的预测头被应用于不同任务的解码器输出。在训练阶段，通过在迭代中随机选择特定任务来联合训练所有任务。实验表明，UniT 使用一组紧凑的模型参数在每项任务上都取得了令人满意的性能。</p>
<p>总之，当前基于 Transformer 的多模态模型展示了其在统一数据和各种模态任务方面的架构优势，这展示了 Transformer 构建通用智能代理以应对大量应用程序的潜力。未来的研究可以在探索多模态 Transformer 的有效训练或可扩展性方面进行。</p>
<h3 id="3-6-Transformer效率"><a href="#3-6-Transformer效率" class="headerlink" title="3.6 Transformer效率"></a>3.6 Transformer效率</h3><p>尽管 Transformer 模型在各种任务中取得了成功，但其对内存和计算资源的高要求阻碍了其在手机等资源受限设备上的实现。 在本节中，我们回顾了为有效实施<strong>压缩</strong>和<strong>加速</strong> Transformer 模型而开展的研究。 这包括包括<strong>网络修剪</strong>、<strong>低秩分解</strong>、<strong>知识蒸馏</strong>、<strong>网络量化</strong>和<strong>紧凑架构设计</strong>。 表4 列出了基于 Transformer 模型压缩的一些代表性作品。</p>
<h4 id="3-6-1-剪枝和压缩"><a href="#3-6-1-剪枝和压缩" class="headerlink" title="3.6.1 剪枝和压缩"></a>3.6.1 剪枝和压缩</h4><p>在基于 Transformer 的预训练模型（例如，BERT）中，并行执行多个注意力操作以独立建模不同标记之间的关系。但是，特定的任务不需要使用所有的头。例如，michel2019sixteen 提供了经验证据表明可以在测试时移除大部分注意力头而不会对性能产生显着影响不同层所需的头数量各不相同——有些层甚至可能只需要一个头。考虑到注意力头上的冗余，定义重要性分数以估计每个 head 对 michel2019sixteen 中最终输出的影响，并且可以删除不重要的 heads 以进行有效部署。 prasanna2020bert 从两个角度分析了预训练 Transformer 模型中的冗余：一般冗余和任务特定冗余。遵循 frankle2018lottery 的彩票假设，prasanna2020bert 分析了 BERT 中的彩票，表明基于 Transformer 的模型中也存在良好的子网络，减少了 FFN 层和注意力头以获得高压缩率。对于将图像分割成多个patch的vit，tang2021patch提出减少patch计算以加速推理，并通过考虑它们的贡献自动发现冗余patch到有效的输出特征。 zhu2021visual 将网络瘦身方法 liu2017learning 扩展到视觉Transformer，以减少 FFN 和注意力模块中线性投影的维度。</p>
<p>除了Transformer模型的宽度，深度（即层数）也可以减少，以加速推理过程。与Transformer模型中不同注意力头可以并行计算的概念不同，不同层必须顺序计算，因为下一层的输入取决于前一层的输出。 fan2019reducing 提出了一种逐层删除策略来规范模型的训练，然后在测试阶段将所有层一起删除。</p>
<p>除了在 Transformer 模型中直接丢弃模块的剪枝方法之外，矩阵分解旨在基于低秩假设用多个小矩阵来逼近大矩阵。 例如 wang2019structured 分解了 Transformer 模型中的标准矩阵乘法，提高了推理效率。</p>
<h4 id="3-6-2-知识蒸馏"><a href="#3-6-2-知识蒸馏" class="headerlink" title="3.6.2 知识蒸馏"></a>3.6.2 知识蒸馏</h4><p>知识蒸馏旨在通过从大型教师网络转移知识来训练学生网络。与教师网络相比，学生网络通常具有更薄更浅的体系结构，更容易部署在资源有限的资源上。神经网络的输出和中间特征也可用于将有效信息从教师传递给学生。专注于 Transformer 模型，mukherjee2020xtremedistil 使用预训练的 BERT 作为教师来指导小模型的训练，利用大量未标记的数据。 wang2020minilm 训练学生网络模仿预训练教师模型中自注意力层的输出。引入值之间的点积作为一种新的知识形式来指导学生。老师的助手 mirzadeh2020improved 在 wang2020minilm 中也被引入，减少了大型预训练Transformer模型和紧凑学生网络之间的差距，从而促进了模仿过程。由于 Transformer 模型中的层类型多种多样（即自注意力层、嵌入层和预测层），tinybert 设计了不同的目标函数来将知识从教师传递给学生。例如，学生模型嵌入层的输出通过 MSE 损失模仿教师的输出。对于vision Transformer，jia2021efficient 提出了细粒度的流形蒸馏方法，通过图像与图像之间的关系挖掘有效知识分割块。</p>
<h4 id="3-6-3-量化"><a href="#3-6-3-量化" class="headerlink" title="3.6.3 量化"></a>3.6.3 量化</h4><p>量化旨在减少表示网络权重或中间特征所需的位数。对一般神经网络的量化方法进行了详细讨论，并实现了与原始网络相当的性能。 最近，人们对如何专门量化 Transformer 模型越来越感兴趣。 例如，shridhar2020end 建议将输入嵌入到二进制高维向量中，然后使用二进制输入表示来训练二进制神经网络。cheong2019Transformers 通过低位（例如，4 位）表示法表示 Transformer 模型中的权重。zhao2020investigation 对各种量化方法进行了实证研究，表明 k-means 量化具有巨大的发展潜力。 针对机器翻译任务，prato2020fully 提出了一种完全量化的Transformer，正如论文所称，这是第一个在翻译质量上没有任何损失的 8 位模型。 此外，liu2021post 探索了一种训练后量化方案，以减少视觉Transformer的内存存储和计算成本。</p>
<h4 id="3-6-4-紧凑结构设计"><a href="#3-6-4-紧凑结构设计" class="headerlink" title="3.6.4 紧凑结构设计"></a>3.6.4 紧凑结构设计</h4><p>除了将预定义的 Transformer 模型压缩成更小的模型外，一些作品还尝试直接设计紧凑的模型。jiang2020convbert 通过提出一个新模块（称为基于跨度的动态卷积）来简化自注意力的计算，该模块结合了全连接层和卷积层。 anonymous2021is 中提出了有趣的``汉堡包’’层，使用矩阵分解来替代原来的self-attention层。与标准的self-attention操作相比，矩阵分解可以更有效地计算，同时清楚地反映不同 token 之间的依赖关系。高效 Transformer 架构的设计也可以通过神经架构搜索(NAS)自动搜索，它会自动搜索如何组合不同的组件。比如su2021vision 搜索的 patch size 线性投影的尺寸和注意模块的头部数量以获得高效的视觉Transformer。 li2021bossnas 探索了一种自我监督的搜索策略，以获得由卷积模块和自注意力模块组成的混合架构。</p>
<p>Transformer 模型中的自注意力操作计算给定序列中不同输入标记的表示之间的点积（图像识别任务中的块vit），其复杂度为 $O(N)$，其中 $N$是序列的长度。最近，有一个目标是将大型方法的复杂性降低到 $O(N)$，以便 Transformer 模型可以扩展到长序列。例如，katharopoulos2020 将自注意力近似为内核特征映射的线性点积，并通过 RNN 揭示标记之间的关系。 zaheer2020big 将每个 token 视为图中的一个顶点，并将两个 token 之间的内积计算定义为一条边。受图论的启发，将各种稀疏图组合起来逼近Transformer模型中的稠密图，可以达到$O(N)$的复杂度。</p>
<p><strong>讨论</strong><br>前面的方法采用不同的方法来尝试识别 Transformer 模型中的冗余~（见图13）。 修剪和分解方法通常需要具有冗余的预定义模型。 具体来说，剪枝侧重于减少 Transformer 模型中组件（例如，层、头）的数量，而分解表示具有多个小矩阵的原始矩阵。 还可以手动（需要足够的专业知识）或自动（例如，通过 NAS）直接设计紧凑型模型。 获得的紧凑模型可以通过量化方法进一步用低位表示，以便在资源有限的设备上有效部署。</p>
<h2 id="4，总结和讨论"><a href="#4，总结和讨论" class="headerlink" title="4，总结和讨论"></a>4，总结和讨论</h2><p>Transformer 因其与 CNN 相比具有竞争力的性能和巨大的潜力而成为计算机视觉领域的热门话题。 为了发现和利用 Transformer，正如本次调查所总结的那样，近年来提出了许多方法。 这些方法在广泛的视觉任务上表现出出色的性能，包括主干、高级/中级视觉、低级视觉和视频处理。 然而，Transformer 用于计算机视觉的潜力尚未得到充分发掘，这意味着仍有一些挑战需要解决。 在本节中，我们将讨论这些挑战并提供对未来前景的见解。</p>
<h3 id="4-1-挑战"><a href="#4-1-挑战" class="headerlink" title="4.1 挑战"></a>4.1 挑战</h3><p>尽管研究人员提出了许多基于 Transformer 的模型来处理计算机视觉任务，但这些工作只是该领域的第一步，还有很大的改进空间。例如，ViT 中的 Transformer 架构遵循 NLP 的标准 Transformer，但专门为 CV 设计的改进版本仍有待探索。此外，有必要将 Transformer 应用于除前面提到的任务之外的更多任务。</p>
<p>用于计算机视觉的 Transformer 的泛化和鲁棒性也具有挑战性。与 CNN 相比，纯 Transformer 缺乏一些归纳偏差，并且严重依赖海量数据集进行大规模训练，因此，数据质量对 Transformer 的泛化性和鲁棒性有重大影响。尽管 ViT 在 CIFAR 和 VTAB 等下游图像分类任务上表现出色，但直接将 ViT 主干应用于对象检测未能取得比 CNN 更好的结果。为了在更通用的视觉任务上更好地泛化预训练的Transformer，还有很长的路要走。从业者关心 Transformer 的健壮性（例如漏洞问题），虽然鲁棒性已经在 zhang2020adversarial 等中进行了研究，但它仍然是一个有待解决的开放性问题。</p>
<p>尽管许多著作已经解释了 Transformer 在 NLP 中的使用，但要清楚地解释为什么 Transformer 在视觉任务上表现良好仍然是一个具有挑战性的课题。归纳偏差，包括翻译等方差和局部性，都带来了 CNN 的成功，但 Transformer 没有任何归纳偏差。目前的文献通常以直观的方式分析效果。例如，Dosovitskiy 声称大规模训练可以超越归纳偏差，位置嵌入被添加到图像块中以保留位置信息，这在计算机视觉任务中很重要。受 Transformer 中大量参数使用的启发，过度参数化可能是视觉 Transformer 可解释性的一个潜在点。</p>
<p>最后但同样重要的是，为 CV 开发<strong>高效</strong>的 Transformer 模型仍然是一个悬而未决的问题，Transformer 模型通常非常庞大且计算量大。例如，基础 ViT 模型需要 180 亿次 FLOP 处理，相比之下，轻量级 CNN 模型 GhostNet 仅需约 6 亿次 FLOP 即可达到类似的性能。尽管已经提出了几种压缩 Transformer 的方法，但它们仍然非常复杂，而这些本来是为 NLP 设计的方法，不一定适合CV。因此，迫切需要高效的 Transformer 模型，以便可以将视觉 Transformer 部署在资源有限的设备上。</p>
<h3 id="4-2-未来展望"><a href="#4-2-未来展望" class="headerlink" title="4.2 未来展望"></a>4.2 未来展望</h3><p>为了推动视觉Transformer的发展，我们为未来的研究提供了几个潜在的方向。</p>
<p>一个方向是 Transformer 在计算机视觉中的有效性和效率。目标是开发高效的视觉Transformer，特别是具有高性能和低资源成本的 Transformer 。性能决定了模型能否应用于实际应用，而资源成本则影响了在设备上的部署。有效性通常与效率相关，因此确定如何在两者之间取得更好的平衡是未来研究的一个有意义的课题。</p>
<p>大多数现有的视觉 Transformer 模型都设计为仅处理单个任务。许多 NLP 模型，例如 GPT-3 已经展示了 Transformer 如何在一个模型中处理多个任务。 IPT 在CV领域也能处理多个低级视觉任务，如超分辨率、图像去噪、去雨等。 Perceiver 和 Perceiver IO 是开创性的模型，可以在多个领域工作，包括图像、音频、多模式、点云，我们相信更多的任务可以只涉及一个模型。在一个Transformer（即大统一模型）中统一所有视觉任务甚至其他任务是一个令人兴奋的话题。</p>
<p>出现了各种类型的神经网络，例如 CNN、RNN 和 Transformer。在CV领域，CNNs曾经是主流选择，但是现在Transformer开始流行了。 CNN 可以捕获归纳偏差，例如翻译等方差和局部性，而 ViT 使用大规模训练来超越归纳偏差。根据目前可用的证据，CNN 在小型数据集上表现良好，而 Transformer 在大型数据集上表现更好。未来的问题是使用 CNN 还是 Transformer。</p>
<p>通过使用大型数据集进行训练，Transformer 可以在 NLP 和 CV 基准测试上实现最先进的性能，神经网络可能需要大数据而不是归纳偏差。最后，我们留给大家一个问题：Transformer 是否可以通过非常简单的计算范式（例如，仅使用全连接层）和海量数据训练来获得令人满意的结果？</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>三维重建调研</title>
    <url>/2023/06/28/3D_reconstruction/3d-reconstruction-survey/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>3D重建的调研文章，正在进行中～</p>
<p><strong>3D重建领域调研~</strong><br><span id="more"></span></p>
<h1 id="传统三维重建"><a href="#传统三维重建" class="headerlink" title="传统三维重建"></a>传统三维重建</h1><p>按传感器是否向物体发射光源分为：</p>
<ul>
<li>主动式：发射光源，依靠返回信息来解析深度信息<ul>
<li>结构光</li>
<li>TOF激光飞行时间</li>
<li>三角测距法</li>
</ul>
</li>
<li>被动式：从多视角的RGB图中用多视图几何原理来解析图像<ul>
<li>单目视觉</li>
<li>双目/多目视觉</li>
<li>基于消费级的RGB-D相机</li>
</ul>
</li>
</ul>
<p><strong>经典三维重建算法：SFM，多视图几何</strong></p>
<h1 id="结构光"><a href="#结构光" class="headerlink" title="结构光"></a>结构光</h1><p>将编码的光源照射到物体上，由相机拍摄后通过运算单元来转换成深度信息</p>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>容易受环境光干扰，所以室外体验差<br>检测距离增加，精度变差</p>
<h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><p>增大功率<br>改变编码方式</p>
<h1 id="TOF激光飞行时间法"><a href="#TOF激光飞行时间法" class="headerlink" title="TOF激光飞行时间法"></a>TOF激光飞行时间法</h1><p>连续发射光脉冲，根据接收返回光的时间差或相位差来计算目标距离。</p>
<h2 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h2><p>需要极为精确的时间测量模块，成本高</p>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><p>测量距离远，受环境光干扰少</p>
<h2 id="改进-1"><a href="#改进-1" class="headerlink" title="改进"></a>改进</h2><p>提高计时器良品率，降低成本<br>算法性能优化</p>
<h1 id="三角测距法"><a href="#三角测距法" class="headerlink" title="三角测距法"></a>三角测距法</h1><h2 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h2><p>近距离精度高</p>
<h2 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h2><p>精度与距离有关，距离越大，精度越差</p>
<h1 id="单目视觉"><a href="#单目视觉" class="headerlink" title="单目视觉"></a>单目视觉</h1><p>一部摄像头作为采集设备，一段时间内获得的连续图像的视差来进行三维重建</p>
<h2 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h2><p>低成本，易部署<br>单张图像可能对应无数真实物理世界场景，这是个病态的问题</p>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><ul>
<li>离线重建<ul>
<li>运动恢复法，SFM</li>
</ul>
</li>
<li>在线重建<ul>
<li>渐进式<ul>
<li>REMODE</li>
<li>SVO</li>
</ul>
</li>
<li>直接式</li>
</ul>
</li>
</ul>
<h2 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h2><p>目前这种算法广泛应用于手机等移动设备中，常见的算法有SfM，REMODE和SVO等</p>
<h1 id="双目-多目视觉"><a href="#双目-多目视觉" class="headerlink" title="双目/多目视觉"></a>双目/多目视觉</h1><p>利用左右相机得到两幅矫正图像，找到左右图片的匹配点，然后根据几何原理恢复环境的三维信息。</p>
<h2 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h2><p>左右相机的图片匹配算法精确度直接影响重建效果<br>多目的缺点，多个相机的图片匹配，需要消耗更多时间，实时性差</p>
<h2 id="分类-1"><a href="#分类-1" class="headerlink" title="分类"></a>分类</h2><ul>
<li>全局匹配, GM</li>
<li>本全局匹配, SGBM</li>
<li>局部匹配, BM</li>
</ul>
<h2 id="现状-1"><a href="#现状-1" class="headerlink" title="现状"></a>现状</h2><p>理论上，可以较精确的恢复精度信息，实际受拍摄环境约束无法保证精度，常见方法有SGM，SGBM等</p>
<h1 id="基于消费级的RGB-D"><a href="#基于消费级的RGB-D" class="headerlink" title="基于消费级的RGB-D"></a>基于消费级的RGB-D</h1><p>微软的Kinect v1和v2</p>
<h2 id="现状-2"><a href="#现状-2" class="headerlink" title="现状"></a>现状</h2><p>最早，由帝国理工大学的Newcombe等人于2011年提出的Kinect Fusion开启了RGB相机实时三维重建的序幕。此后有 Dynamic Fusion, Elastic Fusion, Fusion 4D, Volumn, Bundle Fusion等算法。</p>
<h1 id="基于深度学习的三维重建算法"><a href="#基于深度学习的三维重建算法" class="headerlink" title="基于深度学习的三维重建算法"></a>基于深度学习的三维重建算法</h1><p>主要在三个方向的应用</p>
<h2 id="给传统的重建算法提供优化思路"><a href="#给传统的重建算法提供优化思路" class="headerlink" title="给传统的重建算法提供优化思路"></a>给传统的重建算法提供优化思路</h2><p>典型文章：Code SLAM1，CVPR 2018年的best paper提名奖</p>
<p>利用CNN提取img几何信息，实现单目相机的稠密SLAM，主要贡献在于用DL方法从单张图像中提取若干基函数来表示场景的深度，这些基函数可以较大解决传统方法中的优化问题。</p>
<h2 id="传统方法与DL方法结合，优势互补"><a href="#传统方法与DL方法结合，优势互补" class="headerlink" title="传统方法与DL方法结合，优势互补"></a>传统方法与DL方法结合，优势互补</h2><p>多传感器，多算法的融合来提高算法的鲁棒性，比如，深度学习算法在不可见部分的建模有天然优势，传统算法是很难做到的。</p>
<h2 id="模仿动物视觉，直接进行三维建模"><a href="#模仿动物视觉，直接进行三维建模" class="headerlink" title="模仿动物视觉，直接进行三维建模"></a>模仿动物视觉，直接进行三维建模</h2><p>动物或人类根据大脑而不是严格的几何计算来进行三维重建，深度学习中，有一系列利用单张图像来来三维重建，原理上单张图失去了深度信息而无法回复，DL做的是利用一些经验信息来进行估计。</p>
<h1 id="深度学习算法给传统算法做优化"><a href="#深度学习算法给传统算法做优化" class="headerlink" title="深度学习算法给传统算法做优化"></a>深度学习算法给传统算法做优化</h1><p>利用CNN在图像特征匹配上的优势</p>
<ul>
<li><p>DeepVO，其基于深度递归卷积神经网络（RCNN）直接从一系列原始RGB图像（视频）中推断出姿态，而不采用传统视觉里程计中的任何模块，改进了三维重建中的视觉里程计这一环。</p>
</li>
<li><p>BA-Net，其将 SfM 算法 BA（Bundle Adjustment）优化算法作为神经网络的一层，以便训练出更好的基函数生成网络，从而简化重建中的后端优化过程。</p>
</li>
<li><p>Code SLAM，如之前所提，其通过神经网络提取出若干个基函数来表示场景的深度，这些基函数可以简化传统几何方法的优化问题。</p>
</li>
</ul>
<h1 id="深度学习与传统算法优势互补"><a href="#深度学习与传统算法优势互补" class="headerlink" title="深度学习与传统算法优势互补"></a>深度学习与传统算法优势互补</h1><p>CNN-SLAM13将CNN预测的致密深度图和单目SLAM的结果进行融合，在单目SLAM接近失败的图像位置，如低纹理区域，其融合方案给予更多权重于深度方案，提高了重建的效果。</p>
<h1 id="深度学习单挑大梁，直接进行三维重建"><a href="#深度学习单挑大梁，直接进行三维重建" class="headerlink" title="深度学习单挑大梁，直接进行三维重建"></a>深度学习单挑大梁，直接进行三维重建</h1><p>三维重建领域主要的数据格式有四种：</p>
<p>• 深度图(depth map)，2D图片，每个像素记录从视点到物体的距离，以灰度图表示，越近越黑；</p>
<p>• 体素(voxel)，体积像素概念，类似于2D之于像素定义；</p>
<p>• 点云(point cloud)，每个点都含有三维坐标，乃至色彩、反射强度信息；</p>
<p>• 网格(mesh)，即多边形网格，容易计算。</p>
<p>因而，依据处理的数据形式不同我们将研究简要分为三部分：1）基于体素；2）基于点云；3）基于网格。而基于深度图的三维重建算法暂时还没有，因为它更多的是用来在2D图像中可视化具体的三维信息而非处理数据。</p>
<h2 id="基于体素（voxel）"><a href="#基于体素（voxel）" class="headerlink" title="基于体素（voxel）"></a>基于体素（voxel）</h2><p>讲2D的卷积拓展到3D进行三维重建</p>
<ul>
<li><p>Depth Map Prediction from a Single Image using a Multi-Scale Deep Network, 2014<br>  用深度学习做三维重建的开山之作，基于体素形式，其直接用单张图像使用神经网络直接恢复深度图方法，将网络分为全局粗估计和局部精估计，并用一个尺度不变的损失函数进行回归。</p>
</li>
<li><p>3D-R2N2: A unified approach for single and multi-view 3d object reconstruction, 2016<br>  使用Encoder-3DLSTM-Decoder的网络结构建立2D图形到3D体素模型的映射，完成了基于体素的单视图/多视图三维重建（多视图的输入会被当做一个序列输入到LSTM中，并输出多个结果）。</p>
</li>
</ul>
<p>缺点：提升精度即需要提升分辨率，而分辨率的增加将大幅增加计算耗时（3D卷积，立次方的计算量），也就是，计算量大，并且分辨率和精度难平衡</p>
<h2 id="基于点云"><a href="#基于点云" class="headerlink" title="基于点云"></a>基于点云</h2><ul>
<li>A Point Set Generation Network for 3D Object Reconstruction From a Single Image, 2017<br>  用点云做三维重建的开山之作，最大贡献在于解决了训练点云网络时候的损失问题，因为相同的几何形状可能在相同的近似程度上可以用不同的点云表示，如何用恰当的损失函数来进行衡量一直是基于深度学习用点云进行三维重建方法的难题。</li>
<li>Point-Based Multi-View Stereo Network, 2019<br>  对场景的点云进行处理，融合三维深度和二维纹理信息，提高了点云的重建精度</li>
</ul>
<p>优点：简单、统一的结构，在几何变换和变形时更容易操作；</p>
<p>缺点：点云的点之间缺少连接性，缺乏物体表面信息，而直观的感受就是重建后的表面不平整</p>
<h2 id="基于网格"><a href="#基于网格" class="headerlink" title="基于网格"></a>基于网格</h2><p>网格的表示方法具有轻量、形状细节丰富的特点，重要是相邻点之间有连接关系。因而研究者基于网格来做三维重建。我们知道，网格是由顶点，边，面来描述3D物体的，这正好对应于图卷积神经网络的  M=(V,E,F)  所对应。</p>
<ul>
<li>Pixel2Mesh，用三角网格来做单张RGB图像的三维重建</li>
</ul>
<p>Pixel2Mesh的算法：</p>
<ul>
<li>对于任意图像都初始化一个椭球体作为初始三维形状；</li>
<li>网格分为两个部分：<ul>
<li>CNN来提取图像特征；</li>
<li>用图卷积来表示三维网格结构；</li>
</ul>
</li>
<li>对三维网格不断变形，最终输出物体形状</li>
</ul>
<p>模型通过四种损失函数来约束形状，取得了很好的效果。贡献在于用端到端的神经网络实现了从单张彩色图直接生成用网格表示的物体三维信息。</p>
<h1 id="各自有缺点和使用范围"><a href="#各自有缺点和使用范围" class="headerlink" title="各自有缺点和使用范围"></a>各自有缺点和使用范围</h1><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>方式</th>
<th>原理</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>主动</td>
<td>结构光</td>
<td>编码结构光</td>
<td>在一定范围内精度高；<br> 对物体本身的颜色和纹理稳健</td>
<td>容易受环境光干扰，室外体验差; <br> 随检测距离增加而精度变差</td>
</tr>
<tr>
<td></td>
<td>TOF飞行时间</td>
<td>反射光时间差</td>
<td>检测距离远；<br>受环境光干扰较少</td>
<td>整体价格昂贵；<br> 分辨率较低</td>
</tr>
<tr>
<td></td>
<td>三角测距法</td>
<td>几何三角相似性</td>
<td>成本较低；<br>近距离精度高</td>
<td>远距离误差大；<br> 计算量相对较大；</td>
</tr>
<tr>
<td>被动</td>
<td>单目视觉</td>
<td>单目视觉</td>
<td>算法成熟，计算量较小；<br> 成本最低，易部署；</td>
<td>单图可对应多个实际物理场景，歧义大；<br>精度和算法的稳定性较差；</td>
</tr>
<tr>
<td></td>
<td>双目/多目视觉</td>
<td>多目几何视觉</td>
<td>成本较低；<br>理论上可精确恢复精度信息；<br>多摄像头可提高重建精度;</td>
<td>计算量大，实时性差；<br>环境干扰导致精度难保证；</td>
</tr>
</tbody>
</table>
</div>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://zhuanlan.zhihu.com/p/108422696">三维重建算法综述|传统+深度学习方式</a></p>
]]></content>
      <categories>
        <category>3DReconstruction</category>
      </categories>
      <tags>
        <tag>reconstruction</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Residual Learning for Image Recognition</title>
    <url>/2023/06/09/CNN/Deep%20Residual%20Learning%20for%20Image%20Recognition/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>残差连接，让深层神经网络能够正常训练而不至于陷入梯度消失困境的有力武器，Kaiming He的又一神作之一。</p>
<p><strong>理解残差连接的由来和贡献，以及后续还有什么优化方向</strong><br><span id="more"></span></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>更深的神经网络更难训练，本文提出了一个残差学习框架，简化更深的神经网络训练。 本文明确地将网络层重新表述为考虑了输入的learning redidual functions，而不是learning unreferenced functions。 本文提供了全面的经验证据，表明这些残差网络更容易优化，并且可以从显著增加的深度中获得更好准确性。 </p>
<p>在 ImageNet 数据集上，本文评估了深度高达 152 层的残差网络，比 VGG 网络 [41] 深 8 倍，但仍然具有较低的复杂性，这些残差网络的集合在 ImageNet 测试集上达到了 3.57% 的错误率，该结果在 ILSVRC 2015 分类任务中获得第一名。本文还对具有 100 层和 1000 层的 CIFAR-10 进行了分析。</p>
<p>特征表示的深度对于许多视觉识别任务至关重要，仅由于本文极深的特征表示，本文在 COCO 对象检测数据集上获得了 28% 的相对改进。 深度残差网络是本文提交给 ILSVRC 和 COCO 2015 竞赛的基础，本文还在 ImageNet 检测、ImageNet 定位、COCO 检测和 COCO 分割任务上获得了第一名。</p>
<h1 id="一，介绍"><a href="#一，介绍" class="headerlink" title="一，介绍"></a>一，介绍</h1><p>深度卷积神经网络 [22、21] 已经在图像分类领域取得了一系列突破 [21、50、40]。 深度网络以端到端的多层方式自然地集成了低、中、高级特征[50]和分类器，特征的“层次”可以通过堆叠层数（深度）来丰富。 最近的文章 [41、44] 表明网络深度至关重要，在具有挑战性的 ImageNet 数据集 [36] 上的领先结果 [41、44、13、16] 都利用“非常深”的 [41] 模型，如具有深度为16 [41] 至30 [16]。 许多其他重要的视觉识别任务 [8、12、7、32、27] 也极大地受益于非常深的模型。</p>
<p>在深度重要性的驱动下，出现了一个问题：是否堆叠更多层就能学习更好的网络？</p>
<p>回答这个问题的一个障碍是臭名昭著的梯度消失、爆炸问题 [1,9]，它从一开始就阻碍了收敛。 然而，这个问题已在很大程度上通过对初始化参数进行归一化 [23、9、37、13] 和对中间层归一化 [16] 得到解决，这使得具有数十层的网络开始收敛以进行随机梯度下降 (SGD) 反向传播[22]。</p>
<p>当更深的网络能够开始收敛时，一个退化问题就暴露出来了：随着网络深度的增加，准确率达到饱和（这可能不足为奇），然后迅速退化。 出乎意料的是，这种退化不是由过度拟合引起的，并且向适当深度的模型添加更多层会导致更高的训练误差，如 [11,42] 中所报告的那样，并通过本文的实验进行了彻底验证。 图 1 显示了一个典型的例子。</p>
<p>训练精度的下降表明并非所有系统都一样容易优化，让本文考虑一个较浅的架构及其更多网络层的深度对应物。 对于更深层次的模型存在一种构造解决方案：添加的层是身份映射，其他层是从学习到的较浅模型中复制的。 这种构造的解决方案表明，较深的模型不应比其较浅的模型产生更高的训练误差。 但实验表明，本文目前手头的求解器无法找到与构建的解决方案相当或更好的解决方案（或无法在可行的时间内这样做）。</p>
<p>在本文中，本文通过引入深度残差学习框架来解决退化问题，本文不是希望每几个堆叠层直接适合所需的底层映射，而是明确让这些层适合残差映射。 形式上，将所需的底层映射表示为 $\mathcal{H}(x)$，本文让堆叠的非线性层 $\mathcal F(x) := \mathcal H(x) − x$ 拟合另一个映射，原始映射被重铸为 $\mathcal F(x)+x$。 本文假设优化残差映射比优化原始的（original）、未引用的（unreferenced）映射更容易。 在极端情况下，如果恒等映射是最优的，则就是残差为零，比通过一堆非线性层拟合恒等映射更容易。</p>
<p>$\mathcal F (x) + x$ 的公式可以通过具有“快捷连接”（short connection）的前馈神经网络来实现（图 2），快捷连接 [2,34,49] 是那些跳过一层或多层的连接。 在本文的例子中，快捷连接简单地执行身份映射，并且它们的输出被添加到堆叠层的输出中（图 2），身份快捷方式连接既不增加额外参数也不增加计算复杂性，整个网络仍然可以通过带有反向传播的 SGD 进行端到端训练，并且可以使用通用库（例如 Caffe [19]）轻松实现，而无需修改求解器。</p>
<p>本文在 ImageNet [36] 上进行了综合实验，以显示退化问题并评估本文的方法，结果表明：1）本文的极深残差网络很容易优化，但对应的“plain”网络（简单地堆叠层）在深度增加时表现出更高的训练误差； 2）本文的深度残差网络可以很容易地从大大增加的深度中获得精度增益，产生比以前的网络好得多的结果。</p>
<p>在 CIFAR-10 集 [20] 上也显示了类似的现象，这表明本文的方法的优化困难和效果不仅仅适用于特定的数据集。 本文使用超过 100 层的模型在多个数据集上成功训练，并探索超过 1000 层的模型。</p>
<p>在 ImageNet 分类数据集 [36] 上，本文通过极深的残差网络获得了出色的结果，本文的 152 层残差网络是 ImageNet 上有史以来最深的网络，同时其复杂性仍低于 VGG 网络 [41]。 本文的集成在 ImageNet 测试集上的 top-5 错误率为 3.57%，并在 ILSVRC 2015 分类竞赛中获得第一名。 极深的表示在其他识别任务上也具有出色的泛化性能，并带领本文在 ILSVRC &amp; COCO 2015 竞赛中进一步赢得了 ImageNet 检测、ImageNet 定位、COCO 检测和 COCO 分割方面的第一名。 这个有力的证据表明残差学习原则是通用的，本文期望它适用于其他视觉和非视觉问题。</p>
<h1 id="二，相关工作"><a href="#二，相关工作" class="headerlink" title="二，相关工作"></a>二，相关工作</h1><h2 id="residual-representations"><a href="#residual-representations" class="headerlink" title="residual representations"></a>residual representations</h2><p>在图像识别中，VLAD [18] 是一种通过残差向量对字典进行编码的表示，Fisher Vector [30] 可以表示为 VLAD 的概率版本 [18]，它们都是用于图像检索和分类的强大的浅层表示 [4, 48]。 对于矢量量化，编码残差矢量[17]被证明比编码原始矢量更有效。</p>
<p>在低级视觉和计算机图形学中，为了求解偏微分方程 (PDE)，广泛使用的多重网格方法 [3] 将系统重新表述为多个尺度的子问题，其中每个子问题在较粗和较细的尺度之间负责残差解。 Multigrid 的替代方法是分层基础预处理 [45,46]，它依赖于表示两个尺度之间的残差向量的变量，已经表明 [3,45,46] 这些求解器比不知道解的剩余性质的标准求解器收敛得更快。 这些方法表明，良好的重新制定或预处理可以简化对应的优化问题。</p>
<h2 id="shortcut-connection"><a href="#shortcut-connection" class="headerlink" title="shortcut connection"></a>shortcut connection</h2><p>导致快捷连接 [2,34,49] 的实践和理论已经被研究了很长时间，训练多层感知器 (MLP) 的早期实践是添加一个从网络输入连接到输出的线性层 [34、49]。 在 [44,24] 中，一些中间层直接连接到辅助分类器以解决梯度消失、爆炸问题。 [39、38、31、47] 等论文提出了通过快捷连接实现中间层响应、梯度和传播误差的方法，在 [44] 中，“初始”层由一个快捷分支和几个更深的分支组成。</p>
<p>与本文的工作同时进行的是，“高速公路网络”[42、43] 提供了具有门控功能的快捷连接 [15]。 这些门依赖于数据并具有参数，这与本文的无参数恒等式快捷方式形成对比。 当门控捷径“关闭”（接近零）时，高速公路网络中的层表示非残差函数。 相反，本文的公式总是学习残差函数； 本文的身份捷径永远不会关闭，所有信息总是通过，还有额外的剩余功能需要学习。 此外，高速公路网络并没有表现出随着深度的极大增加（例如，超过 100 层）而获得的准确性提升。</p>
<h1 id="三，深度残差学习"><a href="#三，深度残差学习" class="headerlink" title="三，深度残差学习"></a>三，深度残差学习</h1><h2 id="3-1-残差学习"><a href="#3-1-残差学习" class="headerlink" title="3.1 残差学习"></a>3.1 残差学习</h2><p>让本文将 H(x) 视为由几个堆叠层（不一定是整个网络）拟合的基础映射，其中 x 表示这些层中第一层的输入。 如果假设多个非线性层可以渐近逼近复杂函数2，那么就等价于假设它们可以渐近逼近残差函数，即 H(x) − x（假设输入和输出是 尺寸相同）。 因此，本文不是期望堆叠层近似 H(x)，而是明确地让这些层近似残差函数 F(x) := H(x) − x。 因此，原始函数变为 F(x)+x。 尽管这两种形式都应该能够渐近逼近所需的函数（如假设的那样），但学习的难易程度可能不同。<br>这种重新表述的动机是关于退化问题的违反直觉的现象（图 1，左）。 正如本文在介绍中所讨论的，如果可以将添加的层构造为恒等映射，则更深的模型的训练误差应该不大于其较浅的对应层。 退化问题表明求解器可能难以通过多个非线性层来近似恒等映射。 通过残差学习重构，如果恒等映射是最优的，求解器可以简单地将多个非线性层的权重推向零以接近恒等映射。<br>在实际情况下，恒等映射不太可能是最优的，但本文的重新表述可能有助于解决问题。 如果最优函数更接近恒等映射而不是零映射，求解器应该更容易找到参考恒等映射的扰动，而不是将函数作为新函数学习。 本文通过实验（图 7）表明，学习到的残差函数通常具有较小的响应，这表明恒等映射提供了合理的预处理。</p>
<h2 id="3-2-快捷方式的身份映射（Identity-Mapping-by-Shortcuts）"><a href="#3-2-快捷方式的身份映射（Identity-Mapping-by-Shortcuts）" class="headerlink" title="3.2 快捷方式的身份映射（Identity Mapping by Shortcuts）"></a>3.2 快捷方式的身份映射（Identity Mapping by Shortcuts）</h2><p>本文对每几个堆叠层采用残差学习。 构建块如图 2 所示。正式地，在本文中，本文考虑将构建块定义为：</p>
<script type="math/tex; mode=display">y=\mathcal F(x,\{W_i\})+x</script><p>这里 x 和 y 是所考虑层的输入和输出向量。 函数 F (x, {Wi }) 表示要学习的残差映射。 对于图 2 中具有两层的示例，F = W2σ(W1x)，其中 σ 表示 ReLU [29]，并且为了简化符号而省略了偏差。 操作 F + x 是通过快捷连接和逐元素加法执行的。 本文采用相加后的第二个非线性（即 σ(y)，见图 2）。<br>Eqn.(1) 中的快捷连接既不引入额外参数也不引入计算复杂度。 这不仅在实践中很有吸引力，而且在本文比较普通网络和残差网络时也很重要。 本文可以公平地比较同时具有相同数量的参数、深度、宽度和计算成本（除了可忽略的逐元素加法）的普通/残差网络。<br>在等式（1）中 x 和 F 的维数必须相等。 如果不是这种情况（例如，当更改输入/输出通道时），本文可以通过快捷连接执行线性投影 Ws 以匹配维度：<br>y = F (x, {Wi }) + Ws x。 (2)<br>本文也可以在等式（1）中使用方阵Ws。 但本文将通过实验表明恒等映射足以解决退化问题并且是经济的，因此 Ws 仅在匹配维度时使用。<br>残差函数F的形式是灵活的。 本文中的实验涉及具有两层或三层的函数 F（图 5），但更多层也是可能的。 但是如果 F 只有一个层，Eqn.(1) 类似于一个线性层：y = W1 x + x，本文没有观察到优势。<br>本文还注意到，尽管为简单起见，上述符号是关于全连接层的，但它们适用于卷积层。 函数 F(x,{Wi}) 可以表示多个卷积层。 逐个元素的加法是在两个特征图上逐个通道执行的。</p>
<h2 id="3-3-网络框架"><a href="#3-3-网络框架" class="headerlink" title="3.3 网络框架"></a>3.3 网络框架</h2><p>本文测试了各种普通/残差网络，并观察到了一致的现象。 为了提供讨论的实例，本文描述了 ImageNet 的两个模型如下。</p>
<h3 id="原始网络"><a href="#原始网络" class="headerlink" title="原始网络"></a>原始网络</h3><p>本文的普通基线（图 3，中间）主要受到 VGG 网络哲学的启发 [41]（图 3，左）。 卷积层大多具有 3×3 过滤器，并遵循两个简单的设计规则：（i）对于相同的输出特征图大小，层具有相同数量的过滤器； (ii) 如果特征图大小减半，则过滤器的数量加倍，以保持每层的时间复杂度。 本文直接通过步长为 2 的卷积层执行下采样。网络以全局平均池化层和具有 softmax 的 1000 路全连接层结束。 图 3（中间）中的加权层总数为 34。<br>值得注意的是，本文的模型比 VGG 网络 [41]（图 3，左）具有更少的过滤器和更低的复杂性。 本文的 34 层基线有 36 亿次 FLOP（乘加），仅为 VGG-19（196 亿次 FLOP）的 18%。</p>
<h3 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h3><p>基于上述普通网络，本文插入快捷连接（图 3，右），将网络变成对应的残差版本。 当输入和输出具有相同维度时，可以直接使用身份快捷方式（Eqn.（1））（图 3 中的实线快捷方式）。 当维度增加时（图 3 中的虚线快捷方式），本文考虑两种选择：(A) 快捷方式仍然执行身份映射，为增加维度填充额外的零条目。 这个选项没有引入额外的参数； (B) Eqn.(2) 中的投影快捷方式用于匹配维度（通过 1×1 卷积完成）。 对于这两个选项，当快捷方式跨越两种大小的特征图时，它们以 2 的步幅执行。</p>
<h2 id="3-4-实现"><a href="#3-4-实现" class="headerlink" title="3.4 实现"></a>3.4 实现</h2><p>本文对 ImageNet 的实现遵循 [21, 41] 中的做法。 调整图像大小，其短边在 [256、480] 中随机采样以进行缩放 [41]。 从图像或其水平翻转中随机采样 224×224 裁剪，减去每个像素的平均值 [21]。 使用[21]中的标准颜色增强。 本文在每次卷积之后和激活之前采用批量归一化 (BN) [16]，遵循 [16]。 本文像 [13] 中一样初始化权重，并从头开始训练所有普通/残差网络。 本文使用 mini-batch 大小为 256 的 SGD。学习率从 0.1 开始，当误差趋于平稳时除以 10，模型训练最多 60 × 104 次迭代。 本文使用 0.0001 的权重衰减和 0.9 的动量。 本文不使用 dropout [14]，遵循 [16] 中的做法。<br>在测试中，为了进行比较研究，本文采用标准的 10-crop 测试 [21]。 为了获得最佳结果，本文采用 [41, 13] 中的全卷积形式，并在多个尺度上平均得分（调整图像大小，使短边位于 {224, 256, 384, 480, 640}）。</p>
<h1 id="四，实验"><a href="#四，实验" class="headerlink" title="四，实验"></a>四，实验</h1><h2 id="4-1-ImageNet-分类任务"><a href="#4-1-ImageNet-分类任务" class="headerlink" title="4.1 ImageNet 分类任务"></a>4.1 ImageNet 分类任务</h2><p>本文在包含 1000 个类的 ImageNet 2012 分类数据集 [36] 上评估本文的方法。 这些模型在 128 万张训练图像上进行训练，并在 5 万张验证图像上进行评估。 本文还获得了测试服务器报告的 100k 测试图像的最终结果。 本文评估 top-1 和 top-5 错误率。</p>
<h3 id="原始网络-1"><a href="#原始网络-1" class="headerlink" title="原始网络"></a>原始网络</h3><p>本文首先评估 18 层和 34 层普通网络。 34 层平面网如图 3（中）所示。 18层素网的形式类似。 有关详细架构，请参见表 1。<br>表 2 中的结果表明，较深的 34 层普通网络比较浅的 18 层普通网络具有更高的验证错误。 为了揭示原因，在图 4（左）中，本文比较了他们在训练过程中的训练/验证错误。 本文已经观察到退化问题——34 层普通网络在整个训练过程中具有更高的训练误差，即使 18 层普通网络的解空间是 34 层网络的解空间的子空间。<br>本文认为这种优化困难不太可能是由梯度消失引起的。 这些普通网络是用 BN [16] 训练的，它确保前向传播的信号具有非零方差。 本文还验证了 BN 的反向传播梯度表现出健康的范数。 所以前向或后向信号都不会消失。 事实上，34 层普通网络仍然能够达到具有竞争力的精度（表 3），这表明求解器在一定程度上起作用。 本文推测深层普通网络可能具有指数级的低收敛率，这会影响训练误差的减少 3。 这种优化困难的原因将在未来进行研究。</p>
<h3 id="残差网络-1"><a href="#残差网络-1" class="headerlink" title="残差网络"></a>残差网络</h3><p>接下来本文评估 18 层和 34 层残差网络 (ResNets)。 基线架构与上述普通网络相同，除了将快捷连接添加到每对 3×3 滤波器，如图 3（右）所示。 在第一个比较中（表 2 和图 4 右），本文对所有快捷方式使用恒等映射，对增加的维度使用零填充（选项 A）。 所以与普通的对应物相比，它们没有额外的参数。<br>本文从表 2 和图 4 中得到了三个主要观察结果。首先，残差学习的情况是相反的——34 层 ResNet 优于 18 层 ResNet（2.8%）。 更重要的是，34 层 ResNet 表现出相当低的训练误差，并且可以推广到验证数据。 这表明退化问题在此设置中得到了很好的解决，本文设法从增加的深度中获得准确度增益。<br>其次，与其普通对应物相比，34 层 ResNet 将 top-1 错误减少了 3.5%（表 2），这是由于成功减少了训练错误（图 4 右与左）。 这种比较验证了残差学习在极深系统上的有效性。<br>最后，本文还注意到 18 层普通网络/残差网络相对准确（表 2），但 18 层 ResNet 收敛得更快（图 4 右与左）。 当网络“不太深”（此处为 18 层）时，当前的 SGD 求解器仍然能够为普通网络找到好的解决方案。 在这种情况下，ResNet 通过在早期提供更快的收敛来简化优化。</p>
<h3 id="Identity-vs-Projection-Shortcuts"><a href="#Identity-vs-Projection-Shortcuts" class="headerlink" title="Identity vs. Projection Shortcuts"></a>Identity vs. Projection Shortcuts</h3><p>本文已经证明，无参数的恒等式快捷方式有助于训练。 接下来本文研究投影捷径（Eqn.（2））。 在表 3 中，本文比较了三个选项：(A) 零填充快捷方式用于增加维度，所有快捷方式都是无参数的（与表 2 和图 4 右相同）； (B) projection shortcuts用于增加维度，其他shortcuts是identity； (C) 所有捷径都是投影。<br>表 3 显示所有三个选项都比普通选项好得多。 B 略好于 A。本文认为这是因为 A 中的零填充维度确实没有残差学习。 C 略好于 B，本文将此归因于许多（十三个）投影快捷方式引入的额外参数。 但是 A/B/C 之间的微小差异表明投影捷径对于解决退化问题并不是必不可少的。 因此，本文在本文的其余部分不使用选项 C，以减少内存/时间复杂性和模型大小。 身份捷径对于不增加下面介绍的瓶颈架构的复杂性尤为重要。</p>
<h3 id="更深的bottleneck架构"><a href="#更深的bottleneck架构" class="headerlink" title="更深的bottleneck架构"></a>更深的bottleneck架构</h3><p>接下来，本文将描述 ImageNet 的更深层网络。 由于担心本文可以承受的训练时间，本文将构建块修改为瓶颈设计 4。 对于每个残差函数 F，本文使用 3 层堆栈而不是 2 层（图 5）。 三层分别是 1×1、3×3 和 1×1 卷积，其中 1×1 层负责减少然后增加（恢复）维度，而 3×3 层成为输入/输出维度较小的瓶颈 . 图 5 显示了一个示例，其中两种设计具有相似的时间复杂度。<br>无参数恒等式捷径对于瓶颈架构尤为重要。 如果将图 5（右）中的恒等捷径替换为投影，可以看出时间复杂度和模型大小增加了一倍，因为捷径连接到两个高维端。 因此，身份捷径可以为瓶颈设计带来更高效的模型。</p>
<h3 id="50层resnet"><a href="#50层resnet" class="headerlink" title="50层resnet"></a>50层resnet</h3><p>本文用这个 3 层瓶颈块替换 34 层网络中的每个 2 层块，从而产生 50 层 ResNet（表 1）。 本文使用选项 B 来增加维度。 这个模型有 38 亿次 FLOP。</p>
<h3 id="101层和152层resnet"><a href="#101层和152层resnet" class="headerlink" title="101层和152层resnet"></a>101层和152层resnet</h3><p>本文通过使用更多的 3 层块构建 101 层和 152 层 ResNet（表 1）。 值得注意的是，虽然深度显着增加，但 152 层 ResNet（113 亿 FLOPs）的复杂性仍然低于 VGG-16/19 网络（153/196 亿 FLOPs）。<br>50/101/152 层 ResNets 比 34 层 ResNets 的准确度高出相当大的幅度（表 3 和 4）。 本文没有观察到退化问题，因此可以从显着增加的深度中获得显着的精度提升。 所有评估指标都见证了深度的好处（表 3 和 4）。</p>
<h3 id="对比sota模型"><a href="#对比sota模型" class="headerlink" title="对比sota模型"></a>对比sota模型</h3><p>在表 4 中，本文与之前的最佳单模型结果进行了比较。 本文的基线 34 层 ResNets 已经达到了非常有竞争力的精度。 本文的 152 层 ResNet 的单模型前 5 验证错误率为 4.49%。 这个单一模型的结果优于所有以前的整体结果（表 5）。 本文将六个不同深度的模型组合成一个集成（提交时只有两个 152 层的模型）。 这导致测试集上的 top-5 错误率为 3.57%（表 5）。 此条目在 ILSVRC 2015 中获得第一名。</p>
<h2 id="4-2-CIFAR-10和分析"><a href="#4-2-CIFAR-10和分析" class="headerlink" title="4.2 CIFAR-10和分析"></a>4.2 CIFAR-10和分析</h2><p>本文对 CIFAR-10 数据集 [20] 进行了更多研究，该数据集由 10 个类别的 50k 训练图像和 10k 测试图像组成。 本文提出了在训练集上训练并在测试集上进行评估的实验。 本文的重点是极深网络的行为，而不是推动最先进的结果，因此本文有意使用如下简单的架构。<br>普通/残差架构遵循图 3（中/右）中的形式。 网络输入是 32×32 图像，减去每个像素的平均值。 第一层是 3×3 卷积。 然后，本文在大小为 {32、16、8} 的特征图上分别使用 6n 层和 3×3 卷积，每个特征图大小有 2n 层。 过滤器的数量分别为{16、32、64}。 子采样由步幅为 2 的卷积执行。网络以全局平均池、10 路全连接层和 softmax 结束。 总共有 6n+2 个堆叠的加权层。 下表总结了架构：</p>
<p>当使用快捷方式连接时，它们连接到成对的 3×3 层（总共 3n 个快捷方式）。 在这个数据集上，本文在所有情况下都使用恒等捷径（即选项 A），因此本文的残差模型与普通模型具有完全相同的深度、宽度和参数数量。<br>本文使用 0.0001 的权重衰减和 0.9 的动量，并采用 [13] 和 BN [16] 中的权重初始化但没有丢失。 这些模型在两个 GPU 上以 128 的小批量大小进行训练。 本文从 0.1 的学习率开始，在 32k 和 48k 迭代时将其除以 10，并在 64k 迭代时终止训练，这是根据 45k/5k train/val split 确定的。 本文按照 [24] 中的简单数据增强进行训练：每边填充 4 个像素，并从填充图像或其水平翻转中随机采样 32×32 裁剪。 为了测试，本文只评估原始 32×32 图像的单个视图。<br>本文比较 n = {3, 5, 7, 9}，得到 20、32、44 和 56 层网络。 图 6（左）显示了普通网络的行为。 deep plain nets 受深度增加的影响，并且在更深时表现出更高的训练误差。 这种现象类似于 ImageNet（图 4，左）和 MNIST（见 [42]）上的现象，表明这种优化困难是一个基本问题。<br>图 6（中）显示了 ResNet 的行为。 同样类似于 ImageNet 案例（图 4，右），本文的 ResNets 设法克服了优化困难，并在深度增加时展示了精度增益。<br>本文进一步探索导致 110 层 ResNet 的 n = 18。 在这种情况下，本文发现初始学习率 0.1 有点太大，无法开始收敛5。 所以本文使用 0.01 来预热训练，直到训练误差低于 80%（大约 400 次迭代），然后回到 0.1 继续训练。 学习计划的其余部分如前所述。 这个 110 层网络收敛良好（图 6，中间）。 它比 FitNet [35] 和 Highway [42]（表 6）等其他深度和薄网络的参数更少，但却是最先进的结果之一（6.43%，表 6）。</p>
<h3 id="层反馈分析"><a href="#层反馈分析" class="headerlink" title="层反馈分析"></a>层反馈分析</h3><p>图 7 显示了层响应的标准偏差 (std)。 响应是每个 3×3 层的输出，在 BN 之后和其他非线性（ReLU/加法）之前。 对于 ResNets，该分析揭示了残差函数的响应强度。 图 7 显示 ResNets 的响应通常比它们的普通对应物小。 这些结果支持本文的基本动机（第 3.1 节），即残差函数通常可能比非残差函数更接近于零。 本文还注意到，更深的 ResNet 具有更小的响应幅度，如图 7 中 ResNet-20、56 和 110 之间的比较所证明的那样。当有更多层时，单个 ResNet 层倾向于修改信号 较少的。</p>
<h3 id="探索超过1000层模型"><a href="#探索超过1000层模型" class="headerlink" title="探索超过1000层模型"></a>探索超过1000层模型</h3><p>本文探索了一个超过 1000 层的深度模型。 本文将 n = 200 设置为 1202 层网络，该网络按上述方式训练。 本文的方法没有优化困难，这个 103 层网络能够实现训练误差 &lt;0.1%（图 6，右）。 它的测试误差仍然相当不错（7.93%，表 6）。<br>但是在如此激进的深度模型上仍然存在未解决的问题。 这个 1202 层网络的测试结果比本文的 110 层网络差，尽管两者的训练误差相似。 本文认为这是因为过度拟合。 对于这个小数据集，1202 层网络可能不必要地大 (19.4M)。 应用诸如 maxout [10] 或 dropout [14] 之类的强正则化以获得此数据集上的最佳结果 ([10, 25, 24, 35])。 在本文中，本文不使用 maxout/dropout，只是通过设计的深度和瘦架构简单地施加正则化，而不会分散对优化困难的关注。 但是结合更强大的正则化可能会改善结果，本文将在未来进行研究。</p>
<h2 id="4-3-PASCAL-和-MS-COCO上的目标检测"><a href="#4-3-PASCAL-和-MS-COCO上的目标检测" class="headerlink" title="4.3 PASCAL 和 MS COCO上的目标检测"></a>4.3 PASCAL 和 MS COCO上的目标检测</h2><p>本文的方法在其他识别任务上具有良好的泛化性能。 表 7 和表 8 显示了 PASCAL VOC 2007 和 2012 [5] 以及 COCO [26] 的对象检测基线结果。 本文采用 Faster R-CNN [32] 作为检测方法。 在这里，本文对用 ResNet-101 替换 VGG-16 [41] 的改进感兴趣。 使用两种模型的检测实现（见附录）是相同的，因此收益只能归功于更好的网络。 最引人注目的是，在具有挑战性的 COCO 数据集上，本文获得了 COCO 标准指标 (mAP@[.5, .95]) 6.0% 的提高，相对提高了 28%。 这种增益完全归功于学习到的表征。<br>基于深度残差网络，本文在 ILSVRC &amp; COCO 2015 竞赛的几个赛道上获得了第一名：ImageNet 检测、ImageNet 定位、COCO 检测和 COCO 分割。 详情载于附录。</p>
]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>Resnet</tag>
      </tags>
  </entry>
  <entry>
    <title>Numerical Methods for algorithmic Systems and Neural Networks</title>
    <url>/2023/05/17/tools/Numerical_Methods_for_algorithmic_Systems_and_Neural_Networks/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>数值方法和DNN的交叉学科所包含的知识，可以逐步补充完整的～</p>
<p><strong>作者</strong><br>Sebastian Kinnewig, Leon Kolditz, Julian Roth, Thomas Wick   </p>
<p><strong>参考链接</strong><br><a href="https://www.ifam.uni-hannover.de/en/wick/">https://www.ifam.uni-hannover.de/en/wick/</a><br><a href="https://www.ifam.uni-hannover.de/en/">https://www.ifam.uni-hannover.de/en/</a></p>
<p><strong>文章百度云链接</strong><br><a href="https://pan.baidu.com/s/1YU0E964JNArGj80lc1BPEg">https://pan.baidu.com/s/1YU0E964JNArGj80lc1BPEg</a><br>提取码: r9hx </p>
<p><strong>多多补充～</strong><br><span id="more"></span></p>
<p><img src="Numerical_Methods_for_algorithmic_Systems_and_Neural_Networks.jpeg" alt="img"></p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>numerical methods</tag>
      </tags>
  </entry>
  <entry>
    <title>Training Neural Networks by Lifted Proximal Operator Machines</title>
    <url>/2023/05/10/CNN/Training%20Neural%20Networks%20by%20Lifted%20Proximal%20Operator%20Machines/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>title: Training Neural Networks by Lifted Proximal Operator Machines</p>
<p>论文百度云盘链接:<a href="https://pan.baidu.com/s/14x2KrlBVhfdWWy8oQBjROw?pwd=axgn">https://pan.baidu.com/s/14x2KrlBVhfdWWy8oQBjROw?pwd=axgn</a> </p>
<p><strong>提升近端算子机对前馈神经网络的训练问题～</strong><br><span id="more"></span></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>我们提出了提升近端算子机 (LPOM) 来训练完全连接的前馈神经网络。 LPOM 将激活函数表示为等效的近端算子，并将近端算子作为惩罚添加到网络的目标函数中。 LPOM 在所有层级权重和激活中都是块多凸的。 这使我们能够开发一种新的具有收敛保证的块坐标下降（BCD）方法来解决它。 由于新颖的公式和求解方法，LPOM 仅使用激活函数本身，不需要任何梯度步骤。 因此，它避免了梯度消失或爆炸问题，这些问题通常归咎于基于梯度的方法。 此外，它还可以处理各种非递减 Lipschitz 连续激活函数。 此外，LPOM 几乎与随机梯度下降一样具有内存效率，并且其参数调整相对容易。 我们进一步实现和分析了 LPOM 的并行解决方案。 我们首先提出了一种具有收敛保证的通用异步并行 BCD 方法。 然后我们用它来解决 LPOM，从而得到异步并行 LPOM。 为了更快的速度，我们开发了同步并行 LPOM。 我们验证了 LPOM 在各种网络架构和数据集上的优势。 我们还将同步并行 LPOM 应用于自动编码器训练，并展示了其快速收敛和卓越的性能。</p>
<h1 id="一，介绍"><a href="#一，介绍" class="headerlink" title="一，介绍"></a>一，介绍</h1><p>神经网络 (NN) 是强大的模型，在许多应用中取得了巨大成功，包括图像识别 [1]、语音识别 [2]、自然语言理解 [3] 和构建围棋游戏学习系统 [4] . 然而，由于它们的目标函数是高度非凸的，NN 的训练仍然是一个挑战，其中包括病态的 Hessian 和许多鞍点和局部最小值的存在 [5]。</p>
<p>作为 NN 训练中的主要方法，基于梯度的方法主要包括 vanilla 【？】随机梯度下降 (SGD) [6] 和具有自适应学习率和动量项的 SGD，如 Nesterov momentum [7]、AdaGrad [8]、RMSProp [9] ]、Adam [10] 和 AMSGrad [11]。 SGD 及其变体使用小批量训练样本来估计全批量梯度，所以每一步的权重更新都很简单，此外估计的梯度也有噪声，这有助于避开鞍点 [12]。 尽管基于梯度的方法取得了巨大成功，但它们也有几个严重的缺点。 主要缺陷是它们遭受梯度消失或爆炸问题的困扰，这会减慢或破坏训练的稳定性。 已经开发了一些方法来缓解这样的问题，例如，引入整流线性单元 (ReLU)、批量归一化 (BN) [13] 或残差网络 (ResNets) [14]。 然而，嵌套计算目标函数梯度的问题仍然存在，而且它们的参数调整很困难，例如设置学习率和停止标准 [15]。 此外，它们不能直接处理不可微分的激活函数（例如，二值化神经网络 [16]），也不能并行更新跨层的权重 [15]。 有关 SGD 局限性的更多详细信息，请读者参考 [17] 和 [15]。</p>
<p>由于上面讨论的 SGD 的局限性，在为神经网络开发新的训练方法方面有非常活跃的研究。 一种值得注意的方法是引入与网络激活相关的辅助变量，并将 NN 的训练制定为等式约束优化问题 [18]，这种方法用标准优化方法解决了产生的约束问题。 这种方法的另一个优点是它可以并行实现，因此可以解决分布式大规模问题。 已经提出了这种方向的几种方法，它们在如何放松约束和向目标函数添加惩罚方面有所不同。 </p>
<p>Carreira-Perpinan 和 Wang [18] 使用二次惩罚来近似执行等式约束并解决了一系列无约束最小化问题。 zeng等[19] 也使用二次惩罚来近似强制执行等式约束，但为每一层引入了更多的辅助变量块。 受乘法器交替方向法 (ADMM) [20] 的启发，Taylor 等人[17] 和zhang等人[21]采用增广拉格朗日方法获得满足非线性等式约束的序列。 然而，引入的拉格朗日乘数需要更多的内存，而且 ADMM 不是为处理非线性等式约束而设计的。 Zhang 和 Brand [22] 以及 Askari 等人[23] 使用激活函数的等效表示，消除了非线性约束，然而[22]中的方法仅限于 ReLU 激活函数，虽然 [23] 中的公式可以处理一般的激活函数，但其求解方法需要针对每个激活函数进行调整，并且所提出的求解方法仅适用于 ReLU，而且这两种方法在速度或错误率方面无法与 SGD 竞争。 此外，在测试阶段，他们需要解决优化问题以预测新样本的标签，这在计算上是负担不起的。 相比之下，SGD 使用前馈传递来预测标签，即将激活从输入层传播到输出层并使用输出层的激活进行预测。 最近，Gu等人[24] 介绍了 [23] 的后续工作，他们的公式（见[24]中的（4）和（9））等同于我们的（见（19）或（20）），然而，他们的解决方法不如我们的方法有效和通用（见第 4.1 节的最后一段）。</p>
<p>本文的主要贡献可归纳如下。</p>
<ul>
<li>我们开发了一种新的优化方法来训练完全连接的前馈神经网络，我们称之为提升近端算子机 (LPOM)，LPOM 允许我们使用简单的前馈传递来推断新样本，这与 SGD 相同。 此外，LPOM 是块多凸的，即在保持剩余权重和激活固定的前提下，对当前层的权重或者激活来说它是凸。 相比之下，大多数现有的 NN 训练方法不具有此属性。 这对神经网络的训练非常有利。</li>
<li>相应地，我们提出了一种新的块坐标下降（BCD）方法来求解 LPOM 并证明其收敛性。 我们的BCD求解方法只用到了激活函数本身的映射，没有用到它的导数。 因此它避免了梯度消失或爆炸问题，这在基于梯度的方法中是众所周知的。 同时，LPOM 适用于各种非递减 Lipschitz 连续激活函数，它们可以是饱和的（例如，sigmoid 和 tanh），也可以是不可微的（例如，ReLU 和 leaky ReLU）。 此外，LPOM 不需要比分层激活更多的辅助变量，因此它的内存占用与 SGD 几乎相同，它还存储所有用于梯度计算的激活，而且在 LPOM 中调整惩罚参数很容易。</li>
<li>我们以异步和同步方式实现和分析了 LPOM 的并行解决方案。 我们首先提出了一种通用的异步并行 BCD 方法。 我们证明了它的收敛性并用它来解决 LPOM，从而得到异步并行 LPOM，得到的结论也是同步并行LPOM的基础。 然后我们提出了同步并行 LPOM，并表明它在串行 LPOM 上实现了令人满意的加速，而不会降低性能。</li>
</ul>
<p>由于 SGD 是常用的 NN 训练方法，我们将其作为我们的主要竞争对手。 如表 1 所示，与 SGD 相比，LPOM 表现出一些有利的特性。 目前，我们仅在全连接 NN 上实施 LPOM，卷积神经网络 (CNN) 是最流行的前馈网络。 然而，由于我们还没有解释 pooling operators 和 skip-connections，我们将来会在 CNNs 上实现 LPOM。 我们注意到，大多数现有的基于非梯度的方法也首先关注完全连接的神经网络 [17]、[18]、[19]、[21]、[22]、[23]。</p>
<h1 id="二，相关工作"><a href="#二，相关工作" class="headerlink" title="二，相关工作"></a>二，相关工作</h1><p>在本节中，我们将回顾一些与我们的工作最相关的、基于无梯度的 NN 训练方法，在表 2 中总结了整篇论文中使用的主要符号。</p>
<p>考虑监督学习中的 $N$ 层标准前馈神经网络，其中第一层是输入层，第 $N$ 层是输出层，令 $X^1\in \mathbb R^{n_1\times m}$ 为一批训练样本，其中 $n_1$ 是训练样本的维数，也是输入层神经元的数量，$m$ 是批量大小。 设 $D\in \mathbb R^{c\times m} $ 是 $X^1$ 的标签，其中 $c$ 是类数。 设 $W^i$ 为第 $i$ 层和第 $i + 1$ 层之间的权重矩阵，其中我们将额外的一列堆叠到 $W^i$ 并将全行堆叠到 $X^i$ 并省略相应的偏差。 令 $\phi(\cdot)$ 为逐元素激活函数（例如，sigmoid、tanh 和 ReLU）。 让 $\ell(\cdot,\cdot)$ 成为损失函数（例如，均方误差 (MSE) 或交叉熵）。 借助这些符号，NN 的批量训练问题可以表示为以下最小化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{W^i}\ell(\phi(W^{n-1}\phi(\cdots\phi(W^2\phi(W^1X^1))\cdots)),D) \tag{1}
\end{align*}</script><p>在上面的公式中，NN 的目标函数是嵌套的，其中第 $i$ 层的输出是第 $i+1$ 层的输入。问题 (1) 可以通过 SGD 求解，它计算 $\{W^i\}^{n-1}_{i=1}$ 传播的梯度，然后通过梯度下降更新 $\{W\}^{n-1}_{i=1}$。</p>
<p>为了使问题 (1) 在计算上更易于处理，最常见的方法是将每一层的激活作为辅助变量块引入。 那么问题（1）可以等效地改写为以下等式约束最小化问题[18]：</p>
<script type="math/tex; mode=display">
\begin{align*}
&\min_{\{W^i\},\{X^i\}}\ell(X^i,D)\\
&s.t. \; X^i=\phi(W^{i-1}X^{i-1})\quad i=2,3...n\tag{2}
\end{align*}</script><p>其中 $X^i$ 是第 $i$ 层的激活值，$X^n$ 是神经网络的输出，其他符号同（1）。 与问题（1）相比，问题（2）的目标函数没有嵌套，所以我们可能会使用更优雅的优化方法来解决它。 我们要注意，在使用 SGD 解决问题 (1) 时，还需要记录激活值 $\{X^i\}^n_{i=2}$ 以计算梯度。</p>
<p>Carreira-Perpinan 和 Wang [18] 提出了辅助坐标（MAC）的方法来近似解决问题（2）。 MAC 不是直接求解 (2)，而是通过使用二次惩罚来放宽等式约束并求解以下形式的无约束问题</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{\{W^i\},\{X^i\}}\ell(X^n,D)+\frac{\mu}{2}\sum^n_{i=2}\|X^i-\phi(W^{i-1}X^{i-1})\|^2_F \tag{3}
\end{align*}</script><p>随着迭代逐渐增加，其中 $\mu &gt; 0$。为了解耦非线性激活并获得更有效的子问题求解方法，Zeng 等人[19] 为问题（2）的每一层引入了一个新的辅助变量块，并提出了以下问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
&\min_{\{X^i\},\{W^i\},\{U^i\}}\ell(X^n,D)\\
&s.t.\; U^i=W^{i-1}X^{i-1},X^i=\phi(U^i) \tag{4}
\end{align*}</script><p>按照MAC的方法，他们没有直接解决上面的问题，而是优化了如下形式的问题</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{\{X^i\},\{W^i\},\{U^i\}}\ell(X^n,D)+\frac{\mu}{2}(\|U^i-W^{i-1}X^{i-1}\|^2_F+\|X^i-\phi(U^i)\|^2_F) \tag{5}
\end{align*}</script><p>泰勒等[17] 也试图优化问题 (4)，受 ADMM[20] 的启发，他们只在输出层的约束中添加了一个拉格朗日乘子，而不是在每个等式约束中添加了一个拉格朗日乘子，从而得到</p>
<script type="math/tex; mode=display">
\begin{align*}
&\min_{\{X^i\},\{W^i\},\{U^i\},M}\ell(X^n,D)\\
&+\langle U^n,M\rangle+\frac{\beta}{2}\|U^n-W^{n-1}X^{n-1}\|^2_F\\
&+\sum^{n-1}_{i=2}\frac{\mu_i}{2}(\|U^i-W^{i-1}X^{i-1}\|^2_F+\|X^i-\phi(U^i)\|^2_F) \tag{6}
\end{align*}</script><p>其中 $M$ 是拉格朗日乘数，$\beta &gt; 0$ 和 $\mu &gt; 0$ 是常数，请注意，输出层使用线性激活函数（即 $\phi(x)=x$），所以他们只是试探性地使用了 ADMM。 同样受到 ADMM 的启发，Zhang 等人[21] 使用了另一种变量拆分方案：</p>
<script type="math/tex; mode=display">
\begin{align*}
&\min_{\{X^i\},\{W^i\},\{U^i\}}\ell(X^n,D)\\
&s.t.\; U^{i-1}=X^{i-1},X^i=\phi(W^{i-1}U^{i-1}) \tag{7}
\end{align*}</script><p>与 ADMM 一样，他们为 (7) 中的每个约束添加了拉格朗日乘子，那么增广拉格朗日问题就变成了</p>
<script type="math/tex; mode=display">
\begin{align*}
&\min_{\{X^i\},\{W^i\},\{U^i\},\{A^i\},\{B^i\}}\ell(X^n,D)\\
&+\frac{\mu}{2}\sum^{n}_{i=2}\big(\|U^{i-1}-X^{i-1}+A^{i-1}\|^2_F\\
&+\|X^i-\phi(W^{i-1}U^{i-1}+B^{i-1})\|^2_F\big) \tag{8}
\end{align*}</script><p>其中 $A^i$ 和 $B^i$ 是拉格朗日乘数，然而，问题（7）包含非线性等式约束，ADMM 旨在处理线性约束问题。</p>
<p>Zhang 和 Brand [22] 开发了一种新方法，仅使用 ReLU 激活函数来解决问题 (2)。 最值得注意的是，他们将 ReLU 激活函数表示为凸最小化问题。 即，他们使用 ReLU 激活函数将问题 (2) 中的等式约束重新解释为以下问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
X^i &= \phi(W^{i-1}X^{i-1})\\
&= max(W^{i-1}X^{i-1},0)\\
&= \mathop{argmin}_{U^i\geq 0}\|U^i-W^{i-1}X^{i-1}\|^2_F \tag{9}
\end{align*}</script><p>其中 $max$ 是基于元素的最大值运算符。 根据解释，他们使用 ReLU 激活函数对问题 (2) 进行了近似，如下所示：</p>
<script type="math/tex; mode=display">
\begin{align*}
&\min_{\{W^i\},\{X^i\}}+\sum^n_{i=2}\frac{\mu_i}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F\\
&s.t.\; X^i\geq 0,\; i=2,3...n \tag{10}
\end{align*}</script><p>与基于 MAC 和 ADMM 的方法不同，这种松弛不包括非线性等式约束，此外，问题（10）是分块多凸的，即当其余块固定时，每个变量块它是凸的。 他们提出了一种新的 BCD 方法来求解。 然而，测试时，他们必须在固定 $\{W^i \}^{n-1}_{i=1}$ 情况下求解问题（10）或者在输出层重新训练 $W^{n-1} $。 Askari i1⁄41 等人[23] 遵循了 Zhang 和 Brand [22] 提出的想法，他们演示了如何将更一般的激活函数转换为凸最小化问题。 然而，他们训练的权重只能用于初始化 SGD 的前馈神经网络。 顾等[24] 介绍了 [23] 的后续工作，它们的公式等同于 LPOM。 然而，他们的解决方法需要解决更新基于层的激活函数约束问题，效率不高。 另外，他们的求解方法需要针对每个激活函数进行裁剪，所以不具有通用性。 值得注意的是，基于卷积和平均池化的组合也是线性的这一事实，他们结合了一个经验技巧来扩展他们的公式以直接处理卷积和平均池化。</p>
<h1 id="三，LPOM"><a href="#三，LPOM" class="headerlink" title="三，LPOM"></a>三，LPOM</h1><p>在本节中，我们介绍了 LPOM 的概念，并讨论了它相对于现有 NN 训练方法的优势。</p>
<h2 id="3-1-近端算子重构"><a href="#3-1-近端算子重构" class="headerlink" title="3.1 近端算子重构"></a>3.1 近端算子重构</h2><p>LPOM旨在解决问题（2），基本思想如下，考虑只有一个约束的问题 (2) 的简化版本</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{x,y}s(x),s.t.\; x=\phi(y) \tag{11}
\end{align*}</script><p>其中 $x$ 和 $y$ 是单变量，我们首先构造一个函数 $h(x,y)$，使得 $x=\phi(y)=\mathop{argmin}_xh(x,y)$。 那么我们将问题（11）放宽为以下问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{x,y}s(x)+\mu h(x,y) \tag{12}
\end{align*}</script><p>其中 $\mu &gt; 0$ 是惩罚参数。</p>
<p>我们首先描述问题 (11) 中 $h(x,y)$ 的构造，近端算子[26]</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathop{prox}_f(y)=f(x)+\frac{1}{2}(x,y)^2 \tag{13}
\end{align*}</script><p>这是优化算法中更新变量的基本操作，所以我们考虑用它来构造 $h(x,y)$。 这里我们假设激活函数 $\phi$ 是非递减的，那么 $\phi^{-1}(x)=\{y|x=\phi(y)\}$ 是一个凸集，$\phi^{-1}(x)$ 是单例，当且仅当 $\phi$ 在 $\phi(y)$ 处严格递增。 定义如下所示 $f(x)$</p>
<script type="math/tex; mode=display">
\begin{align*}
f(x)=\int^x_0(\phi^{-1}(y)-y)dy 
\end{align*}</script><p>请注意，如果允许取 $+\infty $ ，那么$f(x)$ 是适定的，即使 $\phi^{-1}(y)$  对于 $0$ 和 $x$ 之间的某些 y 是非唯一的。 无论如何，我们没有在计算中明确使用 $\phi^{-1}、f$ 和 $g$（稍后定义）。 很容易证明(13)的最优性条件是 $0\in(\phi^{-1}(x)-x)+(x-y)$，所以(13)的解正好是 $x=\phi(y)$。 因此，我们可以选择 $h(x,y)=f(x)+\frac{1}{2}(x,y)^2$，其中 $f(x)$ 是单变量函数。</p>
<p>下面我们考虑原问题（2）， 我们首先扩展上面的 $h(x,y)$ 到矩阵形式 $h(X,Y)$，其中 $X$ 和 $Y$ 是大小相同的矩阵。 因为我们已经有了 $h(x,y)$，逐元素函数 $h(X,Y)$ 是最需要的，让下标 $kl$ 指代矩阵的第 $(k,l)$ 项。 我们定义 $h(x,y)=\sum_k\sum_l h(X_{kl},Y_{kl})$。 为了得到更紧凑的 $h(X,Y)$ 表示，对于矩阵 $X=(X_{kl})$，我们定义 $f(x)=(f(X_{kl}))$，即，矩阵 $X$ 是由 $X_{kl}$ 表示的条目数组。 $f(X)$ 是 $X$ 上的逐元素函数，即它是一个与 $X$ 大小相同的矩阵，第 $(k,l)$ 项是 $f(X_{kl})$。 然后我们有</p>
<script type="math/tex; mode=display">
\begin{align*}
h(X,Y) &= \sum_k\sum_l h(X_{kl},Y_{kl})\\
&= \sum_k\sum_l \big\{f(X_{kl})+\frac{1}{2}(X_{kl}-Y_{kl})^2\big\}\\
&= \pmb{1}^\top f(X)\pmb{1}+\frac{1}{2}\|X-Y \|^2_F \tag{x}
\end{align*}</script><p>因此，(13)中的近端算子的矩阵形式对于问题 (2) 变为</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathop{argmin}_{X^i}\;&h(X^i,W^{i-1}X^{i-1})\\
&\equiv\pmb{1}^\top f(X^i)\pmb{1}+\frac{1}{2}\|X^i-W^{i-1}X^{i-1} \|^2_F \tag{14}
\end{align*}</script><p>其中 $X^i$ 和 $W^{i-1}X^{i-1}$ 分别扮演 (13) 中的 $x$ 和 $y$ 的角色。 类似地，我们可以证明问题（14）对于 $X^i$ 的最优性条件是</p>
<script type="math/tex; mode=display">
\begin{align*}
0\in\phi^{-1}(X^i)-W^{i-1}X^{i-1} \tag{15}
\end{align*}</script><p>其中 $\phi^{-1}(X^i)$ 也是按元素定义的， 所以(14)的最优解是 $X^i=\phi(W^{i-1}X^{i-1})$，这正是问题(2)中的约束条件。 因此我们可以自然地放松问题（2）为：</p>
<script type="math/tex; mode=display">
\begin{align*}
&\min_{\{W^i\},\{X^i\}}\ell(X^n,D)\\
&+\sum^n_{i=2}\mu_i(\pmb 1^\top f(X^i)\pmb 1+\frac{1}{2}\|X^i-W^{i-1}X^{i-1} \|^2_F) \tag{16}
\end{align*}</script><p>然而，由于（2）中的递归约束不是独立的，问题（11）和问题（2）之间存在根本区别。 即对于 $i=2…n-1$，$X^i$ 出现在 $X^i=\phi(W^{i-1}X^{i-1})$ 和 $X^{i+1}=\phi(W^iX^i)$ 中。 所以它也出现在 $h(X^i,W^{i-1}X^{i-1})$ 和 $h(X^{i+1},W^iX^i)$ 中。 为了正确逼近问题（2），$X^i=\phi(W^{i-1}X^{i-1})$ 和 $X^{i+1}=\phi(W^iX^i)$ 都应满足最优条件</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{X^i}\mu_i h(X^i,W^{i-1}X^{i-1})+\mu_{i+1}h(X^{i+1},W^iX^i) 
\end{align*}</script><p>对于 $X^i$，如下所示：</p>
<script type="math/tex; mode=display">
\begin{align*}
0\in&\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1}) \\
&+\mu_{i+1}(W^i)^\top(W^iX^i-X^{i+1}),\; i=2,...n-1 tag{17}
\end{align*}</script><p>这也是问题 (16) 中关于 $X^i$ 的最优条件。</p>
<p>不幸的是，我们可以看到 $X^i=\phi(W^{i-1}X^{i-1})$ 和 $X^{i+1}=\phi(W^iX^i)$ 并不都满足上述条件！</p>
<p>为了使 $X^i=\phi(W^{i-1}X^{i-1})$ 和 $X^{i+1}=\phi(W^iX^i)$ 都满足松弛问题的最优条件，我们需要将 (17) 修改为</p>
<script type="math/tex; mode=display">
\begin{align*}
0\in&\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1}) \\
&+\mu_{i+1}(W^i)^\top(\phi(W^iX^i)-X^{i+1}),\; i=2,...n-1\tag{18}
\end{align*}</script><p>这对应于以下问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{\{W^i\},\{X^i\}}&\ell(X^n,D)+\sum^n_{i=2}\mu_i \big(\pmb 1^\top f(X^i)\pmb 1 \\
&+\pmb 1^\top g(W^{i-1}X^{i-1})\pmb 1+\frac{1}{2}\|X^i-W^{i-1}X^{i-1} \|^2_F\big) \tag{19}
\end{align*}</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{align*}
g(x)=\int^x_0(\phi(y)-y)dy
\end{align*}</script><p>类似地，$g(X)$ 是矩阵 $X$ 的逐元素函数，问题 (19) 是 LPOM 的最终公式。 请注意，(2) 中的递归约束确保 $\{X^i\}^n_{i=2}$ 匹配网络的前馈传递 $g$ 的引入诱导出正确的再表述，还允许我们使用简单的前馈过程来推断新样本。 我们要强调的是，这是不平凡且不明显的，在表 3 中给出了一些代表性激活函数的 $f(x)$ 和 $g(x)$.</p>
<h2 id="3-2-LPOM优势"><a href="#3-2-LPOM优势" class="headerlink" title="3.2 LPOM优势"></a>3.2 LPOM优势</h2><p>我们将 $F(W,X)$ 表示为 (19) 中 LPOM 的目标函数，那么我们有下面的定理</p>
<h3 id="定理1"><a href="#定理1" class="headerlink" title="定理1"></a>定理1</h3><p>假设 $\ell(X^n,D) $ 在 $X^n$ 中是凸的并且 $f$ 是非递减的。 然后 $F(W,X)$ 是块多凸的，即在保持其他变量块固定时，它在每个 $X^i$ 和 $W^i$ 中都是凸的。</p>
<h3 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h3><p>$F(W,X)$ 可以等效地重写为</p>
<script type="math/tex; mode=display">
\begin{align*}
F(W,X) &= \ell(X^n,D)+\sum^n_{i=2}\mu_i\big(\pmb 1^\top \tilde{f}(X^i)\pmb 1\\
&+\pmb 1^\top\tilde{g}(W^{i-1}X^{i-1})\pmb 1-\langle X^i,W^{i-1}X^{i-1}\rangle \big) \tag{20}
\end{align*}</script><p>其中 $\tilde{f}(X)=\int^x_0\phi^{-1}(y)dy $，且 $\tilde{g}(x)\int^x_0\phi(y)dy $。由于 $\phi$ 和 $\phi^{-1}$ 都是非递减的，所以 $\tilde{f}(X)$ 和 $\tilde{g}(x)$ 都是凸的。 易证 $\pmb 1^\top\tilde{g}(W^{i-1}X^{i-1})\pmb 1$ 在 $W^{i-1}$ 固定时在$X^{i-1}$ 中是凸的，在 $X^{i-1}$ 固定时在 $W^{i-1}$ 中是凸的。 当其他两个块固定时，$F(W,X)$ 中的项 $\langle X^i,W^{i-1}X^{i-1}\rangle$ 在一个块中是线性的。 证明完成。</p>
<p>上述定理允许我们使用 BCD 方法来求解 LPOM。 由于每个子问题都是凸的，我们可以获得更新 $X_i$ 和 $W_i$ 的最优解。 相比之下，惩罚子问题和基于 ADMM 的方法都是非凸的。</p>
<p>与基于 ADMM 的方法 [17]、[21] 相比，LPOM 不包含拉格朗日乘数，并且不需要比 $\{X^i\}^n_{i=2}$ 更多的辅助变量。 此外，我们的 LPOM 求解方法不需要额外的辅助变量（见第 4 节），所以 LPOM 比基于 ADMM 的方法具有更少的变量，因此需要更少的内存。 实际上，由于 SGD 需要保存 $\{X^i\}^n_{i=2}$ ，因此 LPOM 的内存成本与 SGD 几乎相同。与惩罚方法 [18]、[19] 相比，LPOM 的最优条件更简单。 例如，LPOM 中 $\{X^i\}^{n-1}_{i=2}$  和 $\{W^i\}^n_{i=2}$  的最优条件是 (18) 和</p>
<script type="math/tex; mode=display">
\begin{align*}
(\phi(W^iX^i)-X^{i+1})(X^i)^\top=\pmb 0,\; i=1,...n-1 \tag{21}
\end{align*}</script><p>而用于 MAC 的是</p>
<script type="math/tex; mode=display">
\begin{align*}
&(X^i-\phi(W^{i-1}X^{i-1}))\\
&+(W^i)^\top[(\phi(W^iX^i)-X^{i+1})\circ\phi'(W^iX^i)]=\pmb 0,\; i=2,...n-1 \tag{22}
\end{align*}</script><p>和</p>
<script type="math/tex; mode=display">
\begin{align*}
[(\phi(W^iX^i)-X^{i+1})\circ\phi'(W^iX^i)](X^i)^\top=\pmb 0,\; i=1,...n-1 \tag{23}
\end{align*}</script><p>我们可以看到 MAC 的最优条件有一个额外的 $\phi’(W^iX^i)$，它是非线性的。 [19] 的最优条件可以在补充材料的附录 A 中找到， 他们还有一个额外的 $\phi’(U^i) $。 这可能意味着 MAC 和 [19] 的解集比 LPOM 的解集更复杂，所以 LPOM 可能更容易找到好的解决方案。</p>
<p>与激活函数方法 [22]、[23] 的等效表示相比，LPOM 的公式和求解方法可以处理比 [22] 和 [23] 更通用的激活函数。 请注意，求解方法是通用的意味着它对于所有可行的激活函数都是相同的。 此外，当训练网络的权重时，在 LPOM 中引入 $g$ 使我们能够应用前馈过程来预测测试样本的标签。 相比之下，Zhang 和 Brand [22] 只考虑了 ReLU 激活函数，为了获得测试样本的潜在标签，他们必须解决优化问题或重新训练输出层。 尽管 [23] 中的公式适用于一般激活函数，但其求解方法并不通用，所提出的求解方法仅适用于 ReLU。 此外，Askari 等人的表述[23] 是不正确的，因为它对 $\{X^i\}^{n-1}_{i=2} $ 和 $\{W^i\}_{i=2}^{n-1} $ 的最优条件分别是</p>
<script type="math/tex; mode=display">
\begin{align*}
0\in\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})-\mu_{i+1}(W^i)^\top X^{i+1}
\end{align*}</script><p>和</p>
<script type="math/tex; mode=display">
\begin{align*}
X^{i+1}(X^i)^\top = 0,\; i=1,...n-1
\end{align*}</script><p>可以看出，式（2）中的递归等式约束不满足上述条件。 此外，不知何故 Askari 等人[23] 对于任何激活函数，都添加了额外的约束 $X^i\geq 0$， 所以他们的公式不能很好地逼近原始问题（2），这可以解释为什么 Askari 等人的结果[23] 不好。 实际上，他们只能用 SGD 的 ReLU 激活来初始化前馈神经网络的权重。</p>
<p>与基于梯度的方法（例如 SGD）相比，LPOM 可以处理任何非递减 Lipschitz 连续激活函数而没有计算困难，包括饱和（例如，sigmoid 和 tanh）和不可微分（例如，ReLU 和 leaky ReLU）并且可以并行更新分层权重和激活（参见第 5 节）。 相比之下，基于梯度的方法只能处理有限的激活函数（例如 ReLU、leaky ReLU 和 softplus），以避免梯度消失或爆炸问题，并且在计算梯度时它们不容易跨层并行化和激活。 此外，基于梯度的方法需要很多参数调整，这并不容易[15]，而 LPOM 中惩罚参数 $\mu$ 的调整要简单得多。</p>
<h1 id="四，求解LPOM"><a href="#四，求解LPOM" class="headerlink" title="四，求解LPOM"></a>四，求解LPOM</h1><p>块多凸性（定理 1）启发了 BCD 方法来解决 LPOM。 即，我们通过固定所有其他变量块来更新 $X^i$ 或 $W^i$。 我们在算法 1 中总结了整个求解过程，其中使用一小批训练样本执行优化。 我们可以证明算法 1.3 的收敛性 下面我们给出算法的细节，它是串行的。</p>
<h2 id="4-1-更新-X-i-n-i-2"><a href="#4-1-更新-X-i-n-i-2" class="headerlink" title="4.1 更新 $\{X^i\}^n_{i=2}$"></a>4.1 更新 $\{X^i\}^n_{i=2}$</h2><p>我们首先描述 $\{X^i\}^n_{i=2} $ 的更新。 我们将 $\{X^i\}^n_{i=2} $ 从 $i=2$ 连续更新到 $n$，就像神经网络的前馈过程一样。 对于 $i=2,…n-1$，固定 $\{W^i\}^{n-1}_{i=1} $ 和其他 $\{X^j\}^n_{j=2,j\neq i} $，问题 (19) 简化为</p>
<script type="math/tex; mode=display">
\begin{align*}
&\min_{X^i}\mu_i\big(\pmb 1^\top f(X^i)\pmb 1+\frac{1}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F \big)\\
&+\mu_{i+1}\big(\pmb 1 ^\top g(W^iX^i)\pmb 1+\frac{1}{2}\|X^{i+1}-W^iX^i\|^2_F \big) \tag{24}
\end{align*}</script><p>优化条件是：</p>
<script type="math/tex; mode=display">
\begin{align*}
0\in&\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})\\
&+\mu_{i+1}((W^i)^\top(\phi(W^iX^i)-X^{i+1})) \tag{25}
\end{align*}</script><p>基于定点迭代[27]，为了避免使用 $\phi^{-1}$，我们可以通过迭代更新 $X^i$</p>
<script type="math/tex; mode=display">
\begin{align*}
X^{i,t+1}=\phi\bigg(W^{i-1}X^{i-1}-\frac{\mu_{i+1}}{\mu_i}(W^i)^\top(\phi(W^iX^{i,t})-X^{i+1})\bigg) \tag{26}
\end{align*}</script><p>直到收敛，其中上标 $t$ 为迭代次数。 收敛分析如下：</p>
<h3 id="定理2"><a href="#定理2" class="headerlink" title="定理2"></a>定理2</h3><p>假设 $\phi$ 是可微的且 $|\phi’(x)|\leq \kappa$。 如果 $\rho &lt; 1$，则迭代收敛且收敛率是线性的，其中，$\rho=\frac{\mu_{i+1}}{\mu_i}\kappa^2\sqrt{\bigg||(W^i)^\top||W^i| \bigg|_1\bigg||(W^i)^\top||W^i| \bigg|_{\infty}}$</p>
<p>请注意，上述定理中 $\rho $ 的选择是相当保守的，所以在我们的实验中，只要迭代收敛，我们就不用服从选择。<br>当考虑 $X^n$ 时，问题 (19) 简化为</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{X^n}\ell(X^n,D)+\mu_n\bigg(\pmb 1^\top f(X^n)\pmb 1+\frac{1}{2}\|X^n-W^{n-1}X^{n-1} \|^2_F \bigg) \tag{27}
\end{align*}</script><p>假设 $\ell(X^n,D)$ 是可微的。 那么最优条件是:</p>
<script type="math/tex; mode=display">
\begin{align*}
0\in\frac{\partial \ell(X^n,D)}{\partial X^n}+\mu_n(\phi^{-1}(X^n)-W^{n-1}X^{n-1}) \tag{28}
\end{align*}</script><p>同样通过定点迭代，我们可以通过迭代更新 $X^n$ 的公式为：</p>
<script type="math/tex; mode=display">
\begin{align*}
X^{n,t+1}=\phi(W^{n-1}X^{n-1}-\frac{1}{\mu_n}\frac{\partial\ell(X^n,D)}{\partial X^n}) \tag{29}
\end{align*}</script><p>直到收敛。 收敛分析如下：</p>
<h3 id="定理3"><a href="#定理3" class="headerlink" title="定理3"></a>定理3</h3><p>假设 $\phi(x)$ 是可微的，$|\phi’(x)|\leq \kappa$，而且 $\bigg||(\frac{\partial^2\ell(X,D)}{\partial X_{kl}\partial X_{pq}})| \bigg|_1\leq \eta$，如果有 $\tau&lt;1$，那么迭代是收敛的，收敛速度是线性的，其中 $\tau=\frac{\kappa\eta}{\mu_n}$.</p>
<p>如果 $\ell(X^n,D)$ 是 MSE 损失函数，也就是 $\ell(X^n,D)=\frac{1}{2}|X^n-D |^2_F $，那么 $\bigg||(\frac{\partial^2\ell(X,D)}{\partial X_{kl}\partial X_{pq}})| \bigg|_1=1$，所以有 $\mu_n&gt;\kappa$.</p>
<p>值得注意的是，（19）中的 $f$ 和 $g$ 是积分，它们的表达式非常复杂，可能无法解析（见表 3）。 所以最好在计算中使用他们的导数而不是他们自己。 然而，$f$ 的导数包括 $\phi^{-1}$ （见（25）和（28）），我们认为在更新 $\{X^i\}^n_{i=2}$ 时最好避免使用 $\phi^{-1}$。 原因如下，对于常用的激活函数 $\phi$，$\phi^{-1}$ 的定义域（或等价于 $\phi$ 的范围）通常不是 $(-\infty,+\infty)$。 所以我们需要在计算中约束 $\phi^{-1}$ 的输入，这导致受约束的问题无法像不受约束的问题那样有效地解决。 使用 $\phi^{-1}$ 的另一大缺点是它可能不是单值的（见表 3），这会给计算和确保收敛带来很大困难。 此外，不同的 $\phi^{-1}$ 可以具有不同的域，由于必须为每个 $\phi^{-1}$ 调整 $\{X^i\}^n_{i=2}$ 的更新，因此整个求解方法不能适用于一般的激活函数，这就是为什么 [23] 和 [24] 中的求解方法需要针对每个激活函数进行调整的原因。 由于我们只使用 $\phi$ 而没有使用 $\phi^{-1}$，我们的方法没有这样的限制。 所以我们的求解方法比[23]和[24]中的方法更实用。</p>
<h2 id="4-2-更新-W-i-n-1-i-1"><a href="#4-2-更新-W-i-n-1-i-1" class="headerlink" title="4.2 更新$\{W^i\}^{n-1}_{i=1}$"></a>4.2 更新$\{W^i\}^{n-1}_{i=1}$</h2><p>$\{W^i\}^{n-1}_{i=1}$ 的更新是完全并行的。 当 $\{X^i\}^n_{i=2}$ 固定，问题（19）减少到：</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{W^i}\pmb 1^\top g(W^iX^i)\pmb 1+\frac{1}{2}\|W^iX^i-X^{i+1} \|^2_F,\;i=1,...n-1 \tag{30}
\end{align*}</script><p>上述求解 $W^i$ 的问题独立于 $\{W^j\}^{n-1}_{j=1,j\neq i}$ 的其他问题，所以可以并行解决，我们将 (30) 重写为</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{W^i}\pmb 1^\top \tilde g(W^iX^i)\pmb 1+\langle X^{i+1},W^iX^i \rangle \tag{31}
\end{align*}</script><p>其中 $\tilde g(x)=\int^x_0\phi(y)dy$，如前所述，假设 $\phi(x)$ 是 $\beta-Lipschitz$ 连续的，这对于几乎所有使用的激活函数都是正确的。 则 $\tilde g(x)$ 是 $\beta-$ 光滑的</p>
<script type="math/tex; mode=display">
\begin{align*}
|\tilde g'(x)-\tilde g'(y)|=|\phi(x)-\phi(y)|\leq\beta|x-y| \tag{32}
\end{align*}</script><p>我们借用 APG [28] 中通过局部线性化 $\hat g(W)=\tilde{g}(WX) $ 来解决问题 (31)。 然而，由于 $\hat g(W)$ 梯度的 $Lipschitz$ 常数 $\beta|x|^2_2$ 可能非常大，收敛速度可能很慢。 因此，我们开发了一种 APG 变体，专为更有效地解决 (31) 问题而设计。</p>
<p>从优化的角度来看，问题（31）可以更一般地表示为</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_xG(x)\equiv\varphi(Ax)+\psi(x) \tag{33}
\end{align*}</script><p>其中，$\varphi(y)$ 和 $\psi(x)$ 都是凸的，$\varphi(y)$ 是 $L_{\varphi}-$ 光滑的，也就是有：$|\nabla \varphi(x)-\nabla\varphi(y)|\leq L_{\varphi}|x-y|,\;\forall x,y$。假设有如下的最小化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
x_{k+1}=\mathop{argmin}_x\langle \nabla\varphi(Ay_k),A(x-y_k)\rangle+\frac{L_{\varphi}}{2}\|\|^2+\Psi(x) \tag{34}
\end{align*}</script><p>对于任何给定的 $y_k$ 很容易求解，我们提出算法 2 来解决 (33)，这是从其收敛定理的证明推导出来的。</p>
<h3 id="定理4"><a href="#定理4" class="headerlink" title="定理4"></a>定理4</h3><p>如果我们用算法2来解决问题（33），那么收敛速度至少为 $O(k^{-2})$</p>
<script type="math/tex; mode=display">
\begin{align*}
G(x_k)-G(x^\ast)+\frac{L_{\varphi}}{2}\|z_k\|^2\leq\frac{4}{k^2}\bigg(G(x_1)-G(x^\ast)+\frac{L_{\varphi}}{2}\|z_1\|^2 \bigg) 
\end{align*}</script><p>其中，$z_k=A[\theta_{k-1}x_{k-1}-x_k+(1-\theta_{k-1})x^\ast]$，$x^\ast$ 是问题（33）的最优解。</p>
<p>当我们考虑解决问题（31）时，子问题（34）的实例化为：</p>
<script type="math/tex; mode=display">
\begin{align*}
W^{i,t+1}=&\mathop{argmin}_W\langle\phi(Y^{i,t}X^i),(W-Y^{i,t})X^i\rangle\\
&+\frac{\beta}{2}\|(W-Y^{i,t})X^i\|^2_F-\langle X^{i+1},WX^i\rangle \tag{35}
\end{align*}</script><p>这是一个最小二乘问题，它的解是</p>
<script type="math/tex; mode=display">
\begin{align*}
W^{i,t+1}=Y^{i,t}-\frac{1}{\beta}(\phi(Y^{i,t}X^i)-X^{i+1})(X^i) \tag{36}
\end{align*}</script><p>其中 $Y^{i,t}$ 在算法 2 中扮演 $y_k$ 的角色。</p>
<h1 id="五，LPOM的并行"><a href="#五，LPOM的并行" class="headerlink" title="五，LPOM的并行"></a>五，LPOM的并行</h1><p>上一节介绍的求解方法是串行的，在本节中，我们研究了 LPOM 的并行解决方案。</p>
<h2 id="5-1-异步并行-LPOM"><a href="#5-1-异步并行-LPOM" class="headerlink" title="5.1 异步并行 LPOM"></a>5.1 异步并行 LPOM</h2><p>如第 4 节所述，求解 LPOM 的原始方法是 BCD。 因此很自然地使用异步-并行 BCD（async-BCD）[29]、[30] 来并行求解 LPOM。 然而，现有的 async-BCD 主要使用梯度下降来仅一次更新每个块，而在 LPOM 中，梯度 （ $X^i$） 涉及激活函数的逆函数，这就是 LPOM 使用定点迭代来代替的原因（参见（26）和（29））。 所以现有的async-BCD实际上并不适用，在下文中，我们提出了一种新的异步 BCD 方法。</p>
<p>假设优化问题是</p>
<script type="math/tex; mode=display">
\begin{align*}
\min_{\{z_i\}}F(z_1,...,z_n) \tag{37}
\end{align*}</script><p>我们的 async-BCD 执行如下：如果 $z_i$ 被选为更新，更新为</p>
<script type="math/tex; mode=display">
\begin{align*}
z_i^{k+1}=&\mathop{argmin}_{z_i}F(z_1,...z_{i-1},z_i,z_{i+1},...,z_n)\\
&+\frac{\gamma}{2}\|z_i-z_i^k\|^2 \tag{38}
\end{align*}</script><p>其中 $z_j,j\neq i$，取 $z_i$ 可用的最新值。 请注意，由于通信延迟等原因，$z_j$ 的此类值可能比正在计算的 $z_j$ 的实际 $j$ 个最新值更旧。 近端项 $\frac{\gamma}{2}|z_i-z_i^k|^2$ 是保证我们的异步 BCD 收敛所必需的。</p>
<p>为了分析我们的异步 BCD 的收敛性，我们做出以下假设。</p>
<h3 id="假设1"><a href="#假设1" class="headerlink" title="假设1"></a>假设1</h3><p>$F(z)\equiv F(z_1,…z_n) $ 具有坐标 $Lipschitz$ 连续梯度，即对于所有 $i\in\{1,…n\}$，我们有</p>
<script type="math/tex; mode=display">
\begin{align*}
\|\nabla_iF(z)-\nabla_iF(\overline z) \|\leq L_i\|z-\overline z\|
\end{align*}</script><h3 id="假设2"><a href="#假设2" class="headerlink" title="假设2"></a>假设2</h3><p>信息的陈旧性（有效性？）是有界的，即</p>
<script type="math/tex; mode=display">
\begin{align*}
k-\tau^i_j(k)\leq \Delta,\; k=0,1...
\end{align*}</script><p>其中 $\tau^i_j(k)$ 是可用于在时间 $k$ 更新 $z_i$ 的 $z_j$ 时间</p>
<p>我们注意到这两个假设都是分析异步算法的标准 [29]、[30]，现在我们要求我们的异步 BCD 的收敛保证。</p>
<h3 id="定理5"><a href="#定理5" class="headerlink" title="定理5"></a>定理5</h3><p>假设 $F(z)$ 是分块多凸的。 在假设 1 和 2 下，通过设置（38）中的逼近参数 $\gamma$ 使 $\frac{\gamma}{2}+\frac{L^2}{2\gamma}(\Delta+1)-\frac{L^2}{2\gamma}(\Delta+1)^2&gt;0 $，其中 $L=max\{L_i\}$，那么有 $|\Delta_i F(z^k)|\to 0,\forall i \in \{1,…n\}$</p>
<p>由于 $\gamma&gt;0$，根据上述定理，我们有 $\gamma&gt;L\sqrt{\Delta(\Delta+1)}$。 因此 $\gamma$ 的选择取决于 $\Delta$ 和 $L$ 的估计。我们的异步 BCD 可用于求解 LPOM，从而得到异步 LPOM，对于 async-LPOM，我们可以将 $\Delta$ 设置为 CPU 核心数。 假设 $\phi(x)$ 是 $\beta-Lipschitz$ 连续的（在（32）中定义），我们可以将 $L$ 设置为 $L=\beta max\{|W^2|^2_2,…|W^{n-1}|^2_2,|X^1|^2_2,…|X^{n-1}|^2_2 \} $。 $\gamma$ 的理论估计值非常大，在实践中，我们通过逐渐增加其值来选择 $\gamma$，直到 async-LPOM 收敛。 然而，async-LPOM 并没有完全利用问题 (19) 的属性，即每个 $X^i$ 仅与 $X^{i-1}$ 相关，并且 $X^i$ 和 $\{W^i\}^{n-1}_{i=1}$ 可以完全并行更新。 所以 async-LPOM 并没有我们预期的那么快。 这促使我们考虑开发同步并行 LPOM (sync-LPOM)，它可以更好地利用 LPOM 的特性。</p>
<h2 id="5-2-同步并行-LPOM"><a href="#5-2-同步并行-LPOM" class="headerlink" title="5.2 同步并行 LPOM"></a>5.2 同步并行 LPOM</h2><p>我们首先分析算法 1 中的串行 LPOM。当 $\{X^i\}_{i=2}^n$ 固定时，每个 $W^i$ 的更新独立于其他 $W^j，j \neq i$（见（30））。 因此算法 1 中的步骤 c 可以完全并行化完成。 我们生成 $n-1$ 个线程，每个线程独立地求解每个 $W^ i$ 。 在 $\{W^i\}^{n-1}_{i=1} $ 固定的情况下，$X^i (i\in\{2,…n-1\})$ 的更新与 $X^{i-1}$ 和 $X^{i+1}$ 有关（参见 (26)），而 $X^n$ 的更新与 $X^{n-1}$ 有关 （见（29））。 所以我们只需要同步并行化步骤 a 和 b 。</p>
<p>我们将 $X^i$ 的更新并行化如下，我们创建了 $n-1$ 个线程。 每个线程迭代方程（26）或方程(29) 来更新 $X^i$， 该步骤由每个线程独立且同时执行。 当所有线程完成该步骤时，我们重复该过程多次，直到达到最大时间。 然而，我们注意到更新相邻 $X^i$ 时的耦合（参见 (26) 和 (29)， 解耦块有助于并行算法的收敛，最大化并行度也是并行算法所追求的。 那么更新 $X^i$ 的奇偶交替方案是唯一可以同时具有最大并行度和最小耦合度的选择。 即，我们首先同步更新奇数子序列 $\{X^3,X^5,…\}$，然后使用更新的 $\{X^3,X^5,…\}$，我们同步更新偶数子序列 $\{X^2,X^4,…\}$。 此外，受 SGD 中使用的动量的启发，我们考虑了每个 $X^i$ 的先前值，导致更新后的 $X^i$ 序列的移动平均值，sync-LPOM 的整个算法在算法 3 中进行了总结。请注意，sync-LPOM 是 async-LPOM 的特例，其中允许更新块的顺序是非随机的。 所以定理 5 的结论也适用于 sync-LPOM。</p>
<h1 id="六，实验"><a href="#六，实验" class="headerlink" title="六，实验"></a>六，实验</h1><p>在本节中，我们对所提出的 LPOM 和 sync-LPOM 进行了广泛的实验评估。 我们首先评估 LPOM 在图像分类任务上的性能，也进行了消融研究来分析 LPOM。 然后，我们研究了 sync-LPOM 在自动编码器训练任务上的计算效率和性能。</p>
<p>我们在自动编码器训练任务上测试 sync-LPOM 的原因有如下几个：首先，目前 LPOM 和 sync-LPOM 仅适用于全连接前馈神经网络。 其次，我们的主要目标是评估 sync-LPOM 的加速。 但是，如果存在具有主导计算的层，则无法加速 sync-LPOM， 比如784-10-10-10网络，$X^2$ 的更新比 $X^3$ 和 $X^4$ 慢16倍，串行 LPOM 和同步 LPOM 都将以更新 $X^2$ 为主，因此同步 LPOM 不会比串行 LPOM 有加速。 因此，sync-LPOM 有利于等宽网络，这在分类网络中并不常见。 这促使我们选择自动编码器。 最后，深度自动编码器具有挑战性，因为它们必须在一层非常低维的约束下重建输入，困难主要来自网络架构而不是使用的数据集，在基准数据集上训练自动编码器被认为是评估前馈神经网络优化方法的标准问题 [18]，[32]。 出于上述原因，我们使用各种自动编码器架构和数据集测试 sync-LPOM。</p>
<h2 id="6-1-LPOM结果"><a href="#6-1-LPOM结果" class="headerlink" title="6.1 LPOM结果"></a>6.1 LPOM结果</h2><p>我们通过与三种具有代表性的基于梯度的方法（SGD、Adam [10] 和 AMSGrad [11]）和三种基于非梯度的方法 [17]、[23]、[24] 进行比较来测试 LPOM。 其他基于非梯度的方法不训练用于分类任务的完全连接的前馈神经网络（例如，使用跳跃连接 [22]、训练自动编码器 [18] 和哈希学习 [21]）。 所以我们不跟他们比较。 为简单起见，我们使用 MSE 损失函数。 由于几种方法[23]，[24]只给出了ReLU的求解方法，我们也使用了ReLU激活函数。 与 [23] 和 [24] 不同，我们不对权重 $\{W^i \}^{n-1}_{i=1}$ 使用任何正则化，因为它通常无助于减少训练损失。 我们使用相同的输入和随机初始化运行 LPOM 和三种基于梯度的方法 [33]。 我们使用 MATLAB 实现 LPOM、SGD、Adam 和 AMSGrad。 对于 LPOM，我们在 (19) 中为所有网络设置 $\mu_i=2$。 对于 SGD，学习率设置为最大可行值，并在总 epoch 的一半和四分之三处乘以 $0.1$。 对于 Adam 和 AMSGrad，我们仔细调整参数以达到最佳性能。 对于 [23] 和 [24]，我们使用作者提供的具有默认参数设置的源代码。 对于 [17]，我们阅读了论文图 1b 中的结果。</p>
<h3 id="6-1-1-与基于梯度的方法比较"><a href="#6-1-1-与基于梯度的方法比较" class="headerlink" title="6.1.1 与基于梯度的方法比较"></a>6.1.1 与基于梯度的方法比较</h3><p>我们在 MNIST [34]、CIFAR-10 [35] 和 ImageNet [36] 数据集上进行实验。 MNIST 数据集有 60,000 个训练图像和 10,000 个测试图像，这些图像与来自 10 个类别的标签相关联。 我们使用 $28\times 28=784$ 个原始像素作为输入，不使用任何预处理或数据增强。 对于所有比较的方法，在每个时期，整个训练样本都经过一次。 网络架构可能会影响性能。 由于过度参数化在训练 [37]、[38] 时极大地促进了基于梯度的方法，因此我们与过度参数化网络进行了比较。 在 [19] 之后，我们使用 784-2048-2048-2048-10 前馈网络。 我们以固定的批量大小 100 运行 100 个 epoch 的所有方法。训练和测试精度如图 1 和 2 所示。 1a 和 1b。 我们可以看到所有方法的最终训练准确率都大约等于 100%。 然而，LPOM 的测试精度与 Adam、AMSGrad 和 SGD 的测试精度相当或略好。 最终的测试准确率是：Adam $98.1%$，AMSGrad $98.5%$，SGD $98.5%$，LPOM $98.6%$。</p>
<p>CIFAR-10 数据集有 50,000 个训练图像和 10,000 个测试彩色图像，与来自 10 个类别的标签相关联。 我们使用 32 32 31⁄43072 个原始像素作为输入。 与 [19] 中一样，我们使用 3072-4000-1000-4000-10 前馈网络。 我们通过分别减去训练数据集的红色、绿色和蓝色通道的均值来标准化每个彩色图像。 我们不使用任何数据扩充。 我们以固定的批量大小 100 运行 100 个 epoch 的所有方法。训练和测试精度如图 1 和 2 所示。 1c 和 1d。 我们可以看到只有 LPOM 的训练准确率大约等于 $100%$。 基于梯度的方法没有实现零训练损失。 这可能是因为使用的网络具有低维隐藏层，难以优化。 在比较测试精度时，我们可以看到 LPOM 能够匹配或击败其他方法。 最终的测试准确率是：Adam $53.5%$，AMSGrad $54.7%$，SGD $54.7%$，LPOM $54.7%$。 请注意，LPOM 获得了近乎完美的训练精度，但测试精度与其他训练精度低得多的方法相当。 我们要注意，这种情况不是过度拟合的问题。 首先，过度拟合用于评估可训练模型的性能而不是优化方法。 所以过拟合的定义不适用。 其次，如果一个模型达到了相当或更好的训练精度，但测试精度比其他模型差得多，我们可能会认为它过度拟合了训练数据集。 虽然 LPOM 训练的网络具有更好的训练精度，但其测试精度与其他方法训练的网络相当或略好。 所以过拟合现象不适用于这种情况。 第三，LPOM 是一种神经网络优化方法。 所以它的主要目标是拟合训练数据集。 测试准确性在很大程度上取决于训练期间使用的权重正则化的类型和数量，这是实践中的一个重要问题。 由于我们专注于优化，因此权重正则化不在我们的讨论范围之内。</p>
<p>ImageNet 数据集包含来自 1,000 个类别的 1,281,167 张训练图像和 50,000 张验证图像。 ImageNet 数据集中的图像通常为 $224\times 224\times 3$ 像素。 为了进行快速实验，我们改用 ImageNet32x32 数据集 [39]，该数据集可以从 ImageNet 项目的官方网站下载。ImageNet32x32 数据集保留了原始 ImageNet 数据集的复杂性，在自动化机器学习 (AutoML) 中被广泛采用 . 它是 ImageNet 数据集的下采样版本，大小为 $32\times 32\times 3$ 像素。 它具有与 ImageNet 数据集相同数量的图像和类别。 我们将所有验证图像作为测试集。 我们对每个图像使用与 CIFAR-10 图像相同的预处理。 我们使用原始像素作为输入，不使用任何数据增强。 我们以固定批量大小 100 运行 100 个 epoch 的所有方法。我们使用 3072-1024-1024-1024-1000 前馈网络。 训练和测试精度如图 2 所示。我们可以看到 LPOM 达到了最好的训练和测试精度。 最终的训练精度为：Adam $0.102%$、AMSGrad $0.611%$、SGD $1.112%$ 和 LPOM $17.916%$。 最终测试精度为：Adam $0.100%$、AMSGrad $0.318%$、SGD $0.596%$ 和 LPOM $2.064%$。 与文献中报道的精度相比，这些精度非常低。 原因如下。 首先，我们使用全连接 NN 而不是 CNN。 其次，较低的图像分辨率会使分类任务变得更加困难。 第三，找到适合 ImageNet32x32 的全连接神经网络的最佳深度和隐藏层宽度需要付出很多努力，这不是我们论文的目标，我们在文献中没有找到这样的参考，所以我们只选择一个相对 简单的一个。 第四，我们不使用数据扩充来提高测试性能。 第五，我们没有对所有比较方法使用额外的学习技巧，例如权重正则化、BN 和动量。</p>
<h3 id="6-1-2-与其他不基于梯度的方法比较"><a href="#6-1-2-与其他不基于梯度的方法比较" class="headerlink" title="6.1.2 与其他不基于梯度的方法比较"></a>6.1.2 与其他不基于梯度的方法比较</h3><p>我们首先在 MNIST 数据集上使用相同的架构与 [23] 和 [24] 进行比较。 与 [23] 中一样，我们使用固定批大小 100 运行 17 个 epoch 的所有方法。我们不使用任何预处理或数据扩充。 表 4 总结了三种方法的测试精度，其中最佳值以粗体显示。 由于 [24] 中的公式等同于 LPOM 的公式，我们可以看到 [24] 和具有 ReLU 激活函数的 LPOM 比 [23] 表现得更好。 这符合我们在第 3.2 节中的分析。 尽管使用等效公式，LPOM 仍然优于 [24]，这证明了我们求解方法的优越性。<br>按照 [17] 中的数据集和网络架构设置，我们在街景门牌号 (SVHN) 数据集 [40] 上测试 LPOM。 表 5 总结了 SGD、[17]、[23]、[24] 和 LPOM 的测试精度，其中最佳值以粗体显示。 我们可以看到 LPOM 实现了最佳性能。 这进一步证明了LPOM的优势。</p>
<h3 id="6-1-3-LPOM-的消融研究"><a href="#6-1-3-LPOM-的消融研究" class="headerlink" title="6.1.3 LPOM 的消融研究"></a>6.1.3 LPOM 的消融研究</h3><p>我们对 LPOM 进行消融研究，以评估 (19) 中的惩罚参数 $\mu_i$ 和所用激活函数的影响。 我们使用 MSE 损失函数和 MNIST 数据集，而不使用任何预处理或数据扩充。 我们使用 784-400-200-100-50-10 前馈网络，这是 [23] 中最深的网络。 对于所有情况，我们使用相同的源代码和随机初始化 [33]。 对于每种情况，我们运行 LPOM 20 个周期，固定批大小为 100。</p>
<p>我们使用表 3 中显示的所有激活函数测试 LPOM。我们分别为 leaky ReLU 和 ELU 激活函数设置 $\alpha=0.01$ 和 $\alpha=1$。 对于 (19) 中的惩罚参数 $\mu_i$，我们为每个激活函数设置 $\mu_i=\mu$，其中 $\mu\in\{2, 5, 10, 20, 50, 100\}$。 我们不调整不同 $\mu_i$ 的原因将在后面给出。 表 6 显示了具有各种 $\mu$ 和激活函数的 LPOM 的最终训练和测试精度。 我们可以看到，具有每个激活函数的 LPOM 在相当大的 $\mu$ 范围内保持稳定。 这进一步证实了LPOM的参数设置是非常容易的。 对于分类任务，当 $\mu \leq 50$ 时，具有 sigmoid、tanh、ReLU 或 leaky ReLU 激活函数的 LPOM 的性能优于其余两个函数。在图 3 中，我们将 LPOM 的行为与 $\mu_i=5$ 进行比较 不同的激活函数。 可以看出，具有 ReLU 或 leaky ReLU 激活函数的 LPOM 收敛得更快。</p>
<p>我们对 LPOM 的经验参数设置（即在 (19) 中设置 $\mu_i =\mu$）实际上具有理论依据。 顾等。 [24] 开发了一种可变缩放策略来证明我们对 LPOM 的参数设置在理论上保证了 ReLU 激活函数。 也就是说，理论上我们可以将（19）中的惩罚参数 $\mu_i$ 减少到 ReLU 的一个参数 $\mu$。 实际上，变量缩放策略仅适用于正齐次函数。 即，$\phi(\lambda x)=\lambda\phi(x) $ 对于所有 $\lambda\geq 0$. 这样的函数可以写成</p>
<script type="math/tex; mode=display">
\begin{align*}
\phi(x)=\begin{cases} 
\alpha x,\; & x\geq 0\\
\beta x,\; & x<0
\end{cases}\tag{39}
\end{align*}</script><p>其中 $\alpha &gt; 0$ 和 $\beta \geq 0$. 我们在补充材料的附录 G 中提供证明，可在线获取。 (39) 中的形式包括线性、ReLU 和 leaky ReLU 激活函数。 我们在 (19) 中的经验设置 $\mu_i = \mu$ 尚未在理论上证明适用于所有可行的激活函数，例如 ELU、softplus、sigmoid 和 tanh。 然而，我们要注意，正齐次激活函数在实践中是最流行的。 ELU 和 softplus 激活函数是正齐次对应函数的平滑近似。 所以使用单一参数 $\mu$ 也大致适用于 ELU 和 softplus。 至于 sigmoid 和 tanh 激活函数，我们可以从表 6 中看出，当在 (19) 中将 $\mu_i=\mu$ 时，它们也能很好地工作。 因此，我们认为在 LPOM 中使用单个惩罚参数应该足以满足大多数常用激活函数的需求。 最后，我们要强调的是，定理 3 为选择 $\mu_n$ 提供了理论参考。 例如，在使用MSE损失函数和tanh激活函数时，我们应该为LPOM设置 $\mu_n &gt; 1$。 总的来说，我们可以得出结论，LPOM 中惩罚参数的调整非常简单。</p>
<h2 id="6-2-同步-LPOM-的结果"><a href="#6-2-同步-LPOM-的结果" class="headerlink" title="6.2 同步 LPOM 的结果"></a>6.2 同步 LPOM 的结果</h2><p>我们在 C++ 中实现了串行 LPOM、sync-LPOM、SGD、Adam 和 AMSGrad。 没有对权重 $\{W^i\}^{n-1}_{i=1} $ 使用正则化。 我们使用 MSE 损失函数和 ReLU 激活函数。 较小的训练或测试损失值表示更好的性能。 对于所有方法，我们使用相同的输入、随机初始化 [33] 和批量大小（此处为 100）。 对于 sync-LPOM，我们使用 POSIX 线程作为并行编程框架。 对于 LPOM 和 sync-LPOM，我们设置 $\mu_i = 20$。对于 SGD，学习率设置为最大可行值，并在总 epoch 的一半和四分之三处乘以 $0.1$。 对于 Adam 和 AMSGrad，我们调整参数以获得最佳性能。 所有实验均在一台 Intel(R) Core(TM) i7-8700 CPU 3.20 GHz 12 核计算机上进行。 与 [32] 一样，我们在三个灰度图像数据集上进行测试，总结在表 7 中。Curves 数据集由合成曲线组成，MNIST 数据集包含手写数字，Faces 数据集是增强的 Oli - vetti 人脸数据集。</p>
<h3 id="6-2-1-CPU-的计算效率"><a href="#6-2-1-CPU-的计算效率" class="headerlink" title="6.2.1 CPU 的计算效率"></a>6.2.1 CPU 的计算效率</h3><p>所有方法的编码和解码时间都相等。 所以我们只比较训练中的时间成本。 我们在 MNIST 数据集上进行了三组实验。</p>
<p>为了编程方便，我们将每一层分配给一个 CPU 内核，这样可以减少不同层更新之间的干扰，从而可以更好地比较加速性能。 因此，对于 sync-LPOM，理想的核心数等于网络层数。 我们首先在各种等宽自动编码器（此处维度为 784）上测试具有理想 CPU 内核的 sync-LPOM 的加速。 层加速比定义为</p>
<script type="math/tex; mode=display">
\begin{align*}
\text{layer speedup ratio}=\frac{\text{serial time per epoch}}{\text{parallel time per epoch}}
\end{align*}</script><p>训练时间和层加速比如图 1 和 2 所示。 分别为 4a 和 4b。 我们可以看到，随着隐藏层的增加，sync-LPOM 的训练时间比串行 LPOM 增加得更慢，并且层加速比几乎是线性的。<br>然后使用固定深度和宽度（这里是 784）的自动编码器，我们测试了具有不同数量 CPU 内核的同步 LPOM 的加速。 这里的核心加速比定义为</p>
<script type="math/tex; mode=display">
\begin{align*}
\text{core speedup ratio}=\frac{\text{serial time per epoch}}{\text{parallel time per epoch using d CPU cores}}
\end{align*}</script><p>结果示于图1和2中。 4c 和 4d， 令 n 为层数。 当$d\leq n/2$ 时，我们可以看到所有核心加速比都随着CPU 核心的增加而线性增长。 当 $d &gt; n/2$ 时，核心加速比变为常数（见图 4d）。 理由如下。 由于sync-LPOM更新Xi的奇偶交替方案，我们只需要 $n/2$ 个核来同步更新 $X^i$。 同步更新 $W^i$ 需要 $n -1$ 个核。 然而，更新 $X^i$ 比更新 $W^i$ 更昂贵。 因此，$n/2$ 内核的内核加速比与 $n$ 内核具有竞争力。</p>
<p>在图 4e 中，我们使用自动编码器 784-1024-500-1024-784 [41] 绘制了 Adam、AMSGrad、SGD、LPOM 和 sync-LPOM 的训练损失与运行时间的关系图。 对于 SGD，学习率在第 300 和 450 轮时乘以 $0.1$。 我们可以看到 sync-LPOM 收敛最快，训练损失最低。</p>
<h3 id="6-2-2-优化性能"><a href="#6-2-2-优化性能" class="headerlink" title="6.2.2 优化性能"></a>6.2.2 优化性能</h3><p>我们使用 [32]、[41] 中考虑的两个浅自动编码器和三个深自动编码器测试 sync-LPOM。 表 7 给出了使用的数据集、训练集和测试集的大小以及测试的编码器网络架构。 请注意，我们使用对称自动编码器，其中解码器架构是编码器的镜像。 我们运行 100 个 epoch 的所有方法。 对于 SGD，学习率在第 50 和第 75 轮时乘以 $0.1$。</p>
<p>单层和多层自编码器的训练和测试损失如图 1 和 2 所示。 分别为 5a 和 5b。 我们可以看到 sync-LPOM 确实具有最好的性能。 Adam 和 AMSGrad 在前几个 epoch 中实现了比 SGD 更低的训练和测试损失。 然而，它们在训练结束时不如 SGD。 一些在测试集上使用多层自动编码器重建的样本如图 6 所示。我们可以看到 sync-LPOM 具有最好的重建性能。 深度自动编码器问题比影子问题更难。 训练和测试损失如图 1 和 2 所示。 5c、5d 和 5e。 可以看出，在 Curves 和 MNIST 数据集上，sync-LPOM 略优于 SGD。 然而，当考虑 Faces 数据集时，我们可以看到 sync-LPOM 明显优于 SGD、Adam 和 AMSGrad。</p>
<h3 id="6-2-3-讨论"><a href="#6-2-3-讨论" class="headerlink" title="6.2.3 讨论"></a>6.2.3 讨论</h3><p>我们还使用表 7 中所示的五个自动编码器测试了 sync-LPOM 相对于 SGD 的实际加速。对于每个反编码器，我们首先运行 sync-LPOM 50 个时期并记录其运行时间和训练损失。 我们将其时间和损失序列分别表示为 $\{t_i^1\}^{50}_{i=1}$ 和 $\{e_i^1\}^{50}_{i=1}$。 设 $\{e_i^1\}^{50}_{i=1}$ 中第一个小于或等于 $e^1_j$ 的元素，称为初始稳定点。 然后我们运行 SGD，直到它的训练损失 $e^2_k$ 小于或等于 $e^1_j$，我们将相应的运行时间表示为 $t^2_k$。 然后，sync-LPOM 相对于 SGD 的实际加速比定义为：$t^2_k/t^1_j$，其中大于 $1$ 的值意味着 sync-LPOM 比 SGD 更快，可以实现令人满意的训练损失。 如果 SGD 运行时间比 $t^1_j$ 长 $20$ 倍，但其损失仍然高于 $e^1_j$，我们终止 SGD 并将实际加速比标记为：$&gt; 20x$。 每个自动编码器的实际加速比显示在图 5 中相应数字下方，以粗体显示。 我们可以看到 sync-LPOM 使用各种自动编码器实现了优于 SGD 的加速。</p>
<p>我们要注意，我们不与并行 SGD 进行比较。 原因如下。 首先，LPOM 可以跨数据或层并行化。 我们只利用它的层并行化。 实现数据并行化或多或少是一项工程工作。 所以我们不做这方面的工作。 由于 [42] 中的并行 SGD 是数据并行的，因此我们不将其包括在内进行比较。 其次，async-SGD 和 sync-SGD 可以实现加速，但它们的性能比 SGD [43] 差得多。 第三，我们从未声称 sync-LPOM 在一个时期内比 SGD 更快。 我们只声称它比 SGD 更快以实现相对较低的训练损失。 所以我们用serial SGD作为我们的竞争对手，这样比较合理。</p>
<p>sync-LPOM（或 LPOM）实现比 SGD 更低的训练和测试损失是因为它适当地提升了问题的维度并解决了提升的问题。 所以它可以很容易地移动到更高维度空间中的好点。 相反，SGD 直接优化问题。 所以优化路径受损失情况的限制，非常复杂。 因此，我们将 sync-LPOM（或 LPOM）的成功归功于“维度的祝福”。</p>
<h1 id="七，结论"><a href="#七，结论" class="headerlink" title="七，结论"></a>七，结论</h1><p>在这项工作中，我们开发了 LPOM 来训练全连接的前馈神经网络。 通过将激活函数重写为等效的近端算子，LPOM 将 NN 的训练公式化为块多凸模型。 这引出了我们具有收敛保证的新型 BCD 求解方法。 由于公式和求解方法，LPOM 避免了梯度消失或爆炸问题，适用于一般非递减 $Lipschitz$ 连续激活函数。 此外，它不需要比分层激活更多的辅助变量，并且其参数调整相对简单。 也可以并行解决。 我们首先提出了一种具有收敛保证的新异步 BCD 方法。 然后我们用它来求解LPOM，得到async-LPOM。 为了更快的速度，我们开发了 sync-LPOM。 我们的实验结果表明，LPOM 在全连接前馈 NN 上的效果优于 SGD、Adam、AMS-Grad、[17]、[23] 和 [24]。 我们还验证了 sync-LPOM 在各种自动编码器训练问题上的效率和有效性。 未来的工作包括扩展 LPOM 以训练卷积和递归神经网络，并将 LPOM 应用于网络量化。</p>
]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>LPOM</tag>
      </tags>
  </entry>
  <entry>
    <title>markdown中的符号和使用小技巧</title>
    <url>/2023/05/06/tools/markdown_tools/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>写markdown的小技巧和实用符号随记</p>
<p><strong>多写写会更熟悉～</strong><br><span id="more"></span></p>
<h1 id="一，希腊字母"><a href="#一，希腊字母" class="headerlink" title="一，希腊字母"></a>一，希腊字母</h1><div class="table-container">
<table>
<thead>
<tr>
<th>字母</th>
<th>markdown</th>
<th>字母</th>
<th>markdown</th>
</tr>
</thead>
<tbody>
<tr>
<td>$A$</td>
<td><code>$A$</code></td>
<td>$\alpha$</td>
<td><code>$\alpha$</code></td>
</tr>
<tr>
<td>$B$</td>
<td><code>$B$</code></td>
<td>$\beta$</td>
<td><code>$\beta$</code></td>
</tr>
<tr>
<td>$\Gamma$</td>
<td><code>$\Gamma$</code></td>
<td>$\gamma$</td>
<td><code>$\gamma$</code></td>
</tr>
<tr>
<td>$\Delta$</td>
<td><code>$\Delta$</code></td>
<td>$\delta$</td>
<td><code>$\delta$</code></td>
</tr>
<tr>
<td>$E$</td>
<td><code>$E$</code></td>
<td>$\epsilon$</td>
<td><code>$\epsilon$</code></td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>$\varepsilon$</td>
<td><code>$\varepsilon$</code></td>
</tr>
<tr>
<td>$Z$</td>
<td><code>$Z$</code></td>
<td>$\zeta$</td>
<td><code>$\zeta$</code></td>
</tr>
<tr>
<td>$H$</td>
<td><code>$H$</code></td>
<td>$\eta$</td>
<td><code>$\eta$</code></td>
</tr>
<tr>
<td>$\Theta$</td>
<td><code>$\Theta$</code></td>
<td>$\theta$</td>
<td><code>$\theta$</code></td>
</tr>
<tr>
<td>$I$</td>
<td><code>$I$</code></td>
<td>$\iota$</td>
<td><code>$\iota$</code></td>
</tr>
<tr>
<td>$K$</td>
<td><code>$K$</code></td>
<td>$\kappa$</td>
<td><code>$\kappa$</code></td>
</tr>
<tr>
<td>$\Lambda$</td>
<td><code>$\Lambda$</code></td>
<td>$\lambda$</td>
<td><code>$\lambda$</code></td>
</tr>
<tr>
<td>$M$</td>
<td><code>$M$</code></td>
<td>$\mu$</td>
<td><code>$\mu$</code></td>
</tr>
<tr>
<td>$N$</td>
<td><code>$N$</code></td>
<td>$\nu$</td>
<td><code>$\nu$</code></td>
</tr>
<tr>
<td>$\Xi$</td>
<td><code>$\Xi$</code></td>
<td>$\xi$</td>
<td><code>$\xi$</code></td>
</tr>
<tr>
<td>$O$</td>
<td><code>$O$</code></td>
<td>$\omicron$</td>
<td><code>$\omicron$</code></td>
</tr>
<tr>
<td>$\Pi$</td>
<td><code>$\Pi$</code></td>
<td>$\pi$</td>
<td><code>$\pi$</code></td>
</tr>
<tr>
<td>$P$</td>
<td><code>$P$</code></td>
<td>$\rho$</td>
<td><code>$\rho$</code></td>
</tr>
<tr>
<td>$\Sigma$</td>
<td><code>$\Sigma$</code></td>
<td>$\sigma$</td>
<td><code>$\sigma$</code></td>
</tr>
<tr>
<td>$\Psi$</td>
<td><code>$\Psi$</code></td>
<td>$\psi$</td>
<td><code>$\psi$</code></td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>$\ell$</td>
<td><code>$\ell$</code></td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>$\tau$</td>
<td><code>$\tau$</code></td>
</tr>
<tr>
<td>$\Phi$</td>
<td><code>$\Phi$</code></td>
<td>$\varphi$</td>
<td><code>$\varphi$</code></td>
</tr>
</tbody>
</table>
</div>
<h1 id="二，花体字"><a href="#二，花体字" class="headerlink" title="二，花体字"></a>二，花体字</h1><p>主要关注的花体字类型有：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$\mathbb$</span><br><span class="line">$\mathcal$</span><br><span class="line">$\mathscr$</span><br><span class="line">$\mathrm$</span><br><span class="line">$\mathbf$</span><br><span class="line">$\mathit$</span><br><span class="line">$\mathsf$</span><br><span class="line">$\mathtt$</span><br><span class="line">$\mathfrak$</span><br></pre></td></tr></table></figure></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>$\mathbb$</code></td>
<td>$\mathbb A$</td>
<td>$\mathbb B$</td>
<td>$\mathbb C$</td>
<td>$\mathbb D$</td>
</tr>
<tr>
<td><code>$\mathcal$</code></td>
<td>$\mathcal A$</td>
<td>$\mathcal B$</td>
<td>$\mathcal C$</td>
<td>$\mathcal D$</td>
</tr>
<tr>
<td><code>$\mathscr$</code></td>
<td>$\mathscr A$</td>
<td>$\mathscr B$</td>
<td>$\mathscr C$</td>
<td>$\mathscr D$</td>
</tr>
<tr>
<td><code>$\mathrm$</code></td>
<td>$\mathrm A$</td>
<td>$\mathrm B$</td>
<td>$\mathrm C$</td>
<td>$\mathrm D$</td>
</tr>
<tr>
<td><code>$\mathbf$</code></td>
<td>$\mathbf A$</td>
<td>$\mathbf B$</td>
<td>$\mathbf C$</td>
<td>$\mathbf D$</td>
</tr>
<tr>
<td><code>$\mathit$</code></td>
<td>$\mathit A$</td>
<td>$\mathit B$</td>
<td>$\mathit C$</td>
<td>$\mathit D$</td>
</tr>
<tr>
<td><code>$\mathsf$</code></td>
<td>$\mathsf A$</td>
<td>$\mathsf B$</td>
<td>$\mathsf C$</td>
<td>$\mathsf D$</td>
</tr>
<tr>
<td><code>$\mathtt$</code></td>
<td>$\mathtt A$</td>
<td>$\mathtt B$</td>
<td>$\mathtt C$</td>
<td>$\mathtt D$</td>
</tr>
<tr>
<td><code>$\mathfrak$</code></td>
<td>$\mathfrak A$</td>
<td>$\mathfrak B$</td>
<td>$\mathfrak C$</td>
<td>$\mathfrak D$</td>
</tr>
</tbody>
</table>
</div>
<h1 id="二，数学符号"><a href="#二，数学符号" class="headerlink" title="二，数学符号"></a>二，数学符号</h1><div class="table-container">
<table>
<thead>
<tr>
<th>含义</th>
<th>示例</th>
<th>markdown代码</th>
<th>含义</th>
<th>示例</th>
<th>markdown代码</th>
</tr>
</thead>
<tbody>
<tr>
<td>恒等于</td>
<td>$\equiv$</td>
<td><code>$\equiv$</code></td>
</tr>
</tbody>
</table>
</div>
<h1 id="other"><a href="#other" class="headerlink" title="other"></a>other</h1><ul>
<li><p>向量形式给字母加粗<br>eg: $\pmb{a},\pmb{1}$<br>markdown代码：<code>$\pmb&#123;a&#125;,\pmb&#123;1&#125;$</code></p>
</li>
<li><p>矩阵转置<br>eg：$\mathbf{A}^\mathrm{T}$<br>markdown: <code>$\mathbf&#123;A&#125;^\mathrm&#123;T&#125;$</code><br>eg: $\mathbf{A}^\top$<br>markdown: <code>$\mathbf&#123;A&#125;^\top$</code><br>eg：$\mathbf{A}^\mathsf{T}$<br>markdown: <code>$\mathbf&#123;A&#125;^\mathsf&#123;T&#125;$</code><br>eg: $\mathbf{A}^\intercal$<br>markdown: <code>$\mathbf&#123;A&#125;^\intercal$</code></p>
</li>
<li><p>字母上的各种符号<br>$\grave{a}$<br>$\check{a}$<br>$\acute{a}$<br>$\breve{a}$<br>$\tilde{a}$<br>$\vec{a}$  向量<br>$\overline{a}$ 平均值<br>$\widehat{a}$ (线性回归，直线方程) y尖<br>$\widetilde{a}$ 颚化符号  等价无穷小<br>$\dot{a}$   一阶导数<br>$\ddot{a}$  二阶导数</p>
</li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么读论文？</title>
    <url>/2023/05/05/tools/how_to_read_paper/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>又言论文十问，由沈向洋博士提出</p>
<p><strong>加油读文章</strong><br><span id="more"></span></p>
<h2 id="哪十个问题呢？"><a href="#哪十个问题呢？" class="headerlink" title="哪十个问题呢？"></a>哪十个问题呢？</h2><ul>
<li>Q1论文试图解决什么问题？</li>
<li>Q2这是否是一个新的问题？</li>
<li>Q3这篇文章要验证一个什么科学假设？</li>
<li>Q4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</li>
<li>Q5论文中提到的解决方案之关键是什么？</li>
<li>Q6论文中的实验是如何设计的？</li>
<li>Q7用于定量评估的数据集是什么？代码有没有开源？</li>
<li>Q8论文中的实验及结果有没有很好地支持需要验证的科学假设？</li>
<li>Q9这篇论文到底有什么贡献？</li>
<li>Q10下一步呢？有什么工作可以继续深入？</li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformers from an Optimization Perspective</title>
    <url>/2023/05/05/Transformer/Transformers%20from%20an%20Optimization%20perspective/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>从优化的角度理解Transformer模型</p>
<p>论文百度云链接: <a href="https://pan.baidu.com/s/1fQiIXRyUw4LBhXIfqCVUKA">https://pan.baidu.com/s/1fQiIXRyUw4LBhXIfqCVUKA</a><br>提取码: 3w62 </p>
<p><strong>加油读文章</strong><br><span id="more"></span></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><p>诸如 Transformer 之类的深度学习模型通常是通过启发式方法和经验构建的。 为了补充基础研究，在这项工作中，我们关注了以下问题：是否有可能找到 Transformer 模型的能量函数，使得该能量函数的下降步骤与 Transformer 模型的前向传递相对应？ 通过找到这样的函数，我们可以将 Transformer 视为跨迭代的可解释优化过程的展开。 过去经常采用这种展开的观点来阐明更直接的深度模型，例如 MLP 和 CNN，然而，到目前为止，对于带自注意力机制（如 Transformer）的复杂模型，获得类似的等价性仍然难以捉摸。 为此，我们首先概述了几个主要障碍，然后提供了至少部分解决这些障碍的配套技术，首次证明了能量函数最小化与带自注意力机制的深层模型之间的密切联系。 这种解释有助于我们对 Transformer 的直觉和理解，同时可能为新模型设计奠定基础。</p>
<h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>建立Transformer模型的能量函数，期待能量最小化的过程对应模型的前向传播过程，已经在 CNN 和 MLP 上有了一定成果，暂时没有在Transformer 模型上的研究成果出现，这是本文的出发点。</p>
<h1 id="一，介绍"><a href="#一，介绍" class="headerlink" title="一，介绍"></a>一，介绍</h1><p>尽管深度学习在实践中取得了广泛的成功 [26、27]，但模型架构通常是启发式创建，多数时候仍然难以理解，因此，最近投入了大量精力来分析和重新解释流行的深度网络结构的各种方法。 出于我们的目的，这里需要特别注意深度模型层和一些优化过程展开迭代之间的一对一关联，这些优化过程旨在最小化可解释的能量函数 [14、15、53、55、41]。 通过这种方式，前向传递至少可以近似地被视为计算，所以具有最小能量的表示，其中模型参数决定了这种能量的形式，并且可以通过反向传递来训练下游感兴趣的任务，例如分类或回归等。</p>
<p>站在这种所谓的展开优化视角，可以深入了解不同架构中归纳偏差的性质，同时可能作为一种指南，通过所涉及的基础能量函数的性质来指导深度网络结构的实例化。事实上，对后者的修改可以导致可预测的模型行为，这些行为在过去会被剪枝优化来保证包含有用的正则化因素 [59]，避免微妙的退化方案 [55]，或组合起来设计有效的新模型 [1、14]。</p>
<p>尽管取得了这些成功，但该领域的大多数前置工作要么解决了相对简单的网络结构，例如 MLP [53]，要么将问题限制在不同于大系统的单个模型组件 [41] 上。 因此，诸如广泛使用的 Transformer [49] 之类的复杂模型大多被忽略了，这在很大程度上是因为很难同时将自注意力和前馈网络 (FFN) 模块映射到一个集成的优化过程，而且可以进行最小化的单个、统一的能量函数。</p>
<p>我们通过以下步骤将展开的优化视角扩展到完整的 Transformer 堆栈：</p>
<ul>
<li><p>在提供了展开优化的背景细节和现有示例之后，第 2 节形式化了将 Transformer 层映射到优化步骤的四个关键挑战，降低了一个显式表示的能量函数。 包括：(i) 处理自注意力，(ii) 集成异构 Transformer 层模块（即自注意力和 FFN），(iii) 考虑非线性激活，以及 (iv) 允许任意、非结构化权重矩阵。</p>
</li>
<li><p>随后，第 3 节推导了一个能量函数，该函数紧密地再现了挑战(i)中的自注意力 ，第 4 节严格推导了收敛结果，展示了如何通过一种新颖的交替最小化形式近似处理挑战 (ii)，以及第 5 节 提供通过近端方法解决挑战 (iii) 的明确细节； 对挑战 (iv) 的考虑与其他支持分析一起推迟到补充部分。</p>
</li>
<li><p>上述贡献最终形成了一个能量函数，当某些技术条件成立时，该能量函数被 Transformer 层最小化。 为了提供进一步的动力，第 6 节凭经验证明，即使在技术条件难以正式验证的情况下，实际数据也会将这种能量最小化。</p>
</li>
</ul>
<p>总的来说，这些结果为理解 Transformer 模型提供了补充基础，并为未来的增强提出了合理的途径。</p>
<h1 id="二，由展开的优化形成的深层架构简介"><a href="#二，由展开的优化形成的深层架构简介" class="headerlink" title="二，由展开的优化形成的深层架构简介"></a>二，由展开的优化形成的深层架构简介</h1><p>展开优化视角的基本思想是创建一对一的对应关系，对象分别是深度模型的层和模型的能量函数的最小化求解过程的迭代算法。 这样，前向传递可以被视为计算最小能量解的近似值。 同时，假设每个前向迭代都是可微的，这种情况下等能量函数参数，可以通过近似最小化步骤中的反向传递来传递一些元目标函数（例如，分类、回归等）的梯度，如此形成一个可解释的双层优化过程[11、14、24]。 总的来说，展开优化产生最小的能量表示，由此能量函数的特定结构可以在相关下游应用领域进行训练。</p>
<h2 id="2-1-数学表示"><a href="#2-1-数学表示" class="headerlink" title="2.1 数学表示"></a>2.1 数学表示</h2><p>上面描述的双向优化过程可以更正式地表示为：</p>
<script type="math/tex; mode=display">
\begin{align*}
Y^\ast(W,X)&=\mathop{argmin}_YE(Y;W,X)\tag{1}\\
W^\ast(X)&=\mathop{argmin}_W\ell(\psi[Y^\ast(W,X)])\tag{2}
\end{align*}</script><p><strong>理解</strong><br>式子（1）是优化问题，式子（2）是深度模型解决的问题。</p>
<p>其中，$X\in\mathbb{R}^{n\times d}$是模型的输入，在语言模型中就是输入的语句，$n$ 是 token 长度，$d$是token维度。参考 $Y\in\mathbb{R}^{n\times d}$ 是模型的隐藏表征，我们用 $Y^\ast$ 来表示模型的输出结果（也就是Transformer 编码器的单词表证输出），并且 $Y^\ast$ 最小化最小能量函数 $E:\mathbb{R}^{n\times d}\to\mathbb{R}$，能量函数的形式由参数 $W$ 决定，对应于Transformer 中的可训练参数。$\ell$ 是元损失函数，$\psi: \mathbb{R}^{n\times d}\to\mathbb{R}^{n\times d}$ 是将模型的隐藏表达转化为模型输出的函数，当然也可能有可训练的参数。假设 $\partial Y^\ast(W,X)/\partial W$ 和 $\partial\ell/\partial Y^\ast(W,X)$ 是有意义的，当然这个形式也要包含求近似 $Y^\ast(W,X)$ 时所求的梯度，之后上文中提到的双边系统就可以用参数 $W$ 来优化。</p>
<p>请注意，在下文中，如果能量函数不包括令牌间的交互，我们将其视为向量函数，并且为简单起见，我们使用小写字符 $y$ 和 $x$ 来表示 $Y$ 和 $X$ 的任意行。 此外，当清楚上下文信息时，我们省略写函数参数，例如我们可以用 $E(Y)$ 来表示 $E(Y ; W, X)$。</p>
<h2 id="2-2-相关工作和局限"><a href="#2-2-相关工作和局限" class="headerlink" title="2.2 相关工作和局限"></a>2.2 相关工作和局限</h2><p>在本节中，我们介绍了展开优化的相关工作，特别是分析了一些相关的限制，这些限制是我们努力的动力，并强调了所涉及的挑战。</p>
<h3 id="优化诱导的前馈网络"><a href="#优化诱导的前馈网络" class="headerlink" title="优化诱导的前馈网络"></a>优化诱导的前馈网络</h3><p>各种各样的前置工作已经从展开的优化角度研究了前馈结构 [14、16、20、22、47、53]。 在这里，我们仔细研究基于近端方法的 [53]。 [53]中的基本能量函数是</p>
<script type="math/tex; mode=display">
\begin{align*}
E(y)=\pmb{1}^\top\tilde{\sigma}^\ast(W^{-\top}y)-\langle f(x),W^{-\top}y\rangle-\frac{1}{2}\|y\|^2\tag{3}
\end{align*}</script><p>其中 $\pmb{1}$ 是一个所有元素都为 1 的向量，$\tilde{\sigma}^\ast$ 是 $\tilde{\sigma}:a\to\int^a_0\sigma(r)dr$ 的凸共轭 [42],$\sigma$ 是激活函数，$f(x)$ 是输入表示 $x$ 的某种变换。 式子(3) 的近端算子 [12] 是</p>
<script type="math/tex; mode=display">
\begin{align*}
y^{(t+1)}=prox_E\big(y^{(t)}\big)=W^\top\sigma(W^{(t)}+f(x))\tag{4}
\end{align*}</script><p>这类似于 MLP 的前馈层。 请注意，只要维数适当对齐，虽然 $W$ 原则上可以是任意矩阵，但在堆叠该模型的多个层后，实际上强制约束了有效变换：</p>
<script type="math/tex; mode=display">
\begin{align*}
y^{(t+2)}=W^\top\sigma[WW^\top\sigma(Wy^{(t)}+f(x))+f(x)]\tag{5}
\end{align*}</script><p>很明显，在第一层之后，实际的前馈变换变成了 $WW^\top$，这必然是半正定的 (PSD)。 因此，在此范例中实际上层权重不可能不受约束。</p>
<h3 id="优化诱导的注意力机制"><a href="#优化诱导的注意力机制" class="headerlink" title="优化诱导的注意力机制"></a>优化诱导的注意力机制</h3><p>也有部分有限的工作试图从展开优化的角度解释注意力机制 [15, 41]。 例如，若有如下形式的能量函数</p>
<script type="math/tex; mode=display">
\begin{align*}
E(y)=-\beta^{-1}log\big(\sum^d_{i=1}e^{\beta S_iy}\big)+\frac{1}{2}\|y\|^2\tag{6}
\end{align*}</script><p>在[41]中提出，随后使用凹凸过程[57]进行更新，有形式如下的迭代</p>
<script type="math/tex; mode=display">
\begin{align*}
y^{(t+1)}=Ssoftmax(\beta SY^{(t)})\tag{7}
\end{align*}</script><p>其中 $S\in\mathbb{R}^{n\times n}$ 对应于注意力机制中的键，而 $y$ 映射到注意力机制中的查询，$β$ 是常数标量。 但至关重要的是，这种注意力机制实际上是交叉注意力（使用 $y$ 来关注 $S$），这与 Transformers 的典型自注意力用例不一致。 此外，这项工作没有考虑聚合的 Transformer 前馈网络模块和随之而来的非线性。</p>
<h3 id="优化诱导的图神经网络"><a href="#优化诱导的图神经网络" class="headerlink" title="优化诱导的图神经网络"></a>优化诱导的图神经网络</h3><p>从类似的优化角度 [8、31、34、37、54、55、58、60] 也开发了各种图神经网络架构。 例如，图注意力机制是使用 [55] 中的迭代重新加权最小二乘法 (IRLS) 算法推导出来的。 虽然这个结果与自注意力相关，可以将其视为全连接图上的图注意力 [5]，但它无法产生 Transformer softmax 项或组合的自注意力/前馈 Transformer 堆栈。 尽管如此，我们稍后将展示如何利用 [55] 中的想法来通过 softmax 获得自我关注。</p>
<h2 id="2-3-扩展到通用Transformer的主要挑战"><a href="#2-3-扩展到通用Transformer的主要挑战" class="headerlink" title="2.3 扩展到通用Transformer的主要挑战"></a>2.3 扩展到通用Transformer的主要挑战</h2><p>在本文中，我们关注比之前工作更复杂的模型，即 Transformer 编码器。 Transformer 层 [49] 通常由两个主要组件组成：自注意力层和前馈网络 (FFN) 层。 如果我们将 FFN 简化为具有非线性激活的单个线性变换，并忽略层归一化和残差连接，一个 Transformer 层将简化为基本形式：</p>
<script type="math/tex; mode=display">
\begin{align*}
Y^{(t+1)}=ReLU[softmax(Y^{(t)}W_aY^{(t)\top})Y^{(t)}W_f]\tag{8}
\end{align*}</script><p>其中 $W_a$ 和 $W_f$ 分别代表self-attention和FFN层权重矩阵。 关于 (8) 和上面讨论的先前工作的局限性，我们提出了将展开优化正式应用于 Transformer 设置的<strong>四大挑战</strong>：</p>
<ol>
<li><p>Token-Level Interactions：如前所述，Transformer 模型不仅包括前馈过程，还包括跨 token 交互，这反映在被认为是 Transformers 的必要条件的自我注意机制上。 到目前为止，之前的工作还没有衍生出 Transformer 风格的自注意力，而是要么放松到交叉注意力 [41]，要么未能产生无处不在的 softmax 运算符 [55]。</p>
</li>
<li><p>异构层类型：每个 Transformer 编码器层都由两个完全不同的组件组成：self-attention 和 FFN。 我们将这种结构称为“异构层类型”，其中每个组件都有自己的参数，可以看作是一个独特的展开优化过程。 然而，在聚合前向传递过程中是否有可能结合这两个分量的相应能量以获得具有任何类型的收敛保证（甚至近似）的统一目标仍然未知。</p>
</li>
<li><p>非线性激活：至少在一些相对简单的前馈模式的孤立上下文中，可以从近端算子 [29、55] 的角度理解常见神经网络架构中使用的许多激活函数。 然而，当集成到上述异构 Transformer 层类型中时，优化过程变得相当复杂，并且不知道在包含近端步骤后是否仍然存在任何收敛特性。</p>
</li>
<li><p>权重不对称：正如在 (5) 的讨论中提到的，大多数处理前馈网络的工作范围实际上仅限于具有对称（或更严格的 PSD）权重变换的模型 [1、14、53] ，这限制了由此产生的普遍性。 此外，图神经网络文献中的相关模型通常基于无向图 [54、55]，其中图传播也是对称的。 因此，尽管据我们所知之前没有讨论过，但仍然不知道如何为更一般的不对称变换构造能量函数。</p>
</li>
</ol>
<p>在本文中，我们提出了至少部分解决挑战 1、2 和 3 的技术，而对于 4，我们将初步讨论推迟到补充，将正式调查作为未来的方向。</p>
<h1 id="三，Transformer-Self-Attention的一种新推导"><a href="#三，Transformer-Self-Attention的一种新推导" class="headerlink" title="三，Transformer Self-Attention的一种新推导"></a>三，Transformer Self-Attention的一种新推导</h1><p>现在，我们通过构建一个能量函数来解决挑战1，该函数的迭代优化步骤与 Transformer 风格的 softmax 自注意力完全匹配，这是惯例。</p>
<h2 id="3-1-展开的优化步骤-vs-基本的-Softmax-自注意力"><a href="#3-1-展开的优化步骤-vs-基本的-Softmax-自注意力" class="headerlink" title="3.1 展开的优化步骤 vs 基本的 Softmax 自注意力"></a>3.1 展开的优化步骤 vs 基本的 Softmax 自注意力</h2><p>考虑能量函数</p>
<script type="math/tex; mode=display">
\begin{align*}
E_1(Y)=\sum^n_{i=1}\sum^n_{j=1}\rho\bigg(\frac{1}{2}\|y_i-y_j\|^2\bigg)+R(Y)\tag{9}
\end{align*}</script><p>其中 $y_i\in\mathbb{R}^{d\times1}$ 是矩阵 $Y\in\mathbb{R}^{n\times d}$ 的第 $i$ 行，$\rho: \mathbb R^+ \to \mathbb R$ 是凹非减函数，$R:\mathbb R^{n\times d} \to \mathbb R$ 是凸函数。 有趣的是，尽管由于 $\rho$ 的非凸性，$E_1$ 不一定是凸的，但在 $\rho$ 和 $R$ 的特定选择下，它可以通过类似 softmax 的结构进行优化，如下所示：</p>
<h3 id="定理3-1"><a href="#定理3-1" class="headerlink" title="定理3.1"></a>定理3.1</h3><p>假设 $\rho(z)=-exp\{-z\},\quad R(Y)=\frac{1}{2}|Y|^2_{\mathcal F}$，且 $\beta_i=exp\big\{-\frac{1}{2}|y_i^{(t)}|^2\big\}$，令 $Y^{(t)}$ 表示 $Y$ 中的任意固定值，则有如下更新策略：</p>
<script type="math/tex; mode=display">
\begin{align*}
y^{(t+1)}_i=\frac{\sum^n_{j=1}\beta_j exp\{y_i^{(t)\top}y_j^{(t)}\}y^{(t)}_j}{\sum^n_{j=1}\beta_j exp\{y_i^{(t)\top}y_j^{(j)}\}},\;\forall i\tag{10}
\end{align*}</script><p>满足 $E_1(Y^{(t+1)})\leq E_1(Y^{(t)})$，等式成立的条件是当且仅当 $Y^{(t)}$ 是 $E_1$的固定点。</p>
<p>值得一提的是，虽然不是很明显，但更新步骤 (10) 可以通过最大-最小化 (MM) 算法生成 [48]，其中最大化步骤产生凸上界，最小化步骤沿上界的梯度下降。 因此，本次更新的核心本质上是凸函数上的梯度步，这将与后续章节的讨论相关； 请参阅补充文件中的证明和更多详细信息。</p>
<h3 id="remark-3-2"><a href="#remark-3-2" class="headerlink" title="remark 3.2"></a>remark 3.2</h3><p>定理 3.1 有几个值得注意的地方。 首先，虽然在定理 3.1 中我们采用特定形式的 $\rho$ 和 $R$ 来恢复 softmax 算子，但是通过这些函数的其他选择，相应的展开优化算法可以生成不同类型的注意机制。 其次，获得（10）依赖于梯度步长的特定选择； 然而，对于更广泛的选择，由此产生的收敛更新会导致残余连接作为优化轨迹的自然副产品（见补充）。 最后，定理 3.1 可以很容易地修改，以适应完全 Transformer 连接受某些图结构约束的情况，如 [10, 18] 中所示（同样，详见补充）。</p>
<h2 id="3-2-拓展到包含可训练参数"><a href="#3-2-拓展到包含可训练参数" class="headerlink" title="3.2 拓展到包含可训练参数"></a>3.2 拓展到包含可训练参数</h2><p>在聚合成矩阵形式后，我们到目前为止已经证明了迭代：</p>
<script type="math/tex; mode=display">
\begin{align*}
Y^{(t+1)}=softmax_{\beta}\big(Y^{(t)}Y^{(t)\top}\big)Y^{(t)}\tag{11}
\end{align*}</script><p>将减少（或保持不变）来自 (9) 的能量，其中 $softmax_{\beta}(y)_i = \frac{\beta_i exp\{y_i\}}{\sum_j \beta_j exp\{y_j\}}$ 表示具有重新加权系数向量 $\beta$ 的 softmax 运算符。 如果 $\beta_i$ 独立于 $i$，即$|y_i^{(t)}|$ 是常数（它可以通过层归一化强制执行，如前所述，层归一化也可以包含在我们的框架中），那么这个重新加权的 softmax 相当于 Transformers 中使用的规范 softmax。</p>
<p>现在考虑重新参数化 $Y = ZW_a$，其中 $W_a \in \mathbb R^{d\times d}$ 是可逆矩阵。 由此得出 $Z^{(t+1)}W_a = softmax_{\beta} (Z^{(t)}W_aW_a^\top Z^{(t)\top}) Z^{(t)}W_a$，导致修改后的更新规则：</p>
<script type="math/tex; mode=display">
\begin{align*}
Z^{(t+1)} = softmax_\beta (Z^{(t)}W_a^sZ^{(t)\top})Z^{(t)} \tag{12}
\end{align*}</script><p>其中 $W_a^s = W_aW_a^\top$，我们采用上标“$s$”表示该矩阵是对称的。 总的来说，本节的结果直接解决了挑战 1，在有和没有可训练参数的情况下（对称权重的主要挥之不去的限制被降级为挑战 4）密切再现 softmax 风格的自注意力。</p>
<h1 id="四，通过交替不精确最小化组合Transformer组件"><a href="#四，通过交替不精确最小化组合Transformer组件" class="headerlink" title="四，通过交替不精确最小化组合Transformer组件"></a>四，通过交替不精确最小化组合Transformer组件</h1><p>接下来我们将解决挑战2 和异构 Transformer 层类型。 带着这个目标，我们首先介绍一个通用的优化场景。 具体来说，我们提出以下问题：<br>给定两个（凸）目标 $f(y)$ 和 $g(y)$，在什么条件下或者在什么程度上，将交替使用单独的梯度步骤，也就是说 $f$ 和 $g$ 优化聚合函数 $f + g$ ？</p>
<p>我们将这种优化策略称为交替不精确最小化 (alternating inexact minimization, AIM)，我们将很快展示它收敛到包含 $f + g$ 最佳点的有限半径球的条件。 稍后我们将讨论这些结果如何有助于解决挑战2。</p>
<p>我们预先强调，如此定义的 AIM 与文献中通常所说的交替最小化完全不同 [19, 33]。 后者指的是在固定其他变量的情况下，以交替方式一次最小化具有多个变量的统一目标函数的场景，这样可以轻松实现下降。 相比之下，我们的 AIM 场景涉及多个带有共享变量的客观项，我们使用相同的变量交替最小化每个项，这是一个更具挑战性的分析过程。</p>
<h2 id="4-1-一般交替不精确最小化"><a href="#4-1-一般交替不精确最小化" class="headerlink" title="4.1 一般交替不精确最小化"></a>4.1 一般交替不精确最小化</h2><p>给定两个目标 $f, g:\mathbb R^d \to \mathbb R$，AIM 形式化为算法 1，其中 $\alpha_1$ 和 $\alpha_2$ 构成步长。 我们现在将研究算法 1 如何与由 $h(y) = f(y) + g(y)$ 定义的组合目标的最小化相关。</p>
<p>在下文中，我们假设 $f、g$ 都是 Lipschitz 连续且强凸的 [42]，分别具有 Lipschitz 常数 $L_f$ 和 $L_g$，以及凸性参数 $c_f$ 和 $c_g$。 因此，$h$ 也将是 Lipschitz 光滑且强凸的，具有 Lipschitz 常数 $L_h$ 和凸性参数 $c_h$。 我们将 $f、g$ 和 $h$ 的最优点分别表示为 $y_f^\top、y_g^\top$ 和 $y_h\top$。</p>
<p>整合算法1中的两个步骤，也就是：</p>
<script type="math/tex; mode=display">
\begin{align*}
y^{(t+1)} = y^{(t)}-\alpha_1 \nabla f(y^{(t)})-\alpha_2 \nabla g[y^{(t)}-\alpha_1 \nabla f(y^{(t)})] \tag{13}
\end{align*}</script><p>而目标 $h$ 上的规范梯度下降步骤是 $y^{(t+1)} = y^{(t)}−\alpha_2[\nabla f(y^{(t)}) + \nabla g( y^{(t)})]$ 。 比较两个更新规则很明显，算法 1 可以被视为具有步长 $\alpha_2$ 和噪声因子的噪声梯度下降步骤，</p>
<script type="math/tex; mode=display">
\begin{align*}
\Delta_t = \nabla h(y^{(t)})-\frac{\alpha_1}{\alpha_2}\nabla f(y^{(t)})-\nabla g[y^{(t)}-\alpha_1\nabla f(y^{(t)})] \tag{14}
\end{align*}</script><p>在某些情况下，噪声梯度下降是一个经过充分研究的问题 [4, 36]。 然而，在我们的特定场景中，我们需要一种新颖的、不同的（实际上更严格的）界限，而不是简单地应用现有结果所能得到的界限。</p>
<p>具体来说，我们证明当 $\delta(y^{(t)}) = |\Delta_t| / |\nabla h(y^{(t)})|$ 有界时，(13) 保证下降目标 $h$ 如下：</p>
<h3 id="定理4-1"><a href="#定理4-1" class="headerlink" title="定理4.1"></a>定理4.1</h3><p>当 $\alpha_1 \leq \alpha_2 \leq L_h^{-1}$ ，假设 $y^{(t)}$ 和 $y^{(t+1)}$ 的关系由(13)给出，而且有 $\delta(y^{(t)})\leq \mathcal L$ 和 $\mathcal L = \frac{\alpha_2}{\alpha_2-\alpha_1+\alpha_2\alpha_1L_g}$，那么有 $h(y^{(t+1)})\leq h(y^{(t)}) $。</p>
<h3 id="Remark-4-2"><a href="#Remark-4-2" class="headerlink" title="Remark 4.2"></a>Remark 4.2</h3><p>尽管 $\mathcal L$ 的定义可能看起来相当复杂，但当 $\alpha_1、\alpha_2、L_f$ 或 $L_g$ 之一足够小时，$\mathcal L$ 表现为 $\Omega([\alpha_1\alpha_2L_fL_g]^{−1})$ 。</p>
<p>我们进一步考虑如何解释约束 $\delta (y) ≤ \mathcal L$ 以及 (13) 可以优化 $h$ 的区域，如下所示：</p>
<h3 id="引理4-3"><a href="#引理4-3" class="headerlink" title="引理4.3"></a>引理4.3</h3><p>可令 $S(\mathcal L) = \{y|\frac{|y-y_f^\ast|}{|y-y_h^\ast|}\leq\frac{c_h\mathcal L}{L_f} \}$，若有 $y^{(t)}\in S(\mathcal L),\;\delta(y)\leq\mathcal L $。 </p>
<p>结合定理 4.1 和引理 4.3，我们因此可以得出结论，当 $y^{(t)} \in S(\mathcal L)$ 时， $h(y^{(t+1)}) ≤ h(y^{(t)})$ 。 请注意，引理 4.3 中给出的 $S(\mathcal L )$ 的边界称为阿波罗圆[43]。 当 $\mathcal L≤\frac{L_f}{c_h}$ 时，$S(\mathcal L)$ 是一个以 $y_f^\ast$ 为中心的球，当 $\mathcal L\geq \frac{L_f}{c_h}$ 时，$S(\mathcal L)$ 是整个不包括以 $y_h^\ast$ 为中心的球的空间。 图 1 分别提供了每个案例的二维可视化。 此外，请注意，可以通过切换 $f$ 和 $g$ 的角色（因为这两个过程是交替的）并进一步限制例外区域来完成相同的分析，尽管这里我们为简单起见省略了这一点。</p>
<h3 id="Remark-4-4"><a href="#Remark-4-4" class="headerlink" title="Remark 4.4"></a>Remark 4.4</h3><p>我们上面的发现可以总结如下：对于足够小的 $\alpha_1$ 和 $\alpha_2$ 值，算法 1 减少了组合目标 $h$，至少假设 $y$ 距离最优点 $y_h^\ast$ 有一定距离。</p>
<p>为了说明这个结论，我们提出了一个合成示例 $f(Y ) = |SY |^2_{\mathcal F} + |Y − B_1|^2_{\mathcal F}$ 和 $g(Y)=|YW|^2_{\mathcal F} +|Y −B_2|^2_{\mathcal F}$。注意这里我们将变量展开到矩阵 $Y$ 并让 $f$ 和 $g$ 分别由 $Y$ 的左变换和右变换组成，带有一个额外的偏置项以防止退化解 ($Y_\ast =0$)。我们随机设置 $\{S,W,B_1,B_2\}$ 的每个条目并以固定步长执行算法 1 并投影 $Y^{(t)}$ 的轨迹 到带有 PCA 的二维空间进行可视化。 $Y^{(t)}$ 的轨迹显示在图 3 中，组合目标 $h$ 跨迭代显示在图 2 中。从这些图中，我们的理论预测的行为可以得到验证： 当 $Y^{(t)}$ 与 $Y_h^\ast$ 有足够的距离时 （能量比较高），优化轨迹向 $Y_h^\ast$ 靠拢（能量下降），当 $Y^{(t)}$ 足够接近 $Y_h^\ast$ 后（且 $h$ 处于比较低的水平），能量震荡在一定范围内的最优解。</p>
<h2 id="4-2-异构层类型模型的展开优化"><a href="#4-2-异构层类型模型的展开优化" class="headerlink" title="4.2 异构层类型模型的展开优化"></a>4.2 异构层类型模型的展开优化</h2><p>我们现在回到特定于 Transformer 的模型。 令 $E_2(Y ) = \frac{1}{2} Tr(Y W_f Y^\top) + \frac{1}{2} |Y |^2_{\mathcal F}$ 并考虑组合能量 $E(Y ) = E_1(Y W_a) + E_2(Y )$（类似于上一节中的 $h$ ），或者 执行以下 AIM 更新：</p>
<script type="math/tex; mode=display">
\begin{align*}
U^{(t)}=\;& softmax_{\beta} (Y^{(t)}W_a^sY)Y^{(t)} \tag{15}\\
Y^{(t+1)}=\;& U^{(t)}-\alpha_2\frac{\partial E_2}{\partial Y}|_{Y=U^{(t)}}=U^{(t)}W_f^s \tag{16}
\end{align*}</script><p>其中 $W_f^s =(1−\alpha_2)I−\alpha_2\frac{W_f+W_f^\top}{2}$ 在 softmax 之后提供了额外的线性变换。<br>从第 3 节中，我们知道 (15) 本质上是在主要化之后使用逐行梯度下降步骤来优化 $E_1$。 同样，(16) 也是步长为 $\alpha_2$ 的梯度步。 因此，组合规则属于算法1的范畴，从4.1节的分析，我们可以得出如下结论：</p>
<h3 id="命题4-5"><a href="#命题4-5" class="headerlink" title="命题4.5"></a>命题4.5</h3><p>如果 $Y^{(t+1)}$ 通过 (15) 和 (16) 以输入 $Y^{(t)}$ 计算，则 $E(Y^{(t+1)}) \leq E(Y^{(t)})$，当 $Y^{(t)} \notin \mathcal S$，其中 $\mathcal S$ 是一个有限半径的球，包含 $Y_{\hat E}^\ast$，是 $\hat E$ 的最优点，$E$ 的凸上界。</p>
<p>请注意，在组合 (15) 和 (16) 之后，更新聚合更新规则已经与我们来自 (8) 的目标非常相似，唯一的区别是缺少非线性和对称权重（注意我们使用对称权重矩阵 $W_a^s$ 和 $W_f^s$ 而不是 $W_a$ 和 $W_f$），分别对应 2.3 节中的挑战 3 和 4； 接下来我们解决前者。</p>
<h1 id="五，展开范式中集成的非线性激活"><a href="#五，展开范式中集成的非线性激活" class="headerlink" title="五，展开范式中集成的非线性激活"></a>五，展开范式中集成的非线性激活</h1><p>之前已经考虑过在展开的优化设置中处理非线性激活 [16、47、51、55]。 然而，之前的工作在很大程度上依赖于近端算子来创建与简单线性滤波器匹配的非线性激活，并且分析无法转移到本文中的 Transformer 案例。 虽然我们将在本节中将注意力集中在添加 ReLU 激活上，但为了简化说明以及它们在 Transformer 模型中的普遍性，我们的结果可以推广到更广泛的选择。【意思是也可以考虑其他的激活函数？】</p>
<p>与第 4 节类似，我们首先通过将 ReLU 激活引入算法 1 来研究优化问题的一般形式，将此修改形式化为具有近端步骤的 AIM，然后分析适当约束下的收敛性。 稍后我们将这些结果应用到 Transformer。</p>
<h2 id="5-1-近端交替不精确最小化"><a href="#5-1-近端交替不精确最小化" class="headerlink" title="5.1 近端交替不精确最小化"></a>5.1 近端交替不精确最小化</h2><p>已经确定 [29, 47] 可以将 ReLU 激活建模为近端算子，如下所示：</p>
<script type="math/tex; mode=display">
\begin{align*}
ReLU(y)=argmin_z\frac{1}{2\lambda}\|z-y\|^2+\phi(z) \tag{17}
\end{align*}</script><p>其中 $\phi$ 是第一象限的指示函数，且 $\phi(z)=\begin{cases}&amp;+\inf \quad &amp;if\;z&lt;0\\&amp;0 \quad &amp;if\; z\geq0\end{cases}$ 。此外，<br>在第 4 节中，我们演示了算法 1 的步骤如何一起形成损失 $h(y) = f(y) + g(y)$ 的不精确梯度下降迭代，其中噪声项 $\Delta_t$ 在（14）中定义。 在这里，通过在算法 2 中添加近端步骤，我们获得了近端梯度下降的不精确版本。 事实上，算法 2 的一轮相当于</p>
<script type="math/tex; mode=display">
\begin{align*}
y^{(t+1)}=argmin_z\frac{1}{2\lambda}\|z-y^{(t)}+\alpha_2\nabla h(y^{(t)})-\alpha_2\Delta_t\|^2+\phi(z) \tag{18}
\end{align*}</script><p>与精确版本 $y^{(t+1)} = argmin_z P(z;y^{(t)})$ 相比，这是噪声为 $\Delta_t$ 的典型近端梯度步长的不精确版本，其中 $P(z;y)$ 是近端问题</p>
<script type="math/tex; mode=display">
\begin{align*}
P(z;y^{(t)})=\frac{1}{2\lambda}\|z-y^{(t)}+\alpha_2\nabla h(y^{(t)})\|^2+\phi(z) \tag{19}
\end{align*}</script><p>虽然过去研究了各种形式的不精确近端梯度下降 [13、17、44、52]，但现有工作仍然假设恒定噪声 [13]、随机噪声 [52] 或递减(收敛)噪声 [17、44] . 至关重要的是，我们所知道的任何先前工作都不适用于我们的案例，其中噪声可能会随着迭代而增加。 此外，现有文献中的分析主要关注收敛到一个固定点，而在我们的场景中，我们反而考虑进入围绕特定点形成的特定区域。</p>
<p>除了像第 4 节中那样限制 $\delta(y^{(t)})$ 之外，我们还需要类似限制当前位置 $y(t)$ 和梯度 $\alpha_2\nabla h(y^{(t)})$，定义为 $\mathfrak D(\xi_1,\xi_2) =\frac{1}{|\xi_1|^2}\sum^d_{i=1}min(\xi^2_{2,i}-\xi^2_{1,i},0)$。直观地，$\mathfrak D(\xi_1;\xi_2)$ 被定义为每一项为负，但只有当 $\xi_1$ 和 $\xi_2$ 都很大时才接近于 $0$。 然后我们有以下内容：</p>
<h3 id="定理-5-1"><a href="#定理-5-1" class="headerlink" title="定理 5.1"></a>定理 5.1</h3><p>如果 $\alpha_1 \leq \alpha_2 \leq L^{−1}_h,\;\mathfrak D(\alpha_2\nabla h(y^{(t)});y^{(t)})\geq -\kappa$，对于任何 $\kappa \in (0,1)$，并且$\delta(y^{(t)})\leq\mathscr{L}’$，其中 $\mathscr{L}’=\frac{\alpha_2c_P\lambda\sqrt{1-\kappa}}{\sqrt{2}(\alpha_2-\alpha_1+\alpha_1\alpha_2L_g)}$，我们有 $h(y^{(t+1)}) +\phi(y^{(t+1)})\leq h(y^{(t)})+\phi(y^{(t)}) $。</p>
<p>直观地，定理 5.1 表明保证 $h(y)+\phi(y)$ 下降的区域是 $S(\mathscr{L}’)$ 与引理 4.3 中定义的 $S$ 的交集，并且区域 $\mathcal{T}(\kappa) = \{y|\mathscr{D}(\alpha_2\nabla h(y);y) \geq −\kappa\}$。 虽然 $\mathcal{T}(\kappa)$ 的形状通常仍然难以指定，但我们注意到当 $\alpha_2 \to 0$ 或 $\kappa\to 1$ 时，$\mathcal{T} (\kappa)$ 趋向于整个空间。我们用二维合成示例，其中能量函数为 $h(y) = |W_y|^2_{\mathcal F} + |y − b|^2$，$W$ 和 $b$ 的条目是随机生成的。 参见图 4 使用不同值 $\kappa$ 的可视化，它表明当 $\kappa$ 足够小时，$\mathcal{T}(\kappa)$ 几乎是整个空间（原点周围的一小块区域除外），这确保了 $h(y) + \phi(y)$ 在大多数情况下的下降。</p>
<h2 id="5-2-在Transformer模型中嵌入非线性"><a href="#5-2-在Transformer模型中嵌入非线性" class="headerlink" title="5.2 在Transformer模型中嵌入非线性"></a>5.2 在Transformer模型中嵌入非线性</h2><p>通过在前面的能量上加上惩罚项，并分别从 3.1 和 4.2 节中得到 $E_1$ 和 $E_2$，我们最终得到 Transformer 总能量 $E(Y) = E_1(Y) + E_2(Y) + \phi(Y)$。 并且 $E(Y)$ 的展开优化落入算法 2 的范围，使得前面的分析和定理 5.1 适用，我们可以得出结论，聚合更新规则</p>
<script type="math/tex; mode=display">
\begin{align*}
Y^{(t+1)} = ReLU[softmax_{\beta}(Y^{(t)}W^s_{\alpha}Y^{(t)\top})] \tag{20}
\end{align*}</script><p>是 $E$ 的下降算法，除了一个有限测度集区域。 此外，如果我们修改底层梯度下降算法使步长 $\alpha_1$ 和 $\alpha_2$ 足够小，异常区域的大小将趋于零，并且更新规则将配备残差项（见补充；类似地 关于 $\beta$ 的讨论）。 除了这些问题之外，展开的更新 (20) 与 (8) 的唯一区别在于它对对称权重的依赖。 这对应于我们尚未完全解决的挑战 4，尽管已经证明如果我们扩大表示维度 [23、56]，对称权重可以模拟不对称权重。</p>
<h1 id="六，实践验证"><a href="#六，实践验证" class="headerlink" title="六，实践验证"></a>六，实践验证</h1><p>尽管我们严格推导了收敛标准，即 Transformer 层将明确指定的能量函数下降到最优解周围的区域，但不可否认，该分析依赖于难以在现实世界数据集上正式验证的条件。 然而，我们的结果仍然适用于有针对性的经验证实，由此我们可以检查所提出的能量是否确实在典型基准的 Transformer 正向传递期间确实下降了。</p>
<p>为此，我们实现了一个 Transformer 模型，满足已知的限制，如对称权重。 我们将该模型应用于两个基准测试，IMDB [35] 和 SST2 [46]，它们都是依赖 Glove-840b-300d [39] 作为词嵌入的常用情感分类数据集。 图 5 和图 6 显示了 Transformer 每一层的输出能量（如 (8) 中所定义）平均超过测试集中 200 个随机选择的样本。 图 5 使用随机初始化的权重，而图 6 涉及使用 SGD 和学习率 0.01 训练 2000 步的权重。 此外，对于经过训练的模型，我们将 $E_1$ 中的项 $|Y|^2_{\mathcal F}$ 更改为 $|Y − X|^2_{\mathcal F}$ 以避免退化表示（有时会发生在经过训练的变形金刚 [45] 中），并指出这种修改同样有效 我们的理论涵盖了这一点，并在最终的 Transformer 架构中产生了一种常用的残差连接形式。 详情见补充。</p>
<p>从这些数字可以清楚地看出，即使使用真实世界的数据，我们得出的 Transformer 能量（平均）跨层单调递减，与我们的分析预测相符。 此外，对于 12 层，代表在实践中并不少见的深度范围（例如 BERT），模型没有进入波动区域。 此外，这种观察甚至适用于由实践中使用的许多/大多数典型组件组成的训练有素的 Transformer 模型，即具有自注意力的层、非线性变换后的线性变换和残差连接等。因此，尽管看似复杂， 引理 4.3 和定理 5.1 采用的条件在许多实际环境中仍然可能成立。</p>
<h1 id="七，结论"><a href="#七，结论" class="headerlink" title="七，结论"></a>七，结论</h1><p>虽然我们在这里的贡献主要是理论性质的，但仍然有一些可能具有实际意义的重要信息。 首先，由于我们的首要目标是使用展开的优化视角尽可能接近地再现 Transformer 层，因此在构建核心底层能量函数时做出了非常具体的设计选择。 然而，在实践中，我们可以自由选择替代能源，这些替代能源可以导致不同形式的定制自注意力，这在特定应用程序的基础上可能是有利的。<br>作为后者的一个简短的代表性示例，请考虑以下内容：规范的 Transformer 使用 softmax 对注意力系数进行归一化。 然而，多项工作质疑 softmax 归一化的适当性，例如 [32、40]。 在我们的优化展开视角下，我们可以看到 Transformer 中的 softmax 归一化源自特定选择的 $\rho$ 函数，即 $\rho(z) = −e^{−z}$（第 3.1 节和定理 3.1）。 但是，如果我们选择不同的 $\rho$，就会产生一种新的标准化方法，而不是具有可解释属性的 softmax。 例如，如果 $\rho(z) = log(z + 2)$，则注意力系数将表现为（为简单起见，我们假设所有 $y_i$ 都具有以下单位范数）：</p>
<script type="math/tex; mode=display">
\begin{align*}
a_{i,j} = \frac{1}{2-y_i^\top y_j}(\sum^n_{k=1}\frac{1}{2-y_i^\top y_k})^{-1}
\end{align*}</script><p>其中 $a_{i,j}$ 是第 $i$ 个和第 $j$ 个 token 之间的注意力系数。 由于 $log(z + 2)$ 对于 $z \in [0, 1]$ 的增长速度比 $−e^{−z}$ 慢，因此相关的注意力公式往往会鼓励标记之间出现更多不同的表示。 这是因为在新能源中，较大的 $|y_i − y_j |$（意味着不同的 $y_i$ 和 $y_j$ ）对能量的贡献比以前少，因此优化过程减少它的动力较小。 相反，如果我们选择在规定范围内增长快于 $−e^{−z}$ 的 $\rho$（例如 $\rho(z) = log(z + 1)）$，那么派生模型可能会鼓励令牌之间有更多相似的表示。</p>
<p>这个总体框架还有其他具有实际意义的分支。 例如，注意力权重的实际分布可能会被产生它们的能量函数的属性更好地理解或影响。 此外，特别是对于数据有限且因此自由模型参数较少的机制，展开视角可用于设计具有与下游任务一致的归纳偏差的架构，以帮助弥补模型灵活性较低的问题。</p>
<p>最后，我们注意到本文中介绍的技术在某种意义上可以被视为构建更广泛的展开式体系结构系列的通用工具。 事实上，许多/大多数深度学习模型，包括具有前馈结构 [30]、令牌级交互 [50]、残差连接 [21] 和异构层类型的模型，都可以通过使用展开优化的镜头重新解释 我们引入的框架，至少要达到一些潜在的约束，比如对称权重（可以用其他方式处理）。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>A Mathematical Framework for Transformer Circuits</title>
    <url>/2023/04/27/Transformer/A%20Mathematical%20Framework%20for%20Transformer%20Circuits/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><a href="https://transformer-circuits.pub/2021/framework/index.html">link</a></p>
<p><strong>transformerd的数学框架？</strong><br><span id="more"></span></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Transformer语言模型在新兴产业中的使用越来越广泛，比如GPT-3，LaMDA，Codex，Meena，Gopher和其他类似模型。<br>然而，随着这些模型的扩展，它们的开放性和高容量为意想不到的甚至有害的行为创造了越来越大的范围。 即使在大型模型经过训练多年后，创建者和用户仍会定期发现他们以前没有意识到的模型功能（包括有问题的行为）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">【GPT-3】</span><br><span class="line">Language models are few-shot learners  [PDF]</span><br><span class="line">T.B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, others.</span><br><span class="line">arXiv preprint arXiv:2005.14165. 2020.</span><br><span class="line">【LaMDA】</span><br><span class="line">LaMDA: our breakthrough conversation technology  [link]</span><br><span class="line">E. Collins, Z. Ghahramani. 2021.</span><br><span class="line">【Codex】</span><br><span class="line">Evaluating large language models trained on code</span><br><span class="line">M. Chen, J. Tworek, H. Jun, Q. Yuan, H.P.d.O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, others.</span><br><span class="line">arXiv preprint arXiv:2107.03374. 2021.</span><br><span class="line">【Meena】</span><br><span class="line">Towards a human-like open-domain chatbot</span><br><span class="line">D. Adiwardana, M. Luong, D.R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, others.</span><br><span class="line">arXiv preprint arXiv:2001.09977. 2020.</span><br><span class="line">【Gopher】</span><br><span class="line">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher  [PDF]</span><br><span class="line">J.W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G.v.d. Driessche, L.A. Hendricks, M. Rauh, P. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X.L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C.d.M. d’Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D.d.L. Casas, A. Guy, C. Jones, J. Bradbury, M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, G. Irving.</span><br><span class="line">Preprint. 2021.</span><br></pre></td></tr></table></figure>
<p>解决这些问题的一种途径是寻求可解释性，试图对Transformer模型的详细计算进行逆向工程，类似于程序员将复杂的二进制文件反编译为人类可读的源代码。 如果这是可能的，它可能会提供一种更系统的方法来解释当前的安全问题、识别新的问题，甚至可能预测尚未构建的强大的未来模型的安全问题。 之前的一个项目，Distill Circuits, 曾尝试对视觉模型进行逆向工程，但到目前为止还没有类似的Tranformer或语言模型项目。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Thread: Circuits</span><br><span class="line">N. Cammarata, S. Carter, G. Goh, C. Olah, M. Petrov, L. Schubert, C. Voss, B. Egan, S.K. Lim.</span><br><span class="line">Distill. 2020.</span><br><span class="line">DOI: 10.23915/distill.00024</span><br></pre></td></tr></table></figure>
<p>在本文中，我们尝试采取原始、初级的步骤来对Transformer模型进行逆向工程，因为现代语言模型令人难以置信的复杂性和规模，我们发现从最简单的模型开始并从那里开始工作是最有成效的。 我们的目标是发现简单的算法模式、主题或框架，这些模式、主题或框架随后可以应用于更大、更复杂的模型。 具体来说，在本文中，我们将研究只有两层或更少层且只有注意力块的Transformer——这与像 GPT-3 这样的大型现代Transformer形成对比，GPT-3有 96 层，且注意力块与 MLP 块交替出现。</p>
<p>我们发现，通过以一种新的但在数学上等效的方式将Transformer的操作概念化，我们能够理解这些小模型，并对它们的内部运作方式获得重要的理解。 特别值得注意的是，我们发现我们称之为“感应头”的特定注意力头可以解释这些小模型中的上下文学习，并且这些头只在至少具有两个注意力层的模型中出现。 我们还介绍了对特定数据进行操作的一些示例。</p>
<p>在第一篇论文中，我们并未尝试将我们的见解应用于更大的模型，但在即将发表的一篇论文中，我们将证明我们理解Transformer的数学框架和感应头的概念仍然至少部分相关，对于更大、更真实的模型——尽管我们距离能够完全逆向工程这些模型还有很长的路要走。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="REVERSE-ENGINEERING-RESULTS"><a href="#REVERSE-ENGINEERING-RESULTS" class="headerlink" title="REVERSE ENGINEERING RESULTS"></a>REVERSE ENGINEERING RESULTS</h3><p>为了挑战Tranformer的逆向工程，我们先在其他算法和仅有注意力的模型上进行了尝试，有以下结论：</p>
<h3 id="CONCEPTUAL-TAKE-AWAYS"><a href="#CONCEPTUAL-TAKE-AWAYS" class="headerlink" title="CONCEPTUAL TAKE-AWAYS"></a>CONCEPTUAL TAKE-AWAYS</h3><h2 id="Transformer回顾"><a href="#Transformer回顾" class="headerlink" title="Transformer回顾"></a>Transformer回顾</h2>]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>Choose a Transformer-Fourier or Galerkin</title>
    <url>/2023/04/27/Transformer/Choose%20a%20Transformer-Fourier%20or%20Galerkin/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><a href="https://arxiv.org/pdf/2105.14995.pdf">paper link</a></p>
<p><strong>在算子学习的角度，将Transformer用在PDE的求解上</strong><br><span id="more"></span></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p><strong>原文翻译</strong><br>在本文中，我们首次将 Attention Is All You Need [88] 中的自注意力机制应用于与偏微分方程相关的数据驱动算子学习问题。 努力解释启发式，并提高注意机制的功效。 通过在希尔伯特空间中使用算子逼近理论，首次证明了缩放点积注意力中的 softmax 归一化是充分的，但不是必需的。 在没有 softmax 的情况下，线性化 Transformer 变体的近似能力可以证明与 Petrov-Galerkin 投影分层相当，并且其估计与序列长度无关。 提出了一种模仿 Petrov-Galerkin 投影的新层归一化方案，以允许缩放通过注意力层传播，这有助于模型在使用非归一化数据的算子学习任务中实现显着的准确性。 最后，我们提出了三个算子学习实验，包括粘性 Burgers 方程、界面 Darcy 流和逆界面系数识别问题。 新提出的简单且基于注意力的算子学习器 Galerkin Transformer 与其 带softmax 归一化的相应模型相比，在训练成本和评估准确性方面都有显着改进。</p>
<p><strong>主要做了三个方面的事情：</strong></p>
<ul>
<li>尝试解释self-attention，提升其运算效率；</li>
<li>softmax的作用和存在的必要性；</li>
<li>层归一化的新替换方案；</li>
</ul>
<h1 id="一，介绍"><a href="#一，介绍" class="headerlink" title="一，介绍"></a>一，介绍</h1><p>在我们的世界中，从宇宙星体的运动，到温度风速的气象预报，再到分子原子间的相互作用，很多工程学、自然科学、经济和商业过程都可以通过偏微分方程（PDE）描述。几个世纪以来，科学家和工程师一直致力于逼近这些物理系统的控制PDE，计算机辅助模拟的出现为研究这些具有挑战性的问题提供了一种成本友好的方法。 传统方法，如有限元、有限差分法 [20、22]、谱方法 [12] 等，利用离散结构将无限维算子映射简化为有限维近似问题。 同时，在许多科学实践中，离散网格上可用的、PDE 控制现象的大量数据使现代黑盒模型（如物理信息神经网络（PINN）[71、62、49]）能够利用配置点上的测量 近似 PDE 解。</p>
<p>尽管如此，对于传统方法或数据驱动的函数学习器（如 PINN），给定 PDE，重点是逼近单个实例，例如，求解具有固定边界条件的某个系数的近似解，这个系数的微小变化会让数据驱动的函数学习器需要进行昂贵的再训练。 相比之下，算子学习者的目标是学习无限维函数空间之间的映射，这要困难得多但也有回报。 一个训练有素的算子学习者可以在没有重新训练或配置点的情况下评估许多实例，从而节省宝贵的资源，并且从长远来看将自己定位为更有效的方法。 数据驱动的分辨率不变算子学习是一个蓬勃发展的新研究方向 [60, 5, 56, 64, 90, 57, 61, 91, 37, 74]，开创性模型 DeepONet [60] 在架构上归因于运算符 [18] 的通用逼近定理。傅立叶神经算子 (FNO) [57]在某些基准测试中，展示了一种令人敬畏的最先进的性能，比 [100] 中的经典模型好几个数量级。</p>
<p>在监督学习下，算子学习者接受算子的输入函数及其对输入的响应作为目标的训练，由于两个函数都是在离散网格点上采样的，因此这是 seq2seq 问题的特例 [81]。 当前最先进的 seq2seq 模型是在 [88] 中首次引入的 Transformer。 作为 Transformer 的核心和灵魂，缩放点积注意力机制能够通过捕获远程交互信息来挖掘算子的隐藏结构。 受到 Transformers [50、19、75、84、96、97、95、59、76、66] 中许多富有洞察力的开创性工作的启发，我们以数学上深刻的方式对注意力机制进行了最低限度的修改，以更好地服务于算子学习。</p>
<p>在我们对缩放点积注意力在希尔伯特空间中的改编中，第一个也是最重要的变化是：没有 softmax 或其近似值。 在 vanilla attention [88] 中，矩阵乘法之后的 softmax 凸化了组合不同位置潜在表示的权重，这被认为是注意力机制正核解释中不可或缺的组成部分 [84]。 然而，softmax 全局作用于注意力矩阵的每一行的序列长度维度，进一步增加了经典 Transformer 中注意力的二次复杂度。 从理论上讲，与自然语言处理 (NLP) 传统中的“一行≈一词”不同，查询、键、值的列被视为离散网格上希尔伯特空间中的函数采样。 因此，去掉 softmax 允许我们验证离散的 Ladyzhenskaya–Babuška–Brezzi (LBB) 条件，这进一步证明了新提出的 Galerkin 类型的注意力可以明确地表示 Petrov-Galerkin 投影，并且这种近似能力是与序列长度无关（定理 4.3）。</p>
<p>在数值上，无 softmax 模型节省了宝贵的计算资源，在训练 FLOP 和内存消耗方面优于使用 softmax 的模型（第 5 节）。 然而在消融研究中，无 softmax 模型的训练变得不稳定（表 8），为了解决这个问题，提出了一种新的 Galerkin 投影类型层归一化方案，作为在 Petrov-Galerkin 解释（等式（40））证明中明确导出的归一化廉价对角替代方案。 由于现在可以通过编码器层传播可学习的缩放比例，因此具有这种新层归一化方案的基于注意力的算子学习器表现出对与 PDE 相关的某些物理特性（例如能量衰减）的更好理解。 结合其他受近似理论启发的技巧，包括投影矩阵的对角占优重新缩放初始化和位置编码的逐层丰富，各种算子学习任务的评估精度得到显着提高。</p>
<h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ul>
<li><p>无softmax的注意力机制<br>我们提出了一种新的简单自注意算子及其没有 softmax 归一化的线性变体。 提供了两种新的数学解释，以及证明与 Petrov-Galerkin 投影相当的线性变体的近似能力。</p>
</li>
<li><p>参数化PDE的算子学习<br>我们将新提出的注意力算子与当前最先进的算子学习器傅立叶神经算子 (FNO) [57] 相结合，以显著提高其在 PDE 算子学习基准问题中的评估准确性。 此外，新模型能够根据传统方法或 FNO 无法完成的噪声测量恢复系数。</p>
</li>
<li><p>实验结果<br>我们提出了三个基准问题，以表明使用新提出的注意力机制的算子学习器在计算、内存效率以及准确性方面优于传统的 softmax 归一化。 用于重现我们结果的 PyTorch 代码可作为开源软件使用。</p>
</li>
</ul>
<h1 id="二，相关工作"><a href="#二，相关工作" class="headerlink" title="二，相关工作"></a>二，相关工作</h1><h2 id="PDE相关的算子学习"><a href="#PDE相关的算子学习" class="headerlink" title="PDE相关的算子学习"></a>PDE相关的算子学习</h2><p>在 [4, 5] 中，参数 PDE 的解算子的某些核形式是使用图神经网络来近似的。 另一个值得注意的并发方法是 DeepONet [60、61]。 [56] 通过利用多级网格结构进一步改进了内核方法。 [57] 提出了一种离散化不变的运算符学习器，以在某些基准问题中实现最先进的性能。 [90, 91] 提出了一种大致等同于附加注意力的 DeepONet，类似于 [7] 中的神经图灵机 (NMT)。 模型/降维与神经网络相结合是另一种流行的学习参数 PDE 解算子的方法 [10、64、55、24]。 深度卷积神经网络 (DCNN) 被广泛应用于学习具有固定离散化大小的解图 [1、9、40、36、35、100、86]。 最近，DCNN 已成功应用于各种反问题 [35, 47]，例如电阻抗层析成像 (EIT)。 据我们所知，对于一类具有随机界面几何形状的系数，还没有关于数据驱动方法的反界面系数识别的工作。</p>
<h2 id="attention机制和变体"><a href="#attention机制和变体" class="headerlink" title="attention机制和变体"></a>attention机制和变体</h2><p>除了 [88] 中开创性的缩放点积注意力之外，早期的 [7] 提出了一种基于附加内容的注意力，然而，由于多重非线性组合，梯度消失问题。 [25] 展示了在投影后移除 [7] 中的 softmax 归一化的第一个努力，然而，它在加性插值传播阶段之前仍然使用 Sigmoid 非线性，并且表现比它的 softmax 对应物差。 当前流行的将注意力线性化的方法利用特征映射的存在假设来近似 softmax 内核 [50、19、70]。 另一种类型的线性化利用矩阵乘积的低阶特性，使用各种方法，例如采样或投影 [73、11、79、92] 或快速多极分解 [65]。 [75] 中的猜想启发我们移除 softmax 整体。 [76] 首先提出了在没有 softmax 的情况下针对线性复杂度注意力的逆序列长度缩放归一化，但是，缩放归一化尚未在示例中得到广泛研究并且表现更差。</p>
<h2 id="Transformer的变量学习"><a href="#Transformer的变量学习" class="headerlink" title="Transformer的变量学习"></a>Transformer的变量学习</h2><p>[84] 中的内核解释启发我们使用 Galerkin 投影重新表述注意力。 [95，定理 2] 给出了去除 softmax 归一化以制定傅里叶型注意力的理论基础。 Nyström 近似 [97] 本质上承认注意力矩阵和积分核之间的相似性。 [96, 66, 59] 启发我们尝试不同的层归一化和重新缩放的对角占优初始化方案。 在我们的工作中反复使用位置编码来丰富潜在表示的做法可以追溯到 [2, 26]，最近，有助于 AlphaFold 2 [48] 的成功，因为如果 目标在坐标系和/或变换组中具有依赖性 ansatz，但难以明确量化。 其他关于调整注意机制以保存重要物理特性的研究在 [82, 31, 44] 中。</p>
<h1 id="三，PDE的算子学习"><a href="#三，PDE的算子学习" class="headerlink" title="三，PDE的算子学习"></a>三，PDE的算子学习</h1><p>紧跟[56,57]中的步骤，我们考虑用一个数据驱动模型来近似密度算子 $T:\mathcal H_1\to \mathcal H_2$，模型定义在两个Hilbert空间之上，且对于空间的约束为 $\mathbf{\Omega}\subset\mathbb R^m$。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>seven concepts in designing and analysising of algoritms</title>
    <url>/2023/04/25/math/seven_concepts_algorithms/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>design and analysis of algorithms can be broadly achieved with the help of seven concepts</p>
<p>《Numerical Methods for Algorithms Systems and Neural Networks》<br><a href="https://www.ifam.uni-hannover.de/en">reference link</a><br><a href="http://dx.doi.org/10.15488/11897">doi</a></p>
<p><strong>从七个要素来看算法的设计和分析……</strong><br><span id="more"></span></p>
<h2 id="算法定义"><a href="#算法定义" class="headerlink" title="算法定义"></a>算法定义</h2><p>算法是一系列指令，有一个对严谨数学问题的解决方案，其主要目的就是制定一个可以在计算机中实施的方案，可以进行所谓的数值模拟。 </p>
<p>直接求解方法对给定问题求解直到要求的舍入误差（例如高斯消除）。 迭代求解会逼近到一定精度（例如，用于求解线性方程系统的Richardson迭代、不动点迭代、梯度下降、牛顿法……）。 </p>
<p>算法在准确性、稳健性和效率方面各不相同。</p>
<h2 id="1-Approximation"><a href="#1-Approximation" class="headerlink" title="1, Approximation"></a>1, Approximation</h2><p>由于正如我们在上一节中刚刚了解到的那样，通常无法获得解析解，因此通过数值近似获得解。</p>
<h2 id="2-Convergence"><a href="#2-Convergence" class="headerlink" title="2, Convergence"></a>2, Convergence</h2><p>收敛是一个定性表达式，它告诉我们序列 $(a_n)_{n\in N}$ 的成员 $a_n$ 何时足够接近极限 $a$。 在数值数学中，这个极限通常是我们正在寻找的解决方案。</p>
<h2 id="3-Order-of-convergence"><a href="#3-Order-of-convergence" class="headerlink" title="3, Order of convergence"></a>3, Order of convergence</h2><p>在分析中，我们通常对收敛本身感兴趣，但在数值数学中，我们必须注意数值解具有足够精度所需的时间。 模拟时间越长，消耗的时间和能源（运行计算机的电力、服务器的空调等）就越多。 为了判断一个算法是否快，我们必须确定收敛的顺序。</p>
<h2 id="4-Error"><a href="#4-Error" class="headerlink" title="4, Error"></a>4, Error</h2><p>数值数学可以被认为是“误差数学”的分支。 </p>
<p>这是什么意思？ 并不是说数值建模错误、不准确或不精确！ 由于我们在最后几步之后切割序列或接受从我们的软件获得的足够准确的解决方案，我们需要说明这个数值解到精确解的近似程度。 换句话说，我们需要确定误差，而误差可能以各种形式出现。</p>
<h2 id="5-Error-Estimation"><a href="#5-Error-Estimation" class="headerlink" title="5, Error Estimation"></a>5, Error Estimation</h2><p>这是数值数学中最大的分支之一。 我们需要导出误差公式来判断数值模拟的结果，并衡量数值解与（未知）精确解在某个范数下的差异。</p>
<h2 id="6-Efficiency"><a href="#6-Efficiency" class="headerlink" title="6, Efficiency"></a>6, Efficiency</h2><p>一般来说，我们可以说，算法的收敛阶数越高，算法的效率就越高。 因此，我们可以更快地获得给定问题的数值解。 但是数值效率并不自动与资源有效计算相关。 例如，使用 MPI（消息传递接口）、硬件优化（CPU、GPU）、软件优化（以某种最佳方式排序 for 循环、算术评估等）开发并行代码可以进一步降低计算成本。</p>
<h2 id="7-Stability"><a href="#7-Stability" class="headerlink" title="7, Stability"></a>7, Stability</h2><p>最后，必须研究算法和实现在参数（模型、材料、数值）变化、边界条件、初始条件、不确定性方面的稳健性。 稳定性在最广泛的意义上与阿达玛的第三个条件相关。</p>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title>General datasets for deep learning in CV</title>
    <url>/2023/03/23/CNN/datasets/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通用检测任务的常用数据集，包含VOC，COCO，ImageNet，不断更新其他分支任务的数据集。</p>
<p><strong>big data is all you need</strong><br><span id="more"></span></p>
<h2 id="一，PASCAL-VOC"><a href="#一，PASCAL-VOC" class="headerlink" title="一，PASCAL VOC"></a>一，PASCAL VOC</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>由Mark Everingham (University ofLeeds)、Luc van Gool (ETHZ, Zurich)等人创立，有1.7W+张图片，分为20类。</p>
<p>PASCAL VOC挑战赛是计算机视觉竞赛的鼻祖，从2005年到2012年一共举办了8届，其任务涵盖：目标分类，目标检测，分割，人体部位，动作识别。</p>
<p>VOC图片集包括20个类别：人类;动物(鸟、猫、牛、狗、马、羊);交通工具(飞机、自行车、船、公共汽车、小轿车、摩托车、火车);室内(瓶子、椅子、餐桌、盆栽植物、沙发、电视)。</p>
<p><a href="http://host.robots.ox.ac.uk/pascal/VOC/">official download</a></p>
<h3 id="1-2-组织结构"><a href="#1-2-组织结构" class="headerlink" title="1.2 组织结构"></a>1.2 组织结构</h3><p>以 VOC 2007 为例，解压后的文件为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── Annotations 进行 detection 任务时的标签文件，xml 形式，文件名与图片名一一对应</span><br><span class="line">├── ImageSets 包含三个子文件夹 Layout、Main、Segmentation，其中 Main 存放的是分类和检测的数据集分割文件</span><br><span class="line">├── JPEGImages 存放 .jpg 格式的图片文件</span><br><span class="line">├── SegmentationClass 存放按照 class 分割的图片</span><br><span class="line">└── SegmentationObject 存放按照 object 分割的图片</span><br><span class="line"></span><br><span class="line">├── Main</span><br><span class="line">│   ├── train.txt 写着用于训练的图片名称， 共 2501 个</span><br><span class="line">│   ├── val.txt 写着用于验证的图片名称，共 2510 个</span><br><span class="line">│   ├── trainval.txt train与val的合集。共 5011 个</span><br><span class="line">│   ├── test.txt 写着用于测试的图片名称，共 4952 个</span><br></pre></td></tr></table></figure>
<h3 id="1-3-数据集xml文件标注格式"><a href="#1-3-数据集xml文件标注格式" class="headerlink" title="1.3 数据集xml文件标注格式"></a>1.3 数据集xml文件标注格式</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;annotation&gt;</span><br><span class="line">  &lt;folder&gt;17&lt;/folder&gt; # 图片所处文件夹</span><br><span class="line">  &lt;filename&gt;77258.bmp&lt;/filename&gt; # 图片名</span><br><span class="line">  &lt;path&gt;~/frcnn-image/61/ADAS/image/frcnn-image/17/77258.bmp&lt;/path&gt;</span><br><span class="line">  &lt;source&gt;  #图片来源相关信息</span><br><span class="line">    &lt;database&gt;Unknown&lt;/database&gt;  </span><br><span class="line">  &lt;/source&gt;</span><br><span class="line">  &lt;size&gt; #图片尺寸</span><br><span class="line">    &lt;width&gt;640&lt;/width&gt;</span><br><span class="line">    &lt;height&gt;480&lt;/height&gt;</span><br><span class="line">    &lt;depth&gt;3&lt;/depth&gt;</span><br><span class="line">  &lt;/size&gt;</span><br><span class="line">  &lt;segmented&gt;0&lt;/segmented&gt;  #是否有分割label</span><br><span class="line">  &lt;object&gt; 包含的物体</span><br><span class="line">    &lt;name&gt;car&lt;/name&gt;  #物体类别</span><br><span class="line">    &lt;pose&gt;Unspecified&lt;/pose&gt;  #物体的姿态</span><br><span class="line">    &lt;truncated&gt;0&lt;/truncated&gt;  #物体是否被部分遮挡（&gt;15%）</span><br><span class="line">    &lt;difficult&gt;0&lt;/difficult&gt;  #是否为难以辨识的物体， 主要指要结体背景才能判断出类别的物体。虽有标注， 但一般忽略这类物体</span><br><span class="line">    &lt;bndbox&gt;  #物体的bound box</span><br><span class="line">      &lt;xmin&gt;2&lt;/xmin&gt;     #左</span><br><span class="line">      &lt;ymin&gt;156&lt;/ymin&gt;   #上</span><br><span class="line">      &lt;xmax&gt;111&lt;/xmax&gt;   #右</span><br><span class="line">      &lt;ymax&gt;259&lt;/ymax&gt;   #下</span><br><span class="line">    &lt;/bndbox&gt;</span><br><span class="line">  &lt;/object&gt;</span><br><span class="line">&lt;/annotation&gt;</span><br></pre></td></tr></table></figure>
<h2 id="二，COCO"><a href="#二，COCO" class="headerlink" title="二，COCO"></a>二，COCO</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>全称是Microsoft Common Objects in Context，起源于微软于2014年出资标注的Microsoft COCO数据集，与ImageNet竞赛一样，被视为是计算机视觉领域最受关注和最权威的比赛之一。</p>
<p>COCO数据集是一个大型的、丰富的<strong>物体检测，分割和图像描述</strong>数据集。这个数据集以情景理解为目标，主要从复杂的日常场景中截取，图像中的目标通过精确的分割进行位置的标定。该数据集主要解决3个问题：<strong>目标检测</strong>，<strong>目标之间的上下文关系</strong>，<strong>目标的二维上的精确定位</strong>。COCO数据集有91类，虽然比ImageNet类别少，但是每一类的图像多，这有利于获得更多的每类中位于某种特定场景的能力，对比PASCAL VOC，其有更多类和图像。</p>
<p>COCO数据集包含20万个图像，其中80个类别（80个对象类别是stuff91类的子集，如果仅仅是做目标检测，基本只用80类即可。）中有超过50万个目标标注,它是最广泛公开的目标检测数据库，平均每个图像的目标数为7.2。</p>
<p><a href="http://cocodataset.org">website</a></p>
<h3 id="2-2-特点"><a href="#2-2-特点" class="headerlink" title="2.2 特点"></a>2.2 特点</h3><ul>
<li>目标集分割；</li>
<li>图像情景识别；</li>
<li>超像素分割；</li>
<li>330K图像（&gt; 200K标记）；</li>
<li>150万个对象实例；</li>
<li>80个对象类别；</li>
<li>91个stuff类别；</li>
<li>每张图片有5段情景描述；</li>
<li>有关键点的250,000人；</li>
</ul>
<p>三种标注类型：</p>
<ul>
<li>object instances（目标实例）</li>
<li>object keypoints（目标上的关键点）</li>
<li>image captions（看图说话）</li>
</ul>
<h3 id="2-3-组织结构"><a href="#2-3-组织结构" class="headerlink" title="2.3 组织结构"></a>2.3 组织结构</h3><p>以coco2017为例，解压后的文件为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── train2017 所有训练图像文件夹（118287张）</span><br><span class="line">├── val2017  所有验证图像文件夹（5000张）</span><br><span class="line">├── annotations 对应标注文件夹</span><br><span class="line">│   ├── instances_train2017.json 对应目标检测、分割任务的训练集标注文件</span><br><span class="line">│   ├── instances_val2017.json 对应目标检测、分割任务的验证集标注文件</span><br><span class="line">│   ├── captions_train2017.json 对应图像描述的训练集标注文件</span><br><span class="line">│   ├── captions_val2017.json 对应图像描述的验证集标注文件</span><br><span class="line">│   ├── person_keypoints_train2017.json 对应人体关键点检测的训练集标注文件</span><br><span class="line">│   ├── person_keypoints_val2017.json 对应人体关键点检测的验证集标注文件</span><br></pre></td></tr></table></figure>
<h3 id="2-4-标注文件格式"><a href="#2-4-标注文件格式" class="headerlink" title="2.4 标注文件格式"></a>2.4 标注文件格式</h3><p>以Object Instance为例，其json文件为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;info&quot;: info,               # dict</span><br><span class="line">    &quot;licenses&quot;: [license],      # list,内部是dict</span><br><span class="line">    &quot;images&quot;: [image],          # list,内部是dict</span><br><span class="line">    &quot;annotations&quot;: [annotation],# list,内部是dict</span><br><span class="line">    &quot;categories&quot;: [category]    # list,内部是dict</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">info&#123;                           # 数据集信息描述</span><br><span class="line">    &quot;year&quot;: int,                # 数据集年份</span><br><span class="line">    &quot;version&quot;: str,             # 数据集版本</span><br><span class="line">    &quot;description&quot;: str,         # 数据集描述</span><br><span class="line">    &quot;contributor&quot;: str,         # 数据集提供者</span><br><span class="line">    &quot;url&quot;: str,                 # 数据集下载链接</span><br><span class="line">    &quot;date_created&quot;: datetime,   # 数据集创建日期</span><br><span class="line">&#125;</span><br><span class="line">license&#123;</span><br><span class="line">    &quot;id&quot;: int,</span><br><span class="line">    &quot;name&quot;: str,</span><br><span class="line">    &quot;url&quot;: str,</span><br><span class="line">&#125; </span><br><span class="line">image&#123;      # images是一个list,存放所有图片(dict)信息。image是一个dict,存放单张图片信息 </span><br><span class="line">    &quot;id&quot;: int,                  # 图片的ID编号（每张图片ID唯一）</span><br><span class="line">    &quot;width&quot;: int,               # 图片宽</span><br><span class="line">    &quot;height&quot;: int,              # 图片高</span><br><span class="line">    &quot;file_name&quot;: str,           # 图片名字</span><br><span class="line">    &quot;license&quot;: int,             # 协议</span><br><span class="line">    &quot;flickr_url&quot;: str,          # flickr链接地址</span><br><span class="line">    &quot;coco_url&quot;: str,            # 网络连接地址</span><br><span class="line">    &quot;date_captured&quot;: datetime,  # 数据集获取日期</span><br><span class="line">&#125;</span><br><span class="line">annotation&#123; # annotations是一个list,存放所有标注(dict)信息。annotation是一个dict,存放单个目标标注信息。</span><br><span class="line">    &quot;id&quot;: int,                  # 目标对象ID（每个对象ID唯一），每张图片可能有多个目标</span><br><span class="line">    &quot;image_id&quot;: int,            # 对应图片ID</span><br><span class="line">    &quot;category_id&quot;: int,         # 对应类别ID，与categories中的ID对应</span><br><span class="line">    &quot;segmentation&quot;: RLE or [polygon],   # 实例分割，对象的边界点坐标[x1,y1,x2,y2,....,xn,yn]</span><br><span class="line">    &quot;area&quot;: float,              # 对象区域面积</span><br><span class="line">    &quot;bbox&quot;: [xmin,ymin,width,height], # 目标检测，对象定位边框[x,y,w,h]</span><br><span class="line">    &quot;iscrowd&quot;: 0 or 1,          # 表示是否是人群</span><br><span class="line">&#125;</span><br><span class="line">categories&#123;                     # 类别描述</span><br><span class="line">    &quot;id&quot;: int,                  # 类别对应的ID（0默认为背景）</span><br><span class="line">    &quot;name&quot;: str,                # 子类别名字</span><br><span class="line">    &quot;supercategory&quot;: str,       # 主类别名字</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三，ImageNet"><a href="#三，ImageNet" class="headerlink" title="三，ImageNet"></a>三，ImageNet</h2><h3 id="3-1-简介"><a href="#3-1-简介" class="headerlink" title="3.1 简介"></a>3.1 简介</h3><p>ImageNet是目前深度学习图像领域应用得非常多的一个图像集，由斯坦福大学李飞飞创立，有1400W+张样例图片，分为27大类和2W+小类，只能用于非商业研究和教学使用。与ImageNet图像集相应的是著名的ILSVRC竞赛，各种新机器学习算法脱颖而出（AlexNet、ZFNet、GoogleNet、ResNet、…），图像识别率得以显著提高，在ILSVRC竞赛上一举成名是近几年来计算机视觉从业者的梦想。</p>
<p>对于如基于ImageNet的图像识别的结果评估，往往用到两个准确率的指标，一个是top-1准确率，一个是top-5准确率。Top-1准确率指的是输出概率中最大的那一个对应的是正确类别的概率；top-5准确率指的是输出概率中最大的5个对应的5个类别中包含了正确类别的概率。</p>
<p><a href="http://www.image-net.org/download-imageurls">official download</a></p>
<h3 id="3-2-特点"><a href="#3-2-特点" class="headerlink" title="3.2 特点"></a>3.2 特点</h3><ul>
<li>ImageNet拥有用于分类、定位和检测任务评估的数据。 </li>
<li>与分类数据类似，定位任务有1000个类别。准确率是根据最高五项检测结果计算出来的。 </li>
<li>所有图像中至少有一个边框。对200个目标的检测问题有470000个图像，平均每个图像有1.1个目标。</li>
</ul>
]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>datasets</tag>
      </tags>
  </entry>
  <entry>
    <title>General activate functions</title>
    <url>/2023/03/23/CNN/activation%20functions/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>激活函数汇总对比~</p>
<p><strong>随时加更中~</strong><br><span id="more"></span></p>
<h2 id="一，定义"><a href="#一，定义" class="headerlink" title="一，定义"></a>一，定义</h2><p>激活函数（又叫激励函数）是模型整个结构中的非线性扭曲力，神经网络的每层都会有一个激活函数。</p>
<p>激活函数的主要作用是提供网络的非线性建模能力，如果没有激活函数，那么神经网络只能表达线性映射，此刻即便是有再多的隐藏层，其整个网络跟单层神经网络也是等价的。因此可以说，只有加入了激活函数之后，深度神经网络才具备了分层的非线性的学习能力。</p>
<p>常用的激活函数：Sigmoid函数、tanh函数、Relu函数、Leaky ReLU函数、ELU (Exponential Linear Units) 函数、SoftMax函数、MaxOut函数。</p>
<p><strong>激活函数特点</strong></p>
<ul>
<li>可微性：因为优化方法是基于梯度的，这个性质是必须的</li>
<li>单调性：当激活函数是单调的时候，能够保证单层网络是凸函数</li>
<li>输出值的范围：激活函数的输出值的范围可以有限也可以无限。当输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更加显著；当输出值是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的learning rate</li>
<li>非饱和性：（饱和函数有Sigmoid、Tanh等，非饱和函数ReLU等）例如Sigmoid函数求导以后的值很小，两端的值接近为零在反向传播的时候，如果网络的层次过大便会发生梯度消失的问题，使得浅层的参数无法更新。</li>
<li>非线性：激活函数必须是非线性的。</li>
<li>计算简单：神经元都要经过激活运算的，在随着网络结构越来越庞大、参数量越来越多，激活函数如果计算量小就节约了大量的资源。</li>
</ul>
<h2 id="二，经典激活函数"><a href="#二，经典激活函数" class="headerlink" title="二，经典激活函数"></a>二，经典激活函数</h2><h3 id="1，sigmoid函数"><a href="#1，sigmoid函数" class="headerlink" title="1，sigmoid函数"></a>1，sigmoid函数</h3><p>Sigmoid 函数的图像看起来像一个 S 形曲线。</p>
<p><img src="sigmoid.jpg" alt="sigmoid">   </p>
<p>原函数：</p>
<script type="math/tex; mode=display">\mathrm{sigmoid}(x)= \frac{1}{1+e^{-x}}</script><p>导数：</p>
<script type="math/tex; mode=display">f^{'}(x)=f(x)(1-f(x))</script><p><strong>缺点</strong></p>
<ul>
<li>有梯度消失的风险；</li>
<li>函数输出不是以 0 为中心，这会降低权重的更新效率；</li>
<li>Sigmoid 函数执行指数运算，运行较慢。</li>
</ul>
<p><strong>适用场景</strong></p>
<ul>
<li>值域$(0,1)$，因此它对每个神经元的输出进行了归一化；</li>
<li>值域$(0,1)$，Sigmoid 非常合适将预测概率作为输出的模型</li>
<li>梯度平滑，避免<strong>跳跃</strong>的输出值；</li>
<li>函数是可微的。这意味着可以找到任意两个点的 sigmoid 曲线的斜率；</li>
</ul>
<h3 id="2，tanh函数"><a href="#2，tanh函数" class="headerlink" title="2，tanh函数"></a>2，tanh函数</h3><p>tanh 激活函数的图像也是 S 形，可以看做放大并平移的Logistic函数</p>
<p><img src="tanh.jpg" alt="tanh"></p>
<p>原函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathrm{Tanh}(x)&=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
    &=\frac{2}{1+e^{-2x}}-1 \\
    &=2\mathrm{sigmoid}(2x)-1
\end{align*}</script><p>导数：</p>
<script type="math/tex; mode=display">f^{'}(x)=1-f^2(x)</script><p><strong>优点</strong><br>值域$(-1,1)$，并且整个函数以 0 为中心，比 sigmoid 函数更好；</p>
<p><strong>缺点</strong><br>输入较大或较小时，输出几乎是平滑的并且梯度较小，这不利于权重更新</p>
<p>注意：在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。</p>
<h3 id="3，ReLU函数"><a href="#3，ReLU函数" class="headerlink" title="3，ReLU函数"></a>3，ReLU函数</h3><p><img src="ReLU.jpg" alt="ReLU"></p>
<p>原函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathrm{ReLU}(x)&=
    \begin{cases}
        x & x\geq 0\\
        0 & x<0
    \end{cases}\\
    &=max(0,x)
\end{align*}</script><p>导数：</p>
<script type="math/tex; mode=display">
f^{'}(x)=
    \begin{cases}
        1 & x\geq 0\\
        0 & x<0
    \end{cases}</script><p><strong>优点</strong></p>
<ul>
<li>当输入为正时，不存在梯度饱和问题。</li>
<li>计算速度快，因为 ReLU 函数中只存在线性关系。</li>
<li>ReLU函数的形式非常简洁，由两段线性函数组合起来后却是非线性的，看似简单，但ReLU的组合却可以逼近任何函数。</li>
<li>ReLU提出的最大作用是解决sigmoid函数导致的梯度消失问题的，ReLU有单侧抑制，会使一部分神经元的输出为0，这样就造成了网络的稀疏性，减少了参数的相互依存关系，缓解了过拟合问题，另外这也更符合生物神经元的特征。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>Dead ReLU 问题。当输入为负时，ReLU 完全失效，在正向传播过程中，这不是问题。但是在反向传播过程中，如果输入负数，则梯度将完全为零，sigmoid 函数和 tanh 函数也具有相同的问题；</li>
<li>可能会导致神经元死亡，权重无法更新的情况，这种死亡是不可逆转的。</li>
<li>ReLU 函数不是以 0 为中心的函数。</li>
</ul>
<p><strong>解释神经元死亡问题</strong><br>训练神经网络的时候，一旦学习率没有设置好，第一次更新权重的时候，输入是负值，那么这个含有ReLU的神经节点就会死亡，再也不会被激活。因为：ReLU的导数在x&gt;0的时候是1，在x&lt;=0的时候是0。如果x&lt;=0，那么ReLU的输出是0，那么反向传播中梯度也是0，权重就不会被更新，导致神经元不再学习。 也就是说，这个ReLU激活函数在训练中将不可逆转的死亡，导致了训练数据多样化的丢失。</p>
<p>在实际训练中，如果学习率设置的太高，可能会发现网络中40%的神经元都会死掉，且在整个训练集中这些神经元都不会被激活。所以，<strong>设置一个合适的较小的学习率</strong>，会降低这种情况的发生。为了解决神经元节点死亡的情况，有人提出了Leaky ReLU、P-ReLu、R-ReLU、ELU等激活函数。</p>
<h3 id="4，Leaky-RELU函数"><a href="#4，Leaky-RELU函数" class="headerlink" title="4，Leaky RELU函数"></a>4，Leaky RELU函数</h3><p>Leaky ReLU函数是一种专门设计用于解决神经元“死亡”问题的激活函数。</p>
<p><a href="http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">Rectified NonlinearitiesImprove Neural Network Acoustic Models </a></p>
<p><img src="LeakyReLU.jpg" alt="Leaky RELU"></p>
<p>原函数：</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathrm{LeakyReLU}(x)= 
\left\{ 
    \begin{array}{ll}
    x & x>0 \\ 
    \alpha x & x\leq 0
    \end{array}
\right.
\end{equation}</script><p>导数：</p>
<script type="math/tex; mode=display">
\begin{equation}
f^{'}(x)= 
\left\{ 
    \begin{array}{ll}
    1 & x>0 \\ 
    \alpha & x\leq 0
    \end{array}
\right.
\end{equation}</script><p><strong>优点</strong></p>
<ul>
<li>神经元不会出现死亡的情况。</li>
<li>Leaky对于所有的输入，神经元不会饱和。</li>
<li>由于Leaky的线性、非饱和，在SGD中能够快速收敛。</li>
<li>计算速度要快很多，因为Leaky ReLU函数只有线性关系。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>Leaky ReLU函数中的α，需要通过先验知识人工赋值，通常取0.01。</li>
</ul>
<p><strong>为什么比ReLU好？</strong></p>
<ul>
<li>Leaky ReLU 通过把 x 的非常小的线性分量给予负输入（0.01x）来调整负值的零梯度问题；</li>
<li>Leaky ReLU 的函数范围是$\infty$（负无穷到正无穷）。</li>
</ul>
<p>从理论上讲，Leaky ReLU 具有 ReLU 的所有优点，而且 Dead ReLU 不会有任何问题，但在实际操作中，尚未完全证明 Leaky ReLU 总是比 ReLU 更好。</p>
<h3 id="5，ELU函数"><a href="#5，ELU函数" class="headerlink" title="5，ELU函数"></a>5，ELU函数</h3><p>paper：<a href="https://arxiv.org/abs/1511.07289v5">Fastand accurate deep network learning by exponential linear units (elus)</a></p>
<p>ELU 的提出也解决了 ReLU 的问题。与 ReLU 相比，ELU 有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。</p>
<p><img src="ELU.bmp" alt="ELU"></p>
<script type="math/tex; mode=display">
\begin{equation}
\mathrm{ELU}(x)=
\left\{
    \begin{array}{ll}
        x & x>0\\
        \alpha(e^x-1) & x\leq 0
    \end{array}
\right.
\end{equation}</script><p><strong>优点</strong></p>
<ul>
<li>没有 Dead ReLU 问题，输出的平均值接近 0，以 0 为中心；</li>
<li>通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习；</li>
<li>ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>计算强度更高，计算量较大</li>
</ul>
<p>显然，ELU 具有 ReLU 的所有优点，同样与 Leaky ReLU 类似，尽管理论上比 ReLU 要好，但目前在实践中没有充分的证据表明 ELU 总是比 ReLU 好。</p>
<h3 id="6，Softmax函数"><a href="#6，Softmax函数" class="headerlink" title="6，Softmax函数"></a>6，Softmax函数</h3><p>Softmax函数是用于<strong>多类分类问题</strong>的激活函数，在多类分类问题中，超过两个类标签则需要类成员关系。对于长度为 $K$ 的任意实向量，Softmax函数可以将其压缩为长度为 $K$，值在 $[0,1]$ 范围内，并且向量中元素的总和为1的实向量。</p>
<p>Softmax函数与正常的max函数不同：max函数仅输出最大值，但Softmax函数确保较小的值具有较小的概率，并且不会直接丢弃。Softmax函数的分母结合了原始输出值的所有因子，这意味着Softmax函数获得的各种概率彼此相关。</p>
<script type="math/tex; mode=display">\mathrm{Softmax}(x)=\frac{e^{x_i}}{\sum_ie^{x_i}}</script><p><strong>缺点</strong></p>
<ul>
<li>在零点不可微；</li>
<li>负输入的梯度为零，这意味着对于该区域的激活，权重不会在反向传播期间更新，因此会产生永不激活的死亡神经元。</li>
</ul>
<h3 id="7，Maxout函数"><a href="#7，Maxout函数" class="headerlink" title="7，Maxout函数"></a>7，Maxout函数</h3><p>在 Maxout 层，激活函数是输入的最大值，因此只有 2 个 Maxout 节点的多层感知机就可以拟合任意的凸函数。</p>
<p>单个 Maxout 节点可以解释为对一个实值函数进行分段线性近似 (PWL) ，其中函数图上任意两点之间的线段位于图（凸函数）的上方。</p>
<p>可以理解为是神经网络中的一层网络，类似于池化层、卷积层一样。我们也可以把Maxout函数看成是网络的激活函数层。</p>
<p>Maxout激活函数并不是一个固定的函数，不像Sigmod、Relu、Tanh等函数，是一个固定的函数方程.它是一个可学习的激活函数，因为我们 W 参数是学习变化的。它是一个分段线性函数</p>
<p>paper：<a href="https://arxiv.org/abs/1302.4389">Maxout Networks</a></p>
<script type="math/tex; mode=display">\mathrm{Maxout}(x)=max(w_ix_i+b_i)</script><p><strong>优点</strong><br>Maxout的拟合能力非常强，可以拟合任意的凸函数。Maxout具有ReLU的所有优点，线性、不饱和性。同时没有ReLU的一些缺点。如：神经元的死亡。</p>
<p><strong>缺点</strong><br>从上面的激活函数公式中可以看出，每个神经元中有两组(w,b)参数，那么参数量就增加了一倍，这就导致了整体参数的数量激增。</p>
<h3 id="8，swish函数"><a href="#8，swish函数" class="headerlink" title="8，swish函数"></a>8，swish函数</h3><p>Swish 的设计受到了 LSTM 和高速网络中 gating 的 sigmoid 函数使用的启发。我们使用相同的 gating 值来简化 gating 机制，这称为 self-gating。</p>
<p>self-gating 的优点在于它只需要简单的标量输入，而普通的 gating 则需要多个标量输入。这使得诸如 Swish 之类的 self-gated 激活函数能够轻松替换以单个标量为输入的激活函数（例如 ReLU），而无需更改隐藏容量或参数数量。Swish 具备无上界、有下界、平滑、非单调的特性。</p>
<p>paper：<a href="https://arxiv.org/abs/1710.05941">Searching for Activation Functions</a></p>
<p><img src="swish1.jpg" alt="swish original"><br><img src="swish2.jpg" alt="swish derivatives"></p>
<p>原函数：</p>
<script type="math/tex; mode=display">\mathrm{Swish}(x)=x*\mathrm{sigmoid}(\beta x)</script><ul>
<li>$\beta=0$ Swish激活函数变为线性函数$f(x)=\frac{x}{2} $；</li>
<li>$\beta=\infty$ Swish激活函数变为 $0$ 或 $x$，相当于Relu;</li>
<li>因此，Swish函数可以看作是介于线性函数与ReLU函数之间的平滑函数。</li>
</ul>
<p>导数：</p>
<script type="math/tex; mode=display">f^{'}(x)=\beta f(x)+\mathrm{sigmoid}(x)(1-\beta f(x))</script><p>优点：</p>
<ul>
<li>无界性：有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和；（同时，有界性也是有优势的，因为有界激活函数可以具有很强的正则化，并且较大的负输入问题也能解决）；</li>
<li>导数恒为正；</li>
<li>平滑度在优化和泛化中起了重要作用。</li>
</ul>
<h3 id="9，mish激活函数"><a href="#9，mish激活函数" class="headerlink" title="9，mish激活函数"></a>9，mish激活函数</h3><p>一种自正则的非单调神经激活函数，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。</p>
<p>paper:<a href="https://www.bmvc2020-conference.com/assets/papers/0928.pdf">Mish: A Self Regularized Non-Monotonic Neural Activation Function</a></p>
<p><img src="mish.jpg" alt="mish original"><br><img src="mish2.png" alt="mish derivatives"></p>
<script type="math/tex; mode=display">\mathrm{Mish} = x*tanh(ln(1+e^x))</script><p><strong>优点</strong></p>
<ul>
<li>无上限，没有上限，这样可以保证没有饱和区域，因此在训练过程中不会有梯度消失的问题，这个和relu后面的激活函数一样；</li>
<li>有下限，有下限的话能够保证具有一定的regularization effect，这对于神经网络训练来说是一个很好的特性；</li>
<li>非单调性，这个在swish里面也强调过，文章说这种特性能够使得很小的负input在保持负output的同时也能够提高表达能力和梯度流；</li>
<li>光滑性，这个主要是相比relu的，relu在0点处不光滑，会在实际优化中遇到一些求解的问题，当然这个应该还是和具体算法有关，大部分我们就把relu=0的时候梯度也变成0了，然后文章继续提到mish的光滑的特性使得在求解和模型泛化性方面相比其他激活函数表现要优良。</li>
</ul>
<h2 id="三，参考文献"><a href="#三，参考文献" class="headerlink" title="三，参考文献"></a>三，参考文献</h2><p><a href="https://ieeexplore.ieee.org/document/9577874">activate or not:learning customized activation</a><br><a href="https://zhuanlan.zhihu.com/p/85971385">常见的激活函数及其特点</a><br><a href="https://www.cnblogs.com/wj-1314/p/12015278.html">深度学习笔记——常用的激活（激励）函数</a></p>
<h2 id="四，常见问题"><a href="#四，常见问题" class="headerlink" title="四，常见问题"></a>四，常见问题</h2><h3 id="1，如何选择合适的激活函数"><a href="#1，如何选择合适的激活函数" class="headerlink" title="1，如何选择合适的激活函数"></a>1，如何选择合适的激活函数</h3><ul>
<li>通常来说，不能把各种激活函数串起来在一个网络中使用。</li>
<li>如果使用ReLU，那么一定要小心设置学习率(learning rate),并且要注意不要让网络中出现很多死亡神经元。如果死亡神经元过多的问题不好解决，可以试试Leaky ReLU、PReLU、或者Maxout。</li>
<li>尽量不要使用sigmoid激活函数，可以试试tanh。</li>
</ul>
<h3 id="2，相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？"><a href="#2，相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？" class="headerlink" title="2，相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？"></a>2，相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？</h3><p>对于sigmoid函数而言，其输出始终为正，这会导致在深度网络训练中模型的收敛速度变慢，因为在反向传播链式求导过程中，权重更新的效率会降低。</p>
<p>此外，sigmoid函数的输出均大于0，作为下层神经元的输入会导致下层输入不是0均值 的，随着网络的加深可能会使得原始数据的分布发生改变。而在深度学习的网络训练中， 经常需要将数据处理成零均值分布的情况，以提高收敛效率，因此tanh函数更加符合这个要求。</p>
<p>sigmoid函数的输出在[0,1]之间，比较适合用于二分类问题。</p>
<h3 id="3，为什么RNN中常用tanh函数作为激活函数而不是ReLU？"><a href="#3，为什么RNN中常用tanh函数作为激活函数而不是ReLU？" class="headerlink" title="3，为什么RNN中常用tanh函数作为激活函数而不是ReLU？"></a>3，为什么RNN中常用tanh函数作为激活函数而不是ReLU？</h3><p>RNN中将tanh函数作为激活函数本身就存在梯度消失的问题，而ReLU本就是为了克服梯度消失问题而生的。因为ReLU的导数只能为0或1，而导数为1的时候在RNN中很容易造成梯度爆炸问题。</p>
<h3 id="4，为什么会出现梯度爆炸的问题呢？"><a href="#4，为什么会出现梯度爆炸的问题呢？" class="headerlink" title="4，为什么会出现梯度爆炸的问题呢？"></a>4，为什么会出现梯度爆炸的问题呢？</h3><p>因为在RNN中，每个神经元在不同的时刻都共享一个参数W（这点与CNN不同，CNN中每一层都使用独立的参数Wi），因此在前向和反向传播中，每个神经元的输出都会作为下一个时刻本神经元的输入，从某种意义上来讲相当于对其参数矩阵W作了连乘，如果W中有其中一个特征值大于1，则多次累乘之后的结果将非常大，自然就产生了梯度爆炸的问题。</p>
<h3 id="5，那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？"><a href="#5，那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？" class="headerlink" title="5，那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？"></a>5，那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？</h3><p>因为在CNN中，每一层都有不同的参数$W_i$，有的特征值大于1，有的小于1，在某种意义上可以理解为抵消了梯度爆炸的可能。</p>
<h3 id="6，什么是神经元“死亡”？"><a href="#6，什么是神经元“死亡”？" class="headerlink" title="6，什么是神经元“死亡”？"></a>6，什么是神经元“死亡”？</h3><p>Relu的输入值为负的时候，输出始终为0，其一阶导数也始终为0，这样会导致神经元不能更新参数，也就是神经元不学习了，这种现象叫做“Dead Neuron”。</p>
<h3 id="7，如何解决ReLU神经元“死亡”的问题？"><a href="#7，如何解决ReLU神经元“死亡”的问题？" class="headerlink" title="7，如何解决ReLU神经元“死亡”的问题？"></a>7，如何解决ReLU神经元“死亡”的问题？</h3><p>①采用Leaky ReLU等激活函数 ②设置较小的学习率进行训练 ③使用momentum优化算法动态调整学习率</p>
]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>activation</tag>
      </tags>
  </entry>
  <entry>
    <title>Unraveling Attention via Convex Duality-Analysis and Interpretations of Vision Transformers</title>
    <url>/2023/03/22/Transformer/Unraveling%20Attention%20via%20Convex%20Duality-Analysis%20and%20Interpretations%20of%20Vision%20Transformers/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>文章：Unraveling Attention via Convex Duality-Analysis and Interpretations of Vision Transformers</p>
<p><a href="https://arxiv.org/abs/2205.08078">essay link</a></p>
<p><strong>通过凸对偶性解释Transformer网络~</strong><br><span id="more"></span></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>使用自注意力或其替代方案的视觉Transformer已经在许多图像相关任务中展示了有潜力的结果，然而，注意力的基础归纳偏差尚不清楚。为了解决这个问题，本文从凸对偶的角度分析了注意力。对于非线性点积自注意力，以及 MLP 混合器和傅里叶神经算子 (FNO) 等替代机制，我们推导出等效的有限维凸问题，这些问题可解释并可求全局最优解。凸程序带来的块核范数正则化促使了潜在特征和token维度的低秩，特别是，我们展示了自注意力网络如何根据token的潜在相似性隐式地对其进行聚类。我们通过微调各种凸注意力头进行实验，采用的是 CIFAR-100 分类问题的预训练主干，结果表明与现有的 MLP 或线性头相比，注意力机制占优。</p>
<h1 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h1><p>Transformers 最近在语言和视觉的表示学习方面取得了巨大的成功，这主要是由于注意机制有效地混合了各层对token的表示以学习输入的语义。 在点积自注意力出现后，出现了几种有效的替代方法，可以很好地适应大型预训练任务的序列大小。然而，可学习的注意力归纳偏置没有得到很好的探索，对注意力归纳偏置的理论理解可以激发设计更高效的架构，并可以解释这些网络的泛化能力。</p>
<p>自注意力是首个视觉Transformer (ViT)中的基本构建块，它由两个线性函数的外积组成，后跟一个非线性函数以及与另一个线性函数的积，这使得它是非凸和不可解释的。理解注意力的一种方法是设计新的自注意力替代方案，其表现同样出色，这可能有助于解释其潜在机制。一组工作涉及基于多层感知器 (MLP) 的架构，tolstikhin2021mlp 等，而另一系列工作提出基于傅立叶的模型 lee2021fnet 等，其他人提出用矩阵分解 geng2021attention等 代替 self-attention。虽然所有这些作品都有吸引人的应用，利用了注意力结构的一般概念，但它们缺乏从优化角度对这些架构及微调后的应用做理论分析。</p>
<p>为了解决这个缺点，我们利用凸对偶性来分析具有 ReLU 激活的单个自注意力块。 由于自注意力会导致序列的平方复杂度，因此我们选择分析更高效的模块。 作为更高效模块的代表，我们专注于 MLP 混合器和傅立叶神经运算符（FNO）。 MLP 混合器（纯粹）在token和特征维度上使用 MLP 投影来混合token，相比之下，FNO 基于 2D 傅里叶变换来实现利用循环卷积混合token。</p>
<p>我们发现所有这三个分析模块都等同于有限维凸优化问题，表明“可证明地将它们优化到全局最优值”是有保障的。此外，我们对凸模型引起的偏差进行了观察，特别是，自注意力和 MLP-Mixer 模块的凸等价物类似于 MLP 的加权组合，但具有额外的自由度（例如，更高维度的归纳参数），以及将它们的各个子模块联系在一起以利用全局信息的独特块核范数正则化操作。相比之下，凸化的 FNO 混合器相当于循环卷积，而对 FNO 架构的轻微修改可以诱导出等效的分组卷积。我们在CIFAR-100 分类问题上进行Transformer的迁移学习，微调单个凸注意头来进行注意力头的实验测试和比较。我们观察到这些注意力模块的归纳偏差优于传统的凸模型。</p>
<p>本文的主要贡献总结如下：</p>
<ul>
<li>我们通过证明带线性（或 ReLU ）激活函数的自注意力、MLP-Mixer 和 FNO 模块与凸优化问题的等价性来保障其能求得到全局最优解。</li>
<li>通过分析这些等效的凸应用，本文为这些注意力模块的优化目标提供了可解释性。</li>
<li>实验验证了（凸）视觉 Transformer 在迁移学习任务中的表现优于基线凸方法。</li>
</ul>
<h2 id="1-1-相关工作"><a href="#1-1-相关工作" class="headerlink" title="1.1 相关工作"></a>1.1 相关工作</h2><p>这项工作主要与两条研究路线有关。</p>
<p><strong>自注意力的解释</strong><br>一种方法是通过实验观察注意力网络的特性来理解它们，例如，DINO 提出了一种针对 ViTs 的对比自监督学习方法。 据观察，学习到的注意力映射保留了图像的语义区域。 另一项工作比较了经过训练的 ViT 和 CNN 的跨层对齐，得出的结论是 ViT 在网络的各个层之间具有更统一的表示结构。 另一项工作使用深度泰勒分解方法来可视化输入图像中会引起特定 ViT 预测的部分。</p>
<p>另一种方法是分析注意力网络的表现力，一项工作将多头自注意力解释为贝叶斯推理，并提供了工具来决定使用多少个头，以及如何在不同的头中强化特征的区别表达。 其他分析表明，稀疏Transformer可以逼近任何函数，多头自注意力网络至少跟卷积网络的表达力相当，点积自注意力网络不是Lipschitz 连续的。</p>
<p><strong>凸神经网络</strong><br>从 pilanci2020neural 开始，已有大量工作证明各种 ReLU 及其变体的激活神经网络架构具有等效的凸优化问题。 这些包括双层卷积和矢量输出网络、更深层次的网络、具有批量标准化的网络和 Wasserstein GANs。最近的工作还展示了如何有效地优化这些等效凸网络的最简单形式，并结合额外的约束来增强对抗性鲁棒性。 然而，这些工作都没有分析过Transformer的构建块，而Transformer是许多最先进的视觉和语言处理任务中的主要方法。</p>
<h1 id="2，预备工作"><a href="#2，预备工作" class="headerlink" title="2，预备工作"></a>2，预备工作</h1><p>一般来说，我们分析监督学习问题，其中输入训练数据 $\{\mathbf{X}_i \in \mathbb{R}^{s \times d}\}_{i=1}^n$ 是嵌入层patch的结果，我们有相应的任意大小的标签 $\{\mathbf{Y}_i \in \mathbb{R}^{r \times c}\}_{i=1}^n$。 对于任意<strong>凸损失函数</strong> $\mathcal{L}(\cdot, \cdot)$，我们求解优化问题</p>
<script type="math/tex; mode=display">
p^\ast:=\min_{\theta}\sum\limits^n_{i=1}\mathcal{L}(f_{\theta}(\mathbf{X}_i),\mathbf{Y}_i)+\mathcal{R}(\theta) 
\tag{1}</script><p>$\theta$ 是可学习的参数，$f_{\theta}(\cdot)$ 是神经网络，$\mathcal{R}(\cdot)$ 是正则化器。 请注意，此公式包括了去噪和分类场景：在 $r = 1$ 的分类设置中，可以将全局平均池化吸收到凸损失 $\mathcal{L}$ 中，而如果 $r = s$，可以直接使用平方损失或其他凸损失函数，也可以使用这个公式来应用于监督学习和自监督学习。</p>
<p>在本文中，我们将 $(\cdot)_+ := \max \{0, \cdot\}$ 表示为 非线性的ReLU。 我们使用上标，比如 $\mathbf{A}^{(i_i, i_2)}$ 来表示矩阵块，用方括号，比如 $\mathbf{A}[i_1, i_2]$ 来表示矩阵的元素，其中参数指的是行（或行块）$i_1$ 和列（或列块）$i_2$。</p>
<h2 id="2-1-线性和-ReLU-MLPs-的隐式凸性"><a href="#2-1-线性和-ReLU-MLPs-的隐式凸性" class="headerlink" title="2.1 线性和 ReLU MLPs 的隐式凸性"></a>2.1 线性和 ReLU MLPs 的隐式凸性</h2><p>之前，已经证明标准的两层 ReLU MLP 等价于凸优化问题，简要描述相关背景，为本文中的大部分分析提供背景。 特别是，我们表示一个网络的隐藏层中有 $m$ 个神经元，权重衰减参数 $\beta &gt;0$ 和数据 $\mathbf{X} \in \mathbb{R} ^{n \times d}$, $\mathbf{Y} \in \mathbb{R}^{n \times c}$ 为:</p>
<script type="math/tex; mode=display">
p^\ast_{RMLP}:=\min_{\mathbf{w}_{2j},\mathbf{w}_{2j}}\mathcal{L}\bigg ( \sum\limits^m_{j=1}(\mathbf{X}\mathbf{w}_{1j})_+\mathbf{w}^\top_{2j},\mathbf{Y} \bigg )+\frac{\beta}{2}\sum\limits^m_{j=1}(\|\mathbf{w}_{1j}\|^2_2+\|\mathbf{w}_{2j}\|^2_2) 
\tag{2}</script><p>虽然这个问题如上所述是非凸的，但已经证明目标等同于等价凸优化问题的解决方案，并且两个问题的解决方案之间存在一对一的映射。 特别是，该分析利用了 <strong>hrperplane arrangements</strong>，枚举了 所有可能的非线性 ReLU 激活模式：</p>
<script type="math/tex; mode=display">
\mathcal{D}:=\{\mathrm{diag}(\mathbb{1}\{\mathbf{Xu}\geq 0\}):\mathbf{u}\in\mathcal{R}^d \} 
\tag{3}</script><p>集合 $\mathcal{D}$ 显然是有限的，其基函数的界为 $P := |\mathcal{D}| \leq 2r\left(\frac{e(n-1)}{r}\right)^r$，其中 $r := \mathrm{rank}(\mathbf{X})$。 通过凸对偶分析，我们可以通过枚举有限排列集 $\{\mathbf{D}_j\}_{j=1}^P$ 来表示一个等价的凸优化问题。 我们定义以下范数：</p>
<script type="math/tex; mode=display">
\begin{align*}
&\| \mathbf{Z} \|_{*,K}  :=\min_{t\geq 0}t\quad s.t.\mathbf{Z}\in t\mathcal{C} \\
&\mathcal{C} :=conv\{\mathbf{Z}=uv^\top\; : \; \mathbf{K}u\geq 0,\|\mathbf{Z}\|_\ast\leq 1,v\in\mathbb{R}^c \}
\end{align*}
\tag{4}</script><p>这个范数是一个准核范数，它与标准核范数的不同之处在于它所依赖的因式分解对其左因子施加了约束，在我们的例子中这将是一个仿射约束。 在凸 ReLU 神经网络中，选择 $\mathrm{K}$ 来强制存在 $\{\mathbf{u}_k, \mathbf{v}_k\}$ 这样 $\mathbf{Z} = \sum_k \mathbf{u}_k \mathbf{v}_k^\top$ 和 $\mathbf{D}_{j}\mathbf{X}\mathbf{Z} = \sum_k (\mathbf{X}\mathbf{u}_k)_+\mathbf{v}_k^\top$, 并惩罚 $\sum_k |\mathbf{u}_k|_2 |\mathbf{v}_k|_2$。</p>
<p>有了这个成立，可以证明</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{RMLP}:=\min_{\{\mathbf{z}_j\}^P_{j=1}}\mathcal{L}( \sum\limits^P_{j=1}(\mathbf{D}_j\mathbf{X}\mathbf{Z}_j,\mathbf{Y})+\beta\sum\limits^P_{j=1}\|\mathbf{Z}_j\|_{*,\mathbf{K}_j} \\
\mathbf{K}_j:=(2\mathbf{D}_j-\mathbf{I})\mathbf{X}
\end{align*}
\tag{5}\label{eq5}</script><p>双层 ReLU MLP 优化问题因此表示为具有核范数正则化约束的分片线性模型，这与双层线性激活 MLP 形成对比，后者的凸等价式为：</p>
<script type="math/tex; mode=display">
p^\ast_{RMLP}=\min_{\mathbf{z}}\mathcal{L}(\mathbf{X}\mathbf{Z},\mathbf{Y})+\beta\|\mathbf{Z}\|_\ast 
\tag{6}\label{eq6}</script><p>众所周知，这种核范数惩罚会诱导出低秩解决方案并出现在矩阵分解问题中。 还可以定义 gated ReLU 激活，其中 ReLU 门固定为 $\{\mathbf{h}_j\}_{j=1}^m$，</p>
<script type="math/tex; mode=display">
g(\mathbf{X}\mathbf{w}_{1j}):=\mathrm{diag}(1\{\mathbf{X}\mathbf{h}_j\geq 0\})(\mathbf{X}\mathbf{w}_{1j}) 
\tag{7}</script><p>然后，定义 $\{\mathbf{D}_j\}_{j=1}^m := \{\mathrm{diag}(1\{\mathbf{X}\mathbf{h}_j \geq 0\} )\}_{j=1}^m$, 相应的凸门控 ReLU 激活双层网络目标直接从 ReLU 和线性情况下得出，由下式给出</p>
<script type="math/tex; mode=display">
p^\ast_{RMLP}=\min_{\mathbf{z}^m_{j=1}}\mathcal{L}\bigg (\sum\limits^m_{j=1}\mathbf{D}_j\mathbf{X}\mathbf{Z}_j,\mathbf{Y}\bigg )+\beta\sum\limits^m_{j=1}\|\mathbf{Z}_j\|_\ast 
\tag{8}</script><p>我们注意到，对于线性和门控 ReLU 公式，凸权重的正则化成为了标准核范数，因为不再需要强制 ReLU 约束。已经证明，门控 ReLU 和 ReLU 网络之间存在小的近似差距，并且 ReLU 网络可以从门控 ReLU 问题的解决方案中形成。</p>
<p>在解决这些问题的有效算法方面，使用应用于凸线性和门控 ReLU 的加速近端梯度下降算法，可以在 $\mathcal{O}(1 /\sqrt{\epsilon})$ 迭代下获得 $\epsilon$ 的精度。对于凸 ReLU 公式，sahiner2020mathbftor 提出了适用于这种情况的凸 MLP 的 Frank-Wolfe 算法，对于 $\epsilon$ 精度，在一般情况下需要 $\mathcal{O}(1/\epsilon)$ 迭代。</p>
<p>在后续部分中，我们将通过类似的凸对偶技术演示具有线性和 ReLU 激活的常见视觉Transformer块如何与等效凸优化问题相关联。</p>
<h1 id="3，自注意力的隐式凸性"><a href="#3，自注意力的隐式凸性" class="headerlink" title="3，自注意力的隐式凸性"></a>3，自注意力的隐式凸性</h1><p>规范的 Vision Transformer (ViT) 使用自注意力和 MLP 作为其主干，特别是，自注意力网络的单个“头”由以下给出：</p>
<script type="math/tex; mode=display">
f_j(\mathbf{X}_i):=\sigma\bigg(\frac{\mathbf{X}_i\mathbf{Q}_j\mathbf{K}^\top_j\mathbf{X}^\top_i}{\sqrt{d}}\bigg)\mathbf{X}_i\mathbf{V}_j 
\tag{9}</script><p>其中 $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ 都是可学习的参数，$\sigma(\cdot)$ 通常（但不总是）表示非线性的 softmax。 在实践中，人们通常使用 $m$ 个注意力“头”，它们沿着特征维度连接在一起，然后是一个“通道混合”层，或者一个分类头：</p>
<script type="math/tex; mode=display">
\begin{align*}
f_{MHSA}(\mathbf{X}_i)& := [f_1(\mathbf{X}_i)\cdots f_m(\mathbf{X}_i)]\mathbf{W} \\
&=\sum\limits^m_{j=1}\sigma\bigg (\frac{\mathbf{X}_i\mathbf{Q}_j\mathbf{K}^\top_j\mathbf{X}^\top_i}{\sqrt{d}}\bigg) \mathbf{X}_i\mathbf{V}_j\mathbf{W}_j 
\end{align*}
\tag{10}</script><p>为了我们的分析，注意到 $\mathbf{Q}_j \mathbf{K}_j^\top$ 和 $\mathbf{V}_j\mathbf{W}_j$ 都可以表示为单个线性层，我们将多头自注意力网络建模为</p>
<script type="math/tex; mode=display">
f_{SA}(\mathbf{X}_i):=\sum\limits^m_{j=1}\sigma\bigg (\frac{\mathbf{X}_i\mathbf{W}_{1j}\mathbf{X}^\top_i}{\sqrt{d}}\bigg) \mathbf{X}_i\mathbf{W}_{2j} 
\tag{11}</script><p>然后我们定义多头自注意力训练问题如下所示：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{SA}:=&\min_{\mathbf{W}_{1j},\mathbf{W}_{2j}}\sum\limits^n_{i=1}\mathcal{L}(f_{SA}(\mathbf{X}_i),\mathbf{Y}_i)\\
&+\frac{\beta}{2}\sum\limits^m_{j=1}\|\mathbf{W}_{1j}\|^2_F+\|\mathbf{W}_{2j}\|^2_F 
\end{align*}
\tag{12}\label{eq12}</script><p>因此在公式中使用了通用的凸损失函数和标准权重衰减。 虽然当 $\sigma(\cdot)$ 表示 softmax 激活时直接凸分析是棘手的，但我们可以针对许多其他激活函数分析这种架构。 特别是，已经提出了具有线性和 ReLU 激活函数的自注意力，其性能与标准 softmax 激活网络相当。因此，我们将分析带线性、ReLU 和门控 ReLU 激活变体的多头自注意力模型。</p>
<p><strong>定理3.1</strong><br>对于带线性激活函数的多头自注意力网络的训练问题\eqref{eq12}，对于 $m \geq m^\ast$，其中 $m^\ast \leq \min\{d^2, dc\}$, 标准的非凸训练目标相当于一个凸优化问题，由下式给出</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{SA}:=&\min_{\mathbf{Z}\in\mathbb{R}^{d^2\times dc}}\sum\limits^n_{i=1}\mathcal{L}\bigg(\sum\limits^d_{k=1}\sum\limits^d_{\ell =1}\mathbf{G}_i[k,\ell ]\mathbf{X}_i\mathbf{Z}^{(k,\ell)},\mathbf{Y}_i \bigg)\\
&+\beta\|\mathbf{Z}\|_\ast
\end{align*}
\tag{13}\label{eq13}</script><p>其中，$\mathbf{G}_i:=\mathbf{X}^\top_i\mathbf{X}_i$，且 $\mathbf{Z}^{(k,\ell )}\in\mathbb{R}^{d\times c}$。</p>
<p>结果表明，线性激活自注意力模型由 Gram（特征相关）矩阵加权线性模型组成，核范数惩罚项将各个模型彼此组合。</p>
<p>还可以将凸模型视为一组具有加权核范数的线性模型，其中每个块 $\mathbf{Z}^{(k, \ell)}$ 具有相应的权重 $1/\mathbf{G}_i[k, \ell]$。因此，具有高相关性的特征将具有相应的较大范数的线性权重。我们注意到，当 $\beta = 0$ 时，线性自注意力模型 \eqref{eq13} 等价于线性两层 MLP \eqref{eq6}。</p>
<p>虽然通常 $\mathbf{Z}$ 上的核范数惩罚项在每个单独的线性模型 $\mathbf{Z}^{(k, \ell)}$ 上没有相应的范数，但以下结果总结了一个实例，核规范可以分解成更小的块。</p>
<p><strong>推论3.2</strong><br>假设 $\mathbf{X}_i$ 的某些特征与所有 $i$ 完全不相关，即 $\mathbf{G}_i$ 是对于所有 $i$ 的块对角线块 $\{\mathbf{ G}_i^{(b)} \in \mathbb{R}^{d_b \times d_b}\}_{b=1}^B$。 然后，凸问题 \eqref{eq13} 简化为以下凸问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{SA}:=&\min_{\mathbf{Z}^{(b)}}\sum\limits^n_{i=1}\mathcal{L}\bigg(\sum\limits^B_{b=1}\sum\limits^{d_b}_{k=1}\sum\limits^{d_B}_{\ell =1}\mathbf{G}_i^{(b)}[k,\ell ]\mathbf{X}_i\mathbf{Z}^{(b,k,\ell)},\mathbf{Y}_i \bigg)\\
&+\beta\|\mathbf{Z}^{(b)}\|_\ast,\mathbf{Z}^{(b)}\in\mathbb{R}^{d_bd\times d_bc}
\end{align*}
\tag{14}\label{eq14}</script><p>因此，这个推论表明，在不相关特征集的假设下，线性自注意力块在这些集上分离。 特别是，对应于 Gram 矩阵 $\mathbf{G}_i$ 中 $0$ 值的 $\mathbf{Z}$ 块将被设置为 $0$，从而消除不相关特征之间的相互作用。 这种现象如图1所示。</p>
<p>虽然这个线性模型为自注意力的基础提供了一个简单、优雅的解释，但我们也可以分析具有非线性的自注意力块。 因此，我们提供了 ReLU 激活自注意力的分析。</p>
<p><strong>定理3.3</strong><br>对于 ReLU 激活多头自注意力训练问题 \eqref{eq12}，我们定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{X}& :=
\begin{bmatrix}
\mathbf{X}_1\otimes\mathbf{X}_1\\
\cdots \\
\mathbf{X}_n\otimes\mathbf{X}_n
\end{bmatrix}\\
\{\mathbf{D_j}\}^P_{j=1}& :=\{\mathrm{diag} (1\{ \mathbf{X}\mathbf{u}_j\geq 0\}):\mathbf{u}_j\in\mathbb{R}^{d^2} \}
\end{align*}</script><p>其中 $P \leq 2r\left(\frac{e(n-1)}{r}\right)^r$ 和 $r := \mathrm{rank}(\mathbf{X})$。</p>
<p>那么，对于 $m \geq m^\ast$，且 $m^\ast \leq n\min\{d^2, dc\}$，标准的非凸训练等价于以下凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{SA}:=&\min_{\mathbf{Z}_j\in\mathbb{R}^{d^2\times dc}}\sum\limits^n_{i=1}\mathcal{L}\bigg(\sum\limits^P_{j=1}\sum\limits^d_{k=1}\sum\limits^d_{\ell =1}\mathbf{G}_{i,j}^{(k,\ell )}\mathbf{X}_i\mathbf{Z}_j^{(k,\ell)},\mathbf{Y}_i \bigg)\\
&+\beta\|\mathbf{Z}_j\|_{\ast,K_j},\mathbf{K}_j:=(2\mathbf{D}_j-\mathbf{I}_{ns^2})\mathbf{X}
\end{align*}
\tag{15}\label{eq15}</script><p>其中 $\mathbf{G}_{i,j} := (\mathbf{X}_i \otimes \mathbf{I}_{s})^\top \mathbf{D}_{j}^{( i)} (\mathbf{X}_i \otimes \mathbf{I}_{s})$, $\mathbf{G}_{i,j}^{(k, \ell)} \in \mathbb{R}^{s \times s}$ 和 $\mathbf{Z}_j^{(k, \ell)} \in \mathbb{R}^{d \times c}$。</p>
<p>有趣的是，虽然标准 ReLU MLP 的超平面排列仅取决于数据矩阵 $\mathbf{X}$，但对于自注意力网络，它们更复杂，而不是取决于 $\mathbf{X}_i \otimes \mathbf{X}_i$。这些超平面排列定义了约束核范数惩罚项的约束。人们可能会将 ReLU 激活自注意力模型视为两个模型的融合—，一种使用 $\mathbf{X}_i \otimes \mathbf{X}_i$ 生成超平面排列，另一种将 $\mathbf{X}_i$ 用于线性预测。因此，与线性自注意力情况不同，即使在 $\beta = 0$ 的情况下，ReLU 自注意力网络 \eqref{eq15} 也不等同于 ReLU MLP 模型 \eqref{eq5}。</p>
<p>此外，在 \eqref{eq13} 中的线性激活情况下，每个线性模型由 $\mathbf{G}_i$ 中的单项进行缩放，而在 ReLU 情况下，每个线性模型由对角矩阵 $\mathbf{G}_{i,j}^{(k, \ell)}$ 进行缩放，将来自 $\mathbf{X}_i$ 的二阶信息与由 ReLU 激活函数诱导的超平面排列联合起来。例如，人们可能会注意到区别项：</p>
<script type="math/tex; mode=display">\mathbf{G}^{(k,\ell )}_{i,j}=\sum\limits^s_{t=1}\mathbf{X}_i[t,k]\mathbf{X}_i[t,\ell ]\mathbf{D}^{(i,t)}_j \tag{16}</script><p>是对于对角线 $\mathbf{D}_{j}^{(i, t)} \in \{0, 1\}^{s \times s}$ 而言。 因此，$\mathbf{G}_{i,j}^{(k, \ell)}$ 可以看作是特征 $k$ 和 $\ell$ 之间的相关性，由对角线 $\{0, 1\}$ 进行加权。换句话说，一种“局部”相关性，其中局部性是通过 $\mathbf{D}_{j}^{(i, t)}$ 中的 $\{0, 1\}$ 值来体现。这种局部相关性对预测的每个token进行缩放，本质上是为未被 $\mathbf{D}_{j}^{(i, t)}$ 掩盖的token赋予权重。</p>
<h1 id="4，替换混合机制"><a href="#4，替换混合机制" class="headerlink" title="4，替换混合机制"></a>4，替换混合机制</h1><p>虽然自注意力是最初提出的用于视觉Transformer的token混合器，但还有许多其他替代方法已显示出产生类似结果，同时计算效率更高，我们在这里处理两个这样的架构。</p>
<h2 id="4-1-MLP-Mixer"><a href="#4-1-MLP-Mixer" class="headerlink" title="4.1 MLP Mixer"></a>4.1 MLP Mixer</h2><p>我们首先分析 MLP-Mixer 架构，这是一种替代自注意力网络的全 MLP 结构，在图像分类基准上具有竞争性能。 该提案很简单，—沿输入的一个维度应用 MLP，然后沿相反维度应用 MLP。 因此，这种 MLP-Mixer 架构的最简单形式可以写成：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{MM}&:=\min_{\mathbf{W}_{1j},\mathbf{W}_{2j}}\sum\limits^n_{i=1}\mathcal{L}(\sum\limits^m_{j=1}\sigma(\mathbf{W}_{1j}\mathbf{X}_i)\mathbf{W}_{2j},\mathbf{Y}_i)\\
&+\frac{\beta}{2}\sum\limits^m_{j=1}\|\mathbf{W}_{1j}\|^2_F+\|\mathbf{W}_{2j}\|^2_F
\end{align*}
\tag{17}\label{eq17}</script><p>其中 $\sigma$ 是激活函数。 虽然 tolstikhin2021mlp 使用 GeLU 的非线性，但我们分析了更简单的线性和 ReLU 对应激活函数，这对 MLP-Mixer 架构的底层结构提供了重要的见解。</p>
<p><strong>定理4.1</strong><br>对于线性激活MLP-Mixer的训练问题\eqref{eq17}，对于 $m \geq m^\ast$，其中 $m^\ast \leq \min\{s^2, dc\}$， 标准的非凸训练目标相当于一个下式的凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{MM}&:=\min_{\mathbf{Z}\in\mathbb{R}^{s^2\times dc}}\sum\limits^n_{i=1}\mathcal{L}([f_1(\mathbf{X}_i)\cdots f_c(\mathbf{X}_i)],\mathbf{Y}_i)\\
&+\beta\|\mathbf{Z}\|_\ast,f_p(\mathbf{X}_i):=\mathbf{Z}^{(p)}\text{vec}(\mathbf{X}_i)
\end{align*}
\tag{18}</script><p>其中 $\mathbf{Z}^{(p)} \in \mathbb{R}^{s \times sd}$ 对于 $p \in [c]$, 和 $\mathbf{Z} ^{(p, t)} \in \mathbb{R}^{s \times d}$ 对于 $t \in [s]$，以及</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{Z}^{(p)}=[\mathbf{Z}^{(p,1)}\cdots \mathbf{Z}^{(p,s)}]\\
\mathbf{Z}=
\begin{bmatrix}
\mathbf{Z}^{(1,1)}\cdots \mathbf{Z}^{(c,1)}\\
\cdots \\
\mathbf{Z}^{(1,s)}\cdots \mathbf{Z}^{(c,s)}
\end{bmatrix}
\end{align*}</script><p>我们可以将线性 MLP-Mixer 的拟合项与标准线性 MLP \eqref{eq6} 进行对比，其中网络输出的每一列 $k$ 为：</p>
<script type="math/tex; mode=display">
\mathbf{X}_i\mathbf{Z}^{(k)} = ({\mathbf{Z}^{(k)}}^\top \otimes \mathbf{I}_s)\text{vec}(\mathbf{X}_i)</script><p>其中 $\mathbf {Z}^{(k)} \in \mathbb{R}^d$。因此，与标准线性 MLP 相比，MLP-Mixer 为网络提供了 $s^2$ 量级多的自由度来拟合 $\mathbf{Y}_i$ 的每一列。这表明，与线性自注意力网络不同，即使 $\beta = 0$，线性 MLP-Mixer 模型也不等同于线性标准 MLP。人们可能会推测，与标准 MLP 相比，这种额外的隐式自由度允许类似 MLP 模型混合器更容易地适应复杂的分布。虽然从拟合项看来 $\mathbf{Y}_i$ 的每个输出类都是独立拟合的，但我们注意到这些输出通过 $\mathbf{Z}$ 上的核范数耦合在一起，这鼓励 $\{ \mathbf{Z}^{(k)}\}_{k=1}^c$ 彼此相似。</p>
<p>凸线性 MLP-Mixer 架构的另一种解释可以通过简单地将 $\mathbf{Z}$ 的列置换为 $\tilde{\mathbf{Z}}$ 来实现，这不会影响核范数，因此不会影响最优解。如果根据块 $\tilde{\mathbf{Z}}^{(t, k)} \in \mathbb{R}^{ s \times c},(t \in [s],k \in [d])$上 $\tilde{\mathbf{Z}}$ 的按列划分，凸优化问题也可以写成： </p>
<script type="math/tex; mode=display">
p^\ast_{MM}=\min_{\tilde{\mathbf{Z}}}\sum\limits^n_{i=1}\mathcal{L}\bigg (\sum\limits^s_{t=1}\sum\limits^d_{k=1}\mathbf{X}_i[t,k]\tilde{\mathbf{Z}}^{(t,k)},\mathbf{Y}_i  \bigg)+\beta\|\tilde{Z} \|_\ast 
\tag{19}\label{eq19}</script><p>在这里，与线性自注意力网络 \eqref{eq13} 的连接变得更加清晰，\eqref{eq13} 是线性模型的加权求和，其中权重对应于 Gram 矩阵项，而 \eqref{eq19} 是预测的加权求和，其中权重对应于数据矩阵项。 我们还注意到，在大多数网络中，通常 $s &lt; d$，因此与自注意力块相比，MLP-Mixer 块的求解复杂度较低，我们还可以将这些结果扩展到 ReLU 激活的 MLP-Mixers。</p>
<p><strong>定理4.2</strong><br>对于 ReLU 激活的 MLP-Mixer 训练问题 \eqref{eq17}，我们定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{X}& :=
\begin{bmatrix}
\mathbf{X}^\top_1\otimes\mathbf{I}_s\\
\mathbf{X}^\top_n\otimes\mathbf{I}_s
\end{bmatrix}\\
\{\mathbf{D}_j\}^P_{j=1}& :=\{\mathrm{diag}(1\{\mathbf{X}\mathbf{u}_j\geq 0 \}):\mathbf{u}_j\in\mathbb{R}^{s^2} \}
\end{align*}</script><p>其中 $P \leq 2r\left(\frac{e(n-1)}{r}\right)^r$ 和 $ r := \mathrm{rank}(\mathbf{X})$。然后，对于 $m \geq m^\ast$，其中 $m^\ast \leq n\min\{s^2, dc\}$，标准的非凸训练目标等价于以下凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  p^\ast_{MM}& =\min_{\mathbf{Z}_j\in\mathbb{R}^{s^2\times dc}}\sum\limits^n_{i=1}\mathcal{L}([f_1(\mathbf{X}_i)\cdots f_c(\mathbf{X}_i)],\mathbf{Y}_i)\\
  & + \beta\sum\limits^P_{j=1}\|\mathbf{Z}_j\|_{\ast,K_j},\mathbf{K}_j:=(2\mathbf{D}_j-\mathbf{I}_{nd})\mathbf{X}
\end{align*}
\tag{20}\label{eq20}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{align*}
    f_p(\mathbf{X_i}) &:= \sum_{j=1}^P \begin{bmatrix}  \mathbf{D}_{j}^{(i, 1)}\mathbf{Z}_j^{(p, 1)} \cdots   \mathbf{D}_{j}^{(i, d)}\mathbf{Z}_j^{(p, d)}\end{bmatrix}  \mathrm{vec}(\mathbf{X}_i)
\end{align*}</script><p>对于 $\mathbf{D}_{j}^{(i,k)} \in \mathbb{R}^{s \times s}$ 和 $\mathbf{Z}_j^{(p, k)} \in \mathbb{R}^{s \times s}$。</p>
<p>现在，与自注意力模型不同，超平面排列的有效数据矩阵是 $\mathbf{X}_i \otimes \mathbf{X}_i$，MLP-mixer 的排列使用 $\mathbf{X}_i^\top \otimes \mathbf{I}_s$，为分区数据提供额外的自由度，同时仍仅包含有关数据的一阶信息。 使用与 \eqref{eq19} 中相同的列置换技巧，可以将 \eqref{eq20} 写为</p>
<script type="math/tex; mode=display">
\begin{align*}
  p^\ast_{MM}& =\min_{\mathbf{\tilde{Z}}_j}\sum\limits^n_{i=1}\mathcal{L}\bigg (\sum\limits^P_{j=1}\sum\limits^s_{t=1}\sum\limits^d_{k=1}\mathbf{X}_i[t,k]\mathbf{D}_j^{(i,k)}\tilde{\mathbf{Z}}_j^{(t,k)},\mathbf{Y}_i\bigg )\\
  & + \beta\sum\limits^P_{j=1}\|\tilde{\mathbf{Z}}_j\|_{\ast,K_j}
\end{align*}
\tag{21}</script><p>现在我们再次清楚地看到与\eqref{eq15} ReLU 自注意力的差异，对角线排列由 $\mathbf{X}_i$ 而不是 Gram 矩阵加权， $\tilde{\mathbf{ Z}}_j^{(t, k)}$ 只是简单的预测，而不是线性模型的权重。</p>
<h2 id="4-2-Fourier神经算子"><a href="#4-2-Fourier神经算子" class="headerlink" title="4.2 Fourier神经算子"></a>4.2 Fourier神经算子</h2><p>与自注意力或类似 MLP 的注意力机制相比，还有一系列基于傅立叶的自注意力替代方案，最近在视觉任务中显示出前景。 我们介绍傅里叶神经算子 (FNO) ，其工作方式如下：</p>
<ul>
<li>i. 二维 DFT 首先应用于空间token； </li>
<li>ii. 每个token乘以自己的权重矩阵； </li>
<li>iii. 逆 DFT 将傅立叶token返回到原始（空间）域。</li>
</ul>
<p>以紧凑矩阵形式表示 FNO，请注意除了标准 MLP 权重 $\mathbf{W}_1 \in \mathbb{R}^{d \times m},\, \mathbf{W}_2 \in \mathbb{R}^{m \times c}$，FNO 块具有第三组权重 $\mathbf{L} \in \mathbb{R}^{s \times d \times d}$。 让我们定义傅里叶变换 $\mathbf{F} := \mathbf{F}_h \otimes \mathbf{F}_w$，它是 $h \times w$ 二维傅里叶变换的矢量化版本。 在傅立叶空间中将权重 $\mathbf{L}$ 写成为 $\mathbf{V}$ 更方便，其中二维傅立叶变换已应用于第一维，即 $\mathbf{V}[ :,i,j]$ 对应每个 $i,j$。</p>
<p>现在，定义 $\mathbf{V}^{(j)}=\mathbf{V}[j,:,:]$。 然后将 $\mathbf{F}\mathbf{X}_i$ 的每一行乘以 $d \times d$ 权重矩阵 $\mathbf{V}^{(j)}$，并转换回图像域如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
f_{FN}(\mathbf{X}_i):=\sigma\bigg ( \bigg ( \mathbf{F}^{-1}
    \begin{bmatrix}
        (\mathbf{F}\mathbf{X}_i)_1^\top\mathbf{V}^{(1)}\\
        \cdots \\
        (\mathbf{F}\mathbf{X}_i)_s^\top\mathbf{V}^{(s)}
    \end{bmatrix}
\bigg )\mathbf{W}_1\bigg )\mathbf{W}_2  
\end{align*}
\tag{22}\label{eq22}</script><p>这种表示可以大大简化。</p>
<p><strong>引理4.3</strong><br>对于权重 $\mathbf{W}_1 \in \mathbb{R}^{sd \times m}$, $\mathbf{W}_2 \in \mathbb{R}^{m \times c}$, FNO 块 \eqref{eq22} 可以等效地表示为</p>
<script type="math/tex; mode=display">
f_{FN}(\mathbf{X}_i)=\sum\limits^m_{j=1}\sigma(\mathrm{circ}(\mathbf{X}_i)\mathbf{w}_{1j})\mathbf{w}_{2j}^\top 
\tag{23}</script><p>其中 $\mathrm{circ}(\mathbf{X}_i) \in \mathbb{R}^{s \times sd}$ 表示由 $\mathbf {X}_i$ 沿着它的第一个维度的所有圆组成的矩阵。</p>
<p>因此，我们可以将 FNO 训练目标写为：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{FN}:=& \min_{\mathbf{w}_{1j},\mathbf{w}_{2j}}\sum\limits^n_{i=1}\mathcal{L}(f_{FN}(\mathbf{X}_i),\mathbf{Y}_i)\\
&+\frac{\beta}{2}\sum\limits^m_{j=1}\|\mathbf{w}_{1j}\|^2_2+\|\mathbf{w}_{2j}\|^2_2
\end{align*}
\tag{24}\label{eq24}</script><p>我们注意到 FNO 实际上非常类似于双层 CNN，其中第一层由具有完整循环填充的卷积层和全局卷积核组成。 与典型的卷积核通常很小并且卷积是局部的 CNN 不同，这里的卷积要大得多，这意味着参数比典型的 CNN 多得多。 之前已经通过凸对偶分析了类似的 CNN 架构。 因此，对于线性和 ReLU 激活，\eqref{eq24} 等价于凸优化问题。</p>
<p><strong>定理4.4</strong><br>对于线性激活FNO训练问题\eqref{eq24}，对于 $m \geq m^\ast$，其中 $m^\ast \leq \min\{sd, c\}$，标准非凸训练问题相当于一个凸优化问题，由下式给出：</p>
<script type="math/tex; mode=display">
p^\ast_{FN}=\min_{\mathbf{Z}\in\mathbb{R}^{sd\times c}}\sum\limits^n_{i=1}\mathcal{L}(\mathrm{circ}(\mathbf{X}_i)\mathbf{Z},\mathbf{Y}_i)+\beta\|\mathbf{Z}\|_\ast 
\tag{25}</script><p><strong>定理4.5</strong><br>对于 ReLU 激活的 FNO 训练问题 \eqref{eq24}，我们定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathbf{X}&:=
    \begin{bmatrix}
        \mathrm{circ}(\mathbf{X}_1)\\
        \cdots \\
        \mathrm{circ}(\mathbf{X}_n)
    \end{bmatrix}\\
    \{\mathrm{D}_j \}^P_{j=1}&:=\{\mathrm{diag}(1\{\mathrm{X}\mathrm{u}_j\geq 0 \}):\mathrm{u}_j\in\mathbb{R}^{sd} \}
\end{align*}</script><p>其中 $P \leq 2r\left(\frac{e(n-1)}{r}\right)^r$ 和 $r := \mathrm{rank}(\mathbf{X})$。 然后，对于 $m \geq m^\ast$，其中 $m^\ast \leq n\min\{sd, c\}$，标准非凸训练问题等价于凸优化问题，由下式给定：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p^\ast_{FN}=\min_{\mathbf{Z}_j\in\mathbb{R}^{sd\times c}}\sum\limits^n_{i=1}\mathcal{L}\bigg(\sum\limits^P_{j=1}\mathbf{D}_j^{(i)}\mathrm{circ}(\mathbf{X}_i)\mathbf{Z}_j,\mathbf{Y}_i)+\\
    \beta\sum\limits^P_{j=1}\|\mathbf{Z}_j\|_{\ast,K_j},\mathbf{K}_j:=(2\mathbf{D}_j-\mathbf{I}_{ns})\mathbf{X} 
\end{align*}
\tag{26}</script><h3 id="4-2-1-块对角FNO"><a href="#4-2-1-块对角FNO" class="headerlink" title="4.2.1 块对角FNO"></a>4.2.1 块对角FNO</h3><p>虽然 FNO 公式非常优雅，但它需要许多参数（每个token的 $d^2$ 个）。 因此，对自适应傅里叶神经算子 (AFNO) 的形式提出了修改。 一个重要的修改涉及强制token权重服从块对角线结构，与标准 FNO 相比，这显着提高了 AFNO 的训练和泛化能力，我们称这种架构为 B-FNO，归结为：</p>
<script type="math/tex; mode=display">
\begin{align*}
    f_{BFN}(\mathbf{X}_i)&:=\sigma\bigg (\mathbf{F}^{-1}
    \begin{bmatrix}
        (\mathbf{F}\mathbf{X}_i)^\top_1\mathbf{V}^{(1)}\\
        \cdots \\
        (\mathbf{F}\mathbf{X}_i)^\top_s\mathbf{V}^{(s)}
    \end{bmatrix}
    \bigg)\mathbf{W}_2\\
    \mathbf{L}^{(l)}&:=
    \begin{bmatrix}
        \mathbf{L}^{(l,1)} & &\\
        &\ddots \\
        &&\mathbf{L}^{(l,B)}
    \end{bmatrix}
    \in\mathbb{R}^{d\times m},l\in[s]\\
    \mathbf{W}_2&:=
    \begin{bmatrix}
        \mathbf{W}_2^{(1)}\\
        &\ddots \\
        && \mathbf{W}^B_2
    \end{bmatrix}\in\mathbb{R}^{m\times c}
\end{align*}
\tag{27}\label{eq27}</script><p><strong>引理4.6</strong><br>对于权重 $\mathbf{W}_{1b} \in \mathbb{R}^{sd/B \times m/B}$ 和 $\mathbf{W}_{2b} \in \mathbb{R}^{m/B \times c/B}$，假设 $\sigma$ 按元素操作，B-FNO 模型 \eqref{eq27} 可以等效地表示为：</p>
<script type="math/tex; mode=display">
\begin{align*}
    f_{BFN}(\mathbf{X}_i)&=[f^{(1)}_{BFN}(\mathbf{X}_i)\cdots f^{(B)}_{BFN}(\mathbf{X}_i)]\\
    f_{BFN}^{(b)}&=\sum\limits^m_{j=1}\sigma(\mathrm{circ}(\mathbf{X}_i^{(b)})\mathbf{w}_{1bj})\mathbf{w}^\top_{2bj}
\end{align*}
\tag{28}</script><p>其中 $\mathrm{circ}(\mathbf{X}_i^{(b)}) \in \mathbb{R}^{s \times sd/B}$ 是由所有 $s$ 来沿 $\mathbf{X}_i^{(b)} \in \mathbb{R}^{s \times d/B}$ 第一维的循环位移。</p>
<p>有趣的是，AFNO 的块对角线权重将 CNN 中的局部卷积与 全局，分组卷积( $B$ 组)进行对比。 我们因此定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p^\ast_{BFN}&:=\min_{\mathbf{W}_{1bj},\mathbf{W}_{2bj}}\sum\limits^n_{i=1}\mathcal{L}(f_{BFN}(\mathbf{X}_i),\mathbf{Y}_i)\\
    &+\frac{\beta}{2}\sum\limits^B_{b=1}\sum\limits^m_{j=1}\|\mathbf{w}_{1bj}\|^2_2+\|\mathbf{w}_{2bj}\|^2_2
\end{align*}
\tag{29}\label{eq29}</script><p><strong>定理4.7</strong><br>对于线性激活B-FNO训练问题\eqref{eq29}，对于 $m \geq m^\ast$ 和 $m^\ast \leq 1/B\min\{sd, c\}$，标准的非凸训练问题相当于一个凸优化问题，由下式给出：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p^\ast_{BFN}&=\min_{\mathbf{Z}_b}\sum\limits^n_{i=1}\mathcal{L}([f^{(1)}(\mathbf{X}_i)\cdots f^{(B)}(\mathbf{X}_i)],\mathbf{Y}_i)\\
    &+\beta\sum\limits^B_{b=1}\|\mathbf{Z}\|_\ast \\
    \mathbf{Z}_b & \in\mathbb{R}^{sd/B\times c/B},f^{(b)}:=\mathrm{circ}(\mathbf{X}_i^{(b)})\mathbf{Z}_b
\end{align*}
\tag{30}</script><p><strong>定理4.8</strong><br>前面给出了 ReLU 激活的 B-FNO 训练问题 \eqref{eq29}, 我们定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathbf{X}_b&:=
    \begin{bmatrix}
        \mathrm{circ}(\mathbf{X}_1^{(b)})\\
        \cdots \\
        \mathrm{circ}(\mathbf{X}_n^{(b)})
    \end{bmatrix}\\
    \{\mathbf{D}_{b,j} \}^{P_b}_{j=1}&:=\{\mathrm{diag}(1\{\mathbf{X}_b\mathbf{u}_j\geq 0 \}):\mathbf{u}_j\in\mathbb{R}^{sd/B} \}
\end{align*}</script><p>其中 $P_b \leq 2r_b\left(\frac{e(n-1)}{r_b}\right)^{r_b}$ 和 $r_b := \mathrm{rank}(\mathbf{X}_b )$。 那么，对于 $m \geq m^\ast$ 和 $m^\ast \leq n/B\min\{sd, c\}$，标准的非凸训练问题等价于以下凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p^\ast_{BFN}&=\min_{\mathbf{Z}_{b,j}}\sum\limits^n_{i=1}\mathcal{L}([f^{(1)}(\mathbf{X}_i)\cdots f^{(B)}(\mathbf{X}_i)],\mathbf{Y}_i) \\
    &+\beta\sum\limits^B_{b=1}\sum\limits^{P_b}_{j=1}\|\mathbf{Z}_{j,b}\|_{\ast,K_{b,j}}
\end{align*}
\tag{31}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathbf{Z}_{b,j} &\in \mathbb{R}^{sd/B \times c/B}, \, f^{(b)}(\mathbf{X}_i) := \sum_{j=1}^{P_b} \mathbf{D}_{b,j}\mathrm{circ}(\mathbf{X}_i^{(b)})\mathbf{Z}_{b,j} \\
    \mathbf{K}_{b,j} &:= (2\mathbf{D}_{b,j} - \mathbf{I}_{ns})\mathbf{X}_b.
\end{align*}</script><h1 id="5，数值结果"><a href="#5，数值结果" class="headerlink" title="5，数值结果"></a>5，数值结果</h1><p>在本节中，我们试图将本文分析的Transformer头的性能与基线凸优化方法进行比较，这种比较使我们能够在一个实际例子中说明这些新颖的头脑所带来的隐性偏置。特别是，我们考虑了训练这些凸头的单个新块以执行图像分类任务。这本质上是无需微调骨干网络的迁移学习，这在边缘计算和内存受限设置中可能是必须的。对于 few-shot 微调 transformer 任务，非凸优化在不同的随机初始化下不稳定，此外，仅微调网络的最后一层是一种常见做法，它在伪相关基准测试中表现非常好。</p>
<p>具体来说，我们试图对来自 CIFAR-100 数据集的图像进行分类，首先在 ImageNet-1k 数据集的 $224 \times 224$ 图像上使用 $16 \times 16$ 的块 ($ s=196$, $d=256$)。然后微调单凸头以对来自 CIFAR-100 的图像进行分类，同时保持预训练的骨干固定。</p>
<p>对于主干 gMLP 架构，我们在使用凸头进行训练之前，将特征维度减少到 $d=100$，并使用平均池作为预处理步骤。同样，为了提高计算效率，我们训练标准 ReLU 架构的门控 ReLU 变体，因为这些门控 ReLU 激活网络是不受约束的。对于 BFNO，我们选择 $B=5$。所有头部都使用相同的维度 $(d=100, s=196, c=100)$，我们选择ReLU头部中的神经元数量为 $m=100$，自注意力网络我们选择 $m=5$ 使参数计数大致相等。作为我们的基线，我们比较了一个简单的线性模型（即逻辑回归）和 MLP 的凸等价物，如第2.1节中所讨论的。</p>
<p>我们在表1中总结了结果，在这里，我们证明注意力变体优于标准凸 MLP 和线性基线。 这表明注意力结构的高阶信息和额外的自由度为困难的视觉任务提供了优势。 令人惊讶的是，对于自注意力、FNO 和 MLP-Mixer，线性和 ReLU 激活性能之间只有微小的差距，这表明这些架构的大部分优势在于它们的基本结构，而不是应用的非线性。 相反，对于 B-FNO，ReLU 和线性激活精度之间存在非常大的差距，这表明当应用组卷积时，这种非线性更为重要。 因此，这些凸化的架构为迁移学习的稳定和透明模型铺平了道路。</p>
<h1 id="6，总结"><a href="#6，总结" class="headerlink" title="6，总结"></a>6，总结</h1><p>我们证明了自注意力块和常见替代方案（如 MLP-Mixer、FNO 和 B-FNO）等价于线性和 ReLU 激活函数下的凸优化问题，这些等效的凸公式隐含地聚类相关特征，并在确保全局表示的情况下，用块核范数正则化器作为惩罚项。 对于未来的工作，仍然需要利用这些独特的正则化器的结构来为这些网络设计高效的近似求解器，可能会找到更快的求解器，例如 FISTA 或相关算法。 从长期的实际采用来看，未来的理论工作还需要对实践中经常使用的更深层次的网络进行分析。 通过选定凸公式，可以将这项工作用于设计新的网络架构。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>non-mercer</tag>
        <tag>convex duality</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines</title>
    <url>/2023/03/22/Transformer/Transformer%20and%20Infinite-Dimensional%20Non-Mercer%20Binary%20Kernel%20Machines/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：Transformer are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines</p>
<p><a href="https://arxiv.org/abs/2106.01506">essay link</a></p>
<p><strong>无限维non-mercer跟Transformer关联的文章，感觉这是最近五篇文章中最难的，主要泛函已经还给老师了，老天爷啊！</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>尽管它们在自然语言处理等核心 AI 领域无处不在，但Transformer等基于深度注意力的神经网络机制尚未得到充分理解。在本文中，我们提出了理解 Transformer 工作原理的新视角。</p>
<p>特别是，我们展示了作为 Transformer 操作核心的“点积注意力”可以表征为一对 Banach 空间上的核学习方法。尤其是，Transformer 的内核具有无限的特征维度。在此过程中，我们考虑将标准内核学习问题扩展到二进制设置，其中数据来自两个输入域，并且为每个跨域对定义了一个响应。</p>
<p>我们为这些具有非 Mercer（不定、不对称）内核的二进制内核机器证明了一个新的表示定理（这意味着学习的函数是再生内核 Banach 空间而不是 Hilbert 空间的元素），并且还证明了一个新的通用逼近定理表明 Transformer 计算可以学习任何二进制非 Mercer 再生内核 Banach 空间对。</p>
<p>我们在 Transformers 中用新内核进行实验，获得的结果表明标准 Transformer 内核的无限维度对其性能有部分影响。本文的结果为现代机器学习中一个非常重要但鲜为人知的模型提供了新的理论理解。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>自从 bahdanau_neural_2015 提出以来，所谓的神经注意力已经成为许多最先进的深度学习模型的支柱。在自然语言处理 (NLP) 中尤其如此，其中 Transformer 模型已经无处不在。这种无处不在的现象使得过去几年 NLP 的大部分突破都归功于为 Transformers 开发新的训练机制。</p>
<p>与大多数现代深度神经网络一样，对 Transformer 的理论理解已经落后于基于 Transformer 的 AI 任务（如 NLP）的性能改进速度。最近，一些作者注意到 Transformer 操作与深度学习理论中其他更容易理解的主题之间的关系，例如注意力和卷积之间的相似性以及多层 Transformer 中残差块的设计；另请参阅原始 Transformer 作者的官方参考代码库和最近的一些研究中对主要学习（全连接或注意）操作、元素非线性和归一化的重新排序用以研究更深层次的 Transformers 到归一化的“预规范”排序、学习操作、非线性，添加现代（“v2”）Resnets 的残差排序。</p>
<p>在本文中，我们提出了一个新的视角来理解 Transformer 的核心组件，即它的“点积注意力”操作。特别是，我们表明点积注意力可以表征为一类特定的内核方法。更具体地说，它是所谓的“非定”和“对称”的核方法，它们指的是经典类核的两个独立推广，不需要对称和（半）正定的经典假设确定性。事实上，我们在下面的定理2中表明，点积注意力可以学习任何不对称的不定核。</p>
<p>这种见解有几个有趣的含义。最直接的是，它为 Transformer 中一个更神秘的组件提供了一些理论依据。它还可能为应用数十年的经典核方法理论打开大门，以理解当今最重要的神经网络模型之一，这可能类似于数字信号处理工具如何广泛用于研究卷积神经网络。我们在本文的最后一点上迈出了第一步，提出了我们称为“二进制”内核机器的先前的内核方法泛化，它学习如何预测两个输入集中的元素对的不同值，类似于注意力模型。</p>
<p>本文的其余部分安排如下。Section2部分回顾了 Transformers 和经典内核方法的数学背景。Section3 节介绍了我们用来表征 Transformer 的再生内核 Banach 空间 (RKBS) 上的内核机器的定义。我们特别注意到，Transformer 可以描述为具有无限维特征的空间。Section4 开始了我们的理论结果，明确地描述了 Transformer 的再生内核，包括 Transformer 的内核特征映射的显式公式及其与先前内核的关系。Section5 部分讨论了作为内核学习器的Transformer，包括一个新的表示定理和将随机梯度下降训练的注意力网络表征为近似内核学习器。在第 Section6 节中，我们提供了经验证据，证明 Transformer 内核的无限维特征可能在某种程度上对模型的有效性负责。 Section7 部分给出结论，并总结了我们的工作。</p>
<h2 id="2，背景和相关工作"><a href="#2，背景和相关工作" class="headerlink" title="2，背景和相关工作"></a>2，背景和相关工作</h2><h3 id="2-1-Transformer神经网络模型"><a href="#2-1-Transformer神经网络模型" class="headerlink" title="2.1 Transformer神经网络模型"></a>2.1 Transformer神经网络模型</h3><p>Transformer 模型在自然语言处理等许多核心 AI 应用中无处不在。在这里，我们回顾了它的核心组件。假设我们有两组有序的向量，一组source元素 $\{s_1, s_2, \dots, s_S\},\;s_j \in \mathbb{R}^{d_s}$ 和一组target元素 $\{t_1, t_2, \dots, t_T\},\; t_i \in \mathbb{R}^{d_t}$。在其一般的形式中，构成 Transformer 模型主干的神经网络“注意力”操作是为每个 $t_i$， 计算源序列 $\{ s_j\}_{j=1}^s$ 的嵌入向量。通常，源集和目标集被认为是相同时，即 $s_i=t_i,\forall i$，这种注意力此时称为自注意力。</p>
<p>Transformer 中使用的特殊函数是所谓的“缩放点积”注意力，其形式为：</p>
<script type="math/tex; mode=display">
a_{ij}=\frac{(W^Qt_i)^T(W^Ks_j)}{\sqrt{d}},\alpha_{ij}=\frac{exp(a_{ij})}{\sum^S_{j=1}exp(a_{ij})}, t'_i=\sum\limits^S_{j=1}\alpha_{ij}W^Vs_j
\tag{1}\label{eq1}</script><p>其中 $W^V, W^K \in \mathbb{R}^{d_s \times d}$, 和 $W^Q \in \mathbb{R}^{d_t \times d}$ 是可学习的权重矩阵，通常称为分别为“value”、“key”和“query”权重矩阵。</p>
<p>通常具有独立参数矩阵的多个所谓的“注意力头”实现了方程(1)的几个并行计算，几个 $d$ 维头输出的笛卡尔积（向量级联）形成最终输出 $t’_i$。通常未归一化的 $a_{ij}$ 称为注意力分数或注意力对数，而归一化的 $\alpha_{ij}$ 称为注意力权重。</p>
<p>在本文中，我们将注意力限制在 方程(1) 中所示的注意力点积公式。其他几种注意力替代形式执行大致相同功能， 但Transformer 的点积公式是迄今为止最受欢迎的。</p>
<h3 id="2-2-核方法和生成核"><a href="#2-2-核方法和生成核" class="headerlink" title="2.2 核方法和生成核"></a>2.2 核方法和生成核</h3><p>核方法是一类经典而强大的机器学习方法，关键组成部分是核函数，它允许有效映射低维的输入数据，如用其他线性方案来解决此类问题（如分类或回归等）是不可能的，将低维输入数据映射到高维数据域或无限维嵌入域，这是有线性解决方案的。</p>
<p>给定两个非空集 $\mathcal{X}$ 和 $\mathcal{Y}$，核函数 $\kappa$ 是一个连续函数 $\kappa: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$。在接下来的几节中，我们将回顾经典的对称和（半）正定或 Mercer 内核，然后讨论更一般的形式。</p>
<h4 id="2-2-1-对称和半正定-Mercer-内核"><a href="#2-2-1-对称和半正定-Mercer-内核" class="headerlink" title="2.2.1 对称和半正定 (Mercer) 内核"></a>2.2.1 对称和半正定 (Mercer) 内核</h4><p>如果 $\mathcal{X}=\mathcal{Y}$，并且对于所有 $x_i, x_j \in \mathcal{X}=\mathcal{Y}$，特定内核 $\kappa$ 具有以下属性:</p>
<script type="math/tex; mode=display">
\begin{align*}
& \kappa (x_i,x_j) =\kappa (x_j,x_i)  \tag{2a}\label{eq2a}\\
c^T\mathbf{K}c\geq 0 \quad & \forall c\in\mathbb{R}^n;\quad i,j=1,\cdots n \tag{2b}\label{eq2b}
\end{align*}</script><p>其中 \eqref{eq2b} 中的 $\mathbf{K}$ 是 Gram 矩阵，定义为 $\mathbf{K}_{ij} = \kappa(x_i, x_j)$，那么 $\kappa$ 被称为 Mercer 内核。性质 \eqref{eq2a} 是对称性要求，而 \eqref{eq2b} 是（半）正定性的条件。</p>
<p>对于 Mercer 内核，众所周知，除其他事实外，</p>
<ul>
<li>(i) 我们可以在 $\mathcal{X}$ 上定义函数的 Hilbert 空间，表示为 $\mathcal{H}_\kappa$（称为再生核 Hilbert 空间，或 RKHS，与再生核 $\kappa$ 相关联）；</li>
<li>(ii) $\mathcal{H}_\kappa$ 对于每个 $x$ 都有一个（连续的）唯一元素 $\delta_x$，称为点评估泛函，具有性质 $f(x) = \delta_x(f) \quad \forall f \in \mathcal{H}_\kappa$；</li>
<li>(iii) $\kappa$ 具有所谓的复制属性，$\langle f,\kappa(x, \cdot)\rangle_{\mathcal{H}_\kappa} = f(x) \; \forall f \in \mathcal{H}_\kappa$，其中 $\langle\cdot，\cdot\rangle_{\mathcal{H}_\kappa}$ 是 $\mathcal{H}_\kappa$ 上的内积；</li>
<li>(iv) 我们可以定义一个“特征映射” $\Phi: \mathcal{X} \to \mathcal{F}_\mathcal{H}$，其中 $\mathcal{F}_\mathcal{H}$ 是另一个希尔伯特空间，有时也称为特征空间，$\kappa(x, y) = \langle\Phi(x), \Phi(y)\rangle_{\mathcal{F}_\mathcal{H}}$ （其中 $\langle\cdot, \cdot\rangle_{\mathcal{F}_\mathcal{H}}$ 是与 $\mathcal{F}_\mathcal{H}$ 关联的内积。</li>
</ul>
<p>最后一点指出了 RKHS 的内核技巧。</p>
<p>从机器学习和优化的角度来看，对称和正（半）定（PSD）内核是可取的，因为这些属性保证经验风险最小化内核学习问题（如支持向量机（SVM）、高斯过程等）是凸的。凸性为学习问题的易处理性和解决方案的最优性提供了有吸引力的保证。</p>
<h4 id="2-2-2-用非-Mercer-内核学习"><a href="#2-2-2-用非-Mercer-内核学习" class="headerlink" title="2.2.2 用非 Mercer 内核学习"></a>2.2.2 用非 Mercer 内核学习</h4><p>使用非 Mercer 内核或放松假设 \eqref{eq2a} 和 \eqref{eq2b} 的内核的学习方法已经研究了一段时间。工作 lin_study_2003等专注于使用对称但不正定的内核进行学习，即不满足 \eqref{eq2b}。自 schwartz_sous-espaces_1964 等以来，不定内核已被确定为所谓的再生内核 Krein 空间 (RKKS) 的再生内核。</p>
<p>在学习问题（例如具有不确定内核的 SVM）中替换 Mercer 内核使得优化问题通常是非凸的（因为内核 Gram 矩阵 $\mathcal{K}$ 不再总是 PSD）。一些使用不定内核学习的早期工作试图通过修改 Gram 矩阵的频谱来改善这个问题，使其再次变为 PSD。</p>
<p>最近，loosli_learning_2016 等提出了直接在 RKKS 中学习的优化程序。<br>他们报告说，与流行的 Mercer 核或光谱修改的不定核相比，使用不定核时在某些学习问题上的性能更好，这表明牺牲凸性可以根据经验提高性能。这个结论当然让人联想到深度神经网络的并发体验，由于其高度的非凸性，很难优化，但却提供了优于许多其他方法的性能。</p>
<p>另一项工作探索了内核方法​​在更一般的 Banach 空间中的学习应用，即再生内核 Banach 空间 (RKBS)。已经提出了各种结构作为 Banach 空间的再生核（取代 RKHS 的内积），包括半内积、通过傅立叶变换构造的正定双线性形式，以及其他。在这项工作中，我们考虑 RKBS 的内核可能既不是对称的也不是 PSD 的，接下来介绍这些空间的定义。</p>
<h2 id="3，一般再生内核-Banach-空间"><a href="#3，一般再生内核-Banach-空间" class="headerlink" title="3，一般再生内核 Banach 空间"></a>3，一般再生内核 Banach 空间</h2><p>最近，georgiev_construction_2014等 提出了 RKBS 及其再生内核的类似定义和构造，旨在包含先前的定义。在本文中，我们采用了定义的融合，并尝试使符号尽可能简单以满足我们的目的。</p>
<p><strong>定义1 再生内核 Banach 空间</strong><br>$\mathcal{X}$ 和 $\mathcal{Y}$ 是非空集合，核 $\kappa$ 是个度量函数，$\kappa:\mathcal{X} \times\mathcal{Y}\rightarrow\mathbb{R}$，$\mathcal{B}x$ 和 $\mathcal{B}y$ 是定义在$\mathcal{X}$ 和 $\mathcal{Y}$ 上带实数度量函数的Banach空间，定义 $\langle\cdot,\cdot\rangle:\mathcal{B}x\times\mathcal{B}y\rightarrow\mathbb{R}$ 是一个双线性非退化映射，使得：</p>
<script type="math/tex; mode=display">
\begin{align*}
\kappa(x,\cdot)\in\mathcal{B}y \quad & \forall x\in\mathcal{X}\tag{3a}\label{eq3a}\\
\langle f,\kappa(x,\cdot)\rangle_{\mathcal{B}x\times\mathcal{B}y}=f(x)\quad & \forall x\in\mathcal{X},f\in\mathcal{B}x \tag{3b}\label{eq3b}\\
\kappa(\cdot,y)\in\mathcal{B}x\quad & \forall y\in\mathcal{Y} \tag{3c}\label{eq3c}\\
\langle\kappa(\cdot,y),g\rangle_{\mathcal{B}x\times\mathcal{B}y}=g(y)\quad & \forall y\in\mathcal{Y},g\in\mathcal{B}y \tag{3d}\label{eq3d}
\end{align*}</script><p>那么，$\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 分别是 $\mathcal{X}$ 和 $\mathcal{Y}$ 上的一对再生核Banach空间（RKBS），$\kappa$ 是它们的再生核。</p>
<p>式 \eqref{eq3a} (或者\eqref{eq3c})的含义为，如果我们取 $\kappa$，两个变量 $x \in \mathcal{X}$ 和 $y \in \mathcal{Y}$ 的函数 , 并固定 $x$ (或者 $y$), 然后我们得到一个变量的函数。这个函数的一个变量必须是 $\mathcal{B}_\mathcal{Y}$ (或者 $\mathcal{B}_\mathcal{X}$) 的一个元素。<br>式子 \eqref{eq3b} 和 \eqref{eq3d} 是 $\kappa$ 再生属性。</p>
<p>出于我们的目的，扩展此定义以包含类似于 RKHS 的某些解释中针对特征使用的“特征映射”将很有用。</p>
<p><strong>定义2(RKBS 的特征映射)</strong><br>对于定义1中定义的一对 RKBS，假设存在映射 $\Phi_\mathcal{X}: \mathcal{X} \to \mathcal{F}_\mathcal{X}, \Phi_\mathcal{Y}: \mathcal{Y} \to \mathcal{F}_\mathcal{Y}$，其中 $\mathcal{F}_\mathcal{X}$ 和 $\mathcal{F}_\mathcal{Y}$ 是我们称为特征空间的 Banach 空间，以及一个非退化双线性映射 $\langle\cdot,\cdot\rangle_{\mathcal{F}_\mathcal{X} \times \mathcal{F}_\mathcal{Y}}: \mathcal{F}_\mathcal{X} \times \mathcal{F}_\mathcal{Y}\to \mathbb{R}$ 这样就有：</p>
<script type="math/tex; mode=display">
\kappa(x,y)=\langle\Phi_{\mathcal{X}(x)},\Phi_{\mathcal{Y}(y)}\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}}\quad \forall x\in\mathcal{X},y\in\mathcal{Y} 
\tag{4}\label{eq4}</script><p>在这种情况下，空格 $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 可以定义如下，可参考文献 xu_generalized_2019 。</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathcal{B}_{\mathcal{X}} & =\{f_v:\mathcal{X}\rightarrow\mathbb{R}:f_v(x)\triangleq\langle\Phi_{\mathcal{X}}(x),v\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}};v\in\mathcal{F}_{\mathcal{Y}},x\in\mathcal{X} \tag{5a}\label{eq5a}\\
\mathcal{B}_{\mathcal{Y}} & =\{g_u:\mathcal{Y}\rightarrow\mathbb{R}:g_u(y)\triangleq\langle u,\Phi_{\mathcal{Y}}(y)\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}};u\in\mathcal{F}_{\mathcal{X}},y\in\mathcal{Y} \tag{5b}\label{eq5b}
\end{align*}</script><p><strong>注意1</strong><br>我们简要讨论如何理解 \eqref{eq5a}和\eqref{eq5b} 给出的空间，以 \eqref{eq5a} 为例。它是一个变量 $x$ 的实值函数空间，其中该函数也由 $v$ 参数化。在 \eqref{eq5a} 中选取 $v \in \mathcal{F}_\mathcal{Y}$ 定义 $\mathcal{B}_\mathcal{X}$ 中的函数流形。这种具有固定 $v$ 的函数流形随函数 $\Phi_\mathcal{X}$ 而变化。通过取 $\Phi_\mathcal{X}(x)$ 和选定的 $v$ 的双线性积来定义在点 $x$ 处计算此流形中的函数 $f_v$。这也意味着我们可以结合 \eqref{eq4},\eqref{eq5a} 和 \eqref{eq5b}有：</p>
<script type="math/tex; mode=display">
\kappa(x,y)=\langle\Phi_{\mathcal{X}}(x),\Phi_{\mathcal{Y}}(y)\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}}=\langle f_{\Phi_{\mathcal{X}}(x)},g_{\Phi_{\mathcal{Y}}(y)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}} 
\tag{6}\label{eq6}</script><p>对所有的 $x\in \mathbb{X}, y\in\mathbb{Y}$</p>
<p><strong>注意2</strong><br>如果 $\Phi_\mathcal{X}(x)$ 和 $\Phi_\mathcal{Y}(y)$ 可以表示为实值可测函数的可数集，$\{\phi_\mathcal{X}(x)_\ell\}_{\ell \in \mathbb{N}}$ 和 $\{\phi_\mathcal{Y}(y)_\ell\}_{\ell \in \mathbb{N}}$ 对于 $(\phi_{\mathcal{X}})_\ell: \mathcal{X} \to \mathbb{R}$ 和 $(\phi_{\mathcal{Y}})_\ell: \mathcal{Y} \to \mathbb{R}$ （即 $\mathcal{F}_\mathcal{X}, \mathcal{F}_\mathcal{Y} \subset \prod_{\ell \in \mathbb{N}} \mathbb{R}$）； 而且 $\langle u,v\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}} = \sum_{\ell \in \mathbb{N}} u_\ell v_\ell$ 对 $u \in \mathcal{F}_\mathcal{X}, v \in \mathcal{F}_\mathcal{Y}$ 都成立。 那么我们从 lin_reproducing_2019 借用其“特征映射”构造对应于 xu_generalized_2019 的“广义 Mercer 内核”。</p>
<h2 id="4，作为-RKBS-内核的点积注意力"><a href="#4，作为-RKBS-内核的点积注意力" class="headerlink" title="4，作为 RKBS 内核的点积注意力"></a>4，作为 RKBS 内核的点积注意力</h2><p>我们现在正式陈述作为 RKBS 学习者的点积注意力公式。与 RKHS 非常相似，对于给定的内核及其关联的 RKBS 对，特征映射（以及双线性映射）不是唯一的。在下文中，我们基于其他内核的经典特征（例如 RBF 内核）展示了一个特征映射。</p>
<p><strong>命题1</strong><br>方程\eqref{eq1} 的（缩放）点积注意力计算是定义1和2意义上的 RKBS 的再生内核，输入集 $\mathcal{X}$ 和 $\mathcal{Y}$ 分别是目标元素 $\{t_i\}_{i=1}^T, \; t_i \in \mathbb{R}^{d_t}$ 和源元素 $\{s_j\}_{j=1}^S, s_j \in \mathbb{R}^{d_s}$ 所在的向量空间。那么特征映射为：</p>
<script type="math/tex; mode=display">
\begin{align*}
\Phi_{\mathcal{X}}(t) & =\sum\limits^{\infty}_{n=0}\sum\limits_{p_1+\cdots+p_d=n}\frac{\sqrt{\frac{n!}{p_1!\cdots p_d!}}\prod^d_{l=1}(q_\ell)^{p_\ell}}{d^{1/4}} \tag{7a}\label{eq7a}\\
\Phi_{\mathcal{Y}}(s) & =\sum\limits^{\infty}_{n=0}\sum\limits_{p_1+\cdots+p_d=n}\frac{\sqrt{\frac{n!}{p_1!\cdots p_d!}}\prod^d_{l=1}(k_\ell)^{p_\ell}}{d^{1/4}} \tag{7b}\label{eq7b}
\end{align*}</script><p>其中 $q_\ell$ 是 $q = W^Qt$ 的第 $\ell$ 个元素，$k_\ell$ 是 $k = W^Ks$ 的第 $\ell$ 个元素，其中 $W^Q \in \mathbb{R}^{d \times d_t}, W^K \in \mathbb{R}^{d \times d_s}$，在 $d \leq d_s$ 的条件下，而且 $rank(W^Q) = rank(W^K) = d$。双线性映射为 $\langle\Phi_\mathcal{X}(t),\Phi_\mathcal{Y}(s)\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}} = \Phi_\mathcal{X}(t) \cdot \Phi_\mathcal{Y}(s)$，巴拿赫空间定义为：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathcal{B}_{\mathcal{X}} & =\{f_k(t)=exp((W^Qt)^Tk/\sqrt{d});\; k\in\mathcal{F}_{\mathcal{Y}},t\in\mathcal{X} \} \tag{8a}\label{eq8a}\\
\mathcal{B}_{\mathcal{Y}} & =\{g_q(s)=exp(q^T(W^Ks)/\sqrt{d});\; q\in\mathcal{F}_{\mathcal{X}},s\in\mathcal{Y} \} \tag{8b}\label{eq8b}
\end{align*}</script><p>使用“幂级数query-key内核”相关的再生内核，定义如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
\kappa(t,s) & =\langle\Phi_{\mathcal{X}}(t),\Phi_{\mathcal{Y}}(s)\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}}\tag{}\\ 
&  =\langle f_{\Phi_{\mathcal{Y}}(s)},g_{\Phi_{\mathcal{X}}(t)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}}\tag{}\\
& =exp(\frac{(W^Qt)^T(W^Ks)}{\sqrt{d}}) \tag{9}\label{eq9}
\end{align*}</script><p>命题1的证明很简单，涉及通过将 \eqref{eq9} 中的两个无限级数相乘来验证 \eqref{eq7a}和\eqref{eq7b}，然后使用多项式定理和指数的泰勒展开。</p>
<p>在上面，当特别提到 Transformer 类型的模型而不是一般的 RKBS 时，我们使用 $t,s,q$ 和 $k$ 来表示 $x、y、u$ 和 $v$ ，分别绘制 RKBS 的元素与广泛使用的术语“目标”、“源”、“查询”和“键”之间的联系。</p>
<p>$W^Q$ 和 $W^K$ 的秩要求，意味着 $span(\{\Phi_\mathcal{X}(t), t \in \mathcal{X}\}) = \mathcal{F}_\mathcal{X}$ 和 $span(\{\Phi_\mathcal{Y}(s), s \in \mathcal{Y}\}) = \mathcal{F}_\mathcal{Y}$。这反过来意味着双线性映射是非退化的。</p>
<p><strong>注意3</strong><br>现在我们有了一对 RKBS 的例子，我们可以更具体地讨论注意1中的一些讨论。例如，验证 \eqref{eq8a}，当我们在 $\mathcal{F}_\mathcal{Y}$ 中选择 $k$ 时，在 $\mathcal{B}_\mathcal{X}$ 中定义了一个流形函数，其中 $k$ 是固定的 , 但 $W^Q$ 可以变化。类似地，选择 $q \in \mathcal{F}_\mathcal{X}$ 在 $\mathcal{B}_\mathcal{Y}$ 中定义流形。从 $\mathcal{F}_\mathcal{X}$ 和 $\mathcal{F}_\mathcal{Y}$ 中选择一个元素会将我们锁定到 $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 中的一个元素，这导致 \eqref{eq6} 中的相等性。</p>
<p><strong>注意4</strong><br>检查 \eqref{eq8a}-\eqref{eq9}，可以看到从 $\mathcal{F}_\mathcal{Y}$ 提取的元素参数化了 $\mathcal{B}_\mathcal{X}$ 的元素，如 \eqref{eq8a} 所示，是 $\Phi_\mathcal{Y}$ 的函数（反之亦然，对于 \eqref{eq8b}）。 这揭示了 Transformer 类型的注意力计算是针对 SVM 等应用程序考虑的 RKBS 泛化的确切机制，这些功能空间的其中之一被认为是固定的。</p>
<p><strong>注意5</strong><br>由于特征映射定义了 Banach 空间 \eqref{eq5a} 和 \eqref{eq5b}，学习到的参数 $W^Q$ 和 $W^K$ 意味着 Transformer 学习 RKBS 自身的参数表示。这与经典内核方法形成对比，在经典内核方法中，内核（以及再生空间）通常是固定的。事实上，在下面的定理2中，我们证明了 Transformer 架构（的变体）可以近似任何 RKBS 映射。</p>
<p><strong>注意6</strong><br>已知指数点积内核的对称版本是所谓的巴格曼空间的再生内核，它出现在量子力学中。</p>
<p><strong>注意7</strong><br>在命题1中值得注意的是，我们将点积注意力的内核定义为包括 softmax 运算的指数。因此，此操作的输出不是注意力分数 $a_{ij}$，而是非标准化的注意力权重 $\bar{\alpha}_{ij} = \alpha_{ij} \sum_j \alpha_{ij}$。将指数视为核运算的一部分表明，Transformer 的特征空间实际上是无限维的，就像 RBF 核被称为具有无限维特征空间一样。在第6节中，我们发现经验证据表明这种无限维度可能是 Transformer 有效性的部分原因。</p>
<h2 id="5，Transformer作为核学习者"><a href="#5，Transformer作为核学习者" class="headerlink" title="5，Transformer作为核学习者"></a>5，Transformer作为核学习者</h2><h3 id="5-1-二元-RKBS-学习问题及其表示定理"><a href="#5-1-二元-RKBS-学习问题及其表示定理" class="headerlink" title="5.1 二元 RKBS 学习问题及其表示定理"></a>5.1 二元 RKBS 学习问题及其表示定理</h3><p>大多数内核学习问题都采用经验风险最小化问题的形式。例如，如果我们有一个有限数据集 $(x_1, z_1), \dots, (x_n, z_n), x_i \in \mathcal{X}, z_i \in \mathbb{R}$ 的学习问题，并且想要学习一个函数 $f： \mathcal{X} \to \mathbb{R}$ 在 RKHS $\mathcal{H}_\mathcal{K}$ 中，学习问题可以写成：</p>
<script type="math/tex; mode=display">
f^\ast=\mathop{argmin}_{f\in\mathcal{H}_{\kappa}}\frac{1}{n}\sum\limits^n_{i=1}L(x_i,z_i,f(x_i))+\lambda R(\|f\|_{\mathcal{H}_{\kappa}}) 
\tag{10}\label{eq10}</script><p>其中 $L: \mathcal{X} \times \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ 是凸损失函数，$R: [0, \infty) \to \mathbb{R}$ 是严格递增的正则化函数，$\lambda$ 是比例常数。最近考虑在 RKBS 中学习的参考文献考虑了与 \eqref{eq10} 类似的问题，但 RKHS 中 $\mathcal{H}$ 替换为 RKBS。</p>
<p>然而，注意力的内核学习问题与 \eqref{eq10} 的不同之处在于，正如我们在上一节中讨论的那样，我们需要对每个参数对 $(t_i, s_j)$ 预测响应 $z_{ij}$（即注意力 logit）。这激发了在输入空间对上运行的经典内核学习问题类的泛化。我们现在讨论这种概括。</p>
<p><strong>定义3(二元核学习问题——正则化经验风险最小化)</strong></p>
<p>设 $\mathcal{X}$ 和 $\mathcal{Y}$ 为非空集，分别定义其上的RKBS $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 。设 $\langle\cdot,\cdot\rangle_{\mathcal{B}_\mathcal{X}\times\mathcal{B}_\mathcal{Y}}: \mathcal{B}_\mathcal{X} \times \mathcal{B}_\mathcal{Y} \to \mathbb{R}$ 是两个 RKBS 上的双线性映射。设 $\Phi_\mathcal{X}: \mathcal{X} \to \mathcal{F}_\mathcal{X}$，$\Phi_\mathcal{Y}: \mathcal{Y} \to \mathcal{F}_\mathcal{Y}$ 是固定特征映射，其属性为 $\langle\Phi_\mathcal{X}(x_i),\Phi_\mathcal{Y}(y_i)\rangle_{\mathcal{F}_\mathcal{X}\times\mathcal{F}_\mathcal{Y}} = \langle f_{\Phi_\mathcal{Y}(y)},g_{\Phi_\mathcal{X}(x)}\rangle_{\mathcal{B}_\mathcal{X}\times\mathcal{B}_\mathcal{Y}}$。若 $\{x_1, \dots, x_{n_x}\}, x_i \in \mathcal{X}$, $\{y_1, \dots, y_{n_y}\}, y_j \in \mathcal{Y}$, 且 $\{z_ {ij}\}_{i=1,\dots,n_x; \; j=1,\dots,n_y},\;z_{ij} \in \mathbb{R}$ 是有限数据集，其中为定义在 $(i,j)$ 的 $x_i$ 和 $y_j$ 数据对定义响应 $z_{ij}$。设 $L: \mathcal{X} \times \mathcal{Y} \times \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ 是固定 $(x_i, y_j, z_{i,j})$ 的凸损失函数，定义 $R_{\mathcal{X}}: [0, \infty) \to \mathbb{R}$ 和 $R_{\mathcal{Y}}: [0, \infty) \to \mathbb{R}$ 是凸的，且为严格增加正则化函数。</p>
<p>用于在一对 RKBS 上学习的二进制经验风险最小化核学习问题采用以下形式</p>
<script type="math/tex; mode=display">
\begin{align*}
f^\ast,g^\ast=\mathop{argmin}_{f\in\mathcal{B_{\mathcal{X}}},g\in\mathcal{B_{\mathcal{Y}}}}[\frac{1}{n_xn_y}\sum\limits_{i,j}L(x_i,y_j,z_{ij},\langle f_{\Phi_{\mathcal{Y}}(y_j)},g_{\Phi_{\mathcal{X}}(x_i)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}})\tag{}\\
+\lambda_{\mathcal{X}}R_{\mathcal{X}}(\|f\|_{\mathcal{B_{\mathcal{X}}}})+\lambda_{\mathcal{Y}}R_{\mathcal{Y}}(\|g\|_{\mathcal{B_{\mathcal{Y}}}})] \tag{11}\label{eq11}
\end{align*}</script><p>其中 $\lambda_\mathcal{X}$ 和 $\lambda_\mathcal{Y}$ 又是缩放常数。</p>
<p><strong>注意8</strong><br>在两个集合组成的成对数据上运行的二进制内核问题的想法并不是全新的：在协同过滤和张量内核方法等文献中都有先前的工作。我们的问题和结果在泛化到 Banach 而不是 Hilbert 空间方面是新的：正如 RKBS 文献中的前置工作指出的那样，RKBS 学习问题与 RKHS 学习问题的不同之处在于它们的额外非线性 和/或非凸性。因此，将二元学习问题扩展到 Banach 空间是由 Transformer 设置推动的，其中内核方法处于非线性和非凸深度神经网络的上下文中，而不是像 SVM 或矩阵完成那样的浅层学习器。有关更多讨论，请参阅附录A。</p>
<p>几乎所有经典的内核学习方法都能找到其形式由所谓的表示定理指定的解决方案。表示定理指出，再生核空间上正则化经验风险最小化问题的解可以表示为再生核对数据集评估的线性组合。因此，内核学习问题的经典解决方案简化为寻找该线性组合的系数。</p>
<p>fasshauer_solving_2015 等为 RKBS 学习问题提供了表示定理。然而，他们的定理只处理学习问题，其中数据点仅来自定义再生内核的集合之一（即，只有 $\mathcal{X}$ 而没有 $\mathcal{Y}$），这意味着寻求的解决方案是只有一个 Banach 空间（例如 $f: \mathcal{X} \to \mathbb{R}, f \in \mathcal{B}_\mathcal{X}$）。在这里，我们陈述并证明了定义3中提出的与 Transformers 更相关的二元情况的定理。</p>
<p><strong>定理1</strong><br>假设我们有一个形式为 \eqref{eq11} 的内核学习问题。设 $\kappa: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ 为 RKBS 上 $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 的再生内核，满足定义1 和 定义2。然后，给定 $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 的一些条件（参见附录B），正则化经验风险最小化问题 \eqref{eq11} 就有一个唯一的解法数据对 $(f^_, g^_)$，具有以下性质：</p>
<script type="math/tex; mode=display">
\iota(f^\ast)=\sum\limits^{n_x}_{i=1}\xi_i\kappa(x_i,\cdot);\quad \iota(g^\ast)=\sum\limits^{n_y}_{j=1}\zeta_j\kappa(\cdot,y_j) 
\tag{12}\label{eq12}</script><p>其中 $\iota(f)$ (或者 $\iota(g)$) 表示 $f$ (resp. $g$) 范数的 Gateaux 导数，约定 $\iota(0) \triangleq 0$，且有 $\xi_i,\zeta_j \in \mathbb{R}$。</p>
<p>证明见附录B。</p>
<h3 id="5-2-一个新的近似核学习问题和通用逼近定理"><a href="#5-2-一个新的近似核学习问题和通用逼近定理" class="headerlink" title="5.2 一个新的近似核学习问题和通用逼近定理"></a>5.2 一个新的近似核学习问题和通用逼近定理</h3><p>按照表示定理的建议，寻找内核学习问题（如 \eqref{eq10} 或 \eqref{eq11} 形式的 \eqref{eq12} 解决方案的缺点是它们很难扩展到大型数据集。众所周知，对于 RKHS 学习问题，找到与内核评估相乘的标量系数需要花费大约数据集大小三次方的时间，而查询模型需要线性时间。最流行的一类近似技术基于所谓的 Nystrom 方法，它构造核 Gram 矩阵的低阶近似并求解由该近似生成的问题。最近的一项工作已将 Nystrom 方法扩展到 RKKS 学习。</p>
<p>在本节中，我们将 Transformer 学习问题描述为一类新的近似核方法——可以称之为“蒸馏”方法。我们现在正式陈述这个想法。</p>
<p><strong>命题2（二进制核学习问题的参数化近似求解）</strong><br>考虑定义3中二进制内核学习问题的陈述，我们想找到解决方案对 $(f^\ast, g^\ast)$ 的近似值。特别地，我们会说我们想要一个近似值 $\hat \kappa: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ 这样就有：</p>
<script type="math/tex; mode=display">
\hat{\kappa}(x,y)\approx \langle f^\ast_{\Phi_{\mathcal{Y}}(y)},g^\ast_{\Phi_{\mathcal{X}}(x)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}} 
\tag{13}\label{eq13}</script><p>对于所有 $x \in \mathcal{X}$ 和 $y \in \mathcal{Y}$ 都成立。</p>
<p>比较 \eqref{eq13} 和 \eqref{eq6} 提出了一个解决方案：学习一个近似 $\kappa$ 的函数 $\hat \kappa$。特别是，\eqref{eq6} 建议学习特征映射的显式近似，即</p>
<script type="math/tex; mode=display">\hat{\kappa}(x,y)\approx\langle f^\ast_{\Phi_{\mathcal{Y}}(y)},g^\ast_{\Phi_{\mathcal{X}}(x)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}}</script><p>事实上，Transformer q-k映射正是这样做的。也就是说，虽然命题1中概述的 Transformer 核计算是有限维的，但它实际上可以近似潜在的无限维最优解 $(f^\ast, g^\ast)$，其特征在于定理1。下面证明这一事实。</p>
<p><strong>定理2</strong><br>设 $\mathcal{X} \subset \mathbb{R}^{d_t}$ 和 $\mathcal{Y} \subset \mathbb{R}^{d_s}$ 是紧致的； $t \in \mathcal{X}, s \in \mathcal{Y}$; 并设 $q_\ell: \mathcal{X} \to \mathbb{R}$ 和 $k_\ell: \mathcal{Y} \to \mathbb{R}$，且 $\ell=1, \dots, d$ 是两层神经网络，其中 $ m$ 是隐藏单元个数。然后，对于任何连续函数 $F: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ 和 $\epsilon &gt;0$，存在整数 $m, d &gt;0$ 使得：</p>
<script type="math/tex; mode=display">\bigg |F(t,s)-\sum^d_{l=1}q_l(t)k_l(s) \bigg |< \epsilon,\quad \forall t\in\mathcal{X},s\in\mathcal{Y}
\tag{14}\label{eq14}</script><p>证明详见附录C。</p>
<p>我们现在概述定理2与Transformer的关系。如果我们将两层神经网络 $\{q_\ell\}_{\ell=1}^d$ 和 $\{k_\ell\}_{\ell=1}^d$ 的输出连接成$d$ 维向量 $q: \mathbb{R}^{d_t} \to \mathbb{R}^d$ 和 $k: \mathbb{R}^{d_s} \to \mathbb{R}^d$，然后点积 $q(t)^Tk(s)$ 由 \eqref{eq14} 中的和表示，可以逼近 $\mathcal{X} \times \mathcal{Y}$ 上的任何实值连续函数。减去通用逼近定理应用中的常见警告（即，在实践中，输出元素共享隐藏单元而不是独立单元），这个点积正是注意力的对数 $a_{ij}$ 的计算结果，即 $F(t,s) \approx \log \kappa(t,s)$ 对于 \eqref{eq14} 中的 $F$ 和 \eqref{eq9} 中的 $\kappa$ 逼近直到一个比例常数 $\sqrt{d}$。</p>
<p>由于注意力对数和 Transformers 中使用的求幂q-k内核之间的指数映射是一对一的映射，如果我们取 $F(t, s) = \log \langle f^\ast_{\Phi_\mathcal{Y}(s)},g^\ast_{\Phi_\mathcal{X}(t)}\rangle_{\mathcal{B_{\mathcal{X}}\times\mathcal{B_{\mathcal{Y}}}}}$，那么我们可以使用 Transformer 的点积注意力很好地逼近任意 RKBS 中的最优解。</p>
<p>基于注意力的深度神经网络的核心思想是通过随机梯度下降学习 $q_\ell$ 和 $k_\ell$ 的参数表示。与传统的基于表示定理的学习函数不同，基于注意力的内核机器（如深度Transformer）的训练时间（通常，但没有保证）随数据集大小呈亚立方体缩放，而无论数据集大小如何，评估时间都保持不变。</p>
<h2 id="6，取幂的点积对Transformer来说是必不可少的吗？"><a href="#6，取幂的点积对Transformer来说是必不可少的吗？" class="headerlink" title="6，取幂的点积对Transformer来说是必不可少的吗？"></a>6，取幂的点积对Transformer来说是必不可少的吗？</h2><p>我们研究了具有经典内核机器中使用的多个内核 Transformer 的修改，训练了两个标准的机器翻译数据集和两个标准的情感分类任务。对于机器翻译，IWSLT14 DE-EN 是一个相对较小的数据集，而 WMT14 EN-FR 是一个相当大的数据集。对于情感分类，考虑 SST-2 和 SST-5。保留标准的非对称查询和关键特征映射，即 $q = W^Qt$ 和 $k = W^Ks$，并且只修改内核 $\kappa : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}_{\geq 0}$。下面，$\tau &gt;0$ 和 $\gamma \in \mathbb{R}$ 是每个头学习的标量。</p>
<p>我们感兴趣的内核是：</p>
<ul>
<li>(缩放)取幂点积 (EDP)，$\kappa(q,t) = \exp(q^Tk/\sqrt{d})$，即标准 Transformer 内核；</li>
<li>径向基函数 (RBF) 内核，$\kappa(q,t) = \exp(|-{\tau}/\sqrt{d} (q - k)|^2_2)$，其中 $| \cdot |_2$ 是标准的 2-范数。众所周知，RBF 内核是取幂点积的规范化版本，规范化使其具有平移不变性；</li>
<li>vanilla L2 距离，$\kappa(q,t) = {\tau}/\sqrt{d} | q - k |_2$;</li>
<li>交集内核的指数版本，$\kappa(q,t) = \exp(\sum_{\ell=1}^d \min( q_\ell, k_\ell ))$。交集内核的对称版本在计算机视觉应用的内核机器中很流行，通常被描述为具有关联的 RKHS，它是函数空间 $L^2$ 的子空间（即，在具有连续函数的特征空间的意义上，它是无限维的，与 EDP和RBF的无限维无限级数相反）；</li>
<li>一个二次多项式内核，$\kappa(q,t) = ( 1/\sqrt{d} q^Tk + \gamma)^2$。</li>
</ul>
<p>附录D中提供了完整的实施细节。</p>
<p>机器翻译的结果显示在表1中，几个结果脱颖而出。首先，据称具有无限维特征空间的取幂点积、RBF 和取幂交集核确实比具有低维特征映射的核（如二次核）表现更好。事实上，RBF 和 EDP 内核的性能大致相同，这表明深度 Transformer 可能不需要使 RBF 内核优于经典内核机器中的 EDP 的平移不变性。有趣的是，在 IWSLT14 DE-EN 上，（非正统的）取幂交集内核与 EDP 和 RBF 内核的性能大致相同，但在 WMT14 EN-FR 上稍差。如前所述，EDP 和 RBF 核具有无穷级数的特征空间，而交集核对应于连续函数的特征空间。在这两个数据集上，二次核比最好的无限维核表现稍差，而 L2 距离表现明显更差。</p>
<p>情绪分类的结果显示在表2中。与机器翻译实验不同，无限维内核在此任务上并不严格优于有限维内核。事实上，这里明显的输家是取幂的交集内核，而在机器翻译中表现最差的 L2 距离与表现最好的内核的标准差不超过一个标准差。然而，值得注意的是，情感分类测试准确度的差异意味着不可能在此任务上选择具有统计显着性的“最佳”。内核间的小变化可能与这个问题的相对简单性（以及数据集的相对较小）与机器翻译有关：也许不需要无限维的特征空间来在更简单的上获得 Transformer 级别的性能问题。</p>
<p>值得注意的是，取幂的点积内核（同样是标准的 Transformer 内核）始终保持高性能。这可能是他们所享有的通用近似属性的实际用途的实验证据（参见定理2）。</p>
<p>内核之间相对较小但具有统计显着性的性能差异让人联想到神经网络的激活函数（ReLU、ELU 等）的相同现象。此外，与 SST 情感分析任务的小得多的性能差异相比，机器翻译的内核间性能差异很大，表明不同复杂性问题的不同表征需求。总的来说，这些结果表明内核选择可能是 Transformer 网络的一个额外设计参数。</p>
<h2 id="7，结论"><a href="#7，结论" class="headerlink" title="7，结论"></a>7，结论</h2><p>在本文中，我们将经典内核方法与最先进的 Transformer 网络联系起来。除了对开发新的 RKBS 表示定理和其他核理论的理论兴趣之外，我们对什么可能使 Transformers 起作用有了新的认识。</p>
<p>我们的实验结果表明，Transformer 内核的无限维数使其成为应用中的一个很好的选择，类似于 RBF 内核如何成为标准选择，例如支持向量机。我们的工作还揭示了 Transformer 研究的新途径。例如，我们的实验结果表明，Transformer 内核的选择与神经网络设计中的激活函数具有相似的设计选择。</p>
<p>新的开放研究问题包括：</p>
<ul>
<li>(1) 指数点积是否应该始终是首选，或者不同的内核是否更适合不同的任务（参见 GELU 最近如何在 Transformers 中替代 ReLU 变得非常流行）；</li>
<li>(2) 用于结构化预测的向量值内核与例如多个注意力头之间的任何关系；</li>
<li>(3) 将 Transformer 类型的深度内核学习器扩展到非欧几里德数据（使用，例如，图形内核或流形上的内核）。</li>
</ul>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>non-mercer</tag>
      </tags>
  </entry>
  <entry>
    <title>Lifted Proximal Operator Machines</title>
    <url>/2023/03/22/CNN/Lifted%20Proximal%20Operator%20Machines/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>title: Lifted Proximal Operator Machines</p>
<p>论文百度云链接: <a href="https://pan.baidu.com/s/1J2hdhLMp_xW6QcmWRi1-zA">https://pan.baidu.com/s/1J2hdhLMp_xW6QcmWRi1-zA</a><br>提取码: ersk </p>
<p><strong>提升近端算子机，将非凸问题进行凸化的利器，已用于分析DNN的网络和训练策略等~</strong><br><span id="more"></span></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>我们提出了一种用于训练前馈神经网络的新优化方法，通过将激活函数重写为等效的近端算子，然后将近端算子以正则项添加到目标函数来近似前馈神经网络，因此称本文提出的方法为提升近端算子机（LPOM）。 </p>
<p>LPOM 在所有权重或者激活层中都是块多凸的，这允许我们使用块坐标下降并行更新逐层权重和激活。最值得注意的是，我们只使用激活函数本身，而不是它的导数，从而避免基于梯度方法中的梯度消失或爆炸问题。所以我们的方法适用于各种非递减 Lipschitz 连续激活函数，而且可以是饱和的和不可微的。 LPOM 不比逐层激活需要更多的辅助变量，因此与 SGD方法使用的内存量大致相同。我们也证明了逐层更新权重和激活的收敛性，数据集 MNIST 和 CIFAR-10 上的实验证明了 LPOM 的优势。</p>
<h1 id="一，介绍"><a href="#一，介绍" class="headerlink" title="一，介绍"></a>一，介绍</h1><p>前馈深度神经网络 (DNN) 是完全级联的连接层，没有反馈连接。在最近年，随着硬件和数据集规模的进步，前馈 DNN 已成为许多任务的标准，例如图像识别, 语音识别，自然语言理解，并作为Go游戏学习系统的块。</p>
<p>几十年来，训练 DNN 是通过优化一个高度非凸且嵌套的网络权重函数来完成的。训练 DNN 的主要方法是随机梯度下降(SGD)，其有效性由DNN 在各种领域的实际应用中已经得到证明。最近，SGD 的许多变体已被提出，它使用自适应学习率和动量项，例如，Nesterov momentum，AdaGrad，RMSProp 和 Adam。 SGD 及其变体使用少量训练样本来估计全局梯度，使得每次迭代计算复杂度小。此外，估计的梯度是有噪声的，有助于鞍点逃逸。然而，他们也有一些缺点。一个主要问题是梯度消失或爆炸问题，即很多层的梯度指数级的减少或增加。这会导致缓慢或不稳定的收敛，尤其是在非常深的网络中更甚。这个缺陷可以通过使用非饱和激活函数来消除，例如整流线性单元 (ReLU) ，以及网络结构的修改，例如 ResNet。但是，根本问题依然存在。此外，他们无法直接处理不可微的激活函数（例如，二值化神经网络） 并且不允许跨层的并行权重更新。有关更多SGD限制的讨论请参考taylor2016training。</p>
<p>SGD 的缺点激发了对训练 DNN 的替代方法研究。最近在训练一个前馈神经网络被表述为带约束的优化问题，其中网络激活作为辅助变量引入，网络配置由分层约束保证。它打破了嵌套函数之间的依赖关系，转变为等式约束，因此可使用许多标准优化方法。许多工作研究了这种方法，不同之处在于如何处理等式约束。carreira2014distributed 将等式约束近似为二次正则项，交替优化网络权重和激活。zeng2018global 提出每层多一个辅助变量块，也通过二次正则项近似等式约束。受乘法器交替方向法启发(ADMM)，taylor2016training等使用了增广拉格朗日方法获得等式约束的精确增强。然而，这两种方法涉及拉格朗日乘数和非线性约束，因此对内存的要求更高，更难优化。ReLU 激活函数等同于一个简单的带约束的凸优化问题，受这一事实的启发，zhang2017convergent 放宽了非线性约束作为正则项，对网络架构和 ReLU 激活函数进行编码。因此，非线性约束不复存在。然而，他们的方法仅限于 ReLU 函数，不适用于其他激活函数。沿着这个思路，askari2018lifted 考虑了更多复杂的凸优化问题并讨论了几种非递减激活函数。然而，他们的方法对权重和激活的更新仍然限于 ReLU 函数。所以他们的方法不能胜过 SGD，只能服务于为 SGD 生成良好的初始化。其实我们已经发现了他们的公式不正确（参见“LPOM 的优势”小节）。</p>
<p>本文做出了以下贡献：</p>
<ul>
<li>我们提出了一种新的公式来训练前馈 DNN，我们称之为提升近端算子机 (LPOM)。 LPOM 是块多凸的，即当剩余的权重和激活是固定时，权重和激活的问题是凸的。相比之下，几乎现有所有的 DNN 训练方法都没有这样的性质。这大大方便了DNN的训练。</li>
<li>相应地，我们应用块坐标下降 (BCD) 来求解 LPOM，其中逐层权重和激活可以并行更新。最值得注意的是，逐层权重或激活的更新仅利用激活函数本身，而不是其导数，从而避免了基于梯度的训练方法中的梯度消失或爆炸问题。此外，LPOM 不需要比逐层激活更多的辅助变量，因此其内存成本接近 SGD。我们进一步证明更新逐层权重或激活的迭代是收敛的。</li>
<li>由于只有激活函数本身参与计算，LPOM 能够处理一般的非递减 Lipschitz 连续激活函数，可以是饱和的（例如 sigmoid 和 tanh）和不可微的（例如 ReLU 和 leaky ReLU）。因此 LPOM 成功地克服了使用大多数现有激活函数时的计算困难。</li>
</ul>
<p>我们在全连接的 DNN 上实施 LPOM 并在基准数据集MNIST 和 CIFAR-10 中对其进行测试，并获得了满意的结果。在卷积神经网络 (CNN)中，因为我们还没有重新制定池化和跳跃连接，将 LPOM 在 CNN 上的实施留作未来的工作。请注意，现有基于非梯度的方法也首先关注全连接的 DNN。</p>
<h1 id="二，相关工作"><a href="#二，相关工作" class="headerlink" title="二，相关工作"></a>二，相关工作</h1><p>在标准的前馈神经网络中，执行分类任务时训练 $n$ 层神经网络的优化问题可以写做：</p>
<script type="math/tex; mode=display">
\min_{W^i}\ell(\phi(W^{n-1}\phi(\cdots \phi(W^2\phi(W^1X^1)))),L) 
\tag{1}\label{eq1}</script><p>其中 $X^1 \in \mathbb{R}^{n_1\times m}$ 是批训练样本，$L \in \mathbb{R}^{c\times m}$ 表示对应标签，$n_1$ 是训练样本的维度，$m$ 是批量大小，$c$ 是类的数量，$\{W^i\}_{i=1}^{n-1}$ 是要学习的权重，为简单起见，省略了偏差，$\phi(\cdot)$ 是逐元素激活函数（例如，sigmoid、tanh 和 ReLU)，而 ${\ell}(\cdot,\cdot)$ 是损失函数（例如，最小二乘误差或交叉熵误差）。这里的神经网络被定义为嵌套函数，其中第一层神经网络的函数是$\phi(W^1X^1)$，第 $i$ 层($i=2,\cdots,n$) 函数的形式为 $\phi(W^iX)$，并且 $X$ 是第 $(i-1)$ 层函数的输出。优化 \eqref{eq1} 的常用方法是 SGD，即计算梯度，而网络的所有权重更新使用反向传播，具体通过梯度下降来更新权重。</p>
<p>通过引入逐层激活作为辅助变量块，神经网络的训练可以等价地公式化为等式约束优化问题:</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i \},\{X^i\} }\ell(X^n,L)\\
  &s.t. X^i=\phi(W^{i-1}X^{i-1}),i=2,3\cdots n
\end{align*}
\tag{2}\label{eq2}</script><p>其中 $X^i$ 是第 $i$ 层的激活，其他层符号与 \eqref{eq1} 中的相同。</p>
<p>问题 \eqref{eq2} 中的约束确保辅助变量 $\{X^i\}_{i=2}^{n}$ 完全匹配网络。 与问题\eqref{eq1}相比，问题\eqref{eq2}是有限制的。 但由于目标函数没有嵌套，因此更简单，这样的等价表达可能会带来更灵活的优化方法。 注意当使用 SGD 解决问题\eqref{eq1}时，它<br>实际上也是隐含地对问题\eqref{eq2}求解，但需要记录激活 $\{X^i\}_{i=2}^{n}$ 以便计算梯度。</p>
<p>受二次正则方法的启发，carreira2014distributed 提出了辅助坐标（MAC）的方法来求解问题\eqref{eq2}。 MAC使用<br>二次正则项近似等式约束，并试图求解以下问题：</p>
<script type="math/tex; mode=display">
\min_{\{W^i\},\{X^i\}}\ell(X^n,L)+\frac{\mu}{2}\sum\limits^n_{i=2}\|X^i-\phi(W^{i-1}X^{i-1})\|^2_F \tag{3}</script><p>其中 $\mu&gt;0$ 是控制约束权重的常数，$|\cdot|_F$ 是 Frobenius 范数。zeng2018global 用新的辅助变量解耦了 \eqref{eq2} 中的非线性激活函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i \},\{X^i\},\{U^i\}}\ell(X^n,L)\\
  &s.t. U^i=W^{i-1}X^{i-1},X^i=\phi(U^i),i=2,3\cdots n
\end{align*}
\tag{4}\label{eq4}</script><p>这称为 3-splitting 公式。相应地，问题\eqref{eq2} 是 2-splitting 公式。 对问题\eqref{eq4}同样适用 MAC 方法，而不是直接求解，可得出他们优化了下面的问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\},\{U^i\}}\ell(X^n,L)\\
  &+\frac{\mu}{2}\sum\limits^n_{i=2}(\|U^i-W^{i-1}X^{i-1}\|^2_F+\|X^i-\phi(U^i)\|^2_F)
\end{align*}
\tag{5}</script><p>他们采用 BCD 方法来解决上述问题。</p>
<p>taylor2016training 也考虑求解问题\eqref{eq4}。 受 ADMM启发，他们在输出层添加了拉格朗日乘子以实现对输出层的等式约束，产生下列问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\},\{U^i\},M}\ell(U^n,L)+\frac{\beta}{2}\|U^n-W^{n-1}X^{n-1}+M\|^2_F\\
  &+\sum\limits^{n-1}_{i=2}\frac{\mu_i}{2}(\|U^i-W^{i-1}X^{i-1}\|^2_F+\|X^i-\phi(U^i)\|^2_F)
\end{align*}
\tag{6}\label{eq6}</script><p>其中 $M$ 是拉格朗日乘子，$\beta&gt;0$ 和 $\mu_i&gt;0$是常数。 注意输出层上没有激活函数。 所以 \eqref{eq6} 是自适应启发式的 ADMM 算法。 zhang2016efficient 采用了类似的技术，但使用了不同的变量拆分方案：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\},\{U^i\}}\ell(X^n,L) \\
  &s.t. U^{i-1}=X^{i-1},X^i=\phi(W^{i-1}U^{i-1}),i=2,3\cdots n
\end{align*}
\tag{7}\label{eq7}</script><p>尽管有非线性等式约束，但 ADMM 并不旨在处理该类问题，他们为\eqref{eq7}中的每个约束添加了一个拉格朗日乘数。然后增强的拉格朗日问题如下所示：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\},\{U^i\},\{A^i\},\{B^i\}}\ell(X^n,L)\\
  &+\frac{\mu}{2}\sum\limits^n_{i=2}(\|U^{i-1}-X^{i-1}+A^{i-1}\|^2_F\\
  &+\|X^i-\phi(W^{i-1}U^{i-1})+B^{i-1}\|^2_F)
\end{align*}
\tag{8}</script><p>其中 $A^i$ 和 $B^i$ 是拉格朗日乘子。</p>
<p>与原始应用正则方法和 ADMM 不同，zhang2017convergent 将 ReLU 激活函数解释为一个简单的平滑凸优化问题。即，问题\eqref{eq2} 中的等式约束使用 ReLU 激活函数可以改写为凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  X^i&=\phi(W^{i-1}X^{i-1})\\
  &=max(W^{i-1}X^{i-1},\textbf{0})\\
  &=\mathop{argmin}_{U^i\geq 0}\|U^i-W^{i-1}X^{i-1}\|^2_F
\end{align*}
\tag{9}</script><p>其中 $\textbf{0}$ 是具有适当大小的零矩阵。基于这个观察，他们用以下方式近似激活函数为 ReLU 的问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\}}\ell(X^n,L)+\sum\limits^n_{i=2}\frac{\mu_i}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F\\
  &s.t.X^i\geq \textbf{0},i=2,3\cdots n
\end{align*}
\tag{10}\label{eq10}</script><p>其中正则项对网络结构和激活函数都进行了编码。与基于 MAC 和 ADMM 的方法不同，它确实不包括非线性激活。 此外，\eqref{eq10}的主要优势是该问题是块多凸的，即， 每个变量块变化时其余的块是固定的。 他们提出了一种新的 BCD 方法来求解这个问题，而且经验性的证明了在Caffe框架下基于 SGD 方法ADMM方法的优先级。askari2018lifted 等继承了同样的想法，通过引入更复杂的凸最小化问题，他们可以处理更一般的激活函数，例如 sigmoid、leaky ReLU 和正弦函数等。</p>
<h1 id="三，提升近端算子"><a href="#三，提升近端算子" class="headerlink" title="三，提升近端算子"></a>三，提升近端算子</h1><p>在本节中，我们描述了 LPOM 的基本思想及其相对于现有 DNN 训练方法的优势。跟 zhang2017convergent 和 askari2018lifted 一样，LPOM 首先将\eqref{eq2} 中的激活函数表示为凸最小化问题。 但是，我们希望这种表示不应仅限于特定激活函数，而且我们希望\eqref{eq2}中的等式约束满足LPOM的KKT条件。</p>
<h2 id="3-1-用近端算子进行重构"><a href="#3-1-用近端算子进行重构" class="headerlink" title="3.1 用近端算子进行重构"></a>3.1 用近端算子进行重构</h2><p>我们假设激活函数 $\phi$ 是非递减的，那么$\phi^{-1}(x)=\{y|x=\phi(y)\}$是一个凸集，$\phi^{-1}(x)$ 是 $\{y\}$ 单例当且仅当 $\phi$ 在 $\phi(y)$ 处是严格增加的。 我们要构建一个目标函数 $h(x,y)$，由 $y$ 参数化，使得它的最小值正好是 $x=\phi(y)$。 因此，我们可以通过最小化 $h(x,y)$ 替换约束 $x=\phi(y)$ ，可以将约束作为正则添加到DNN损失中。</p>
<p>优化问题更新变量的基本操作有两种：梯度更新和近端算子。 我们正在构造的优化问题和近端算子如下所示：</p>
<script type="math/tex; mode=display">
\textrm{prox}_f(y)=\mathop{argmin}_xf(x)+\frac{1}{2}(x-y)^2 
\tag{11}\label{eq11}</script><p>我们考虑使用近端算子来构造优化问题。 定义</p>
<script type="math/tex; mode=display">f(x)=\int^x_0(\phi^{-1}(y)-y)dy</script><p>注意 $f(x)$ 定义明确，即使 $\phi^{-1}(y)$ 对于某些介于 0 和 $x$ 之间的 $y$ 来说不是唯一的。 无论如何，$\phi^{-1}$、$f$ 和 $g$（稍后定义）不会显式用于我们的计算。 很容易证明问题\eqref{eq11}的优化条件是 $0\in (\phi^{-1}(x)-x) + (x-y)$。 所以\eqref{eq11} 的解正好是 $x=\phi(y)$。</p>
<p>请注意 $f(x)$ 是单位变量函数。 对于矩阵 $X=(X_{kl})$，我们定义$f(X)=(f(X_{kl}))$。 然后<br>以下最小化问题的优化性条件：</p>
<script type="math/tex; mode=display">
\mathop{argmin}_{X^i}\mathbf{1}^\top f(X^i)\mathbf{1}+\frac{1}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F \tag{12}\label{eq12}</script><p>其中 $\mathbf{1}$ 是全为1的列向量，是</p>
<script type="math/tex; mode=display">
\mathbf{0}\in\phi^{-1}(X^i)-W^{i-1}X^{i-1} \tag{13}</script><p>其中 $\phi^{-1}(X^i)$ 也是按元素定义的。 所以<br>\eqref{eq12} 的优化解是</p>
<script type="math/tex; mode=display">X^i=\phi(W^{i-1}X^{i-1}) \tag{14}\label{eq14}</script><p>这正是问题\eqref{eq2} 中的约束。 所以我们自然地将问题\eqref{eq2} 近似为：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\}}\ell(X^n,L)\\
  &+\sum\limits^n_{i=2}\mu_i\bigg (\mathbf{1}^\top  f(X^i)\mathbf{1}+\frac{1}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F \bigg)
\end{align*}
\tag{15}</script><p>然而，$\{X^i\}_{i=2}^{n-1}$ 的优化条件是：</p>
<script type="math/tex; mode=display">
\begin{align*}
  \mathbf{0}\in &\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})\\
  &+\mu_{i+1}(W^i)^\top (W^iX^i-X^{i+1}),i=2\cdots, n-1
\end{align*}
\tag{16}\label{eq16}</script><p>我们可以清楚地看到问题\eqref{eq2}的等式约束\eqref{eq14}不满足以上条件。</p>
<p>为了使等式约束 \eqref{eq14}满足逼近问题的最优性条件，我们需要将 \eqref{eq16} 修改为</p>
<script type="math/tex; mode=display">
\begin{align*}
  \mathbf{0}\in &\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})\\
  &+\mu_{i+1}(W^i)^\top (\phi(W^iX^i)-X^{i+1}),i=2\cdots, n-1
\end{align*}
\tag{17}\label{eq17}</script><p>这对应于以下问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\}}\ell(X^n,L)+\sum\limits^n_{i=2}\mu_i\bigg (\mathbf{1}^\top f(X^i)\mathbf{1}\\
  &+\mathbf{1}^\top g(W^{i-1}X^{i-1})\mathbf{1} +\frac{1}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F \bigg)
\end{align*}
\tag{18}\label{eq18}</script><p>其中</p>
<script type="math/tex; mode=display">\int^x_0(\phi(y)-y)dy</script><p>$g(X)$ 是在矩阵 $X$ 逐元素定义的， $f(x)$ 和 $g(x)$的一些有代表性的激活函数显示在表1。 \eqref{eq18} 是我们提出的 LPOM ，其中强调下 $g$ 的引入非常重要且不明显。</p>
<h2 id="3-2-LPOM优势"><a href="#3-2-LPOM优势" class="headerlink" title="3.2 LPOM优势"></a>3.2 LPOM优势</h2><p>将 \eqref{eq18} 中 LPOM 的目标函数表示为 $F(W,X)$。 那么我们有下面的定理：</p>
<p><strong>定理1</strong><br>假设 $\ell(X^n, L)$ 在 $X^n$ 中是凸的并且 $\phi$ 是<br>非递减的。 那么 $F(W,X)$ 是块多凸，也就是，如果所有其他变量块都是固定的，每个 $X^i$ 和 $W^i$ 都是凸的。</p>
<p><strong>证明：</strong><br>$F(W,X)$ 可以简化为</p>
<script type="math/tex; mode=display">
\begin{align*}
  &F(W,X)=\ell(X^n,L)+\sum\limits^n_{i=2}\mu_i\bigg (\mathbf{1}^\top \tilde{f}(X^i)\mathbf{1}\\
  &+\mathbf{1}^\top \tilde{g}(W^{i-1}X^{i-1})\mathbf{1} +\langle X^i,W^{i-1}X^{i-1}\rangle \bigg)
\end{align*}
\tag{19}</script><p>其中 $\tilde{f}(x)=\int_0^x \phi^{-1}(y) dy$ 和 $\tilde{g}(x)=\int_0^x \phi(y) dy$。 因为 $\phi$ 和 $\phi^{-1}$ 是非递减的，所以 $\tilde{f}(x)$ 和 $\tilde{g}(x)$ 是凸的。 很容易验证 $\mathbf{1}^\top\tilde{g}(W^{i-1}!X^{i-1})\mathbf{1}$ 当 $W^{i-1}$ 固定时在 $X^{i-1}$ 中是凸的，当 $X^{i-1}$ 固定时在 $W^{i-1}$ 中是凸的。$F(W,X)$ 中的剩余项 $\langle X^{i},W^{i-1}X^{i-1}\rangle$ 当其他两个块固定时，在一个块中是线性的。证明完成。</p>
<p>由于子问题的凸性，定理1允许使用高效的 BCD 算法求解 LPOM，并保证可以得到更新 $X^i$ 和 $W^i$ 的最佳解决方案。 相反，正则项方法和基于 ADMM 的方法中的子问题都是非凸的。</p>
<p>与基于 ADMM 的方法相比，LPOM 除了 $\{X^i\}_{i=2}^{n}$ 不需要拉格朗日乘子和更多的辅助变量。 此外，我们还设计了精致的算法，这样求解 LPOM 也无需额外的变量。 所以 LPOM 的变量数比基于 ADMM 的方法少，因此大大节省了内存。实际上，它的内存成本接近于 SGD。</p>
<p>与正则项方法相比，LPOM 的优化条件更简单。 例如，LPOM 中的 $\{X^i\}_{i=2}^{n-1}$ 和 $\{W^i\}_{i=1}^{n-1}$ 优化条件是 \eqref{eq17} 和</p>
<script type="math/tex; mode=display">
(\phi(W^iX^i)-X^{i+1})(X^i)^\top\!=0,\; i=1,2\cdots n-1 
\tag{20}</script><p>而那些用于 MAC 的优化条件是</p>
<script type="math/tex; mode=display">
\begin{align*}
  &(X^i-\phi(W^{i-1}X^{i-1}))\\
  &+(W^i)^\top[(\phi(W^iX^i)-X^{i+1})\circ\phi^{'}(W^iX^i)]=\mathbf{0}\\
  &i=2,\cdots n-1
\end{align*}
\tag{21}</script><p>和</p>
<script type="math/tex; mode=display">
\begin{align*}
  [(\phi(W^iX^i)-X^{i+1})\circ\phi^{'}(W^iX^i)](X^i)^\top=\mathbf{0},i=1,\cdots n-1
\end{align*}
\tag{22}</script><p>其中 $\circ$ 表示逐元素乘法。 我们可以看到 MAC 的优化条件有额外的 $\phi’(W^iX^i)$，这是非线性的。zeng2018global 的优化条件可以在补充材料中找到，他们也还有一个额外的 $\phi’(U^i)$。 这可能意味着 MAC 和 zeng2018global 的解集更复杂，更大。 所以LPOM  可能更容易找到良好解决方案。</p>
<p>与凸优化重构方法相比，LPOM 可处理更一般的激活函数。 注意 zhang2017convergent 只考虑了 ReLU。 虽然 askari2018lifted 声称他们的公式可以处理一般的激活函数，其求解方法还是仅限于 ReLU。 此外 askari2018lifted 关于$\{X^i\}_{i=2}^{n-1}$ 和 $\{W^i\}_{i=1}^{n-1}$的优化条件推导是有误的，也就是：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathrm{0}\in&\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})-\mu_{i+1}(W^i)^\top X^{i+1}\\
&i=2,\dots n-1
\end{align*}</script><p>相应地，$X^{i+1}(X^i)^\top=\mathbf{0},\,i=1,\cdots,n-1,$，很明显，等式约束\eqref{eq14} 不满足以上条件。 此外，不知何故 askari2018lifted 不管激活函数是什么，都添加了额外的约束 $X^i\geq \mathbf{0}$，所以他们的重构不能很好地近似原始 DNN \eqref{eq2}，这可能解释为什么 askari2018lifted 得不到好的结果。实际上，它们只能为 SGD 提供良好的初始化。</p>
<p>与基于梯度的方法（例如 SGD）相比，LPOM 可以使用任何非递减 Lipschitz 连续激活而没有数值求解，包括饱和（例如，sigmoid 和 tanh）和不可微分的（例如，ReLU 和 leaky ReLU），并且可以逐层并行更新权重和激活。 相反，基于梯度的方法只能使用有限的激活函数，例如 ReLU， leaky ReLU 和 softplus，以避免梯度消失或爆炸问题，并且在计算时无法并行化梯度和激活。</p>
<h1 id="四，求解LPOM"><a href="#四，求解LPOM" class="headerlink" title="四，求解LPOM"></a>四，求解LPOM</h1><p>多亏了块多凸性性质(定理1)，LPOM可以用BCD求解。 即，我们通过固定所有其他变量块来更新 $X^i$ 或 $W^i$。可以使用小批量训练数据来进行优化问题的求解，解决 LPOM 的整个算法总结在算法1，下面我们给出更多的细节。</p>
<h2 id="4-1-更新-X-i-n-i-2"><a href="#4-1-更新-X-i-n-i-2" class="headerlink" title="4.1 更新$\{X^i\}^n_{i=2}$"></a>4.1 更新$\{X^i\}^n_{i=2}$</h2><p>我们先介绍串行更新的方法 $\{X^i\}_{i=2}^n$，将 $\{X^i\}_{i=2}^{n}$ 从 $i=2$ 接连不断地更新到 $n$，就像 DNN 的前馈过程一样。 当 $i=2,\cdots,n-1$ 时，在 $\{W^i\}_{i=1}^{n-1}$ 和 $\{X^j\}_{j=2,j\neq i}^n$ 固定时，问题 \eqref{eq18}可以化为：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{X^i}\mu_i\bigg(\mathbf{1}^\top f(X^i)\mathbf{1}+\frac{1}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F \bigg)\\
  &+\mu_{i+1}\bigg(\mathbf{1}^\top g(W^iX^i)\mathbf{1}+\frac{1}{2}\|X^{i+1}-W^iX^i\|^2_F \bigg)
\end{align*}
\tag{23}</script><p>优化条件是：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\mathbf{0}\in \mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})\\
  &+\mu_{i+1}((W^i)^\top(\phi(W^iX^i)-X^{i+1}))
\end{align*}
\tag{24}</script><p>所以用下面迭代方法更新 $X^i$ 直到收敛：</p>
<script type="math/tex; mode=display">
X^{i,t+1}=\phi\bigg(W^{i-1}X^{i-1}-\frac{\mu_{i=1}}{\mu_i}(W^i)^\top(\phi(W^iX^{i,t})-X^{i+1}) \bigg) 
\tag{25}</script><p>其中上标 $t$ 为迭代次序。 收敛分析如下：</p>
<p><strong>定理2</strong><br>假设 $|\phi’(x)|\leq\gamma$，如果 $\rho&lt;1$，则迭代是收敛的并且收敛速率是线性的，其中</p>
<script type="math/tex; mode=display">\rho=\frac{\mu_{i+1}}{\mu_i}\gamma^2\sqrt{\|
 |(W^i)^\top||W^i| \|_1\| |(W^i)^\top| |W^i|\|_\infty}</script><p>证明可以在补充材料中找到。 在上面式子中，$|A|$ 是一个矩阵，其元素是 $A$ 的绝对值，$|\cdot|_1$ 和 $|\cdot|_{\infty}$ 分别是矩阵 1-范数（largest absolute column sum）和矩阵 $\infty$-范数（largest absolute row sum）。</p>
<p>当考虑 $X^n$ 时，问题\eqref{eq18} 简化为</p>
<script type="math/tex; mode=display">
\min_{X^n}\ell(X^n,L)+\mu_n\bigg(\mathbf{1}^\top f(X^i)\mathbf{1}+\frac{1}{2}\|X^n-W^{n-1}X^{n-1}\|^2_F \bigg) 
\tag{26}</script><p>优化条件是：</p>
<script type="math/tex; mode=display">
\mathbf{0}\in\frac{\partial\ell(X^n,L)}{\partial X^n}+\mu_n(\phi^{-1}(X^n)-W^{n-1}X^{n-1}) 
\tag{27}</script><p>所以用下面迭代来更新 $X^n$ 直到收敛</p>
<script type="math/tex; mode=display">
X^{n,t+1}=\phi\bigg(W^{n-1}X^{n-1}-\frac{1}{\mu_n}\frac{\partial\ell(X^{n,t},L)}{\partial X^n} \bigg) 
\tag{28}</script><p>收敛分析如下所示：</p>
<p><strong>定理3</strong><br>假设 $|\phi’(x)|\leq\gamma$ 和 $\bigg|\big(\frac{\partial^2\ell(X,L)}{\partial X_{kl}\partial X_{pq}}\big)\bigg|_1\leq\eta$。如果 $\tau &lt; 1$，则迭代收敛，收敛率为线性，其中 $\tau=\frac{\gamma\eta}{\mu_n}$。</p>
<p>证明也可以在补充材料中找到。 如果 ${\ell}(X^n,L)$ 是最小二乘误差，即 ${\ell}(X^n,L)=\frac{1}{2}|X^n-L|_F^2$，然后 $\bigg|\bigg|\big(\frac{\partial^2\ell(X,L)}{\partial X_{kl}\partial X_{pq}}\big)\bigg|\bigg|_1 = 1 $。 所以我们得到 $\mu_n&gt;\gamma$。</p>
<p>上面的串行更新程序可以很容易地更改为并行更新：每个 $X^i$ 都使用最新的其他 $X^j, j\neq i$ 的信息来更新。</p>
<h2 id="4-2-更新-W-i-n-1-i-1"><a href="#4-2-更新-W-i-n-1-i-1" class="headerlink" title="4.2 更新$\{W^i\}^{n-1}_{i=1}$"></a>4.2 更新$\{W^i\}^{n-1}_{i=1}$</h2><p>$\{W^i\}_{i=1}^{n-1}$ 可以完全并行更新，当 $\{X^i\}_{i=2}^{n}$ 是固定的，问题 \eqref{eq18}化为：</p>
<script type="math/tex; mode=display">
\begin{align*}
  \min_{W^i}\mathbf{1}^\top g(W^iX^i)\mathbf{1}+\frac{1}{2}\|W^iX^i-X^{i+1}\|^2_F\\
  i=1\cdots n-1 
\end{align*}
\tag{29}\label{eq29}</script><p>上述问题可以并行求解。 \eqref{eq29} 可以写做：</p>
<script type="math/tex; mode=display">
\min_{W^i}\mathbf{1}^\top\tilde{g}(W^iX^i)\mathbf{1}-\langle X^{i+1},W^iX^i \rangle\tag{30}
\label{eq30}</script><p>其中，如前所述 $\tilde{g}(x)=\int_0^x \phi(y)dy$。假设 $\phi(x)$ 是 $\beta$-Lipschitz 连续的，这对于几乎所有使用的激活函数都是成立的。 然后 $\tilde{g}(x)$ 是 $\beta$-平滑的：</p>
<script type="math/tex; mode=display">|\tilde{g}^{'}(x)-\tilde{g}^{'}(y)|=|\phi(x)-\phi(y)|\leq\beta|x-y| \tag{31}</script><p>问题 \eqref{eq30} 可以通过APG的局部线性化 $\hat{g}(W)=\tilde{g}(WX)$ 求解。 然而，$\hat{g}(W)$ 梯度的Lipschitz 常数，即 $\beta|X|_2^2$，可能非常大，因此收敛可能很慢。 下面我们提出一个 APG 的改进版本，专为解决问题\eqref{eq30} 而设计的高效算法。</p>
<p>考虑以下问题：</p>
<script type="math/tex; mode=display">
\min_xF(x)\equiv\varphi(Ax)+h(x) 
\tag{32}\label{eq32}</script><p>其中 $\varphi(y)$ 和 $h(x)$ 都是凸的。 而且，$\varphi(y)$ 是 $L_\varphi$-平滑的：$|\nabla\varphi(x)-\nabla \varphi(y)| \leq L_\varphi|x-y|,\forall x,y.$ 我们假设下面的问题：</p>
<script type="math/tex; mode=display">
x_{k+1}=\mathop{argmin}_x\langle\nabla\varphi(Ay_k),A(x-y_k) \rangle +\frac{L_\varphi}{2}\|A(x-y_k)\|^2+h(x)
\tag{33}\label{eq33}</script><p>对任何给定的 $y_k$ 很容易求解，我们提出用算法2求解 \eqref{eq32}，那么我们有下面的定理：</p>
<p><strong>定理4</strong><br>如果我们使用算法2来求解问题\eqref{eq32}，则收敛速度至少为 $O(k^{-2})$：</p>
<script type="math/tex; mode=display">
F(x_k)\!-\!F(x^*)\!+\!\frac{L_\varphi}{2}\|z_k\|^2\!\leq\!\frac{4}{k^2}\!\left(\!F(x_1)\!-\!F(x^*)\!+\!\frac{L_\varphi}{2}\|z_1\|^2\!\right)</script><p>其中，<br>$z_k=A[\theta_{k-1}x_{k-1}-x_k+(1-\theta_{k-1})x^<em>]$，且 $x^</em>$ 是问题\eqref{eq32}的任意优化解。</p>
<p>证明也可以在补充材料中找到。</p>
<p>通过用问题\eqref{eq30}实例化问题\eqref{eq32}，子问题\eqref{eq33} 变为：</p>
<script type="math/tex; mode=display">
\begin{align*}
  W^{i,t+1}&=\mathop{argmin}_W\langle\phi(Y^{i,t}X^i),(W-Y^{i,t})X^i\rangle\\
  &+\frac{\beta}{2}\|(W-Y^{i,t})X^i||^2_F-\langle X^{i+1}-WX^i\rangle
\end{align*}
\tag{34}</script><p>这是一个最小二乘问题，解是：</p>
<script type="math/tex; mode=display">
W^{i,t+1}=Y^{i,t}-\frac{1}{\beta}(\phi(Y^{i,t}X^i)-X^{i+1})(X^i)^{+} 
\tag{35}</script><p>其中 $(X^i)^{+}$ 是 $X^i$ 的伪逆，$Y^{i,t}$ 是算法2中的 $y_k$。</p>
<p>如果 $\phi(x)$ 严格递增且递增率 $\frac{\phi(y)-\phi(x)}{y-x}\, (y\neq x)$ 的下界为 $\alpha&gt;0$，那么 $\tilde{g}(x)$ 是强凸的，并且收敛是线性的，这里我们省略详情。</p>
<h1 id="五，实验"><a href="#五，实验" class="headerlink" title="五，实验"></a>五，实验</h1><p>在本节中，我们通过与 SGD 和两种基于非梯度的方法askari2018lifted 及 taylor2016training 进行比较来评估 LPOM。 其他基于非梯度的方法不为分类任务训练完全连接的前馈神经网络（例如，使用跳跃连​​接，训练自动编码器，以及学习哈希等)，所以我们不能将它们包括在内进行比较。为简单起见，我们使用最小二乘损失函数和 ReLU 激活函数（使用 ReLU 的另一个原因是它可以产生更高的精度，尽管 LPOM 可以没有数值困难的用其他激活函数参与计算）除非另有说明。与 askari2018lifted 不同，我们不对权重 $\{W^i\}_{i=1}^{n-1}$ 使用任何正则化。我们对 LPOM 和 SGD 使用相同的输入和随机运行初始化。我们用 LPOM 的 MATLAB 实现而不优化代码，使用基于 Caffe 的 SGD 求解器。对于 Caffe 求解器，我们修改demo代码，仔细调参实现最好的精度。对于 askari2018lifted 和 taylor2016training，我们引用他们的论文的结果。</p>
<h2 id="5-1-与SGD比较"><a href="#5-1-与SGD比较" class="headerlink" title="5.1 与SGD比较"></a>5.1 与SGD比较</h2><p>我们对两个数据集进行实验，即 MNIST 和 CIFAR-10。对于 MNIST 数据集，我们使用 $28\times28=784$ 个原始像素作为输入，它包括 60,000 张训练图像和 10,000 张测试图像，不使用预处理或数据增强。LPOM和SGD，在每个epoch中都用所有训练样本运行一次。性能取决于网络结构的选择。与 zeng2018global 一样，我们实现了一个784-2048-2048-2048-10 前馈神经网络。对于 LPOM，我们只需在 \eqref{eq18} 中设置 $\mu_i=20$。我们在 LPOM 和 SGD 上都运行 100 个周期的 ，固定批量大小为 100。训练和测试精度如图1(a) 和 (b)，可以看到两种方法的训练精度都约等于 $100\%$。然而，LPOM 的测试准确性略优于 SGD（$98.2\%$ vs. $98.0\%$）。</p>
<p>对于 CIFAR-10 数据集，在 zeng2018global 中我们实现 3072-4000-1000-4000-10 前馈神经网络。我们通过分别减去训练数据集中红色、绿色和蓝色通道的均值来标准化彩色图像，不使用预处理或数据扩充。对于 LPOM，我们设置 \eqref{eq18} 中的 $\mu_i=100$。在 LPOM 和 SGD 上运行 100 个 epochs，批量大小为 100。训练和测试准确度如图1 (c) 和 (d) 所示，可以看到SGD和LPOM的训练精度是约等于 $100\%$。但是，LPOM 的测试精度优于SGD（$52.5\%$ 对 $47.5\%$）。</p>
<h2 id="5-2-与其他非梯度方法比较"><a href="#5-2-与其他非梯度方法比较" class="headerlink" title="5.2 与其他非梯度方法比较"></a>5.2 与其他非梯度方法比较</h2><p>我们用 MNIST 数据集上相同结构的网络与 askari2018lifted 中的结果进行比较。在实际计算中askari2018lifted 只用了 ReLU 激活函数，与 askari2018lifted 一样，我们在 LPOM 上运行 17 个 epochs，固定批量大小为 100，设置 $\mu_i=20$。使用 60,000 张训练图像和 10,000 张测试图像，不要使用预处理或数据增强。这两种方法的测试精度示于表2，可以看到带 ReLU 的 LPOM 表现很大差距的优于askari2018lifted 的方法，这符合我们在“LPOM 的优点”小节的描述。</p>
<p>按照 taylor2016training 中数据集和网络架构的设置，我们在 SVHN 数据集上测试 LPOM，设置 $\mu_i=20$。 SGD、taylor2016training 和 LPOM 的测试精度如表3所示。可以看到 LPOM 优于 SGD 和 taylor2016training。<br>如 taylor2016training 中所述，他们基于 ADMM 的方法和 SGD 的测试精度分别约为 $96.5\%$ 和 $95.0\%$。然而，LPOM 可以在相同的设置下达到 $98.3\%$ 的测试精度。<br>这进一步验证了LPOM的优势。</p>
<h1 id="六，总结"><a href="#六，总结" class="headerlink" title="六，总结"></a>六，总结</h1><p>在这项工作中，我们提出了 LPOM 来训练全连接前馈神经网络，使用近端运算符 LPOM 将神经网络转化为一个新的分块多凸模型，转换适用于一般非递减 Lipschitz 连续激活函数。我们自然地提出了块坐标下降算法，保障每个子问题收敛的情况下求解。 LPOM 可以并行解决，相比分层激活，无需更多辅助变量。 实验结果表明 LPOM 在完全连接的神经网络比 SGD，askari2018lifted 和 taylor2016training 效果更好。未来的工作包括将 LPOM 扩展到训练卷积和递归神经网络并应用 LPOM 到网络压缩。</p>
<h1 id="重要文献"><a href="#重要文献" class="headerlink" title="重要文献"></a>重要文献</h1><p><a href="https://arxiv.org/abs/1805.01532">Lifted neural networks</a><br><a href="https://arxiv.org/abs/1512.03385">Deep residual learning for image recognition</a></p>
]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>LPOM</tag>
      </tags>
  </entry>
  <entry>
    <title>Convexifying Transformers</title>
    <url>/2023/03/22/Transformer/Convexifying%20Transformers/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：Convexifying Transformers: Improving optimization and understanding of transformer networks</p>
<p><a href="https://arxiv.org/abs/2211.11052">essay link</a></p>
<p><strong>凸优化的角度理解和优化Transformer网络~</strong><br><span id="more"></span></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>了解Transformer网络成功背后的基本机制仍然是深度学习中一个悬而未决的问题，尽管它们的出色表现主要归功于自我注意机制，但文献仍然缺乏对这些网络的可靠分析和对它们所学函数的解释。为此，我们研究了注意力Transformer网络的训练问题，并引入了一种新颖的凸分析方法来提高对这些网络的理解和优化。特别是，我们首先引入了自注意力机制的凸替代方案，并用我们的凸注意力重新表述了Transformer网络的正则化训练问题。然后，我们将重构为一个可解释且更易于优化的凸优化问题。此外，作为我们凸分析的副产品，我们揭示了一种隐式正则化机制，它促进了token之间的稀疏性。因此，我们不仅改进了注意力或称Transformer网络的优化，而且还提供了对它们学到的函数的理论理解，并且通过几个数值实验证明了我们理论的有效性。</p>
<h1 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h1><p>vaswani2017attention 提出的 Transformer 网络已成为各种任务中的主导架构，尤其是自然语言处理 (NLP)，因为它们具有非凡的泛化特性和从海量数据中学习的高能力。 尽管有大量关于Transformer网络有效性的经验证据，但由于其高度非线性和非凸结构，揭示其成功背后的潜在理论原因仍然是一个悬而未决的研究问题。</p>
<p>大量研究侧重于通过实证研究分析Transformer网络的某些组件，例如 liu2021analyzingattention 等研究了注意力机制对 transformer 网络的影响。尽管这些研究一致认为注意力是 Transformer 的重要组成部分，但它们也提出了一些有关可解释性和优化的问题。特别是，voita2019analyzing 证明可以删除大多数注意力头而不影响网络性能，这是网络中大量冗余的一个指标。 attentionacrossNLP 提供了一组经验证据表明某些 NLP 任务可能不需要注意力。此外，2021dong_attention 透露，虽然注意力是Transformer网络的核心，但在没有全连接 (FCN) 层和跳跃连接的情况下训练注意力网络极具挑战性，因为没有它们，网络输出会迅速退化。类似地，takase2022layer 讨论了层归一化和跳跃连接对Transformer网络的重要性，因此即使改变它们的位置也可能显着影响Transformer网络的性能。然而，仍然缺乏对这些问题背后的潜在因素的可靠理论分析，这可能是由于Transformer网络的高度复杂和非凸结构。</p>
<p>一系列论文还侧重于设计自注意机制的新替代方案，这些替代方案表现相似，并可能为整体模型提供进一步的解释。一组工作利用基于多层感知器的架构，如tolstikhin2021mlp等，而另一组论文提出基于傅里叶的模型如lee2021fnet等，21adaptive等还提出用矩阵分解 geng2021attention 代替 self-attention 机制。尽管这些工作成功地应用于某些应用，但它们缺乏从优化角度进行扎实的理论分析和理解。最近，sahiner2022convex 尝试通过完全改变自注意力机制的结构并移除 FC 层，通过凸对偶分析 transformer 网络。即使那样，它们也未能为Transformer提供可靠的实际意义，因为它们的公式极具挑战性且在实践中难以解决。</p>
<p>最近，另一项研究侧重于理解 transformer 网络训练过程中出现的结构和模式 如power2022grokking等。 特别是，grokking 现象首先由 power2022grokking 在特定算法任务（例如模除法运算）中观察到。 具体来说，grokking 指的是验证或测试准确性突然过渡到完美泛化，并且这种泛化发生在完美训练准确性点之后。 这个有趣的行为与深度学习模型训练中早期停止的常见做法相矛盾，并且肯定需要进一步了解为什么会出现这种现象。</p>
<p>为了解决与标准Transformer网络相关的问题，在本文中，我们开发了一个凸优化视角来训练、分析和理解Transformer网络。 特别是，我们首先提出了自注意力机制的凸替代方案，然后在结果模型上开发了我们的凸分析框架，如图1所示。</p>
<h2 id="1-1-贡献"><a href="#1-1-贡献" class="headerlink" title="1.1 贡献"></a>1.1 贡献</h2><p>本文贡献如下：</p>
<ul>
<li>我们提出了标准自注意力机制的替代公式，并用它研究了注意力Transformer网络的正则化训练问题。</li>
<li>如图1所示，我们使用提出的注意层凸化了注意Transformer网络的正则化训练问题，因此能够找到全局最优解而不需要任何非凸优化启发式，例如层归一化和跳跃连接。</li>
<li>我们还将我们的凸分析框架应用于各种架构，例如，有或没有 FCN 层的网络。因此，我们能够解释在整个训练过程中每个组件对学习模型的影响。</li>
<li>我们揭示了一种由注意力机制引起的隐式正则化机制，之后进一步将这种正则化描述为跨token的稀疏性诱导因素。</li>
<li>我们通过各种实验结果证明了凸重构的有效性。我们还表明，我们的重新表述显着减轻了最近论文 power2022grokking等，其研究中指出的 grokking 现象。</li>
</ul>
<h2 id="1-2-记号"><a href="#1-2-记号" class="headerlink" title="1.2 记号"></a>1.2 记号</h2><p>我们分别使用小写和大写粗体字母表示向量和矩阵，用下标表示向量或矩阵的某个列元素。 例如，$w_{jk}$ 表示矩阵 $\matrix{W}$ 的第 $jk$ 项，用 $\mathbf{I}_k$ 表示大小为 $k \times k$ 的单位矩阵，使用 $\mathbf{0}$（或 $\mathbf{1}$）表示具有适当尺寸的零（或1）向量或者矩阵，还使用 $[n]$ 表示范围从 $1$ 到 $n$ 的整数集，将 Euclidean 和 Frobenius 范数分别表示为 $|\cdot|_2$ 和 $|\cdot |_{F}$，还使用 $\mathbb{1}[x\geq0]$ 来表示 0-1 值指示函数，在表1中提供了在整篇论文中使用的更多符号。</p>
<h1 id="2，Transformer网络"><a href="#2，Transformer网络" class="headerlink" title="2，Transformer网络"></a>2，Transformer网络</h1><p>给定一个数据样本（或句子）$\mathbf{X} \in \mathbb{R}^{h \times d}$ 作为具有嵌入维度 $d$ 的 $h$ token序列，我们将键（key）、查询（query）和值（value）矩阵定义为：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{Q} & =\mathbf{X}\mathbf{W}_q,\quad \mathbf{W}_q\in\mathbb{R}^{d\times d} \\
\mathbf{K} & =\mathbf{X}\mathbf{W}_k,\quad \mathbf{W}_k\in\mathbb{R}^{d\times d} \\
\mathbf{V} & =\mathbf{X}\mathbf{W}_v,\quad \mathbf{W}_v\in\mathbb{R}^{d\times d}
\end{align*}</script><p>它们是自注意力机制的主要组成部分。 然后，一个基本上是自注意力堆叠的单个Transformer块、残差连接、层归一化和逐点前馈连接可以表示如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{A}_{s,j} &= \text{softmax}(\mathbf{Q}\mathbf{K}^\top\mathbf{V})\\
\mathbf{A}_o &= \mathbf{A}_s\mathbf{W}_o, \mathbf{W}_o\in\mathbb{R}^{d\times d}\\
\mathbf{X}_A &= LayerNorm(\mathbf{A}_o)+\mathbf{X}' \\
\mathbf{X}_B &= \sigma(\mathbf{X}_A\mathbf{W}_1)\mathbf{W}_2 
\end{align*}
\tag{1}\label{eq1}</script><p>其中 $\sigma(\cdot)$ 表示 FCN 层的激活函数，尽管跳跃连接、层归一化和 FCN 在Transformer块中也起着至关重要的作用，但这些网络的成功主要归功于自注意力部分，表示为 $\mathbf{A}_o$ 。 因此，在下一节中，我们首先研究简化的transformer网络的训练问题，网络输出直接为 $\mathbf{A}_o$。 然后，我们将推导扩展到具有 FCN 层的Transformer网络。</p>
<h1 id="3，仅注意力网络"><a href="#3，仅注意力网络" class="headerlink" title="3，仅注意力网络"></a>3，仅注意力网络</h1><p>我们首先考虑一个简化的Transformer网络，它只有一个自注意层，将输入序列 $\mathbf{X}\in \mathbb{R}^{n \times d}$ 映射到 $c$ 维输出序列 $\hat{\mathbf{Y}} \in \mathbb{R}^{n \times c}$ ，即：</p>
<script type="math/tex; mode=display">
\hat{\mathbf{Y}} = \text{softmax}(\mathbf{X}\mathbf{W}_q\mathbf{W}^\top_k\mathbf{X}^\top)\mathbf{X}\mathbf{W}_v\mathbf{W}_o 
\tag{2}\label{eq2}</script><p>我们也称模型 \eqref{eq2} 为仅注意力网络。这是一个有意义的模型，已应用于各种任务，包括机器翻译、语言建模、图像字幕和对象识别。</p>
<p>接下来我们考虑一个带有任意凸损失函数的标准回归框架。 给定训练集 $\{\mathbf{X}_i, \mathbf{Y}_i\}_{i=1}^N$，其中 $\mathbf{X}_i \in \mathbb{R}^{n \times d}$ 和 $\mathbf{Y}_i \in \mathbb{R}^{n \times c}$ 分别表示输入序列和标签输出，\eqref{eq2} 中仅注意力网络的权重衰减正则化训练问题如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{W_q,W_k,W_v,W_o}\sum\limits^N_{i=1}\mathcal{L}(\mathrm{softmax}(\mathbf{X}_i\mathbf{W}_q\mathbf{W}^\top_k\mathbf{X}^\top_i)\mathbf{X}\mathbf{W}_v\mathbf{W}_o,\mathbf{Y}_i)\\
    +\frac{\beta}{2}\sum\limits_{\ast\in\{ q,k,v,o\}}\|\mathbf{W}_\ast\|^2_F
\end{align*}
\tag{3}\label{eq3}</script><p>其中 $\mathcal{L}(\cdot)$ 是任意凸损失函数，包括平方损失和交叉熵，$\beta &gt;0$ 是正则化系数。</p>
<p>尽管 \eqref{eq2} 中的注意力模型在各种 NLP 任务中非常强大，例如，自然语言推理、神经机器翻译和文本分类，\eqref{eq3} 中相应的训练问题是一项极具挑战性的优化任务，需要对各种非凸优化启发式进行充分训练。 为了解决这些问题，在接下来的部分中，我们首先通过用替代凸层替换注意力部分来重新制定训练问题，然后将重新制定的训练问题转换为可解释的凸优化问题，从而实现全局优化网络参数。</p>
<h2 id="3-1-凸注意力层"><a href="#3-1-凸注意力层" class="headerlink" title="3.1 凸注意力层"></a>3.1 凸注意力层</h2><p>我们首先注意到，由于 $\text{softmax}(\cdot)$ 操作是高度非线性和非凸的，因此 \eqref{eq3} 中的训练问题是一个具有挑战性的非凸优化问题。 因此，人们可能无法充分训练注意力网络并在训练结束时获得微不足道的模型。 例如，dong_attention 表明注意力网络在整个训练过程中可能会退化，并且输出会收敛到秩为 1 的矩阵。 因此，他们无法学习基础任务。</p>
<p>为了避免与 \eqref{eq2} 中的非凸公式相关的问题，我们首先用更简单但有效的替代方法替换 $\text{softmax}$ 操作。 特别是，由于  $\text{softmax}$ 将其输入矩阵的行转换为概率分布，因此可以将其放宽为具有单位单纯形约束的线性运算，如下所示</p>
<script type="math/tex; mode=display">
\forall \mathbf{U}\in\mathbb{R}^{n\times n},\exists \mathbf{W}\in\Delta s.t. \quad \text{softmax}(\mathbf{U})\mathbf{X}=\mathbf{W}\mathbf{X}</script><p>其中 $\Delta := \{\mathbf{W} \in \mathbb{R}^{n \times n}: \mathbf{w}_i\geq 0, \matrix{1}^\top\mathbf{w}_i=1 , \forall i \in [ n]\}$ 表示约束的凸集，也称为单位单纯形约束。 因此，我们在不扰乱其结构的情况下简化和凸化了注意力机制。 基于这一观察，\eqref{eq3} 可以重新表述如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{W_1\in\Delta \\ W_2\in\mathbb{R}^{d\times d},\; W_3\in\mathbb{R}^{d\times c}}\sum\limits^N_{i=1}\mathcal{L}(\mathbf{W}_1\mathbf{X}_i\mathbf{W}_2\mathbf{W}_3,\mathbf{Y}_i)+\frac{\beta}{2}\big(\|\mathbf{W}_2\|^2_F+\|\mathbf{W}_3\|^2_F\big) 
\end{align*}
\tag{4}\label{eq4}</script><p>请注意，上面的模型使用单个头部注意力模型，因此，由于其表达能力不足，可能不具有实际意义。 因此，我们在 \eqref{eq4} 中引入 head 的概念如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{W_{1j}\in\Delta \\ W_{2j}\in\mathbb{R}^{d\times d},\; W_{3j}\in\mathbb{R}^{d\times c}}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}\mathbf{W}_{1j}^\top\mathbf{X}_i\mathbf{W}_{2j}\mathbf{W}_{3j},\mathbf{Y}_i\bigg)\\
    +\frac{\beta}{2}\bigg(\sum\limits^h_{j=1}\|\mathbf{W}_{2j}\|^2_F+\|\mathbf{W}_{3j}\|^2_F\bigg)
\end{align*}
\tag{5}\label{eq5}</script><p>现在，我们已准备好将凸分析工具应用于 \eqref{eq5}，详见下一节。</p>
<h2 id="3-2-凸优化应用到仅注意力网络"><a href="#3-2-凸优化应用到仅注意力网络" class="headerlink" title="3.2 凸优化应用到仅注意力网络"></a>3.2 凸优化应用到仅注意力网络</h2><p>作为热身，让我们考虑标量输出预测问题，目标是一维的，即 $y_i \in \mathbb{R}$。 然后，\eqref{eq5} 简化为以下优化问题</p>
<script type="math/tex; mode=display">
\min_{\mathbf{w}_{1j}\in\Delta \\ \mathbf{w}_{2j}\in\mathbb{R}^d,\; w_{3j}\in\mathbb{R}}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}\mathbf{w}_{1j}^\top\mathbf{X}_i\mathbf{w}_{2j}w_{3j},y_i\bigg)+\frac{\beta}{2}\sum\limits^h_{j=1}\big(\|\mathbf{w}_{2j}\|^2_F+(w_{3j})^2 \big) 
\tag{6}\label{eq6}</script><p>接下来，我们首先在参数 $\mathbf{w}_{2j}$ 和 $w_{3j}$ 之间应用重缩放，使得 \eqref{eq6} 可以描述为 $\ell_1$ 正则优化问题。</p>
<p><strong>引理1</strong><br>\eqref{eq6} 中的问题等同于下面的 $\ell_1$ 正则化训练问题：</p>
<script type="math/tex; mode=display">
\min_{\mathbf{w}_{1j}\in\Delta \\ \|\mathbf{w}_{2j}\|_2\leq 1,\; w_{3j}\in\mathbb{R}}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}\mathbf{w}_{1j}^\top\mathbf{X}_i\mathbf{w}_{2j}w_{3j},y_i\bigg)+\beta\|\mathbf{w}\|_1 
\tag{7}</script><p>基于引理1中的等价公式，下一个定理引入了等价于 \eqref{eq6} 的凸优化问题。</p>
<p><strong>定理1</strong><br>非凸优化问题 \eqref{eq6} 可以等效地转换为以下凸优化问题：</p>
<script type="math/tex; mode=display">
\min_{\mathbf{Z}\in\mathbb{R}^{n\times d}}\frac{1}{2}\sum\limits^N_{i=1}\mathcal{L}(trace(\mathbf{Z}^\top\mathbf{X}_i),y_i)+\beta\sum\limits^n_{k=1}\|\mathbf{z}_k\|_2 
\tag{8}\label{eq8}</script><p>请注意，\eqref{eq8} 中的等效凸模型需要单个参数矩阵 $\matrix{Z} \in \mathbb{R}^{n \times d}$，其中每一行是相应token的注意力分数。我们还注意到 \eqref{eq8} 中的正则化，即参数矩阵 $\matrix{Z}$ 行的 $\ell_2$ 范数之和，是一种特定类型的正则化，也称为组 $\ell_1$ 或 Lasso，由 bakin1999adaptive 引入并可促进跨参数的组稀疏性。在我们的例子中，组稀疏度跨越token索引 $k$，因此，可以将 \eqref{eq8} 中的模型解释为稀疏线性模型，其中稀疏性跨token。换句话说，\eqref{eq8} 可以解释为一个模型，它试图使用尽可能少的token来拟合训练标签 $\{y_i\}_{i=1}^N$。</p>
<p>与 \eqref{eq6} 中表示为 $\mathbf{w}_{1j} \in \Delta$的非负注意力得分不同，凸参数 $\matrix{Z} \in \mathbb{R}^{ n \times d}$ 不需要任何约束 . 因此，可以直接应用标准训练算法，如 SGD 和 Adam 来训练凸问题 \eqref{eq8}。 此外，可以从 \eqref{eq8} 的解中恢复 \eqref{eq6} 的一组最佳参数，如以下结果所示。</p>
<p><strong>命题1</strong><br>在求解 \eqref{eq8} 中的凸优化问题后，可以恢复 \eqref{eq6} 中的非凸优化问题的最优解，表示为 $\{\mathbf{w}_{1j}^<em>,\mathbf{w}_{2j}^</em>,w_{3j}^*\}_{j=1}^h$,如下：</p>
<script type="math/tex; mode=display">
\mathbf{w}^*_{1j}=\mathbf{e}_j,\; \mathbf{w}^*_{2j}=\frac{\mathbf{z}_j}{\sqrt{\|\mathbf{z}_j\|_2}},\;\mathbf{w}^*_{3j}=\sqrt{\|\mathbf{z}_j\|_2},\;\forall j\in [h]</script><p>其中 $\mathbf{e}_j \in \mathbb{R}^{n}$ 是第 $j$ 个普通基向量， $\mathbf{z}_j \in \mathbb{R}^{d}$ 是 $\matrix{Z}$矩阵的第 $j$ 行，我们假设由于 \eqref{eq8} 中的稀疏诱导正则化，$\matrix{Z}$ 的 $n$ 行中有 $h$ 非零行。</p>
<p>命题1证明了 \eqref{eq6} 中的非凸公式的参数与 \eqref{eq8} 中凸公式的参数之间存在一对一的映射关系。 因此，无需解决具有挑战性的非凸优化问题 \eqref{eq6} ，该问题还需要对多种启发式优化才能进行充分训练。 相反，可以解决凸问题 \eqref{eq8}，然后使用命题1中的映射来获得 \eqref{eq6} 的最优解。</p>
<h2 id="3-3-拓展到多维输出"><a href="#3-3-拓展到多维输出" class="headerlink" title="3.3 拓展到多维输出"></a>3.3 拓展到多维输出</h2><p>在上一节中，我们考虑了目标变量为标量的问题，即 $y_i \in \mathbb{R}$。 然而，对于某些问题，例如多类分类，目标变量可以是多维的，因此，我们现在将分析扩展到多个向量输出的问题，如下所示</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{\mathbf{w}_{1j}\in\Delta \\ \mathbf{w}_{2j}\in\mathbb{R}^d,\; \mathbf{w}_{3j}\in\mathbb{R}^c}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}\mathbf{w}_{1j}^\top\mathbf{X}_i\mathbf{w}_{2j}\mathbf{w}_{3j},\mathbf{y}_i\bigg)\\
    +\frac{\beta}{2}\bigg(\sum\limits^h_{j=1}\|\mathbf{w}_{2j}\|^2_2+\|\mathbf{w}_{3j}\|^2_1\bigg) 
\end{align*}
\tag{9}\label{eq9}</script><p>其中 $\mathbf{y}_i \in \mathbb{R}^{c}$ 和 $c$ 表示输出类别的数量。 请注意，这里我们在 $\mathbf{w}_{3j}$ 上应用 $\ell_1^2$范数，但这不会影响网络在实践中的性能。 然后，按照相同的推导在下一个结果中产生凸规划。</p>
<p><strong>定理2</strong><br>非凸优化问题 \eqref{eq9} 等价于以下凸优化问题</p>
<script type="math/tex; mode=display">
\min_{\mathbf{Z}_l\in\mathbb{R}^{n\times d}}\sum\limits^N_{i=1}\sum\limits^c_{l=1}\mathcal{L}(trace(\mathbf{Z}^\top_l\mathbf{X}_i),y_{il})+\beta\sum\limits^c_{l=1}\sum\limits^n_{k=1}\|\mathbf{z}_{lk}\|_2 
\tag{10}\label{eq10}</script><p>定理2表明等效的凸模型在输出索引 $l$ 上变得可分离，即，而不是 \eqref{eq8} 中的单个参数矩阵，这里我们有 $c$ 个参数矩阵由于，因为在非凸模型 \eqref{eq9} 中有 $c$ 个输出（详见表2）。 这也说明了网络中输出的数量直接控制了等效凸公式的超参数化水平。</p>
<h2 id="3-4-带FCN层的注意力网络"><a href="#3-4-带FCN层的注意力网络" class="headerlink" title="3.4 带FCN层的注意力网络"></a>3.4 带FCN层的注意力网络</h2><p>尽管 \eqref{eq5} 中的模型在各种应用中表现出有趣的特性，但它基本上是token矩阵 $\mathbf{X}$ 的线性函数。 因此，它很可能会遇到性能不足的问题，尤其是对于 NLP 中的一些具有挑战性的问题。 一系列论文 dong_attention 等也通过广泛的经验证据证实了 FCN 的重要性。 因此，在本节中，我们在 \eqref{eq5} 中将一个 FCN 层添加到我们的注意力模型中，并为这个新模型推导出一个等效的凸公式。</p>
<p>在这里，我们考虑以下优化问题</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{\mathbf{w}_{1j}\in\Delta \\ \mathbf{w}_{2j},\mathbf{w}_{3j}\in\mathbb{R}^c}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sigma\bigg(\sum\limits^h_{j=1}\mathbf{w}_{1j}^\top\mathbf{X}_i\mathbf{w}_{2j}\bigg)\mathbf{w}_{3j},\mathbf{y}_i\bigg)\\
    +\frac{\beta}{2}\bigg(\sum\limits^h_{j=1}\|\mathbf{w}_{2j}\|^2_2+\|\mathbf{w}_{3j}\|^2_1\bigg) 
\end{align*}
\tag{11}\label{eq11}</script><p>其中 $\sigma(\cdot)$ 是激活函数。</p>
<p><strong>定理3</strong><br>具有gated ReLU 激活的非凸优化问题 \eqref{eq11} 等价于以下凸优化问题：</p>
<script type="math/tex; mode=display">
\min_{\mathbf{Z}_{jl}\in\mathbb{R}^{n\times d}}\sum\limits^N_{i=1}\sum\limits^c_{l=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}1_{ij}trace(\mathbf{Z}^\top_{jl}\mathbf{X}_i),y_{il}\bigg)+\beta\sum\limits^c_{l=1}\sum\limits^h_{j=1}\sum\limits^n_{k=1}\|\mathbf{z}_{jlk}\|_2 
\tag{12}\label{eq12}</script><p>其中 $1_{ij} := 1\{\mathbf{u}_{1j}^\top \mathbf{X}_i \mathbf{u}_{2j} \geq 0\}$ 表示gated ReLU 激活的指示函数，这里 $\{\mathbf{u}_{1j},\mathbf{u}_{2j}\}_{j=1}^h$ 是固定向量，可以随机选择。</p>
<p>定理3意味着引入激活函数会进一步增加等效凸公式的超参数化水平。 准确地说，\eqref{eq12} 的参数比 \eqref{eq10} 多 $h$ 倍，如表2所示。</p>
<h1 id="4，数值实验"><a href="#4，数值实验" class="headerlink" title="4，数值实验"></a>4，数值实验</h1><p>在本节中，我们将展示实验结果，以证实我们在前几节中的理论。</p>
<p><strong>BERT中的师生设置</strong><br>我们首先考虑在 Hugging Face 库中使用预训练 BERT 模型的师生设置，即 bert-base-uncased。特别是，我们从 glue 数据集的 mrpc 子集中获取数据，传递给预训练的 BERT 模型，并将输入和输出激活保存在特定层中。然后，我们训练仅注意模型，即标准非凸自注意 \eqref{eq3}、替代非凸注意 \eqref{eq9} 和凸 \eqref{eq10}，从头开始分别前后使用这些激活函数，用前面得到的数据作为我们的训练数据集。</p>
<p>本节中的所有实验都是使用 Google Colab 上的单个 GPU 执行的，还使用相同的正则化系数 $\beta$ 和优化器，即 Adam，并通过对两种算法的验证数据集执行网格搜索来调整学习率和正则化系数。但是，请注意，我们没有对所有实验中的凸模型使用任何非凸优化启发式方法，例如层归一化和跳过连接。在图2中，我们使用从预训练 BERT 模型的第六层提取的数据绘制了目标值（即训练损失 + 正则化项）和测试损失，以秒为时间单位。我们观察到，凸训练方法实现的目标值几乎比标准的非凸训练小一个数量级，后者可能停留在局部最小值，这种训练也能增强泛化能力，即我们的凸训练方法比标准的非凸训练获得更低的测试损失。为了理解每个模型学习的函数，我们还分析了图3中的注意力图。在这里，标准的非凸训练无法学习底层模型并输出跨token的统一注意力图。然而，凸训练输出了一个与地面真实注意力图非常相似的注意力图，因此我们成功地学习了训练数据中的结构。因此，这些实验清楚地说明了凸训练方法在训练和测试中的有效性。</p>
<p><strong>算法数据集和 Grokking</strong><br>受 power2022grokking 中观察到的 grokking 现象的启发，我们接下来使用 \eqref{eq1} 中的自我注意机制，在算法数据集上，验证凸训练方法对标准Transformer网络的有效性。特别是，我们使用在 power2022grokking 中相同的设置，并使用 $\mod 97$ 和 $\mod 15$ 评估模块化除法运算的性能，一直训练框架直至达到 $99\%$ 的测试精度。</p>
<p>在图4中，我们首先复制了 power2022grokking 中的结果，并确认这里确实出现了 grokking 现象，即非凸曲线（图4a中紫色）在 $10^3$ 左右时达到 $100 \%$ 的训练精度，但是图4b中需要超过 $10^5$ 次迭代才能达到完美泛化。我们还比较了非凸和凸训练方法，凸训练方法比图 4b 中的非凸训练方法收敛到完美泛化精度要快 10 倍。此外，凸模型在图 4c 中产生的测试损失也显着降低，这意味着它对测试预测具有更高的置信度，因此比标准非凸训练更稳健。</p>
<p>我们注意到在上一节中，我们理论上只分析了单个注意力Transformer块。然而，由于深度或层数（表示为$L$）的良性影响已经在深度学习文献中得到了经验证明，我们还建议将我们的凸模型扩展到更深的设置。我们在 \eqref{eq12} 中堆叠凸Transformer层以获得任意深度的网络。在图5 中，我们比较了双层Transformer网络与一层网络的性能。在这里，我们观察到虽然增加一层可以显着改善凸模型，尤其是在优化速度方面，但它无法对非凸模型产生任何明显的差异。此外，我们在 $\mod 15$ 操作上运行算法，由于样本数量较少，这基本上是更具挑战性的任务。在这种情况下，如图6 所示，单层模型无法完美地学习底层任务，但我们的凸模型在测试精度和测试损失方面明显更好。通过将层数增加到四层，我们使两个模型都能达到完美的泛化精度。我们的深度模型比非凸模型更快地达到这个水平并且产生更低的测试损失。</p>
<p>接下来，我们根据经验分析我们的凸模型和标准非凸模型上的 grokking 现象。 为此，我们绘制了图7a 中每个实验达到 $99\%$ 测试准确度所需的迭代次数。 请注意，这里我们不包括 $\mod 15$ 案例的单层模型结果，因为在该案例中两个模型都未能实现完美泛化。 图7a 清楚地表明，我们的凸训练方法比标准非凸训练更快地收敛到 $99\%$ 的准确度水平。 因此，我们还减轻了 grokking 现象的影响，如图7b 所示，我们根据迭代次数量化了 grokking 的数量。 基于这个实验，我们还推测 grokking 现象主要归因于标准Transformer模型的高度非线性和非凸结构。</p>
<h1 id="5，结论"><a href="#5，结论" class="headerlink" title="5，结论"></a>5，结论</h1><p>在本文中，我们研究了注意力Transformer网络的正则化训练问题，并开发了一个凸分析框架来训练这些网络。 特别是，我们首先提出了自注意力机制的凸替代方案，然后将这种替代注意力机制的训练问题重新表述为凸优化问题。 由于我们的凸重构，我们全局优化网络参数而不需要任何类型的非凸优化启发式。 此外，我们重构的训练框架学到的函数是透明和可解释的。 更重要的是，重构的问题揭示了数据中跨token的稀疏性诱导正则化机制，这也更清楚地说明了结果函数的结构及其泛化属性。 然后，我们通过几个数值实验，凭经验验证了我们的凸训练方法相对于标准非凸训练的有效性。</p>
<p>我们还注意到，通过凸优化理论的视角分析Transformer网络是极其重要的，因为它可能会大大改善对这些网络的理解和优化。 然而，由于网络模型固有的非凸结构，这也非常具有挑战性。 据我们所知，本文是朝着这个方向迈出的第一步，因此存在一些局限性，希望在未来的工作中消除这些局限性。 具体来说，在本文中，我们主要关注凸分析的理论方面，并在一些小规模问题实例上对理论进行了实证验证。 我们希望后续论文能对我们的理论进行全面、大规模的实证验证。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>convexify</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention is all you need</title>
    <url>/2023/03/22/Transformer/Attention%20is%20all%20you%20need/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：Attention is all you need</p>
<p><a href="https://arxiv.org/abs/1706.03762">essay link</a></p>
<p><strong>Transformer的开山鼻祖~</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>显性序列转换模型基于包括编码器和解码器的复杂递归或卷积神经网络，性能最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构，即 Transformer，它完全基于注意力机制，完全摒弃了循环和卷积。对两项机器翻译任务的实验表明，这些模型在质量上更胜一筹，同时可并行化程度更高，并且需要的训练时间明显减少。我们的模型在 WMT 2014 英德翻译任务中达到了 28.4 BLEU，比现有的最佳结果（包括集成）提高了超过 2 BLEU。在 WMT 2014 英法翻译任务中，我们的模型在八个 GPU 上训练 3.5 天后建立了一个新的单模型，其最先进的 BLEU 分数为 41.8，训练成本约占当时最好模型成本的一小部分。。我们通过将 Transformer 成功应用于具有大量和有限训练数据的英语选区解析，证明 Transformer 可以很好地泛化到其他任务。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>循环神经网络，特别是长短期记忆和门控循环神经网络，已被牢固确立为序列建模和转换问题（如语言建模和机器翻译）的最先进方法。 此后，许多工作继续涌现，努力推动循环语言模型和编码器-解码器架构的边界。</p>
<p>循环模型通常沿着输入、输出序列的符号位置进行因子计算，将位置在计算时间内对齐到步骤中，它们生成一系列隐藏状态 $h_t$，作为前一个隐藏状态 $h_{t-1}$ 和位置 $t$ 的输入函数。这种固有的顺序性质排除了训练样本中的并行化，这在较长的序列长度下变得至关重要，因为内存限制限制了跨样本的批处理。最近的工作通过分解技巧和条件计算显著提高了计算效率，同时后者还提高了模型性能。 然而，顺序计算的基本限制仍然存在。</p>
<p>注意机制已成为序列建模和转换模型的各种任务中不可或缺的一部分，允许依赖性建模而不考虑它们在输入或输出序列中的距离。 然而，在大部分情况（除少数情况外）下，这种注意力机制都与循环网络结合使用。</p>
<p>本文提出了 Transformer，这是一种避免重复出现的模型架构，而是完全依赖注意力机制来绘制输入和输出之间的全局依赖关系。 Transformer 允许显著提高并行化，并且在八个 P100 GPU 上经过短短 12 小时的训练后，可以达到翻译质量的新水平。</p>
<h2 id="2，背景"><a href="#2，背景" class="headerlink" title="2，背景"></a>2，背景</h2><p>减少顺序计算的目标也构成了扩展神经 GPU 、ByteNet  和 ConvS2S 的基础，所有这些都使用卷积神经网络作为基本构建块，并行计算输入和输出所有位置的隐藏表示。在这些模型中，将来自任意两个输入或输出位置信号相关联所需的操作数量随着位置之间的距离而增加，ConvS2S 呈线性增长，By​​teNet 呈对数增长，这使得学习远距离位置之间的依赖关系变得更加困难。在 Transformer 中，这被减少到恒定数量的操作，尽管由于平均注意力加权位置而导致有效分辨率降低的代价，我们用 Multi-Head Attention 抵消了这种效果，如3.2部分所述.</p>
<p>自注意力，有时称为内部注意力，是一种将单个序列的不同位置相关联以计算序列表示的注意力机制。自注意力已成功用于各种任务，包括阅读理解、抽象摘要、文本蕴含和学习与任务无关的句子表示。</p>
<p>端到端记忆网络基于循环注意机制而不是序列对齐循环，并且已被证明在简单语言问答和语言建模任务上表现良好。</p>
<p>然而，据我们所知，Transformer 是第一个完全依靠自注意力来计算其输入和输出表示而不使用序列对齐 RNN 或卷积的转换模型。在接下来的部分中，我们将描述 Transformer，激发自注意力并讨论它相对于 neural_gpu 等模型的优势。</p>
<h2 id="3，模型结构"><a href="#3，模型结构" class="headerlink" title="3，模型结构"></a>3，模型结构</h2><p>大多数有竞争力的神经序列转换模型都具有编码器-解码器结构， 这里，编码器将符号表示的输入序列 $(x_1, …, x_n)$ 映射到连续表示的序列 $\mathbf{z} = (z_1, …, z_n)$。 给定 $\mathbf{z}$，解码器随后逐个生成符号输出序列 $(y_1,…,y_m)$。 在每个步骤中，模型都是自回归的，在生成下一个时将之前生成的符号作为附加输入使用。</p>
<p>Transformer 遵循这种整体架构，为编码器和解码器使用堆叠式自注意力和逐点全连接层，分别如图1的左半部分和右半部分所示。</p>
<h3 id="3-1-编码器和解码器的堆叠"><a href="#3-1-编码器和解码器的堆叠" class="headerlink" title="3.1 编码器和解码器的堆叠"></a>3.1 编码器和解码器的堆叠</h3><p><strong>编码器</strong><br>编码器由 $N=6$ 个相同的层堆叠而成，每一层都有两个子层。第一个是多头自注意力机制，第二个是简单的、按位置的全连接前馈网络。我们在两个子层中都使用了一个残差连接，然后是层归一化，即每个子层的输出为 $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$，其中 $\mathrm{Sublayer}(x)$ 是子层实现的函数层本身。为了促成这些残差连接，模型中的所有子层以及嵌入层都产生维度 $d_{model}=512$ 的输出。</p>
<p><strong>解码器</strong><br>解码器也由 $N=6$ 个相同的层堆叠而成。除了每个编码器层中的两个子层之外，解码器还插入了第三个子层，它对编码器堆栈的输出执行多头注意力。与编码器类似，我们在每个子层上使用残差连接，然后进行层归一化，还修改了解码器堆栈中的自我注意子层，以阻止当前位置关注后续位置信息。这种掩码与输出嵌入偏移一个位置的事实相结合，确保了对位置 $i$ 的预测只能依赖于位置小于 $i$ 的已知输出。</p>
<h3 id="3-2-注意力"><a href="#3-2-注意力" class="headerlink" title="3.2 注意力"></a>3.2 注意力</h3><p>注意力函数可以描述为将 $q$ 和 一组 $k-v$ 对映射到输出，其中 $q$、$k$、$v$ 和 $output$ 都是向量，$output$ 计算为 $v$ 的加权和，其中分配给每个 $v$ 的权重由 $q$ 与相应的 $k$ 的兼容性函数计算。</p>
<h4 id="3-2-1-归一化点积注意力"><a href="#3-2-1-归一化点积注意力" class="headerlink" title="3.2.1 归一化点积注意力"></a>3.2.1 归一化点积注意力</h4><p>将本文的特殊注意力称为“归一化点积注意力”（图2），包含 $q$，$k$ 的输入维度为 $d_k$， $v$ 的维度为 $d_v$。 计算 $q$ 与 $k$ 的点积，逐个除以 $\sqrt{d_k}$，然后应用 softmax 函数来获得 $v$ 的权重。</p>
<p>在实践中，我们同时计算一组 $q$ 的注意力函数，并将它们打包到一个矩阵 $Q$ 中，$k$ 和 $v$ 也一起打包到矩阵 $K$ 和 $V$ 中，我们将输出矩阵计算为：</p>
<script type="math/tex; mode=display">
\begin{align*}
   \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\end{align*}
\tag{1}</script><p>两种最常用的注意力函数是加法注意力和点积（乘法）注意力。点积注意力与我们的算法相同，除了 $\frac{1}{\sqrt{d_k}}$ 的归一化系数。 加法注意力使用具有单个隐藏层的前馈网络计算兼容性函数。 虽然两者在理论上的复杂性相似，但点积注意力在实践中速度更快且空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</p>
<p>虽然对于较小 $d_k$ 的情况，这两种机制的表现相似，但加法注意力在 $d_k$ 较大时因无需缩放而优于点积注意力。 我们怀疑对于较大的 $d_k$ 值，点积的幅度会变大，将 softmax 函数推入梯度极小的区域。 为了抵消这种影响，我们将点积缩放 $\frac{1}{\sqrt{d_k}}$。</p>
<h4 id="3-2-2-多头注意力"><a href="#3-2-2-多头注意力" class="headerlink" title="3.2.2 多头注意力"></a>3.2.2 多头注意力</h4><p>与使用 $d_{model}$ 维 $k$、$v$、$q$ 执行单个注意力头不同，我们发现通过可学习的线性投影分别在 $k$、$v$、$q$ 上执行  $h$ 次是有益的，相应投影后的向量维度为 $d_k$，$d_k $ 和 $d_v$ 。然后，在 $q$ 、 $k$ 和 $v$ 的每个投影版本上，我们并行执行注意力函数，产生 $d_v$ 维输出值，这些值之后被连接起来并再次投影，产生最终值，如图2所示。</p>
<p>多头注意力允许模型共同关注来自不同位置、不同表示子空间的信息，对于单个注意力头，均值化可以抑制这种情况。</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathrm{MultiHead}(Q, K, V) &= \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O\\
    \text{where}\;\mathrm{head_i} &= \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)
\end{align*}</script><p>其中投影指的是参数矩阵 $W^Q_i \in \mathbb{R}^{d_{model} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{model} \times d_k}$, $W ^V_i \in \mathbb{R}^{d_{model} \times d_v}$ 和 $W^O \in \mathbb{R}^{hd_v \times d_{model}}$。</p>
<p>本文中，使用了 $h=8$ 个并行注意力层或头，对于其中的每一个，都使用 $d_k=d_v=d_{model}/h=64$，由于每个头的维数减少，总计算成本与具有全维的单头注意力相似。</p>
<h4 id="3-2-3-本文模型中注意力的应用"><a href="#3-2-3-本文模型中注意力的应用" class="headerlink" title="3.2.3 本文模型中注意力的应用"></a>3.2.3 本文模型中注意力的应用</h4><p>Transformer 以三种不同的方式使用多头注意力：</p>
<ul>
<li><p>在编码解码器注意力的网络层中，$q$ 来自于前面的解码器层，记忆 $k$ 和 $v$ 来自于编码器的输出，这允许解码器中的每个位置都参与输入序列中的位置，模仿了序列到序列模型中典型的编码器-解码器注意力机制。</p>
</li>
<li><p>编码器包含自注意力层，在自注意力层中，所有的 $k$ 、 $v$ 和 $q$ 都来自同一个地方，在这种情况下，前一层的输出在编码器中，编码器中的每个位置都可以关注编码器上一层中的所有位置。</p>
</li>
<li><p>类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的左向信息流，以保持自回归特性，通过屏蔽掉（设置为 $-\infty$）softmax 输入中对应于非法连接的所有值，在缩放点积注意力内部实现这一点。见图2。</p>
</li>
</ul>
<h3 id="3-3-基于位置的前馈网络"><a href="#3-3-基于位置的前馈网络" class="headerlink" title="3.3 基于位置的前馈网络"></a>3.3 基于位置的前馈网络</h3><p>除了注意力子层之外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，它相同地应用于每个对应位置。 这由两个线性变换组成，中间有一个 ReLU 激活。</p>
<script type="math/tex; mode=display">
\begin{align*}
   \mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2
\end{align*}
\tag{2}</script><p>虽然线性变换在不同位置是相同的，但它们在层与层之间使用不同的参数。 另一种描述方式是使用内核大小为 1 的两个卷积，输入和输出的维数为 $d_{model}=512$，内层的维数为 $d_{ff}=2048$。</p>
<h3 id="3-4-Embeddings-和-Softmax"><a href="#3-4-Embeddings-和-Softmax" class="headerlink" title="3.4 Embeddings 和 Softmax"></a>3.4 Embeddings 和 Softmax</h3><p>与其他序列转换模型类似，我们使用学习嵌入将输入标记和输出标记转换为维度 $d_{model}$ 的向量，还使用常用学习中的线性变换和 softmax 函数将解码器输出转换为预测的下一个标记概率。在我们的模型中，我们在两个嵌入层和 pre-softmax 线性变换之间共享权重矩阵，在嵌入层中，我们将这些权重乘以 $\sqrt{d_{model}}$。</p>
<h3 id="3-5-位置编码"><a href="#3-5-位置编码" class="headerlink" title="3.5 位置编码"></a>3.5 位置编码</h3><p>由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于标记在序列中的相对或绝对位置信息。为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入嵌入中，位置编码与嵌入具有相同的维度 $d_{model}$，因此可以将两者相加。位置编码有很多选择，可参见 JonasFaceNet2017等。</p>
<p>在这项工作中，我们使用不同频率的正弦和余弦函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
    PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{model}}) \\
    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{model}})
\end{align*}</script><p>其中 $pos$ 是位置，$i$ 是维度，也就是说，位置编码的每个维度对应一个正弦曲线，波长形成从 $2\pi$ 到 $10000 \cdot 2\pi$ 的几何级数。 我们选择这个函数是因为我们假设它可以让模型很容易地学习相对位置信息，因为对于任何固定偏移 $k$，$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性函数。</p>
<p>我们还尝试使用学习的位置嵌入代替，发现这两个版本产生几乎相同的结果（参见表3），选择正弦版本是因为它可以让模型推断出比训练期间遇到的序列长度更长的序列长度。</p>
<h2 id="4，为什么要自注意力"><a href="#4，为什么要自注意力" class="headerlink" title="4，为什么要自注意力"></a>4，为什么要自注意力</h2><p>在本节中，我们将自注意力层的各个方面与其他模型比较，这些模型通常用于将一个可变长度符号表示序列 $(x_1, …, x_n)$ 映射到另一个等长序列 $(z_1 , …, z_n)$, 其中 $x_i, z_i \in \mathbb{R}^d$, 例如典型序列转换编码器或解码器中的隐藏层，为了激励我们使用自注意力，我们考虑了三个必要条件。</p>
<p>一个是每层的总计算复杂度。另一个是可以并行化的计算量，用所需的最小顺序操作数来衡量。</p>
<p>第三个是网络中远程依赖关系之间的路径长度。学习长程依赖性是许多序列转换任务中的关键挑战，影响学习这种依赖性能力的一个关键因素是前向和后向信号在网络中必须经过的路径长度。输入和输出序列中任何位置组合之间的路径越短，就越容易学习远程依赖。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。</p>
<p>如表1中第3列所述，自注意力层将所有位置与恒定数量的顺序执行操作连接起来，而循环层需要 $O(n)$ 个顺序操作。<br>就计算复杂度而言，当序列长度 $n$ 小于表示维度 $d$ 时，自注意力层比循环层更快，这是句子表示中最常见的情况，通常在最先进的机器翻译模型中出现，例如词片和字节对表示。为了提高涉及非常长序列任务的计算性能，可以将自注意力限制为仅考虑以相应输出位置为中心的输入序列中大小为 $r$ 的邻域，会将最大路径长度增加到 $O(n/r)$，我们计划在未来的工作中进一步研究这种方法。</p>
<p>内核宽度 $k &lt; n$ 的单个卷积层不会连接所有输入和输出位置对，为了增加网络中任意两个位置之间的最长路径，在连续内核的情况下需要一堆 $O(n/k)$ 卷积层，或者在空洞卷积的情况下需要 $O(log_k(n))$。卷积层通常比递归层复杂 $k$ 倍，但是可分离卷积大大降低了复杂性，达到 $O(k \cdot n \cdot d + n \cdot d^2)$。然而，即使 $k=n$，可分离卷积的复杂度也等于自注意力层和逐点前馈层的组合，这是本文模型的方法。</p>
<p>作为附带的好处，自注意力可以产生更多可解释的模型，从我们的模型中检查注意力分布，并在附录中展示和讨论示例。 不仅单个注意力头清楚地学会执行不同的任务，而且多注意力头似乎表现出了与句子的句法和语义结构相关的行为。</p>
<h2 id="5，训练"><a href="#5，训练" class="headerlink" title="5，训练"></a>5，训练</h2><p>本节描述了我们模型的训练机制。</p>
<h3 id="5-1-训练数据和批量化"><a href="#5-1-训练数据和批量化" class="headerlink" title="5.1 训练数据和批量化"></a>5.1 训练数据和批量化</h3><p>我们在标准的 WMT 2014 英德数据集上进行了训练，该数据集包含大约 450 万个句子对。 句子使用字节对编码进行编码，它具有约 37000 个标记的共享source-target词汇表。 对于英语-法语，我们使用了明显更大的 WMT 2014 英语-法语数据集，该数据集由 3600 万个句子组成，并将标记拆分为 32000 个单词词汇表。 句子对按近似序列长度分批在一起，每个训练批次包含一组句子对，其中包含大约 25000 个source标记和 25000 个target标记。</p>
<h3 id="5-2-硬件和策略"><a href="#5-2-硬件和策略" class="headerlink" title="5.2 硬件和策略"></a>5.2 硬件和策略</h3><p>我们在一台配备 8 个 NVIDIA P100 GPU 的机器上训练我们的模型，使用整篇论文中描述的关于超参数的基础模型，每个训练步骤大约需要 0.4 秒。 我们对基本模型进行了总共 100,000 步或 12 小时的训练，对于大模型，（在表3的底行描述），步进时间为 1.0 秒。 大模型训练了 300,000 步（3.5 天）。</p>
<h3 id="5-3-优化器"><a href="#5-3-优化器" class="headerlink" title="5.3 优化器"></a>5.3 优化器</h3><p>我们使用了 Adam 优化器，其中 $\beta_1=0.9$、$\beta_2=0.98$ 和 $\epsilon=10^{-9}$。 我们根据以下公式在训练过程中更新学习率：</p>
<script type="math/tex; mode=display">
lrate=d^{-0.5}_{model}\cdot \min(step\_num^{-0.5},step\_num\cdot warmup\_steps^{-0.5})</script><p>这对应于第一个 $warmup_steps$ 训练步骤线性增加学习率，然后按步数的平方根按反比例减少学习率，本文使用了 $warmup_steps=4000$。</p>
<h3 id="5-4-正则化"><a href="#5-4-正则化" class="headerlink" title="5.4 正则化"></a>5.4 正则化</h3><p>我们在训练期间采用三种类型的正则化：</p>
<p><strong>Residual Dropout</strong><br>我们将 dropout 应用于每个子层的输出，然后再将其添加到子层输入并进行归一化。 此外，我们将 dropout 应用于编码器和解码器堆栈中的嵌入和位置编码的总和。 对于基本模型，我们使用 $P_{drop}=0.1$ 的比率。</p>
<p><strong>Label Smoothing</strong><br>在训练期间，我们采用了值为 $\epsilon_{ls}=0.1$ 的标签平滑。 这会伤害困惑度，因为模型会变得更加不确定，但会提高准确性和 BLEU 分数。</p>
<h2 id="6，结果"><a href="#6，结果" class="headerlink" title="6，结果"></a>6，结果</h2><h3 id="6-1-机器翻译"><a href="#6-1-机器翻译" class="headerlink" title="6.1 机器翻译"></a>6.1 机器翻译</h3><p>在 WMT 2014 英德翻译任务中，大Transformer模型中的 表格2中的Transformer (big)）比之前报告的最佳模型（包括集成模型）高出超过 $2.0$ 个BLEU ，建立了一个新的最先进的 BLEU 分数 $28.4$。该模型的配置列在表3的最后一行，在 8 块 P100 GPU 上训练花费了 3.5 天，甚至我们的基础模型也超过了之前发布的所有模型和集成，其训练成本仅为任何竞争模型的一小部分。</p>
<p>在 WMT 2014 英法翻译任务中，我们的大模型获得了 41.0 的 BLEU 分数，优于之前发布的所有单一模型，训练成本不到之前最好模型的 1/4。为英语到法语训练的 Transformer（大）模型使用 dropout 比例 $P_{drop}=0.1$，而不是 $0.3$。</p>
<p>对于基本模型，我们平均使用了最后 5 个训练checkpoints点获得的单一模型，这些检查点以每 10 分钟的间隔写入。对于大型模型，我们对最后 20 个checkpoints进行了平均。使用束搜索，束大小为 $4$，长度惩罚为 $\alpha=0.6$。这些超参数是在开发集上进行实验后选择的，在推理期间将最大输出长度设置为输入长度 + $50$，但尽可能提前终止。</p>
<p>表2总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较，通过乘以训练时间、使用的 GPU 数量和每个 GPU 的持续单精度浮点容量估计值来估计用于训练模型的浮点运算数。</p>
<h3 id="6-2-模型变体"><a href="#6-2-模型变体" class="headerlink" title="6.2 模型变体"></a>6.2 模型变体</h3><p>为了评估 Transformer 不同组件的重要性，我们以不同的方式改变基础模型，测量开发集 newstest2013 上英语到德语翻译的性能变化。使用了上一节中描述的束搜索，但没有检查点平均，在表3中展示了这些结果。</p>
<p>在表3的行(A)中，我们改变注意力头的数量以及注意力 $k$ 和 $v$ 的维度，保持计算量不变，如第3.2.2节所述。虽然单头注意力比最佳设置差 0.9 BLEU，但如果头太多，质量也会下降。</p>
<p>在表3的行(B)中，我们观察到减小注意力 $k$ 的维度大小 $d_k$ 会损害模型质量，这表明确定兼容性并不容易，比点积更复杂的兼容性函数可能是有益的。我们在 (C) 和 (D) 行中进一步观察到，正如预期的那样，模型越大越好，dropout 对避免过度拟合非常有帮助。在行 (E) 中，我们用学习的位置嵌入替换我们的正弦位置编码，并观察到与基本模型几乎相同的结果。</p>
<h3 id="6-3-英文选区解析"><a href="#6-3-英文选区解析" class="headerlink" title="6.3 英文选区解析"></a>6.3 英文选区解析</h3><p>为了评估 Transformer 是否可以推广到其他任务，我们对英语选区解析进行了实验。这项任务提出了具体的挑战：输出受到强大的结构约束，并且比输入长得多。此外，RNN 序列到序列模型无法在小数据范围内获得最先进的结果。</p>
<p>我们在 Penn Treebank 的华尔街日报 (WSJ) 部分训练了一个 $d_{model} = 1024$ 的 4 层Transformer，大约 40K 个训练句子，还在半监督环境中训练它，使用的数据是来自大约 1700 万个句子的高置信度和 BerkleyParser 语料库。我们为 WSJ only 设置使用了 16K tokens 的词汇表，为半监督设置使用了 32K tokens 的词汇表。</p>
<p>我们只进行了少量实验来选择 dropout，注意力和残差， 22 节开发集上的学习率和光束大小，所有其他参数与英语-德语基础翻译模型的参数保持不变。在推理过程中，我们将最大输出长度增加到输入长度 + $300$，对于仅 WSJ 和半监督设置，我们使用 $21$ 和 $\alpha=0.3$ 的光束大小。</p>
<p>我们在 Table4 中的结果表明，尽管缺乏针对特定任务的调整，我们的模型表现出奇的好，产生的结果比之前报告的所有模型都要好，循环神经网络语法除外。</p>
<p>与 RNN 序列到序列模型相比，Transformer 优于 BerkeleyParser，即使仅在 WSJ 40K 句子训练集上进行训练也是如此。</p>
<h2 id="7，结论"><a href="#7，结论" class="headerlink" title="7，结论"></a>7，结论</h2><p>在这项工作中，我们提出了 Transformer，这是第一个完全基于注意力的序列转换模型，用多头自注意力取代了编码器-解码器架构中最常用的循环层。</p>
<p>对于翻译任务，Transformer 的训练速度明显快于基于循环层或卷积层的架构。在 WMT 2014 英德和 WMT 2014 英法翻译任务中，我们都达到了新的水平。在前一个任务中，我们最好的模型甚至优于所有先前报告的集成模型，还通过英语选区分析实验表明我们模型的更广泛且适用性更好。</p>
<p>我们对基于注意力的模型的未来感到兴奋，并计划将它们应用到其他任务中，计划将 Transformer 扩展到涉及文本以外的输入和输出模式的问题，并研究局部的、受限的注意力机制，以有效处理图像、音频和视频等大型输入和输出，减少生成顺序是我们的另一个研究目标。</p>
<p>我们用于训练和评估模型的代码可在 <a href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a> 获得。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Fourier Transformer</title>
    <url>/2022/11/16/Transformer/Transformer%20with%20Fourier/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：Transformer with Fourier Integral Attentions</p>
<p><a href="https://arxiv.org/abs/2206.00206">essay link</a></p>
<p><strong>无参数核估计，Fourier积分原理等跟Transformer关联的文章，似懂非懂啊，老天爷！</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>多头注意力赋予了Transformer最近的成功，这些最先进的模型在序列建模及其他方面效果显著。这些注意力机制计算q和k之间的成对点积，使用未归一化的Gaussian核并假设q遵循混合Gaussian分布，但实践中并不能保证这个假设成立。</p>
<p>相对应的，本文首先将 Transformer 中的注意力解释为非参数核回归，然后提出了FourierFormer，一类将点积内核用新颖的广义Fourier积分内核取代的新Transformer。与点积核需要选择一个好的协方差矩阵来捕捉数据特征的依赖性不同，广义Fourier积分核可以自动捕捉这种依赖性，无需调试协方差矩阵。我们从理论上证明了本文提出的Fourier积分核可以有效地逼近任何k和q的分布。</p>
<p>与具有点积注意力的传统Transformer相比，FourierFormers 获得了更好的精度并减少了注意力头之间的冗余。我们经验性地证实了 FourierFormers 在包括语言建模和图像分类在内的各种实际应用中优于基线Transformer的优势。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>Transformers是强大的神经网络，在机器学习的许多领域都取得了巨大的成功并成为跨领域广泛应用的最先进模型，视频，点云，以及蛋白质序列。除了在监督学习任务上的出色表现外，Transformer 还可以有效地将学习到的知识从预训练任务转移到有限制或无监督的新任务。Transformers 的核心是点积自注意力，它主要是导致 Transformer 模型成功的原因。</p>
<p>这种点积自注意力通过估计给定标记相对于所有其他标记的相对重要性来学习输入序列中标记之间的自我对齐，然后它将每个标记转换为其他标记的特征表示的加权平均值，其中权重与每对标记之间的重要性分数成正比。 自注意力中的重要性分数使一个标记能够关注序列中的其他标记，从而捕获上下文表示。</p>
<h3 id="1-1-自注意力"><a href="#1-1-自注意力" class="headerlink" title="1.1 自注意力"></a>1.1 自注意力</h3><p>给定输入序列$\mathbf{X}:=[x_1,\cdots x_N]^T\in \mathbb{R}^{N\times D_x} $是$\mathbf{N}$维特征向量，自注意力计算$\mathbf{X}$给定下的$\mathbf{H}$输出序列。</p>
<p><strong>Step 1将输入序列映射到不同子空间</strong><br>输入序列是$\mathbf{X}$，会通过三个线性变换转换为查询矩阵$\mathbf{Q}$，关键矩阵$\mathbf{K}$和值矩阵$\mathbf{V}$，也就是：</p>
<script type="math/tex; mode=display">\mathbf{Q}=\mathbf{X}\mathbf{W}^T_Q;\quad \mathbf{K}=\mathbf{X}\mathbf{W}^T_K;\quad \mathbf{V}=\mathbf{X}\mathbf{W}^T_V</script><p>其中，$\mathbf{W}_Q , \mathbf{W}_K \in \mathbb{R}^{D\times D_x} $，有 $\mathbf{W}_V \in\mathbb{R}^{D_v\times D_x} $ 是权重矩阵。相应的$q,k,v$的矩阵表示如下：</p>
<script type="math/tex; mode=display">\mathbf{Q}:=[q_1,\cdots q_N]^T, \mathbf{K}:=[k_1,\cdots k_N]^T,  \mathbf{V}:=[v_1,\cdots v_N]^T</script><p><strong>Step 2 用加权平均计算输出</strong><br>输出$\mathbf{H}:=[h_1,\cdots h_N]^T $可以表示为：</p>
<script type="math/tex; mode=display">\mathbf{H}=softmax(\mathbf{Q}\mathbf{K}^{T}/\sqrt{\mathbf{D}})\mathbf{V := \mathbf{A}\mathbf{V}} \tag{1}</script><p>其中，softmax函数是作用在矩阵$(\mathbf{Q}\mathbf{K}^T)/\sqrt{D} $的每一行，对每一个查询向量$q_i,\; i=1,\cdots N $，方程(1)可以写成如下的向量形式来求每个输出向量$h_i$：</p>
<script type="math/tex; mode=display">h_{i}=\sum\limits_{j=1}^{N}softmax(q_{i}^{T}k_{j}/\sqrt{\mathbf{D}})v_{j} := \sum\limits_{j=1}^{N}a_{ij}v_{j} \tag{2}</script><p>矩阵$\mathbf{A} \in\mathbb{R}^{N\times N},\; a_{ij}\in\mathbf{A}\; i,j=1\cdots N$分别为注意力矩阵和主力已分值。</p>
<p>自注意力就是用方程(1)和方程(2)来计算的，也叫点乘注意力或者softmax注意力。在本文中，将使用这种注意力的Transformer称为具有点积注意力或点积Transformer的基线Transformer。 训练后的注意力矩阵 $\mathbf{A}$ 的结构决定了自注意力捕获每个标记的上下文表示能力。</p>
<p><strong>多头注意力</strong><br>每个输出序列 $\mathbf{H}$ 形成一个注意力头，多头注意力连接多个头来计算最终输出。令 $H$ 为头的数量，$\mathbf{W}^O \in \mathbb{R}^{HD_v \times HD_v}$ 为输出的投影矩阵。 多头注意力定义为：</p>
<script type="math/tex; mode=display">MultiHead(\{\mathbf{Q},\mathbf{K},\mathbf{V} \}^{H}_{i=1}) = Concat(\mathbf{H}_{1},\cdots \mathbf{H}_{H})\mathbf{W}^{O}</script><p>注意力机制的容量和学习不同句法和语义关系的能力决定了transformer的成功。然而，方程(1)和(2)意味着点积注意力会假设特征$q_i$和$k_j$是独立的。因此，点积注意力无法捕捉这些特征之间的相关性，从而限制了其表示能力并抑制了Transformer在实际任务中的性能，因为无法保证可以从复杂数据中学习到彼此独立的特征。捕获特征 $q_i$ 和 $k_j$ 之间相关性的一种解决方案是将协方差矩阵引入点积注意力的公式中，代价是计算复杂度显著增加，此外，选择好的协方差矩阵也很困难。</p>
<h3 id="1-2-贡献"><a href="#1-2-贡献" class="headerlink" title="1.2 贡献"></a>1.2 贡献</h3><p>在本文中，我们首先建立了自注意力和非参数核回归之间的对应关系。 在这种自注意力的新视角下，我们解释了点积自注意力的局限性，即它可能无法捕获查询向量与关键向量之间的相关性特征。 </p>
<p>然后，我们利用广义Fourier积分定理，它可以自动捕捉这些相关性，并为非参数回归问题推导出广义Fourier积分估计量。 使用这种新的密度估计器，我们提出了 FourierFormer，这是一种新型的Transformer，可以捕获自注意力中$q$和$k$的相关性。 总之，我们的贡献是三方面的：</p>
<ul>
<li>1, 我们从解决非参数核回归问题中推导出了自注意力的公式，从而为研究和进一步发展自注意力提供了非参数回归解释。</li>
<li>2, 我们为非参数回归问题开发了广义Fourier积分估计器，并为这些估计器提供了理论支持。</li>
<li>3, 我们提出了 FourierFormer，它的注意力使用广义Fourier积分估计器来更有效地捕获$q$和$k$的相关性。</li>
</ul>
<p>最后，我们凭经验证明，在包括 WikiText 语言建模和 ImageNet 图像分类在内的各种任务上，FourierFormer 比点积注意力的基线Transformer获得了明显更好的准确度。 我们还在实验中证明 FourierFormer 有助于减少注意力头之间的冗余。</p>
<p><strong>组织</strong><br>我们将本文的结构如下：在Section2中，我们提出了自我注意和非参数核回归之间的对应关系。 在 Section3中，我们讨论了广义Fourier积分估计器并定义了 FourierFormer。 我们在 Section4中验证并实证分析了 FourierFormer 的优势。 我们在 Section5中讨论相关工作。 论文以Section6的总结来结束。 附录中提供了技术证明和更多实验细节。</p>
<p><strong>记号</strong><br>对于任何 $\mathbf{N} \in \mathbb{N}$，我们表示 $[N] = \{1, 2, \ldots, N\}$。 对于任何 $D \geq 1$，我们用$\mathbb{L}_1(\mathbb{R}^D) $ 来表示 $ \mathbb{R}^D $ 上实值函数空间中的全体可积函数。 </p>
<p>对于任意两个序列 $\{a_N\}_{N \geq 1},\{ b_N \}_{N \geq 1} $， 我们用 $a_N = \mathcal{O}(b_N)$ 表示在所有 $N \geq 1$ 的情况下，都有 $a_{N} \leq C b_N$ ，其中 $C$ 是一些通用常数。</p>
<h2 id="2，自注意力的无参数回归解释"><a href="#2，自注意力的无参数回归解释" class="headerlink" title="2，自注意力的无参数回归解释"></a>2，自注意力的无参数回归解释</h2><p>在本节中，我们建立了自注意力和无参数核回归之间的联系。 特别是，我们将方程(2)中的自注意力推导为非参数核回归，其中向量 $k_j$ 和向量 $v_j$ 分别是训练输入和训练目标 , 而查询向量 $q_i$ 和输出向量 $h_i$ 形成了一组新的输入及其对应的待估计目标，且有 $i,j=1,\cdots,N$。 一般来说，我们将训练集 $\{ k_j, v_j\}$ for $j \in [N]$ 视为来自以下非参数回归模型：</p>
<script type="math/tex; mode=display">v_{j}=f(k_{j})+\varepsilon_{j} \tag{3}</script><p>其中 $\varepsilon_1,\cdots,\varepsilon_N$ 是独立的噪声，使得 $\mathbb{E}(\varepsilon_j) = 0$。 此外，我们考虑一个随机设计，其中$k$向量 $k_1,k_2,\cdots ,k_N$ 是来自 $p$ 为密度函数分布的采样。 通过混用符号，我们还将 $p$ 表示为联合密度，其中$k$和$v$向量 $(v_1, k_1), \cdots, (v_N, k_N)$ 也是来自 $p$ 为联合密度函数分布的采样。 在这里，$f$ 是一个真实但未知的函数，我们需要估计它。</p>
<p><strong>Nadaraya-Watson 验证</strong><br>我们估计函数 $f$ 的方法是基于 Nadaraya–Watson 的非参数核回归方法。 特别是，从非参数回归模型，对于所有 $ j \in [N]$，我们有 $\mathbb{E}[v_j|k_j] = f(k_j)$ 。 因此，在给定$k$向量的情况下，估计$v$向量的条件分布就足够了。 给定$k$向量的密度函数 $p$ 以及$k$和$v$向量的联合密度 $p$，对于任何从模型生成的向量对 $(v, k)$ 我们有：</p>
<script type="math/tex; mode=display">\mathbb{E}=\int_{\mathbb{R}^{D}}v\cdot p(v|k)dv=\int \frac{v\cdot p(v,k)}{p(k)}dv \tag{4}</script><p>条件期望的公式表明，只要我们能够估计联合密度函数$p(v, k)$和边际密度函数$p(v)$，我们就能够获得条件期望的估计，从而获得函数$f$。 这种方法被广泛称为 Nadaraya-Watson 的非参数核回归方法。</p>
<p><strong>核密度估计</strong><br>为了估计 $p(v, k)$ 和 $p(k)$，我们采用核密度估计方法。 特别是，通过使用带宽为 $\sigma$ 的各向同性Gaussian核，我们有以下 $p(v, k)$ 和 $p(k)$ 的估计量：</p>
<script type="math/tex; mode=display">\hat{p}_{\sigma}(v,k)=\frac{1}{N}\sum\limits_{j=1}^{N}\varphi_{\sigma}(v-v_{j})\varphi_{\sigma}(k-k_{j});\\ \hat{p}_{\sigma}(k)=\frac{1}{N}\sum\limits_{j=1}^{N}\varphi_{\sigma}(k-k_{j}) \tag{5}</script><p>其中 $\varphi_\sigma(.)$ 是对角协方差矩阵 $\sigma^2 I_D$ 的各向同性多元Gaussian密度函数。 给定核密度估计器，我们得到函数 $f$ 的以下估计：</p>
<script type="math/tex; mode=display">\hat{f}_{\sigma}(k)=\int_{\mathbb{R}^{D}}\frac{v\cdot \hat{p}_{\sigma}(v,k)}{\hat{p}_{\sigma}(k)}dv=\int_{\mathbb{R}^{D}}\frac{v\cdot \sum\limits_{j=1}^{N}\varphi_{\sigma}(v-v_{j})\varphi_{\sigma}(k-k_{j})}{\sum\limits_{j=1}^{N}\varphi_{\sigma}(k-k_{j})}dv \\ =\frac{\sum\limits_{j=1}^{N}\phi_{\sigma}(k-k_{j})\int v\cdot \varphi_{\sigma}(v-v_{j})dv}{\sum\limits_{j=1}^{N}\varphi_{\sigma}(k-k_{j})}=\frac{\sum\limits_{j=1}^{N}v_{j}\varphi_{\sigma}(k-k_{j})}{\sum\limits_{j=1}^{N}\varphi_{\sigma}(k-k_{j})} \tag{6}</script><p><strong>自注意力和无参数回归的关系</strong><br>通过将查询向量 $ q_i $ 代入方程(6) 中的函数  $\widehat{f}_\sigma $，我们得到</p>
<script type="math/tex; mode=display">\widehat{f}_{\sigma}(q_{i})=\frac{\sum\limits_{j}^{N}v_{j}exp(-||q_{i}-k_{j}||^{2}/2\sigma^{2})}{\sum\limits_{j}^{N}exp(-||q_{i}-k_{j}||^{2}/2\sigma^{2})} \\ =\frac{\sum\limits_{j}^{N}v_{j}exp[-(||q_{i}||^{2}+||k_{j}||^{2})/2\sigma^{2}]exp(q_{i}k_{j}^{T}/\sigma^{2})}{\sum\limits_{j}^{N}exp[-(||q_{i}||^{2}+||k_{j^{'}}||^{2})/2\sigma^{2}]exp(q_{i}k_{j}^{T}/\sigma^{2})}\tag{7}</script><p>如果我们进一步假设 $k_j$ 向量被归一化，通常在实践中这样做是为了稳定transformers的训练，方程(7)中$\hat{f}_\sigma (q_i)$的值为：</p>
<script type="math/tex; mode=display">\hat{f}_{\sigma}(q_{i})=\frac{\sum\limits_{j}^{N}v_{j}exp(q_{i}k_{j}^{T}/\sigma^{2})}{\sum\limits_{j}^{N}exp(q_{i}k_{j^{'}}^{T}/\sigma^{2})}=\sum\limits_{j}^{N}softmax(q_{i}^{T}k_{j}/\sigma^{2})v_{j} \tag{8}</script><p>当我们选择 $\sigma^2 = \sqrt{D}$，其中 $D$ 是 $q_i$ 和 $k_j$ 的维数，方程(8)和方程(2)中的自注意力等价，即$\widehat{f}_\sigma (q_i) = h_i$。 因此，我们已经证明自注意力使用各向同性Gaussian核执行非参数回归。</p>
<p><strong>注意1</strong><br>$k_j$ 被归一化的假设是为了恢复Transformer中成对点积的注意力。 一般来说，这个假设是不必要的。 事实上，方程(7)中的各向同性Gaussian核比成对点积注意力的方程(8)中的点积核更可取，因为前者是 Lipschitz 而后者不是。 Lipschitz约束有助于提高模型的鲁棒性并稳定模型训练。</p>
<p><strong>自注意力局限</strong><br>根据我们非参数回归的解释，自注意力源自使用各向同性Gaussian核进行核密度估计和非参数回归估计，这可能无法捕捉 $q_i$ 和 $k_j$中$D$维特征的复杂关联。 使用具有密集协方差矩阵的多元Gaussian核有助于捕捉这种相关性，然而，选择好的协方差矩阵具有挑战性且效率低下。 在下一节中，我们将讨论Fourier积分估计器及其作为计算自注意力的内核以克服这些限制。</p>
<h2 id="3，FourierForm：广义Fourier积分原理的Transformer"><a href="#3，FourierForm：广义Fourier积分原理的Transformer" class="headerlink" title="3，FourierForm：广义Fourier积分原理的Transformer"></a>3，FourierForm：广义Fourier积分原理的Transformer</h2><p>在下文中，我们介绍了能够捕捉$q$和$k$之间复杂特征关系的广义积分定理。 然后，我们将这些定理应用于密度估计和非参数回归问题，我们还建立了这些估计器的收敛速度。 鉴于这些密度估计器，我们引入了一个名为 FourierFormer 的新型Transformer家族，它将广义Fourier积分定理集成到标准Transformer的点积注意力步骤中。</p>
<h3 id="3-1-广义Fourier积分方法和应用"><a href="#3-1-广义Fourier积分方法和应用" class="headerlink" title="3.1 广义Fourier积分方法和应用"></a>3.1 广义Fourier积分方法和应用</h3><p>Fourier积分定理是数学中一个漂亮的结果，最近被用于非参数模式聚类、反卷积问题和生成建模，它是Fourier变换和Fourier逆变换的组合。 特别是，对于任何函数 $p \in \mathbb{L}_{1}(\mathbb{R}^{D})$，Fourier积分定理由下式给出：</p>
<script type="math/tex; mode=display">p(k)=\frac{1}{(2\pi)^{D}}\int_{\mathbb{R}^{D}}\int_{\mathbb{R}^{D}}cos(s^{T}(k-y))p(y)dyds \\ =\frac{1}{\pi^{D}}\lim_{\mathrm{R}\rightarrow\infty}\int_{\mathbb{R}^{D}}\prod_{j=1}^{D}\frac{sin(R(k_{j}-y_j))}{k_{j}-y_{j}}p(y)dy \tag{9}</script><p>其中，$k=(k_1,\cdots k_D), y=(y_1,\cdots y_D) $，上式表明：</p>
<script type="math/tex; mode=display">p_{\mathrm{R}}(k):=\frac{1}{\pi^{D}}\int_{\mathbb{R}^{D}}\prod_{j=1}^{D}\frac{sin(R(y_j-k_{j}))}{y_{j}-k_{j}}p(y)dy</script><p>$p_{\mathrm{R}}(k)$可用于估计函数$p$。</p>
<p><strong>用Gaussian核做Fourier积分的优点</strong><br>估计器 $p_R$ 有两个重要的好处：（i）即使 $p$ 是非常复杂和高维的函数，它也可以自动保留 $p$ 内的相关结构。与基于多元Gaussian核构建的标准核估计器形成鲜明对比，我们需要在多元Gaussian核中选择好的协方差矩阵来保证这样的估计器工作良好。我们注意到，由于标准的 softmax Transformer 是基于多元Gaussian核构建的，因此在点积Transformer中选择好的协方差矩阵的问题是不可避免的； (ii) 当 $R \to \infty$ 时，估计器 $p_{R}$ 中 sinc 核的乘积不会衰减到点质量，它与多元Gaussian核估计量截然不同，后者在协方差矩阵变为 0 时收敛到一个点质量，这表明 $p_R$ 是函数 $p$ 的非平凡估计量。最后，在密度估计和非参数回归问题中Fourier积分优于Gaussian核的这些好处的详细说明，也是我们刚刚证明这些好处与Transformer中的自注意力有关，可以在第 8 节中找到。</p>
<p><strong>广义Fourier积分估计</strong><br>借用Fourier积分估计器 $p_R$ 的上述优点，在本文中，我们想考虑该估计器的泛化，命名为广义Fourier积分估计器，由下式给出：</p>
<script type="math/tex; mode=display">p_{\mathrm{R}}^{\phi}(k)=\frac{\mathrm{R}^{D}}{A^{D}}\int_\mathrm{R^D}\prod^D_{j=1}\phi(\frac{sin(R(y_j-k_j))}{R(y_j-k_j)})p(y)dy \tag{10}</script><p>其中 $A : = \int_{\mathbb{R}} \phi \left(\frac{\sin(z)}{z}\right) dz$ 和 $\phi: \mathbb{R} \to \mathbb {R}$ 是给定的函数。 当 $\phi(k) = k$ 对所有 $k \in \mathbb{R}^D$ 时，广义Fourier积分估计量 $p_{R}^{\phi}$ 变为Fourier积分估计器 $p_R$。 在函数 $\phi$ 的适当条件下，估计器$p_R^{\phi}$收敛到真函数$p$，即，</p>
<script type="math/tex; mode=display">p(k)=\lim_{R\rightarrow \infty}p^{\phi}_R (k)=\lim_{R\rightarrow\infty}\frac{\mathrm{R}^{D}}{A^{D}}\int_\mathrm{R^D}\prod^D_{j=1}\phi(\frac{sin(R(y_j-k_j))}{R(y_j-k_j)})p(y)dy \tag{11}</script><p>我们将上述极限命名为广义Fourier积分定理。 此外，估计器 $p_R^{\phi}$ 也继承了Fourier积分估计器 $p_R$ 的类似优点。 因此，我们将使用广义Fourier积分定理作为构建密度估计器和非参数回归估计器的构建块，这对于开发 Section3.2 中的 FourierFormer 至关重要。</p>
<h4 id="3-1-1-用广义Fourier积分原理估计密度"><a href="#3-1-1-用广义Fourier积分原理估计密度" class="headerlink" title="3.1.1 用广义Fourier积分原理估计密度"></a>3.1.1 用广义Fourier积分原理估计密度</h4><p>我们首先将广义Fourier积分定理应用于密度估计问题。 为了简化表示，我们假设 $k_1, k_2, \ldots, k_N \in \mathbb{R}^D$ 是来自一个分布的样本，其密度函数为 $p$，其中 $D \geq 1$ 是维度。 受广义Fourier积分定理的启发，我们得到以下$p$的广义Fourier密度估计$p_{N,R}^{\phi}$如下：</p>
<script type="math/tex; mode=display">p^\phi_{N,R}(k):= \frac{R^D}{NA^D}\sum\limits^N_{j=1}\prod^{D}_{j=1}\phi\frac{sin(R(k_j-k_{ij}))}{R(k_j-j_{ij})} \tag{12}</script><p>其中 $A = \int_{\mathbb{R}} \phi \left(\frac{\sin(z)}{z}\right) dz$ 和 $k_i = (k_{i1}, \ ldots, k_{iD})$ 对于所有 $i \in [N]$。 为了量化广义Fourier密度估计 $p_{n,R}^{\phi}$ 和真实密度 $p$ 之间的误差，我们利用均方积分平方误差 MISE，给出如下表达式：</p>
<script type="math/tex; mode=display">MISE(p^\phi_{N,R},p):=\int_\mathrm{R^D}(p^\phi_{N,R}(k)-p(k))^2dk \tag{13}</script><p>我们从 $p_{n,R}^{\phi}$ 和 $p$ 之间的 MISE 的以下界限开始。</p>
<p><strong>定理1</strong><br>假设 $\int_{\mathbb{R}} \phi(\sin(z)/z)z^j dz = 0$ 对于所有 $j \in [m]$ 以及 $\int_{\mathbb{ R}} |\phi(\sin(z)/z)| |z|^{m + 1} dz &lt; \infty$ 对于一些 $m \in \mathbb{N}$。 然后，存在取决于 $d$ 和 $A$ 的通用常数 $C$ 和 $C^{‘}$，使得：</p>
<script type="math/tex; mode=display">MISE(p^\phi_{N,R},p)\leq\frac{C}{R^{m+1}}+\frac{C^{'}R^D}{N}</script><p>定理1证明见附录B.1，且以下论述是有序的。 首先，通过选择 $R$ 来平衡定理1中 MISE 边界的偏差和方差，我们得到最优 $R$ 为 $R = \mathcal{O}(N^{1 /(D + m + 1)})$，选择 $R$，$p_{N, R}^{\phi}$ 的 MISE 率为 $\mathcal{O}(N^{-(m+1)/(D + m + 1) })$。 其次，当 $l \geq 4$ 和 $z \in \mathbb{R}$ 的 $\phi(z) = z^{l}$ 时，定理1中的假设满足 $m = 1$。 在这种情况下，$p_{N,R}^{\phi}$ 的 MISE 率为 $\mathcal{O}(N^{-2/(D+2)})$。 但是，当 $\phi(z) = z^{l}$ 和 $l \in \{1,2,3\}$ 时，这些假设不满足，这是由于当前定理1证明技术的限制，是基于估计量 $p_{n,R}^{\phi}$ 的Taylor展开。</p>
<p>为了解决Taylor展开技术的局限性，我们利用Fourier分析中的 Plancherel 定理来建立当 $\phi(z) = z^{l}$ 和 $l \in \{1,2,3\}$时 $p_{N,R}^{\phi}$ 的 MISE 率。 这种设置的理论分析细节在附录A中。</p>
<h3 id="3-2-FourierFormer：Transformer带Fourier注意力"><a href="#3-2-FourierFormer：Transformer带Fourier注意力" class="headerlink" title="3.2 FourierFormer：Transformer带Fourier注意力"></a>3.2 FourierFormer：Transformer带Fourier注意力</h3><p>受广义Fourier积分定理中函数相关结构的保留以及密度估计器的理论保证的启发，在本节中，我们采用了Section2中自注意力的非参数回归解释并在 Section3.2.1 中提出广义Fourier非参数回归估计器，我们还建立了该估计器的收敛特性。 然后，基于广义Fourier非参数回归估计器，我们在 Section3.2.2 中开发了 Fourier 注意力及其对应的 FourierFormer。</p>
<h4 id="3-2-1-广义Fourier积分原理的非参数化回归"><a href="#3-2-1-广义Fourier积分原理的非参数化回归" class="headerlink" title="3.2.1 广义Fourier积分原理的非参数化回归"></a>3.2.1 广义Fourier积分原理的非参数化回归</h4><p>我们现在讨论广义Fourier积分定理在非参数回归设置中的应用，即我们假设 $(v_1, k_1), \ldots, (v_N, k_N)$ 是来自以下非参数回归模型的样本：</p>
<script type="math/tex; mode=display">v_j:=f(k_j)+\varepsilon_j</script><p>其中 $\varepsilon_1, \ldots, \varepsilon_N$ 是独立的噪声，使得 $\mathbb{E}(\varepsilon_j) = 0$ 和$k$向量 $k_1, k_2、\ldots、k_N$ 来自密度函数为 $p$ 的样本。 给定广义Fourier密度估计器，根据第2节中的参数，基于广义Fourier密度估计器的函数 $f$ 的 Nadaraya-Watson 估计器为：</p>
<script type="math/tex; mode=display">f_{N,R}(k):=\frac{\sum\limits^N_{i=1}v_i\prod^D_{j=1}\phi(\frac{sin(R(k_i-k_{ij}))}{R(k_i-k_{ij})})}{\sum\limits^N_{i=1}\prod^D_{j=1}\phi(\frac{sin(R(k_i-k_{ij}))}{R(k_i-k_{ij})})} \tag{14}</script><p>方程(14)中的广义Fourier非参数回归估计器$f_{N,R}$与方程(6)中估计器$\widehat{f}_{\sigma}$ 的主要区别是估计器 $f_{N,R}$ 利用广义Fourier密度估计器来估计给定$k$和$v$向量的条件分布，而不是如 $\widehat{f}_{\sigma}$ 中的各向同性Gaussian核密度估计器 。 </p>
<p>正如我们在 Section3 中强调的那样，广义Fourier密度估计器的一个重要好处是它可以捕获$k$和$v$向量特征的复杂依赖关系，而Gaussian核需要具有良好的协方差矩阵来做到这一点，这在实践中计算成本很高。</p>
<p>我们现在有以下结果，建立了 $f_{N,R}$ 的均方误差 (MSE)。</p>
<p><strong>定理2</strong><br>假设 $\int_{\mathbb{R}} \phi \left(\frac{\sin(z)}{z}\right) z^j dz = 0$ 对于所有 $1 \leq j \leq m $，且 $\int_{\mathbb{R}} \left|\phi \left(\frac{\sin(z)}{z}\right)\right| |z|^j dz &lt; \infty$ 对于任何 $m + 1 \leq j \leq 2m + 2$ 对于一些 $m \in \mathbb{N}$ 成立。 那么，对于任何 $k \in \mathbb{R}^D$，存在一般常数 $C_1, C_2, C_3, C_4$ 使得以下成立 ：</p>
<script type="math/tex; mode=display">\mathbb{E}[(f_{N,R}(k)-f(k))^2]\leq(\frac{C_1}{R^{2(m+1)}}+\frac{(f(k)+C_2)R^D}{N})/(p^2(kJ(R)))</script><p>其中 $J(R) = 1 - \frac{1}{p^2(k)} ( \frac{C_3}{R^{2(m+1)}} + \frac{ C_4 R^d \log (N R)}{N}) $。 这里，外部期望是关于$k$向量 $k_1,\ldots,k_N$ 和噪声 $\varepsilon_1,\ldots,\varepsilon_N$ 的。</p>
<p>定理2证明见附录B.3，且关于定理2的一些评论是有序的。 首先，通过选择 $R$ 来平衡非参数广义Fourier估计器 $f_{N,R}$ 的 MSE 范围内的偏差和方差，我们得到最优半径 $R$ 为 $R = \mathcal{O}(N^{\frac{1}{2(m + 1) + D}})$。 选择最优半径 $R$，$f_{N,R}$ 的比率为 $\mathcal{O}(N^{-\frac{2(m+1)}{D + 2(m +1)}})$。 其次，当 $\phi(z) = z^{l}$ 对 $l \geq 6$ 时，定理2的函数 $\phi$ 的假设满足 $m = 1$。 在这种情况下，$f_{N,R}$ 的比率变为 $\mathcal{O}(N^{-\frac{4}{D + 4}})$。 在附录A中，我们还提供了当 $\phi(z) = z^l$ 对于某些 $l \leq 5$ 时的 $f_{N,R}$ 的比率，其中包括原始Fourier积分定理。</p>
<h4 id="3-2-2-FourierFormer"><a href="#3-2-2-FourierFormer" class="headerlink" title="3.2.2 FourierFormer"></a>3.2.2 FourierFormer</h4><p>给定方程(14)中的广义Fourier非参数回归估计器 $f_{N,R}$，通过将$q$值 $q_1,\ldots,q_N$ 插入该函数 ，我们得到Fourier注意力的以下定义：</p>
<p><strong>定义1(Fourier注意力)</strong><br>Fourier注意力是一种多头注意力，它使用广义Fourier非参数回归估计器 $f_{N,R}$ 进行非参数回归。 Fourier注意力的输出 $\hat{h}_i$计算公式为</p>
<script type="math/tex; mode=display">\hat{h}_i:=f_{N,R}(q_i)=\frac{\sum\limits^N_{i=1}v_i\prod^D_{j=1}\phi(\frac{sin(R(q_{ij}-k_{ij}))}{R(q_{ij}-k_{ij})})}{\sum\limits^N_{i=1}\prod^D_{j=1}\phi(\frac{sin(R(q_{ij}-k_{ij}))}{R(q_{ij}-k_{ij})})} \quad \forall i\in [N]\tag{15}</script><p>给定定义1中的 Fourier注意力，我们接着给出 FourierFormer 的定义如下。</p>
<p><strong>定义2(FourierFormer)</strong><br>FourierFormer 是一种Transformer，它使用Fourier注意力来捕获输入序列中，标记之间的依赖关系，以及每个标记中特征之间的相关性。</p>
<p><strong>注意2 Fourier核的非负性</strong><br>通过第3.1.1节中的广义Fourier积分定理进行的密度估计不需要广义Fourier密度估计器是非负的。 然而，根据经验，我们观察到负密度估计器会导致训练FourierFormer的不稳定。 因此，在 FourierFormer 中，我们选择函数 $\phi$ 作为非负函数，以强制密度估计为非负。 特别是，我们选择 $\phi$ 为 $\phi(x) = x^{2m}$ 形式的幂函数，其中 $m$ 是一个正整数。 请注意，当 $m=2$ 和 $m=4$ 时，我们的广义Fourier积分估计器中的核是著名的 Fejer-de la Vallee Poussin 和 Jackson-de la Vallee Poussin 核。</p>
<h3 id="3-3-Fourier注意力的高效实现"><a href="#3-3-Fourier注意力的高效实现" class="headerlink" title="3.3 Fourier注意力的高效实现"></a>3.3 Fourier注意力的高效实现</h3><p>Fourier内核在 Pytorch 开发的 C++/CUDA 扩展中高效实现。 这个想法类似于函数 cdist ，它计算两个行向量集合中每对之间的 p 范数距离。 在我们的例子中，我们的目标是计算在定义1中代表Fourier注意力的核函数。 这个实现的核心是下面的Fourier度量函数$d_f$：</p>
<script type="math/tex; mode=display">d_f(q_i,k_j)=\prod^D_{d=1}\phi(\frac{sin(R(q_{id}-k_{jd}))}{(q_{id}-k_{jd})})</script><p>我们直接将 $d_f$ 实现为 torch.autograd.Function，其中我们提供了一种计算前向和后向函数的有效方法（$d_f$ 和 $d_f$ 的梯度）。 虽然前向函数的实现是直截了当的，但后向函数更加棘手，因为我们需要优化代码来一次计算与变量 $q$、$k$ 和 $R$ 的所有梯度有关的 $d_f$。 我们可以通过利用 GPU 架构和利用缩减技术来开发具有高度并行计算的后向函数。 计算时间与函数 cdist 相当； 因此，我们的 FourierFormer 实现在计算上是高效的。</p>
<h2 id="4，实验结果"><a href="#4，实验结果" class="headerlink" title="4，实验结果"></a>4，实验结果</h2><p>在本节中，我们在数值上证明了 FourierFormer 在两个大规模任务上优于基线点积Transformer的优势：WikiText-103 上的语言建模和 ImageNet 上的图像分类。 我们的目标是证明：(i) FourierFormer 在具有不同数据模式的各种实际任务中实现比基线 Transformer 更好的精度，并且 (ii) FourierFormer 与基线 Transformer 相比有助于减少头部冗余。</p>
<p>在本章节，我们将 FourierFormers 与相同配置的基线点积Transformer进行比较。 在所有实验中，我们将Fourier注意力中的常数 $R$ 设为可学习的标量，并设置选择函数 $\phi(x) = x^4 $。 我们所有的结果都是使用不同种子进行 5 次运行的平均值。 附录C中提供了有关模型和训练的更多详细信息。 我们还在附录D中提供了额外的实验结果。</p>
<h3 id="4-1-WikiText-103的语言建模"><a href="#4-1-WikiText-103的语言建模" class="headerlink" title="4.1 WikiText-103的语言建模"></a>4.1 WikiText-103的语言建模</h3><p>WikiText-103 是来自 Wikipedia 的文章集合，它们具有长期的上下文依赖关系。 训练集由大约 28K 数量的文章组成，其中包含 1.03M 的运行词；这对应于大约 3600 个单词的文本块。 验证集和测试集分别有 $218K$ 和 $246K$ 的运行词。 每篇文章都包含 60 篇文章和大约 268K 数量的单词。 我们的实验遵循标准设置并将训练数据拆分为$L$-word 独立的长片段。 为了评估，我们使用批量大小为 1，并使用大小为 $L$ 的滑动窗口处理文本序列。 最后一个位置用于计算除了第一段的困惑度（PPL），所有位置都参与评估。</p>
<p><strong>模型和基线</strong><br>我们的实现基于 schlag2021linear 的公共代码，我们在实验中使用他们的中小模型。 特别是对于小模型，key、value和query维度设置为128，训练和评估上下文长度设置为256。对于中模型，key、value和query维度设置为256，并且训练和评估上下文长度设置为 384。在两种配置中，head 的数量都是 8，前馈层维度是 2048，层数是 16。</p>
<p><strong>结果</strong><br>我们在 Table1 中报告了 FourierFormer 与带点积注意力的基线Transformer的验证和测试困惑度 (PPL)。 FourierFormers 在中小模型配置中都比基线获得了更好的 PPL。对于小模型配置，FourierFormer 相对于基线的改进是验证中的 1.29 PPL 和测试中的 1.44 PPL。对于中模型配置，这些改进是验证中的 1.39 PPL 和测试中的 1.59 PPL。这些结果表明，FourierFormer 相对于基线点积Transformer的优势随着模型的大小而增长。这符合我们的预期，因为更大的模型具有更大的$q$和$k$维度，例如本实验中配置中等的语言模型的$q$和$k$的维度为 256，而配置较小的语言模型为 128。由于 FourierFormer 的优势来自于 FourierFormer 可以捕获$q$和$k$之间的特征相关性，因此$q$和$k$维度越大，FourierFormer 的优势就越大。</p>
<h3 id="4-2-ImageNet上的图像分类任务"><a href="#4-2-ImageNet上的图像分类任务" class="headerlink" title="4.2 ImageNet上的图像分类任务"></a>4.2 ImageNet上的图像分类任务</h3><p><strong>数据集和指标</strong><br>ImageNet 数据集包含 1.28M 的训练图像和 50K 的验证图像。 对于这个基准，模型学习在 1000 个类别中预测输入图像的类别。 报告了前 1 和前 5 的分类精度。</p>
<p><strong>模型和基线</strong><br>我们使用 DeiT-tiny 模型，有 12 个transformer 层，每层 4 个注意力头，模型维度为 192。为了训练模型，我们遵循与基线相同的设置和配置。</p>
<p><strong>结果</strong><br>我们在 Table2 中总结了我们的结果。 与语言建模实验相同，对于这个图像分类任务，配备 FourierFormer 的 Deit 模型在 top-1 和 top-5 精度上都显着优于基线 Deit 点积Transformer。 该结果表明 FourierFormer 优于基线点积Transformer的优势适用于不同的数据模式。</p>
<h3 id="4-3-FourierFormer帮助减少多头冗余"><a href="#4-3-FourierFormer帮助减少多头冗余" class="headerlink" title="4.3 FourierFormer帮助减少多头冗余"></a>4.3 FourierFormer帮助减少多头冗余</h3><p>为了研究注意力头之间的多样性，给定 WikiText-103 语言建模任务训练的模型，我们计算每层头之间的平均 $\mathcal{L}_2$ 距离。 我们在 Table3 中显示了头部之间距离的层平均均值和方差。 Table3 中的结果表明，FourierFormer 在注意力头之间获得了比具有点积注意力的基线Transformer更大的 $\mathcal{L}_2$ 距离，因此有助于减少头冗余。<br>请注意，我们对两个模型都使用了 Section4.1 中指定的小模型配置。</p>
<h2 id="5，相关工作"><a href="#5，相关工作" class="headerlink" title="5，相关工作"></a>5，相关工作</h2><p><strong>Transformer中的注意力机制理解</strong><br>最近的工作试图从不同的角度了解Transformer的注意力。tsai2019transformer 认为注意力是在输入上用内核平滑，扩展这种内核方法，katharopoulos2020transformers,choromanski2021rethinking, wang2020linformer 线性化了点积注意力中的 softmax 核，并提出了一系列具有线性计算和内存复杂度的高效Transformer。cao2021choose 然后表明这些线性Transformer与 Petrov-Galerkin投影相当，表明点积注意力中的 softmax 归一化是足够的，但不是必需的。</p>
<p>其他作品通过常/偏微分方程提供了对Transformer注意力的理解，包括 lu2019understanding, sander2022sinkformers。此外，tang2021probabilistic等将Transformer中的注意力与Gaussian混合模型联系起来。一些工作还将注意力机制与图形模型中的图结构学习和消息传递联系起来，如wang2018non。我们的工作重点是推导自注意力和非参数核回归之间的联系，并探索更好的回归估计器，例如广义Fourier非参数回归估计器，以提高Transformer的性能。</p>
<p><strong>Transformer中的冗余</strong><br>dalvi2020analyzing等表明预训练Transformer中的神经元和注意力头是冗余的，并且可以在应用于下游任务时移除。 通过研究预训练网络中的上下文嵌入，已经证明从这些冗余模型中学习到的表示是高度各向异性的。 此外，sanh2019distilbert 等采用知识蒸馏和稀疏近似来提高Transformer的效率。 我们的 FourierFormer 是对这些方法的补充，可以与它们结合使用。</p>
<h2 id="6，结论"><a href="#6，结论" class="headerlink" title="6，结论"></a>6，结论</h2><p>在本文中，我们建立了非参数核回归与 Transformer 中的自注意力之间的对应关系。然后，我们开发了广义Fourier积分估计器并提出了 FourierFormer，这是一类新的Transformer，它使用广义Fourier积分估计器来构建它们的注意力，以有效地捕获$q$和$k$向量中的特征之间的相关性。我们从理论上证明了广义Fourier积分估计器的近似保证，并在精度和头部冗余减少方面通过点积注意验证了 FourierFormer 相对于基线Transformer的优势。将鲁棒内核合并到 FourierFormer 的非参数回归框架中以增强模型在数据扰动和对抗性攻击下的鲁棒性是很有趣的。 </p>
<p>FourierFormer 的一个限制是它仍然具有与具有点积注意力的基线Transformer相同的二次计算和内存复杂性。我们将实现线性计算和内存复杂性的 FourierFormer 线性版本的开发留作未来的工作。值得注意的是，FourierFormer 没有潜在的负面社会影响。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Fourier</tag>
      </tags>
  </entry>
  <entry>
    <title>数学专业课学习资源</title>
    <url>/2022/11/15/math/learning_video_source/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>数学专业课的学习资源，大多是视频，后续会不断增加其他形式~</p>
<p><strong>多年前翘的课，早晚要还回去~</strong><br><span id="more"></span></p>
<h2 id="高等数学"><a href="#高等数学" class="headerlink" title="高等数学"></a>高等数学</h2><ul>
<li><a href="https://www.bilibili.com/video/av84881516/?vd_source=587c86d59d7db86347d0f0b0deaa5031">朱建民，国防科大</a></li>
<li><a href="https://www.bilibili.com/video/BV1FU4y1p7f3/?vd_source=587c86d59d7db86347d0f0b0deaa5031">樊顺厚，天津工业大学</a></li>
<li><a href="https://www.bilibili.com/video/BV1Eb411u7Fw/?spm_id_from=333.999.0.0&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">宋浩</a></li>
</ul>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1aW411Q7x1/?spm_id_from=333.999.0.0&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">宋浩</a></li>
<li><a href="https://www.bilibili.com/video/av58706273/?vd_source=587c86d59d7db86347d0f0b0deaa5031">麻省理工</a></li>
<li><a href="https://www.bilibili.com/video/BV1ys411472E/?spm_id_from=333.999.0.0">3Blue1Brown，线性代数的本质</a></li>
</ul>
<h2 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1UW411k7Jv/?spm_id_from=333.999.0.0">宋浩</a></li>
<li><a href="https://www.bilibili.com/video/BV1qW411N7FU/?spm_id_from=333.999.0.0">3Blue1Brown，微积分的本质</a></li>
</ul>
<h2 id="概率论与梳理统计"><a href="#概率论与梳理统计" class="headerlink" title="概率论与梳理统计"></a>概率论与梳理统计</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1ot411y7mU/?spm_id_from=333.999.0.0">宋浩</a></li>
</ul>
<h2 id="高等代数"><a href="#高等代数" class="headerlink" title="高等代数"></a>高等代数</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1cy4y1V79E/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">宋浩</a></li>
<li><a href="https://www.bilibili.com/video/BV1jR4y1M78W/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">北大丘维声</a></li>
<li><a href="https://www.bilibili.com/video/BV1mJ411r7ZB/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">复旦谢启鸿</a></li>
<li><a href="https://www.bilibili.com/video/BV16Z4y1U7oU/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">麻省理工MIT</a></li>
</ul>
<h2 id="数学分析"><a href="#数学分析" class="headerlink" title="数学分析"></a>数学分析</h2><ul>
<li><a href="https://www.bilibili.com/video/BV15v411g7VP/?vd_source=587c86d59d7db86347d0f0b0deaa5031">陈纪修</a></li>
</ul>
<h2 id="实变函数"><a href="#实变函数" class="headerlink" title="实变函数"></a>实变函数</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1o7411N7qx/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">陈闯老师，程其襄，仅实变部分</a></li>
</ul>
<h2 id="泛函分析"><a href="#泛函分析" class="headerlink" title="泛函分析"></a>泛函分析</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1zW411s7o8/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">孙炯老师</a></li>
</ul>
<h2 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1g7411b7r2/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">华中科技大学</a></li>
</ul>
<h2 id="数学物理方程"><a href="#数学物理方程" class="headerlink" title="数学物理方程"></a>数学物理方程</h2><p><a href="https://www.bilibili.com/video/BV117411m78E/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">马逍也</a></p>
<h2 id="抽象代数，微分几何，拓扑"><a href="#抽象代数，微分几何，拓扑" class="headerlink" title="抽象代数，微分几何，拓扑"></a>抽象代数，微分几何，拓扑</h2><ul>
<li><a href="https://space.bilibili.com/410564174?spm_id_from=333.337.0.0">无尽沙砾</a></li>
</ul>
<h2 id="常微分方程"><a href="#常微分方程" class="headerlink" title="常微分方程"></a>常微分方程</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1Tr4y1w7Ef/?spm_id_from=333.337.search-card.all.click">宋浩</a></li>
<li><a href="https://www.bilibili.com/video/BV1bx411s7pb/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">川大数院</a></li>
<li><a href="https://www.bilibili.com/video/BV1qk4y1R7W8/?spm_id_from=333.337.search-card.all.click">童雯雯</a></li>
</ul>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>source</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读清单来了，下半部分</title>
    <url>/2022/11/15/tools/readinglist_2023_half2/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>想给自己立flag了，能戒掉游戏不？能多读两本书吗？</p>
<p><strong>从2023就开始吧~</strong><br><span id="more"></span></p>
<h2 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h2><ul>
<li>叶圣陶《文心》</li>
<li>韦海生《教自己写作》</li>
<li>毕飞宇《小说课》</li>
<li>刘军强《写作是门手艺》</li>
<li>商务印书馆《新华字典》</li>
</ul>
<h2 id="散文诗歌类"><a href="#散文诗歌类" class="headerlink" title="散文诗歌类"></a>散文诗歌类</h2><ul>
<li>汪曾祺《人间草木》</li>
<li>史铁生《我与地坛》</li>
<li>余秋雨《文化苦旅》</li>
<li>余光中《听听那冷雨》</li>
<li>季羡林《无问东西》</li>
<li>林清玄《境明，千里皆明》</li>
<li>贾平凹《南北笔记》</li>
<li>梁实秋《梁实秋生活美学》</li>
<li>杨绛《将饮茶》</li>
<li>徐志摩《翡冷翠的一夜》</li>
<li>张枣《春秋来信》</li>
<li>林徽因《林徽因文集》</li>
<li>余秀华《月光落在左手上》</li>
</ul>
<h2 id="小说类"><a href="#小说类" class="headerlink" title="小说类"></a>小说类</h2><ul>
<li>钱钟书《围城》</li>
<li>余华《活着》</li>
<li>路遥《平凡的世界》</li>
<li>汪曾祺《受戒》</li>
<li>莫言《生死疲劳》</li>
<li>沈从文《边城》</li>
<li>萧红《呼兰河传》</li>
<li>宗璞《野葫芦引》</li>
<li>白先勇《孽子》</li>
<li>张爱玲《金锁记》</li>
<li>陈忠实《白鹿原》</li>
<li>老舍《我这一辈子》</li>
<li>巴金《寒夜》</li>
<li>刘震云《一句顶一万句》</li>
<li>陈春成《夜晚的潜水艇》</li>
</ul>
<h2 id="历史普本类"><a href="#历史普本类" class="headerlink" title="历史普本类"></a>历史普本类</h2><ul>
<li>傅乐成《中国通史》</li>
<li>马伯庸《长安的荔枝》</li>
<li>刘和平《大明王朝》</li>
<li>孙皓晖《大秦帝国》</li>
<li>黄仁宇《万历十五年》</li>
<li>唐浩明《曾国藩》</li>
<li>二月河《雍正皇帝》</li>
<li>顾城《南明史》</li>
</ul>
<h2 id="思想认知类"><a href="#思想认知类" class="headerlink" title="思想认知类"></a>思想认知类</h2><ul>
<li>王阳明《传习录》</li>
<li>鲁迅文集</li>
<li>王小波《沉默的大多数》</li>
<li>刘慈欣《三体》</li>
<li>韦启昌《人生的智慧》</li>
<li>蒋勋《孤独六讲》</li>
<li>梁文道《常识》</li>
<li>马克.奥勒留《沉思录》</li>
<li>瑞.达丽欧《原则》</li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>readinglist</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读清单来了，上半部分</title>
    <url>/2022/11/15/tools/readinglist_2023_half1%20copy/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>想给自己立flag了，能戒掉游戏不？能多读两本书吗？</p>
<p><strong>从2023就开始吧~</strong><br><span id="more"></span></p>
<ul>
<li><p>《平凡的世界》 路遥 著<br>  高度浓缩了中国西北农村的历史变迁过程</p>
</li>
<li><p>《三体》 刘慈欣 著<br>  “中国科幻文学里程碑”式的作品</p>
</li>
<li><p>《万历十五年》 黄仁宇 著<br>  原来历史可以这样写，原来历史是如此的有趣、复杂、丰富</p>
</li>
<li><p>《如何阅读一本书》 [美]莫提默.J.艾德勒 查尔斯.范多伦 著<br>  一本指导人们如何阅读的名作</p>
</li>
<li><p>《围城》 钱钟书 著<br>  “围在城里的人想逃出来，城外的人想冲进去”</p>
</li>
<li><p>《红楼梦》 曹雪芹 著<br>  一部具有高度思想性和高度艺术性的伟大作品</p>
</li>
<li><p>《四世同堂》 老舍 著<br>  再现了那个时代普通人在国破家亡之际缓慢、痛苦而又艰难的觉醒历程</p>
</li>
<li><p>《从一到无穷大》 [美] 乔治.伽莫夫 著<br>  是当今世界最有影响的科普经典名著之一</p>
</li>
<li><p>《全球通史：从史前史到21世纪》 [美] 斯塔夫里阿诺斯 著<br>  从全球角度考察世界各地区人类文明的产生和发展</p>
</li>
<li><p>《资本论》 [德]马克思 著<br>  一部融哲学、政治经济学、科学社会主义为一体的马克思主义百科全书</p>
</li>
<li><p>《月亮和六便士》[英]毛姆 著<br>  “即使只靠一支画笔，沦陷于赤贫之中，我孤独而炽热的灵魂也无法和画画分开”</p>
</li>
<li><p>《学会提问》 [美]尼尔.布朗 斯图尔特.基利 著<br>  一本授人以渔的智慧之书，独立思考者的七点</p>
</li>
<li><p>《沈从文文集》沈从文 著<br>  他笔下的世界，给我们的心灵开辟了一方净土</p>
</li>
<li><p>《美的历程》 李泽厚 著<br>  中国美学的经典之作，如斯感性，如斯亲切</p>
</li>
<li><p>《我们仨》 杨绛 著<br>  注以简洁而沉重的语言，回忆一家三口那些快乐而艰难、爱与痛交织的日子。</p>
</li>
<li><p>《数学之美》 吴军 著<br>  把高深的数学原理讲得更加通俗易懂，让非专业读者也能领略数学的魅力。</p>
</li>
<li><p>《繁华》 金宇澄 著<br>  “建立了一座与南方有关与城市有关的人情世态的博物馆”</p>
</li>
<li><p>《一句顶一万句》 刘震云 著<br>  从小人物的命运变迁中去寻找破解孤独的钥匙，被誉为中国版《百年孤独》</p>
</li>
<li><p>《牵风记》 徐怀中 著<br>  将现实主义与浪漫主义相结合的方式描写战争，以特别的胆略探寻战火中的爱恋与人性</p>
</li>
<li><p>《主角》 陈彦 著<br>  一部动人心魄的命运之书，一个以中国古典审美方式讲述的“中国故事”</p>
</li>
<li><p>《爱的艺术》[美]艾里希.弗洛姆 著<br>  爱是一门艺术，想要掌握这门艺术的人，需要有这方面的知识并付出努力去学习</p>
</li>
<li><p>《应物兄》李洱 著<br>  借鉴经史子集的叙述方式，记叙了中国当代知识分子的言谈和举止</p>
</li>
<li><p>《乡土中国》费孝通 著<br>  学界共认的中国乡土社会传统文化和社会结构理论研究的代表作</p>
</li>
<li><p>《理想国》 [古希腊]柏拉图 著<br>  人类正义问题的开山之作，涉及柏拉图思想体系的各个方面</p>
</li>
<li><p>《战争与和平》[俄]列夫.托尔斯泰 著<br>  一部百科全书式的恢弘史诗，罗曼.罗兰称之为“伟大作家的伟大作品”</p>
</li>
<li><p>《解忧杂货店》 [日]东野圭吾 著<br>  不是推理小说，却更扣人心弦</p>
</li>
<li><p>《呼兰河传》 萧红 著<br>  一部充满童心、诗趣和灵感的“回忆式”长篇小说</p>
</li>
<li><p>《白鹿原》 陈忠实 著<br>  一部渭河平原的雄奇史诗，一幅中国农村的斑斓画卷</p>
</li>
<li><p>《长恨歌》 王安忆 著<br>  讲述一个女人四十年的情与爱，一座城市四十年的常与变</p>
</li>
<li><p>《霍乱时期的爱情》[哥伦比亚]加西亚.尔克斯 著<br>  讲述了一段跨越半个多世纪的爱情史诗，穷尽了所有爱情的可能性</p>
</li>
<li><p>《经济学原理》[美]曼昆 著<br>  一本享誉全球的经济学经典教科书。内容简明、语言生动、诙谐</p>
</li>
<li><p>《时间简史》[英]史蒂芬.霍金 著<br>  “我们生存在一个奇妙无比的宇宙中”</p>
</li>
<li><p>《百年孤独》[哥伦比亚]加西亚.马尔克 著<br>  魔幻现实主义文学的代表作，反映了拉丁美洲一个世纪以来风云变幻的历史</p>
</li>
<li><p>《人类简史：从动物到上帝》[以色列]尤瓦尔.赫拉利 著<br>  宏大的人类简史，也见微知著，让人类重新审视自己</p>
</li>
<li><p>《小王子》[法]圣一埃克苏佩里 著<br>  这本书是献给长成了大人的从前那个孩子</p>
</li>
<li><p>《思考，快与慢》[美]丹尼尔.卡尼曼 著<br>  指导我们如何作出更好的选择，以及如何应用不同技巧来避免那些常见的思维失误</p>
</li>
<li><p>《季羡林谈人生》 季羡林 著<br>  季羡林的思想像一本厚厚的百科全书，而他的品格却像一目见底的清水，大德大智隐于无形</p>
</li>
<li><p>《语言学的逻辑》 [美]塞缪尔.早川 艾伦.早川 著<br>  一次让人神往的语言学探秘之旅，一堂终身受用的语言学入门课</p>
</li>
<li><p>《西方美学史》 朱光潜 著<br>  西方美学三千年历史展现，奠定中国研究西方美学思想的基础</p>
</li>
<li><p>《心理学与生活》 [美]理查德.格里格 菲利普.津巴多 著<br>  “希望你能像一个心理学家那样去思考”</p>
</li>
</ul>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>readinglist</tag>
      </tags>
  </entry>
  <entry>
    <title>Several spaces in Real Variable Functions</title>
    <url>/2022/11/14/math/several%20spaces/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>实变函数中理不清的几个空间问题，不断更新中，一边学习，一边总结~</p>
<p><strong>实变函数与泛函分析中的几个空间，各种空间，还在学习中…</strong><br><span id="more"></span></p>
<h2 id="一，度量空间"><a href="#一，度量空间" class="headerlink" title="一，度量空间"></a>一，度量空间</h2><p>设 $\mathbf{X}$ 是集合，对 $\forall x,y\in{X}$ 都有实数 $d(x,y)$ 对应，而且满足：</p>
<ul>
<li>$d(x,y)\geqq 0,\quad d(x,y)=0 \; only \; if \; x=y$</li>
<li>$d(x,y)\leqq d(x,z)+d(y,z)\quad for \; all \; z$</li>
</ul>
<p>$d(x,y)$是距离，集合$\mathbf{X}$是<strong>度量空间</strong>，又叫<strong>距离空间</strong>。</p>
<h3 id="1，特例"><a href="#1，特例" class="headerlink" title="1，特例"></a>1，特例</h3><ul>
<li>Euclid空间<ul>
<li>实数集合 $\mathbf{R}^{n}$ 和 $ d(x,y)=(\sum\limits^n_{i=1}(\xi_i-\eta_i)^{2})^{\frac{1}{2}} $ 定义的距离下，$(\mathbf{R}^{n}, d(x,y)) $ 称为n维欧式空间，其中 $ d $ 称为欧几里得距离。</li>
<li>其他的距离定义，如1-范数，$\infty$-范数等。</li>
</ul>
</li>
</ul>
<h3 id="2，完备的度量空间"><a href="#2，完备的度量空间" class="headerlink" title="2，完备的度量空间"></a>2，完备的度量空间</h3><p>如果度量空间$(\mathbf{X},d)$中每个Cauchy点列收敛到该空间中的一点，则可称为完备的度量空间。</p>
<ul>
<li>$l^{\infty}$是完备度量空间</li>
</ul>
<h2 id="二，线性空间"><a href="#二，线性空间" class="headerlink" title="二，线性空间"></a>二，线性空间</h2><p>$\mathbf{X}$是非空集合，在其上定义元素的加法运算及其元素与实数（复数）的乘法运算，且满足一下两个条件：</p>
<ul>
<li>关于加法称为交换群，且满足性质：<ul>
<li>$x+y = y+x$</li>
<li>$(x+y)+z = x+(y+z)$</li>
<li>存在零元素且唯一，$\forall x\in\mathbf{X},\; x+0 = x$</li>
<li>存在负元素且唯一，$\forall x\in\mathbf{X},\; x+(-x)=0$</li>
</ul>
</li>
<li>定义$u=\alpha x$的数积运算，且满足性质:<ul>
<li>$1x=x$</li>
<li>$a(bx)=(ab)x\quad\forall a,b为实数或复数$</li>
<li>$(a+b)x=ax+bx,\quad a(x+y)=ax+ay$</li>
</ul>
</li>
</ul>
<p>则称$\mathbf{X}$按上述加法和数乘称为线性空间或向量空间，其中元素称为向量。</p>
<h3 id="1，特例-1"><a href="#1，特例-1" class="headerlink" title="1，特例"></a>1，特例</h3><ul>
<li>实线性空间$\mathbf{R}^{n}$</li>
<li>复线性空间</li>
</ul>
<h3 id="2，维度"><a href="#2，维度" class="headerlink" title="2，维度"></a>2，维度</h3><ul>
<li>无限维</li>
<li>有限维</li>
<li>零维</li>
</ul>
<h2 id="三，赋范线性空间"><a href="#三，赋范线性空间" class="headerlink" title="三，赋范线性空间"></a>三，赋范线性空间</h2><p>$\mathbf{X}$为实(复)的线性空间，$\forall x\in\mathbf{X}$都有实数(记为$||x||$)与之对应，并且满足：</p>
<ul>
<li>$||x||\geqq 0\quad ||x||=0\;only\; if\;x=0$</li>
<li>$||\alpha x||=|\alpha|||x||,\; \alpha为任意实(复)数$</li>
<li>$||x+y||\leqq ||x||+||y||$<br>则$||x||$为向量$x$的范数，$\mathbf{X}$为按范数$||x||$称为赋范线性空间。</li>
</ul>
<h3 id="1，举例"><a href="#1，举例" class="headerlink" title="1，举例"></a>1，举例</h3><h2 id="四，Banach空间-巴拿赫空间"><a href="#四，Banach空间-巴拿赫空间" class="headerlink" title="四，Banach空间(巴拿赫空间)"></a>四，Banach空间(巴拿赫空间)</h2><p>【完备的赋范线性空间】</p>
<h4 id="1，举例-1"><a href="#1，举例-1" class="headerlink" title="1，举例"></a>1，举例</h4><ul>
<li>欧式空间 $ \mathbf{R}^{n} $ 对任一元素 $ x=(\xi_1,\dots,\xi_n) $ 定义如下范数<script type="math/tex; mode=display">||x||=\sqrt{|\xi_{1}|^{2}+\dots+|\xi_{n}|^{2}}</script></li>
<li>$L^{p}[a,b]$对任一元素$f$定义如下范数<script type="math/tex; mode=display">||f||_{p}=(\int^{b}_{a}|f(t)|^{p}dt)^{\frac{1}{p}}</script></li>
</ul>
<h2 id="五，内积空间"><a href="#五，内积空间" class="headerlink" title="五，内积空间"></a>五，内积空间</h2><p>赋范线性空间只有长度(范数)，但没有角度，两个向量除了有长度概念外，还有夹角的概念。</p>
<script type="math/tex; mode=display">内积\rightarrow 内积空间\rightarrow 内积定义范数\rightarrow 内积中的正交</script><h4 id="1，定义内积"><a href="#1，定义内积" class="headerlink" title="1，定义内积"></a>1，定义内积</h4><p>在复欧式空间中，任意两个向量：</p>
<script type="math/tex; mode=display">a=(\xi_{1},\xi_{2}\dots\xi_{n})\quad b=(\eta_{1},\eta_{2},\dots\eta_{n})</script><p>定义$a$与$b$的内积为：</p>
<script type="math/tex; mode=display"><a,b>=\xi_{1}\bar\eta_{1}+\xi_{2}\bar\eta_{2}+\dots+\xi_{n}\bar\eta_{n}</script><ul>
<li>$\bar\eta_i$是$\eta_i$的复共轭。</li>
<li>若为实欧式空间，类似定义</li>
</ul>
<h4 id="2，定义内积空间"><a href="#2，定义内积空间" class="headerlink" title="2，定义内积空间"></a>2，定义内积空间</h4><p>设 $ \mathbf{X} $ 是复线性空间，对其中任意两个向量 $ x,y $，都有一个复数 $ &lt; x,y &gt; $ 与之对应，且满足如下条件：</p>
<ul>
<li>$ &lt; x,x &gt; \geq 0,\quad &lt; x,x &gt;=0\;only\;if\; x=0 $</li>
<li>$ &lt; \alpha x+\beta y, z &gt;=\alpha &lt; x,z &gt;+\beta &lt; y,z &gt;, \alpha 和\beta 为复数 $</li>
<li>$ &lt; x,y &gt;=\overline{&lt; y,x &gt;} $</li>
</ul>
<p>则称$&lt; x,y &gt;$为$x$与$y$的内积，空间 $ \mathbf{X} $ 为内积空间。</p>
<h4 id="3，用内积定义范数"><a href="#3，用内积定义范数" class="headerlink" title="3，用内积定义范数"></a>3，用内积定义范数</h4><p>若$\mathbf{X}$是内积空间，令<script type="math/tex">||x||=\sqrt{<x,x>}</script>，则$||x||$为$\mathbf{X}$上的范数。</p>
<h4 id="4，内积空间上的特殊（赋范线性空间没有）"><a href="#4，内积空间上的特殊（赋范线性空间没有）" class="headerlink" title="4，内积空间上的特殊（赋范线性空间没有）"></a>4，内积空间上的特殊（赋范线性空间没有）</h4><p>$\mathbf{X}$为内积空间，任意两个元素$a$和$b$正交等价于$&lt; a,b &gt;=0$</p>
<h4 id="5，重要的定义或公式"><a href="#5，重要的定义或公式" class="headerlink" title="5，重要的定义或公式"></a>5，重要的定义或公式</h4><ul>
<li>Schwarz不等式<script type="math/tex; mode=display">|<x,y>|\leq||x||\cdot||y||</script></li>
<li>投影定理</li>
<li>正交基和标准正交基</li>
</ul>
<h2 id="六，Hilbert空间"><a href="#六，Hilbert空间" class="headerlink" title="六，Hilbert空间"></a>六，Hilbert空间</h2><p>【完备的内积空间】<br>是赋范线性空间的特例</p>
<h2 id="理不清的关系"><a href="#理不清的关系" class="headerlink" title="理不清的关系"></a>理不清的关系</h2><ul>
<li>赋范线性空间是一种特殊的度量空间<br>  只要用范数来定义距离，$d(x,y)=||x-y||$</li>
</ul>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>real-variable-funcs</tag>
      </tags>
  </entry>
  <entry>
    <title>ODE Transformer</title>
    <url>/2022/11/14/Transformer/ODE%20transformer/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：ODE Transformer-An Ordinary Differential Equation-Inspired Model for Neural Machine Translation</p>
<p><a href="https://arxiv.org/abs/2104.02308v1">essay link</a></p>
<p><strong>ODE 跟Transformer关联的文章，第一次读文章，先看懂！</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>前面有文献工作发现了，残量连接网络是ODE的Euler离散解法。本文探讨Transformer和ODE数值方法之间的关系，且发现了Transformer可以描述为ODE的高阶解法，同时也带领我们设计了一种新的架构（称为 ODE Transformer），类似于在 ODE 中受到很好启发的 Runge-Kutta 方法。</p>
<p>作为Transformer的自然拓展，ODE Transformer容易实现，参数也更加高效。我们对三个 WMT 任务的实验证明了该模型的通用性，以及在几个强大的基线上的性能大幅改进。本文在数据集WMT14的En-De和En-Fr上分别获得了30.76和44.11的BLEU的分值，这为 WMT’14 En-Fr 任务设置了新的sota。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>残差网络已被用作简化多层神经模型中信息流的标准方法，并取得了巨大成功。 给定输入 $y_{t}$，此类模型将深度 $t$ 处的层的输出定义为：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+F(y_{t},\theta_{t}) \tag{1}</script><p>其中，$F(\cdot,\cdot)$ 是网络层的函数，$\theta_{t}$是参数。</p>
<p>有意思的是，ML中最近的工作指出上式可以理解为ODE的Euler离散方法，类似下式：</p>
<script type="math/tex; mode=display">\frac{dy(t)}{dt}=F(y(t),\theta(t)) \tag{2}</script><p>其中，$y(t)$和$\theta(t)$ 是关于 $t$ 连续的。</p>
<p>用这样的方式，可以将残差网络理解为ODE模块，该发现也给理解残差网络从数值优化的角度提供了新思路。之后，可以尝试在多层网络上利用Euler方法，初始值就给定为$y(0)=y_0 $ 和 $\theta (0)=\theta_0 $。</p>
<p>方程 (2) 的解法前提，只有当 $\theta(t) $ 沿 $t$ 缓慢变化时，才有足够低的误差界限（称之为稳定解）。 但这种假设并不总是适用于最先进的自然语言处理 (NLP) 系统，其中模型是非线性和超参的。 例如，语言建模和机器翻译系统为不同的层学习完全不同的参数，尤其是当层接近模型输入时。 </p>
<p>此外，欧拉方法的截断误差不可忽略，因为它是真实解的一阶近似。 当更多的层被堆叠并且错误通过神经网络传播时，这些问题会使情况变得更糟。 这或许可以解释为什么最近的机器翻译 (MT) 系统无法从极深的模型中受益。</p>
<p>在本文中，我们继续研究 ODE 启发方法，基本思想是使用高阶更精确地对 ODE 进行数值求解，这会导致生成更大的 ODE 块，产生一系列中间近似值。 我们发现较大的 ODE 块足以等价于多个一阶解的 ODE 块，好处是显而易见的：使用较少的 ODE 块降低了在块切换中引入错误的风险，而高阶方法减少了每个 ODE 块中的近似误差。</p>
<p>我们的方法是参数有效的，因为$\theta(t) $在同一个 ODE 块中重复使用了，作为另一个“奖励”，可以通过学习块中不同中间近似的系数来改进模型。 我们在强 Transformer 系统中评估我们的方法，涵盖宽（和大）模型和深度模型。 它在 WMT14 En-De 和 En-Fr 测试集上获得了 30.76 和 44.11 的 BLEU 分数，该结果为 WMT14 En-Fr 任务设置了新的sota。</p>
<h2 id="2，Transformer和ODE"><a href="#2，Transformer和ODE" class="headerlink" title="2，Transformer和ODE"></a>2，Transformer和ODE</h2><p>本文从 Transformer 的描述开始，然后是它与 ODE 的关系。 我们选择 Transformer 进行讨论和实验，因为它是最近 MT(机器翻译) 评估中最先进的模型之一。</p>
<h3 id="2-1-Transformer"><a href="#2-1-Transformer" class="headerlink" title="2.1 Transformer"></a>2.1 Transformer</h3><p>Transformer 是编码器-解码器范式的一个例子，编码器是一堆相同的层，每层由一个自注意力块和一个前馈网络（FFN）块组成，它们都配备了残差连接和层归一化单元。 注意，术语“块”以许多不同的方式使用，在本文中指的是通过残差连接（有时称为残差块）增强的任何神经网络。</p>
<p>遵循 Pre-norm 架构 ，我们将块定义为</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+G(LN(y_{t}),\theta_{t}) \tag{3}</script><p>其中$LN(\cdot)$ 是层归一化函数，为了简单起见，我们去掉了$LN(\cdot)$的参数，$G(\cdot)$是 自注意力或前馈网络。 解码器具有类似的架构，在 self-attention 和 FFN 块之间有一个额外的编码器-解码器注意块。</p>
<h3 id="2-2-ODE"><a href="#2-2-ODE" class="headerlink" title="2.2 ODE"></a>2.2 ODE</h3><p>常微分方程是变量 $t$ 的函数 $y(t)$ 及其导数的方程，ODE 的一种简单形式是定义 $y(t)$ 的一阶导数的方程，如下所示:</p>
<script type="math/tex; mode=display">\frac{dy(t)}{dt}=f(y(t),t) \tag{4}</script><p>其中 $f(y(t),t)$ 定义了一个时间相关向量场，如果我们知道它在 $y$ 的所有点和所有时间点 $t$ 的值。 上述方程涵盖了广泛的问题，因为变量的变化由其当前值和时间变量$t$决定。</p>
<p>这个方程也适用于Pre-norm的Transformer块，为了记号简单，我们将$G(LN(y_t),\theta_t) $表示为一个新函数$F(y_t,\theta_t) $就有了：</p>
<script type="math/tex; mode=display">F(y_{t},\theta_{t})=G(LN(y_{t},\theta_{t})) \tag{5}</script><p>将$y_t $和$\theta_t $转换成时间的连续函数$y(t)$和$\theta(t) $，重写方程(3)为：</p>
<script type="math/tex; mode=display">y(t+\Delta t)=y(t)+\Delta t\cdot F(y(t),\theta(t)) \tag{6}</script><p>其中，$\Delta t$表示$t$的变化，也就是常说的步长，显然，在Transformer中有$\Delta t=1$，但可以使用一个约束来适应步长$\Delta t $，就得到了：</p>
<script type="math/tex; mode=display">\lim\limits_{\Delta t\rightarrow 0}\frac{y(t+\Delta t)-y(t)}{\Delta t}=F(y(t),\theta(t)) \tag{7}</script><p>实际上有$\lim_{\Delta t\rightarrow 0}\frac{y(t+\Delta t)-y(t)}{\Delta t}=\frac{dy(t)}{dt} $，方程(7)是方程(4)的特例，唯一的区别就是在方程(4)的右边引进了$\theta(t)$。</p>
<p>然后，我们说 Pre-norm Transformer 块描述了 ODE。 已经发现方程(3)与求解方程(7)中描述的 ODE 的欧拉方法具有相同的形式。 这建立了 Transformer 和 ODE 之间的关系，在给定 $F(\cdot,\cdot)$ 和学习参数 $\{\theta_t\}$ 的情况下，多块 Transformer 的前向传递是多次来运行欧拉方法。</p>
<h2 id="3-ODE-Transformer"><a href="#3-ODE-Transformer" class="headerlink" title="3 ODE Transformer"></a>3 ODE Transformer</h2><p>在 ODE 的数值方法中，我们希望以最少的计算步骤确保 ODE 的精确解。 但是Euler方法并不“精确”，因为它是一阶方法，自然会出现局部截断误差。 如果我们多次运行它，全局误差可能会更大{全局误差就是我们通常所说的误差，$y(t)$ 与真值之间的差异。 局部误差是单步引入的误差，假设$y(t-1)$为真解，$y(t)$与其的差值}。 Transformer 显然就是这种情况，尤其是当多层神经网络在求解 ODE 时，易出现更不稳定的风险。</p>
<h3 id="3-1-高阶ODE解法器"><a href="#3-1-高阶ODE解法器" class="headerlink" title="3.1 高阶ODE解法器"></a>3.1 高阶ODE解法器</h3><p>在这里，我们使用 Runge-Kutta 方法来获得 ODE 的高阶解，它们是具有不同阶精度的经典迭代方法。 正式地介绍$n$步求解的显式 Runge-Kutta 方法定义为：</p>
<script type="math/tex; mode=display">y_{t+1} =y_{t}+\sum\limits_{i=1}^{n}\gamma_{i}F_{i} \tag{8}</script><script type="math/tex; mode=display">F_{1} =hf(y_{t},t) \tag{9}</script><script type="math/tex; mode=display">F_{i} =hf(y_{t}+\sum\limits^{i-1}_{j=1}\beta_{ij}F_{j},t+\alpha_{i}h) \tag{10}</script><p>其中 $ h $ 是步长，在大多数情况下可能就是 1。 $\mathrm{F}_i$ 是步骤 $t+\alpha_i h$ 处解的中间近似。</p>
<p>$ \alpha $、$ \beta $ 和 $ \gamma $ 是可以由 $ y_{t+1} $ 的泰勒级数确定系数，方程(10)描述了在 $n $ 步 $\{t+\alpha_1 h,…,t+\alpha_n h \}$上的解近似序列 $\{F_1,…,F_n\}$，然后对这些近似值进行插值以形成最终解，如方程(8)。</p>
<p>Runge-Kutta 方法直接适用于 Transformer 模块的设计，我们所需要的只是将函数 $f$ 替换为函数 $F$， 优点是函数 $F$ 在块中被重用。 此外，模型参数$\theta_t$ 可以在块内共享。（虽然我们可以区分一个块中不同步骤的参数，但我们发现它没有帮助并且使模型难以学习。） 这样，可以省略方程(10)中的$t+\alpha_i h$，用下式来计算 $F_i$：</p>
<script type="math/tex; mode=display">F_{i}=F(y_{t}+\sum\limits^{i-1}_{j=1}\beta_{ij}F_{j},\theta_{t}) \tag{11}</script><p>这使得系统参数效率更高，正如我们的实验所示，高阶 Runge-Kutta 方法可以学习具有明显更小的模型和强大 NMT 系统。</p>
<p>Runge-Kutta 方法是通用的，例如欧拉方法就是它们的一阶特例。 对于二阶 Runge-Kutta (RK2) 块，我们有：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+\frac{1}{2}(F_{1}+F_{2}) \tag{12}</script><script type="math/tex; mode=display">F_{1}=F(y_{t},\theta_{t}) \tag{13}</script><script type="math/tex; mode=display">F_{2}=F(y_{t}+F_{1},\theta_{t}) \tag{14}</script><p>这也称为改进的欧拉方法，同样，我们可以将四阶 Runge-Kutta (RK4) 块定义为：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+\frac{1}{6}(F_{1}+2F_{2}+2F_{3}+F_{4}) \tag{15}</script><script type="math/tex; mode=display">F_{1}=F(y_{t},\theta_{t}) \tag{16}</script><script type="math/tex; mode=display">F_{2}=F(y_{t}+\frac{1}{2}F_{1},\theta_{t}) \tag{17}</script><script type="math/tex; mode=display">F_{3}=F(y_{t}+\frac{1}{2}F_{2},\theta_{t}) \tag{18}</script><script type="math/tex; mode=display">F_{4}=F(y_{t}+F_{3},\theta_{t}) \tag{19}</script><p>参见图2 以比较不同的 Runge-Kutta 块，需要注意的是，这里介绍的方法可以从表示细化的角度来解释。 它为函数提供了一种更新函数本身的方法，例如，Universal Transformer 使用相同的函数和相同的参数以块方式来细化输入序列的表示。 在这里，我们展示了内部块细化可以在良好的理论支持下建模。</p>
<h3 id="3-2-参数学习"><a href="#3-2-参数学习" class="headerlink" title="3.2 参数学习"></a>3.2 参数学习</h3><p>在我们的初步实验中，当模型很浅时，RK2 和 RK4 方法产生了有希望的 BLEU 改进，但发现对于更深层次的模型，这种改进并没有持续下去。 为了弄清楚为什么会发生这种情况，让我们从训练的角度回顾一下龙格-库塔方法，以 RK2 方法为例。 我们通过替换 $F_1$ 和 $F_2$重写方程(12)，如下：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+\frac{1}{2}F(y_{t}+F(y_{t},\theta_{t}),\theta_{t}) \tag{20}</script><p>令 $\mathcal{E}$ 为训练损失，$L$ 为模型的块数，$y_{L}$ 为模型输出，$\mathcal{E}$ 在 $y_t$ 处的梯度是</p>
<script type="math/tex; mode=display">\frac{\partial{\varepsilon}}{\partial{y_{t}}}=\frac{\partial\varepsilon}{\partial y_{L}}\cdot\frac{1}{2^{L-t}}\cdot\prod_{k=t}^{L-1}(1+g_{k}) \tag{21}</script><p>其中，</p>
<script type="math/tex; mode=display">g_{k}=(1+\frac{\partial F(y_{k},\theta_{k})}{\partial y_{k}})\cdot (1+\frac{\partial F(y_{k}+F(y_{k},\theta_{k}),\theta_{k})}{\partial y_{k}+F(y_{k},\theta_{k})}) \tag{22}</script><p>从等式(29)看，$\frac{\partial \mathcal{E}}{\partial y_{t}}$ 与因子 $\frac{1}{2^{L-t}}$成正比，这导致当 $L$ 较大时梯度消失的风险更高。</p>
<p>这个问题在某种程度上归因于$F_i$ 的小系数，即$\gamma_1 = \gamma_2 = \frac{1}{2}$。 一个自然的想法是根据经验设置 $\gamma_i = 1$ 以消除梯度计算中小于 1 的乘积因子，尽管这在理论上并不基于标准的龙格-库塔方法。 我们用新系数重写方程(20)，如下：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+F(y_{t},\theta_{t})+F(y_{t}+F(y_{t},\theta_{t}),\theta_{t}) \tag{23}</script><p>然后，可以求梯度为：</p>
<script type="math/tex; mode=display">\frac{\partial\varepsilon}{\partial y_{t}}=\frac{\partial\varepsilon}{\partial y_{L}}\cdot\prod^{L-1}_{k=t}g_{k} \tag{24}</script><p>这个模型很容易优化，因为 $\frac{\partial \mathcal{E}}{\partial_{y_L}}$ 可以传递给没有尺度的低级块。 请注意，这里的方法是参数共享的实例。 例如，在每个 ODE 块中，我们对所有中间步骤使用具有相同参数 $\theta_t$ 和相同函数 $F$。 设置 $\gamma_i = 1$，因为 $F_i$ 以相同的比例传递到下一步。 这里我们称之为隐式参数共享。</p>
<p>另一种缩放 $F_i$ 的方法是在训练数据上自动学习系数（初始值 $\gamma_i = 1$），它帮助系统学习在一个块中流动 $F_i$ 的方式。 我们的实验表明，自动系数学习对于获得更好的结果是必要的。</p>
<h2 id="4，实验"><a href="#4，实验" class="headerlink" title="4，实验"></a>4，实验</h2><h3 id="4-1-实验设置"><a href="#4-1-实验设置" class="headerlink" title="4.1 实验设置"></a>4.1 实验设置</h3><p>我们提出的方法在三个广泛使用的基准上进行了评估：WMT’14 英语-德语 (En-De)、WMT’14 英语-法语 (En-Fr) 和 WMT’16 英语-罗马尼亚语 (En-Ro) 翻译任务。</p>
<h4 id="数据集和评价"><a href="#数据集和评价" class="headerlink" title="数据集和评价"></a>数据集和评价</h4><p>对于 En-De 任务，训练数据由大约 4.5M的标记化句子对组成，所有句子都被分割成子词单元的序列，使用共享词汇表进行了 32K 的合并操作。 我们选择 newstest2013 作为验证数据，选择 newstest2014 作为测试数据。 </p>
<p>对于 En-Fr 任务，我们使用了 Fairseq 提供的数据集，即来自 WMT’14 的 36M 训练句子对，newstest2012+newstest2013 是验证数据，newstest2014 是测试数据。 </p>
<p>对于 En-Ro 任务，我们复制了文章mehta2020delight的设置，分别使用 600K/2K/2K 句子对进行训练、评估和推理。</p>
<p>我们根据 BLEU 来衡量性能，标记化的 BLEU 分数 和 sacrebleu 都是在 En-De 和 En-Fr 任务上报。 此外，我们报告了 En-Ro 任务的标记化 BLEU 分数，En-De 和 En-Fr 的beam尺寸和长度惩罚因子分别为 4 和 0.6 ，En-Ro 为 5 和 1.3 。</p>
<h4 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h4><p>正如 li-etal-2020-shallow 的工作所建议的，我们使用相对位置表示 (RPR) 来获得更强的基线 。 所有实验都在 8 块 GPU 上进行训练，每个 GPU 上有 4,096 tokens。 对于 En-De 和 En-Fr 任务，我们采用梯度累积策略，步长分别为 2 和 8 。 我们使用了 Adam 优化器，其超参数设置为 $(0.9, 0.997)$，学习率的最大点设置为 $0.002$，以实现快速收敛。 我们将 SAN 和 FFN 合并视为默认的 ODE 块。</p>
<p>更多细节参见我们提供的材料。</p>
<h3 id="4-2-结果"><a href="#4-2-结果" class="headerlink" title="4.2 结果"></a>4.2 结果</h3><h4 id="En-De-和-En-Fr-的结果"><a href="#En-De-和-En-Fr-的结果" class="headerlink" title="En-De 和 En-Fr 的结果"></a>En-De 和 En-Fr 的结果</h4><p>表1将 ODE Transformer 与几个最先进的系统进行了比较，RK2-block 和 RK4-block 在不同的模型容量下都大大优于基线。 例如，当深度为 6 时，RK2-block 使用基本配置获得了 0.97 的 BLEU 改进。 RK4 块在 RK2 块之上产生 +$0.17$ BLEU 点的收益， 这一观察从经验上验证了高阶 ODE 函数更有效的猜想。 当我们切换到深度模型时，RK2-block 可与 li-etal-2020-shallow 中报告的 48 层强系统相媲美，但参数显着减少，这表明我们的方法是参数有效的。</p>
<p>宽模型也可以受益于扩大层深度。 RK-2 ODE Transformer 在 En-De 和 En-Fr 任务上的 BLEU 分数分别为 30.76 和 44.11 ，显着超过标准 Big 模型 1.32 和 0.70 的 BLEU 分数。 这为这些任务设置了新的最先进技术，参数更少， 请注意，我们后续将报告更多关于 RK4 块的结果。</p>
<h4 id="Rn-Ro-结果"><a href="#Rn-Ro-结果" class="headerlink" title="Rn-Ro 结果"></a>Rn-Ro 结果</h4><p>表2展示了 En-Ro 任务中几个强大系统的模型参数、总训练步骤和 BLEU 分数。 同样，ODE Transformer 的性能优于这些基线。 如 mehta2020delight 中所述，他们对模型进行了高达 170 epoches 的训练，并通过 DeLight 模型获得了 34.70 的 BLEU 分数。 然而，这里的观察是完全不同的， 在 20 epoch 之后，验证的困惑度开始增加。 因此，我们的基线略逊于他们的基线，但与 lin2020towards 中报告的结果相匹配。 ODE Transformer 使用 DeLight 以更少的训练成本实现了更好的性能， 对于更大的模型（表2中的第 6 行），它获得了 35.28 的 BLEU 分数。</p>
<h4 id="参数有效性"><a href="#参数有效性" class="headerlink" title="参数有效性"></a>参数有效性</h4><p>表3总结了几个高效 Transformer 变体的结果，包括 Lite Transformer 、DeLight和 Evolved Transformer的轻型版本。 正如我们预期的那样，所提出的 ODE Transformer 有望用于较小的模型。 它在 BLEU 中与 DeLight 相当，但参数少了 9M。 在相同的模型容量下，它比 DeLight 高出 0.84 BLEU 点。 这些结果表明，所提出的方法与模型容量正交，它可能为在边缘设备上部署 NMT 系统提供新的选择。</p>
<h3 id="4-3-分析"><a href="#4-3-分析" class="headerlink" title="4.3 分析"></a>4.3 分析</h3><p>在这里，我们调查一些有趣的问题。 为简单起见，在下文中，我们将具有可学习系数的 RK2-block 称为 RK2-block-v2。</p>
<h4 id="BLEU-和编码深度"><a href="#BLEU-和编码深度" class="headerlink" title="BLEU 和编码深度"></a>BLEU 和编码深度</h4><p>图 3（左）描绘了几个 ODE Transformer 变体的 BLEU 分数和不同编码器深度下的基线。当深度 $\leq 24$ 时，所有 ODE Transformer 变体都明显优于基线。而 RK2-block-v2 几乎在所有深度上都达到了最佳性能，尤其是当模型变得更深时。直观地说，与 18 层基线系统相比，6 层 RK2 块能够提供相当的性能。同样，它表明所提出的方法是参数有效的。这里的另一个发现是 RK4 块在浅层模型上表现得很好，在表 1 中观察到了类似的现象。对于更深的模型，它不如 RK2-block，尽管高阶 ODE 求解器可以获得更低的误差。这是因为当模型很深时，原始系数可能会导致反向传播中的优化问题。此外，图 3（右）将 BLEU 绘制为当隐藏大小为 256 时模型大小的函数。我们的 RK2 方法使用更少的参数显着超过了基线。</p>
<h4 id="不同-F-cdot-cdot-上的参数学习"><a href="#不同-F-cdot-cdot-上的参数学习" class="headerlink" title="不同$F(\cdot,\cdot) $上的参数学习"></a>不同$F(\cdot,\cdot) $上的参数学习</h4><p>正如我们所说，$F(\cdot,\cdot)$ 函数可以是子层，例如 SAN、FFN 或两者兼有 (SAN+FFN)。 如图 4 所示，高阶 ODE 与 FFN 协同相比 SAN 协同的效果更好。 一个探索可能是FFN组件的参数比SAN组件多。将 FFN 和 SAN 合并为 ODE 块的模型表现出最佳性能。</p>
<h4 id="训练和验证难度"><a href="#训练和验证难度" class="headerlink" title="训练和验证难度"></a>训练和验证难度</h4><p>图 5 绘制了 RK 块和标准残差块的训练和验证困惑 (PPL) 曲线。 我们基于两种配置（基本模型和宽模型）比较行为。 直观地说，RK2 块在两种配置中都呈现较低的训练和验证 PPL。</p>
<h4 id="梯度归一化后的可视化"><a href="#梯度归一化后的可视化" class="headerlink" title="梯度归一化后的可视化"></a>梯度归一化后的可视化</h4><p>为了研究所提出的 ODE Transformer 的优越性，我们在训练期间收集了几个训练有素的系统的梯度范数。 图 6 绘制了 RK2-block、RK4-block 和标准残差块（基线）的梯度范数。 正如我们所见，Pre-Norm 残差块能够使训练稳定。 由于中间近似之间的隐式参数共享，RK2-block 和 RK4-block 都提供了更丰富的信号。 并且两条学习曲线同样看起来几乎相同，这与表 1 中的结果一致。</p>
<h4 id="不同ODE设计策略的比较"><a href="#不同ODE设计策略的比较" class="headerlink" title="不同ODE设计策略的比较"></a>不同ODE设计策略的比较</h4><p>然后，我们对几种ODE设计模式进行了综合分析。 正如 yiping2018beyond 中所述，计算机视觉中的几个模型，例如 LeapfrogNet、PolyNet 、Multi-step Net 也可以从 ODE 角度进行解释。 相关的 ODE 函数汇总在表 4 中。 在这里，我们使用相同的代码库重新实现这些方法以进行公平比较。 我们按照基本配置将编码器深度设置为 6 ，并对 En-De 任务进行了实验。</p>
<p>在时间 $t$，多步欧拉方法需要先前的状态，例如 $y_{t-1}$，生成当前近似值，而不是基于当前时间状态的迭代细化。 基本上，这些方法参数效率不高，并且性能不如我们的方法。 注意，DLCL也可以看成是多步欧拉法，在deep Transformer中更具竞争力。 但是在浅基线上只有很小的改进。</p>
<p>从理论上讲，Backward Euler 方法在数值分析中略好于 Forward Euler 方法，但改进微乎其微。 请注意，与上述方法相比，我们的 ODE Transformer 实现了一致的 BLEU 改进。 这里的原因是这种迭代细化使参数学习更加高效和有效。 所有模型都可以在我们的附件中找到。</p>
<h4 id="阶段误差量化"><a href="#阶段误差量化" class="headerlink" title="阶段误差量化"></a>阶段误差量化</h4><p>在这里，我们旨在量化截断误差。 但是，我们无法在 NMT 中获得每个块输出的“真实”解，因为我们主要在编码器端进行实验。相反，我们在语言建模任务上进行了实验，也就是单层模型输出与真值之间的损失 相当于没有错误传播的截断错误。</p>
<p>表 5 显示了 PTB 任务的 PPL。 所有 ODE Transformer 变体都显着减少了错误。 RK4-order 在两种设置上都达到了最低的 PPL。 此外，RK2 块甚至可以获得比 2 层残差块更低的 PPL。 这里的观察再次验证了我们的猜想。</p>
<h2 id="5，相关工作"><a href="#5，相关工作" class="headerlink" title="5，相关工作"></a>5，相关工作</h2><h3 id="深度Transformer模型"><a href="#深度Transformer模型" class="headerlink" title="深度Transformer模型"></a>深度Transformer模型</h3><p>最近，Deep Transformer 在机器翻译方面取得了巨大成功。 一种直接的方法是缩短从上层到下层的路径，从而缓解梯度消失或爆炸问题。 对于更深层次的模型，训练成本是不可忽略的。 为了加快训练速度，另一种方法是先训练一个浅层模型，然后逐渐增加模型深度。</p>
<p>除了模型架构改进之外，另一种简化优化的方法是利用精心设计的参数初始化策略，例如 depth-scale 、Lipschitz 约束 ，T-fixup  和 ADMIN 。</p>
<p>请注意，ODE Transformer 与上述方法是正交的，我们将在未来的工作中对这些方法进行测试。</p>
<h3 id="ODE"><a href="#ODE" class="headerlink" title="ODE"></a>ODE</h3><p>ResNet 和 ODE 之间的关系最早由 weinan2017proposal 提出。 这为社区带来了设计有效深度架构的全新视角。 一些有见地的架构zhang2017polynet,larsson2017fractalnet,yiping2018beyond,he2019ode 也可以从ODE的角度来解释。 但是，在自然语言处理中，从 ODE 角度设计模型的研究仍然很少见。 也许与我们最相关的工作是 lu2019understanding 的工作。 他们从多粒子动态系统的角度解释了 Transformer 架构，并将夹在 FFN 中的自注意力重新定位。 与他们的工作不同，我们认为堆叠的一阶 ODE 块可能会导致误差累积，从而阻碍模型性能。 我们通过引入高阶块来解决这个问题，并展示了显著的 BLEU 改进。</p>
<h2 id="6，结论"><a href="#6，结论" class="headerlink" title="6，结论"></a>6，结论</h2><p>在本文中，我们探讨了 Transformer 和 ODE 之间的关系，提出了一种新的架构（ODE Transformer）来帮助模型从高阶 ODE 解决方案中受益。 实验结果表明，在模型容量相同的情况下，ODE Transformer 可以显著优于基线。 它在 WMT’14 En-De 和 En-Fr 测试数据上获得了 30.76 和 44.11 的 BLEU 分数，这为 En-Fr 任务设置了新的sota。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>ODE</tag>
      </tags>
  </entry>
  <entry>
    <title>区块链中的名词解释</title>
    <url>/2022/11/14/tools/clock_chain_new_words/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>区块链中的各种链解释~</p>
<p><strong>区块链，去中心化的新技术~</strong><br><span id="more"></span></p>
<h2 id="1，公链"><a href="#1，公链" class="headerlink" title="1，公链"></a>1，公链</h2><p>比特币、以太坊是时下流行的公链。</p>
<p>公链全称是“公有链”，是指任何人都可读取的、任何人都能发送交易且交易能获得有效确认的、任何人都能参与其中共识过程的区块链。通常被认为是去中心化的。可以简单理解为公开的、公共可用的区块链。完全去中心化的公链通过共识机制和代币奖励机制来鼓励参与者{节点}竞争记账，共同维护链上数据的安全性。</p>
<p>公链具有开源、保护用户免受开发者的影响、访问门槛低、所有数据默认公开等特性。任何人也可以在公共链上发送交易，还可以随时参与网络上形成共识的过程，即决定哪个区块可以加入区块链并记录当前的网络状态。</p>
<h2 id="2，私链"><a href="#2，私链" class="headerlink" title="2，私链"></a>2，私链</h2><p>私有链是与公有链相对的一个概念，就是指不对外开放，仅在组织内部使用的系统，其创建及维护权限由一个组织拥有，非组织成员无法访问或仅拥有小规模访问权限的区块链网络。具有交易速度快、隐私保障性好、信息安全性高、交易成本很低甚至为零等特点。私有链在使用过程中，通常要求注册，即需要提交身份认证并具备一套权限管理系统。节点数量和节点状态通常是可控的，一般不需要通过竞争方式来筛选区块数据的打包者，可以采用更加节能环保的方式。</p>
<p>私有链的价值主要是提供安全、可追溯、不可篡改、自动执行的运算平台，同时防范来自内部和外部对数据安全的攻击。私有链的应用场景一般是企业内部，如企业的票据管理、账务审计、供应链管理。</p>
<h2 id="3，联盟链"><a href="#3，联盟链" class="headerlink" title="3，联盟链"></a>3，联盟链</h2><p>联盟链是由多个组织或者通过联盟形式组建的区块链，联盟参与者之间通过契约或其他形式建立信任和共识机制，构造的区块和链接功能仅限于联盟参与者，访问权限可以对外采取限制性开放。联盟链是一种需要注册许可的区块链，这种区块链也称为许可链，通常是使用在多个成员角色的环境下，比如银行之间的支付结算、企业之间的物流结算。</p>
<p>联盟链可以根据应用场景来决定对公众的开放程度。由于参与共识的节点比较少，联盟链一般不采用工作量证明的挖矿机制，而是多采用权益证明或PBFT（Practical ByzantineFault Tolarant）、RAFT等共识算法。联盟链对交易的确认时间、每秒交易数都与公共链有较大的区别，对安全和性能的要求也比公共链高。</p>
<h2 id="4，单链"><a href="#4，单链" class="headerlink" title="4，单链"></a>4，单链</h2><p>所谓单链，是指能够单独运行的区块链系统，这些区块链系统拥有完备的组件模块并自称一格体系。单链应用程序的运行需要独立的区块链系统的支持。</p>
<h2 id="5，侧链"><a href="#5，侧链" class="headerlink" title="5，侧链"></a>5，侧链</h2><p>侧链属于一种区块链的跨链技术。区块链系统与侧链系统本身都是一格独立的链系统，两者之间可以按照一定的协议进行数据互动，通过这种方式，侧链能起到对主链功能扩展的作用，如很多在主链中不方便实现的功能可以在侧链中实现，而侧链再通过与主链的数据交互增强自己的可靠性。本质上说，侧链并不会指某一区块链，是遵循侧链协议的全部区块链统称。侧链致力于完成双向锚定，让某类加密货币在主链和侧链两者之间互相“转移”。</p>
<h2 id="6，互联链"><a href="#6，互联链" class="headerlink" title="6，互联链"></a>6，互联链</h2><p>针对特定领域的应用可能会形成各自垂直领域的区块链，互联链就是一种通过跨链技术连接不同区块链的基础设施：包括数据结构和通信协议，其本身通常也是区块链。各种不同的区块链通过互联链互联互通并形成更大的区块链生态。与互联网一样，互联链的建立将形成区块链的全球网络。</p>
<p>与传统的软件不同的是，区块链系应用拥有独特的性质，比如数据不可篡改、完全性证明、自动网络共识、智能合约等，从最初的数字货币到未来可能的区块链可编程社会，这些不单单改变生活服务方式，还会促进社会治理的变革。如果说每一条链都是一条神经的话，一旦互联起来，就像是神经系统一样，将会给我们的社会发展带来更新层次的智能化。</p>
<h2 id="7，主链"><a href="#7，主链" class="headerlink" title="7，主链"></a>7，主链</h2><p>也就是部署在生产环境中的真正的区块链系统，软件在正式发布前会经过很多的内部测试版本，用于发现一些可能存在的bug，并且用来内部演示以便于查看结果，最后才会正式发布正式版。主链，也可以说是正式版客户端组成的区块链网络，只有主链才会真正被推广使用的，各项功能的设计都是相对完善的。另外有些时候，区块链系统会由于种种原因导致分叉，比如挖矿的时候临时产生的小分叉等等，因此最长的那条原始链称为主链。</p>
<h2 id="8，测试链"><a href="#8，测试链" class="headerlink" title="8，测试链"></a>8，测试链</h2><p>一是开发者为了方便大家学习使用而提供的测试用途的区块链网络，如比特币测试链、以太坊测试链等；<br>二是用户自行搭建的测试网络。</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>blockchain</tag>
      </tags>
  </entry>
  <entry>
    <title>cpp相关面试题</title>
    <url>/2022/10/17/cpp/interview_1/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>c++的常见面试题，随时加更中～</p>
<p><strong>加油~</strong><br><span id="more"></span></p>
<h1 id="chapter-1"><a href="#chapter-1" class="headerlink" title="chapter-1"></a>chapter-1</h1><h2 id="问题1-incluede-后面使用双引号-“-“-和尖括号-lt-gt-的区别"><a href="#问题1-incluede-后面使用双引号-“-“-和尖括号-lt-gt-的区别" class="headerlink" title="问题1: incluede 后面使用双引号 “ “ 和尖括号 &lt;&gt; 的区别"></a>问题1: incluede 后面使用双引号 “ “ 和尖括号 &lt;&gt; 的区别</h2><p><strong>参考：</strong></p>
<p>预处理器发现 <code>#include</code> 指令后，就会寻找后跟的文件名并把这个文件的内容包含到当前文件中。被包含文件中的文本将替换源代码文件中的<code>#include</code>指令，就像你把被包含文件中的全部内容键入到源文件中的这个位置一样。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span>  <span class="comment">// 文件名放在尖括号中 </span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> “mystuff.h”  <span class="comment">//文件名放在双引号中 </span></span></span><br></pre></td></tr></table></figure>
<p>尖括号<code>&lt; &gt;</code>括起来表明这个文件是一个工程或标准头文件。查找过程会检查预定义的目录，我们可以通过设置搜索路径环境变量或命令行选项来修改这些目录。</p>
<p>如果文件名用一对<code>&quot; &quot;</code>引号括起来则表明该文件是用户提供的头文件，查找该文件时将从当前文件目录（或文件名指定的其他目录）中寻找文件，然后再到标准位置寻找文件。</p>
<h2 id="问题2：请说说new-delete与malloc-free的区别？"><a href="#问题2：请说说new-delete与malloc-free的区别？" class="headerlink" title="问题2：请说说new/delete与malloc/free的区别？"></a>问题2：请说说new/delete与malloc/free的区别？</h2><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ul>
<li><p>new和delete是<strong>C++的关键字</strong>，是一种操作符，<strong>可以被重载</strong></p>
</li>
<li><p>malloc和free是<strong>C语言的库函数</strong>，并且<strong>不能重载</strong></p>
</li>
<li><p>malloc使用时需要自己显示地计算内存大小，而new使用时由编译器自动计算</p>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *q = (<span class="type">int</span> *)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(<span class="type">int</span>) * <span class="number">2</span>);	<span class="comment">//显示计算内存大小</span></span><br><span class="line"><span class="type">int</span> *p = <span class="keyword">new</span> <span class="type">int</span>[<span class="number">2</span>];						<span class="comment">//编译器会自动计算</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>malloc分配成功后返回的是<strong>void*指针</strong>，需要强制类型转换成需要的类型；而new<strong>直接就返回了对应类型的指针</strong></p>
</li>
<li><p>new和delete使用时会分别调用构造函数和析构函数，而malloc和free只能申请和释放内存空间，不会调用构造函数和析构函数</p>
</li>
</ul>
<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>注意：<strong>delete和free被调用后，内存不会立即回收，指针也不会指向空</strong>，delete或free仅仅是告诉操作系统，这一块内存被释放了，可以用作其他用途。但是由于没有重新对这块内存进行写操作，所以内存中的变量数值并没有发生变化，这时候就会出现野指针的情况。因此，释放完内存后，应该把指针指向NULL。<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="type">int</span> *q = (<span class="type">int</span> *)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(<span class="type">int</span>) * <span class="number">2</span>);</span><br><span class="line">	<span class="type">int</span> *p = <span class="keyword">new</span> <span class="type">int</span>[<span class="number">2</span>];</span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p = &quot;</span> &lt;&lt; p &lt;&lt; <span class="string">&quot;  q = &quot;</span> &lt;&lt; q &lt;&lt; endl;</span><br><span class="line">	<span class="built_in">free</span>(p);	<span class="comment">//指针还没指向空</span></span><br><span class="line">	<span class="keyword">delete</span> q;	<span class="comment">//同上</span></span><br><span class="line">	cout &lt;&lt; <span class="string">&quot;p = &quot;</span> &lt;&lt; p &lt;&lt; <span class="string">&quot;  q = &quot;</span> &lt;&lt; q &lt;&lt; endl;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//上面程序运行的结果，可见第二次打印的时候p和q指针还没指向空</span></span><br><span class="line"><span class="comment">//这里不知为何编译器第二次打印的q和第一次不一样</span></span><br><span class="line"><span class="comment">//	p = 000000000039B9B0  q = 000000000039B960</span></span><br><span class="line"><span class="comment">//	p = 000000000039B9B0  q = 0000000000008123</span></span><br></pre></td></tr></table></figure></p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://blog.csdn.net/ii0789789789/article/details/86851445">new 和 malloc free 和 delete 的区别</a></p>
<p><a href="new/delete与malloc/free的区别与联系详解！">new/delete与malloc/free的区别与联系详解！</a></p>
]]></content>
      <categories>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo上的next主题增量优化</title>
    <url>/2022/08/04/ubuntuOS/hexo-next-theme2/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>接着继续优化hexo上next的主题，现在基本就是，缺什么补上，看着什么补顺眼就美化，赞的啦~</p>
<p><strong>立志成为顺眼的博客~</strong><br><span id="more"></span></p>
<h2 id="1，代码折叠"><a href="#1，代码折叠" class="headerlink" title="1，代码折叠"></a>1，代码折叠</h2><h3 id="1-1-添加code-unfold-js"><a href="#1-1-添加code-unfold-js" class="headerlink" title="1.1 添加code-unfold.js"></a>1.1 添加code-unfold.js</h3><p>新增文件，路径为themes/next/source/js/code-unfold.js，代码为：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">var CODE_MAX_HEIGHT = 200;</span><br><span class="line">var containers = [];</span><br><span class="line"></span><br><span class="line">// 展开</span><br><span class="line">$(&#x27;body&#x27;).on(&#x27;click&#x27;, &#x27;.js_unfold_code_btn&#x27;, function () &#123;</span><br><span class="line">  $(this).closest(&#x27;.js_highlight_container&#x27;).addClass(&#x27;on&#x27;);</span><br><span class="line">&#125;);</span><br><span class="line">// 收起</span><br><span class="line">$(&#x27;body&#x27;).on(&#x27;click&#x27;, &#x27;.js_retract_code_btn&#x27;, function () &#123;</span><br><span class="line">  var $container = $(this).closest(&#x27;.js_highlight_container&#x27;).removeClass(&#x27;on&#x27;);</span><br><span class="line">  var winTop = $(window).scrollTop();</span><br><span class="line">  var offsetTop = $container.offset().top;</span><br><span class="line">  $(this).css(&#x27;top&#x27;, 0);</span><br><span class="line">  if (winTop &gt; offsetTop) &#123;</span><br><span class="line">    // 设置滚动条位置</span><br><span class="line">    $(&#x27;body, html&#x27;).animate(&#123;</span><br><span class="line">      scrollTop: $container.offset().top - CODE_MAX_HEIGHT</span><br><span class="line">    &#125;, 600);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line">// 滚动事件，触发动画效果</span><br><span class="line">$(window).on(&#x27;scroll&#x27;, function () &#123;</span><br><span class="line">  var scrollTop = $(window).scrollTop();</span><br><span class="line">  var temp = [];</span><br><span class="line">  for (let i = 0; i &lt; containers.length; i++) &#123;</span><br><span class="line">    var item = containers[i];</span><br><span class="line">    var &#123; $container, height, $hide, hasHorizontalScrollbar &#125; = item;</span><br><span class="line">    if ($container.closest(&#x27;body&#x27;).length === 0) &#123;</span><br><span class="line">      // 如果 $container 元素已经不在页面上, 则删除该元素</span><br><span class="line">      // 防止pjax页面跳转之后，元素未删除</span><br><span class="line">      continue;</span><br><span class="line">    &#125;</span><br><span class="line">    temp.push(item);</span><br><span class="line">    if (!$container.hasClass(&#x27;on&#x27;)) &#123;</span><br><span class="line">      continue;</span><br><span class="line">    &#125;</span><br><span class="line">    var offsetTop = $container.offset().top;</span><br><span class="line">    var hideBtnHeight = $hide.outerHeight();</span><br><span class="line">    // 减去按钮高度，减去底部滚动条高度</span><br><span class="line">    var maxTop = parseInt(height - (hasHorizontalScrollbar ? 17 : 0) - hideBtnHeight);</span><br><span class="line">    let top = parseInt(</span><br><span class="line">      Math.min(</span><br><span class="line">        Math.max(scrollTop - offsetTop, 0), // 如果小于 0 ，则取 0</span><br><span class="line">        maxTop,// 如果大于 height ，则取 height</span><br><span class="line">      )</span><br><span class="line">    );</span><br><span class="line">    // 根据 sin 曲线设置&quot;收起代码&quot;位置</span><br><span class="line">    var halfHeight = parseInt($(window).height() / 2 * Math.sin((top / maxTop) * 90 * (2 * Math.PI/360)));</span><br><span class="line">    $hide.css(&#x27;top&#x27;, Math.min(top + halfHeight, maxTop));</span><br><span class="line">  &#125;</span><br><span class="line">  containers = temp;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">// 添加隐藏容器</span><br><span class="line">function addCodeWrap($node) &#123;</span><br><span class="line">  var $container = $node.wrap(&#x27;&lt;div class=&quot;js_highlight_container highlight-container&quot;&gt;&lt;div class=&quot;highlight-wrap&quot;&gt;&lt;/div&gt;&lt;/div&gt;&#x27;).closest(&#x27;.js_highlight_container&#x27;);</span><br><span class="line"></span><br><span class="line">  // 底部 &quot;展开代码&quot; 与 侧边栏 &quot;收起代码&quot;</span><br><span class="line">  var $btn = $(`</span><br><span class="line">    &lt;div class=&quot;highlight-footer&quot;&gt;</span><br><span class="line">      &lt;a class=&quot;js_unfold_code_btn show-btn&quot; href=&quot;javascript:;&quot;&gt;展开代码&lt;i class=&quot;fa fa-angle-down&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;a class=&quot;js_retract_code_btn hide-btn&quot; href=&quot;javascript:;&quot;&gt;&lt;i class=&quot;fa fa-angle-up&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;收起代码&lt;/a&gt;</span><br><span class="line">  `);</span><br><span class="line"></span><br><span class="line">  $container.append($btn);</span><br><span class="line">  return $container;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">function codeUnfold () &#123;</span><br><span class="line">  $(&#x27;.highlight&#x27;).each(function () &#123;</span><br><span class="line">    // 防止重复渲染</span><br><span class="line">    if (this.__render__ === true) &#123;</span><br><span class="line">      return true;</span><br><span class="line">    &#125;</span><br><span class="line">    this.__render__ = true;</span><br><span class="line">    var $this = $(this);</span><br><span class="line">    var height = $(this).outerHeight();</span><br><span class="line">    if (height &gt; CODE_MAX_HEIGHT) &#123;</span><br><span class="line">      // 添加展开&amp;收起容器</span><br><span class="line">      var $container = addCodeWrap($this, height);</span><br><span class="line">      containers.push(&#123;</span><br><span class="line">        $container,</span><br><span class="line">        height,</span><br><span class="line">        $hide: $container.find(&#x27;.js_retract_code_btn&#x27;),</span><br><span class="line">        hasHorizontalScrollbar: this.scrollWidth &gt; this.offsetWidth,</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h3 id="1-2-添加jquery"><a href="#1-2-添加jquery" class="headerlink" title="1.2 添加jquery"></a>1.2 添加jquery</h3><p>主题配置文件<code>source/_data/next.yml</code>修改配置项<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fancybox: true</span><br></pre></td></tr></table></figure></p>
<h3 id="1-3-引用code-unfold-js"><a href="#1-3-引用code-unfold-js" class="headerlink" title="1.3 引用code-unfold.js"></a>1.3 引用code-unfold.js</h3><ul>
<li><p>修改文件<code>themes/next/layout/_scripts/index.swig</code>，在最后添加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 代码折叠</span><br><span class="line">&#123;&#123;- next_js(&#x27;code-unfold.js&#x27;) &#125;&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加引用<code>themes/next/source/js/next-boot.js</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NexT.boot.refresh = function () &#123;</span><br><span class="line">  // 添加一行代码</span><br><span class="line">  codeUnfold()</span><br><span class="line">  </span><br><span class="line">  // ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="1-4-创建highlight-styl"><a href="#1-4-创建highlight-styl" class="headerlink" title="1.4 创建highlight.styl"></a>1.4 创建highlight.styl</h3><p>添加<code>theme/next/source/css/_common/components/highlight.styl</code>文件<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 展开收起效果</span><br><span class="line">.highlight-container</span><br><span class="line">  position: relative</span><br><span class="line">  background-color: highlight-background</span><br><span class="line">  &amp;.on</span><br><span class="line">    .highlight-footer</span><br><span class="line">      display: none</span><br><span class="line">    .hide-btn</span><br><span class="line">      display: flex</span><br><span class="line">    .highlight-wrap</span><br><span class="line">      max-height: none</span><br><span class="line">  .highlight-wrap</span><br><span class="line">    overflow: hidden</span><br><span class="line">    max-height: 200px</span><br><span class="line">  .highlight-footer</span><br><span class="line">    position absolute</span><br><span class="line">    width: 100%</span><br><span class="line">    left: 0</span><br><span class="line">    bottom: 0</span><br><span class="line">    height: 60px</span><br><span class="line">    background-image: &#x27;linear-gradient(-180deg, rgba(255,255,255,0) 0%, %s 65%)&#x27; % highlight-background;</span><br><span class="line">    text-align: center</span><br><span class="line">  .show-btn</span><br><span class="line">    font-size: 12px</span><br><span class="line">    color: #fff</span><br><span class="line">    position: absolute</span><br><span class="line">    left: 50%</span><br><span class="line">    transform: translateX(-50%)</span><br><span class="line">    bottom: 0</span><br><span class="line">    line-height: 2em</span><br><span class="line">    text-decoration: none</span><br><span class="line">    padding: 0 0.8em</span><br><span class="line">    text-align: center</span><br><span class="line">    border-radius: 4px 4px 0</span><br><span class="line">    &amp;:hover</span><br><span class="line">      text-decoration: none</span><br><span class="line">  .hide-btn</span><br><span class="line">    color: #fff</span><br><span class="line">    font-size: 12px</span><br><span class="line">    width: 22px</span><br><span class="line">    position: absolute</span><br><span class="line">    left: -21px</span><br><span class="line">    top: 0</span><br><span class="line">    line-height: 1em</span><br><span class="line">    text-decoration: none</span><br><span class="line">    text-align: center</span><br><span class="line">    display: none</span><br><span class="line">    flex-direction: column</span><br><span class="line">    background-color: highlight-background</span><br><span class="line">    border-radius: 4px 0 0 4px</span><br><span class="line">    padding: 0.1em 0 0.6em</span><br><span class="line">    transition: top ease 0.35s</span><br><span class="line">  .fa-angle-up,</span><br><span class="line">  .fa-angle-down</span><br><span class="line">    font-style: normal</span><br><span class="line">    color: #fff</span><br><span class="line">  .fa-angle-up:before</span><br><span class="line">    content:&quot;\f106&quot;</span><br><span class="line">  .fa-angle-down:before</span><br><span class="line">    content:&quot;\f107&quot;</span><br><span class="line">    margin-left: 0.5em</span><br><span class="line">  .js_unfold_code_btn, .js_retract_code_btn</span><br><span class="line">    background: rgba(0,0,0,0.5)</span><br><span class="line">    border-bottom: none !important</span><br><span class="line">    &amp;:hover</span><br><span class="line">      border-bottom-color: none !important</span><br></pre></td></tr></table></figure></p>
<h3 id="1-5-引用样式"><a href="#1-5-引用样式" class="headerlink" title="1.5 引用样式"></a>1.5 引用样式</h3><p>修改文件<code>themes/next/source/css/_common/components/components.styl</code>，在最后添加一行<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@import &#x27;post&#x27;;</span><br><span class="line">@import &#x27;pages&#x27;;</span><br><span class="line">@import &#x27;third-party&#x27;;</span><br><span class="line">// 添加这一行</span><br><span class="line">@import &#x27;highlight&#x27;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>ssh环境构建和常用命令</title>
    <url>/2022/08/04/ubuntuOS/ssh-server-on-ubuntu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ubuntu上的ssh服务，包含客户端和服务端的应用环境。在docker中使用ssh服务，若没有自动启动，docker关闭重启后服务会挂掉，再次连接就会报<code>Connection refused</code>问题，怎么解决呢？</p>
<p>本文中的命令主要基于ubuntu的系统，大多为ubuntu20.04。</p>
<p><strong>ssh也是常用的工具呀~</strong><br><span id="more"></span></p>
<h2 id="客户端openssh-client"><a href="#客户端openssh-client" class="headerlink" title="客户端openssh-client"></a>客户端openssh-client</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-client     //安装客户端</span><br></pre></td></tr></table></figure>
<h2 id="服务端openssh-server"><a href="#服务端openssh-server" class="headerlink" title="服务端openssh-server"></a>服务端openssh-server</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dpkg -l | grep ssh  //查看ubuntu是否安装openssh-server服务</span><br><span class="line">sudo apt-get install openssh-server</span><br><span class="line">ps -e | grep ssh    //确认服务启动</span><br><span class="line"></span><br><span class="line">sudo /etc/init.d/ssh start  //启动服务命令1</span><br><span class="line">sudo service ssh start      //启动服务命令2</span><br></pre></td></tr></table></figure>
<p>其中，ssh-server配置文件位于/etc/ssh/sshd_config</p>
<h2 id="docker使用-从头构建镜像"><a href="#docker使用-从头构建镜像" class="headerlink" title="docker使用-从头构建镜像"></a>docker使用-从头构建镜像</h2><p>可以在Dockfile中添加关于ssh自动启动的部分，加上如下代码即可：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RUN mkdir /var/run/sshd</span><br><span class="line">EXPOSE 22</span><br><span class="line">CMD [&quot;/user/sbin/ssh&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure><br>释放22端口给ssh服务，同时启动ssh服务到后台运行。</p>
<h2 id="docker使用-已经在运行的系统或docker"><a href="#docker使用-已经在运行的系统或docker" class="headerlink" title="docker使用-已经在运行的系统或docker"></a>docker使用-已经在运行的系统或docker</h2><p>进入docker内部，手动重启ssh服务<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it xinwen bash</span><br><span class="line">sudo /etc/init.d/ssh start  //启动服务命令1</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>Python替你七夕表白</title>
    <url>/2022/08/04/tools/blessing/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>金风玉露一相逢，便胜却人间无数~</p>
<p><strong>七夕节快乐~</strong><br><span id="more"></span></p>
<h2 id="精彩截图"><a href="#精彩截图" class="headerlink" title="精彩截图"></a>精彩截图</h2><p><img src="first.png" alt="blessing-first"><br><img src="next.png" alt="blessing-next"><br><img src="end.png" alt="blessing-end"><br>实际是个动画，挺惊喜哦~</p>
<h2 id="表白代码"><a href="#表白代码" class="headerlink" title="表白代码"></a>表白代码</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#1.导入</span><br><span class="line">import random</span><br><span class="line">from math import *</span><br><span class="line">import turtle</span><br><span class="line">import time</span><br><span class="line">from turtle import mainloop, hideturtle</span><br><span class="line"></span><br><span class="line">#2.生成斐波契那数列</span><br><span class="line">def FRt(n):</span><br><span class="line">    if n&lt;=0:</span><br><span class="line">        return 0</span><br><span class="line">    elif n==1:</span><br><span class="line">        return 1</span><br><span class="line">    else:</span><br><span class="line">        return FRt(n-1)+FRt(n-2)</span><br><span class="line">def FR(n):</span><br><span class="line">    result_list=[]</span><br><span class="line">    for i in range(1,n+3):</span><br><span class="line">        result_list.append(FRt(i))</span><br><span class="line">    return result_list</span><br><span class="line"></span><br><span class="line">#3.定义生成叶子方法</span><br><span class="line">def leaf(x,y,node):</span><br><span class="line">    til=turtle.heading()</span><br><span class="line">    i=random.random()</span><br><span class="line">    an=random.randint(10,180)</span><br><span class="line">    ye=random.randint(6,9)</span><br><span class="line">    ye=ye/10</span><br><span class="line">    turtle.color(ye,ye*0.9,0)</span><br><span class="line">    turtle.fillcolor(ye+0.1,ye+0.05,0)</span><br><span class="line">    turtle.pensize(1)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.setheading(an+90)</span><br><span class="line">    turtle.forward(8*i)</span><br><span class="line">    px=turtle.xcor()</span><br><span class="line">    py=turtle.ycor()</span><br><span class="line">    turtle.begin_fill()</span><br><span class="line">    turtle.circle(7.5*i,120) #画一段120度弧线</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(px,py) #回到圆点位置</span><br><span class="line">    turtle.setheading(an+90)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.circle(-7.5*i,120)</span><br><span class="line">    turtle.setheading(an+100)</span><br><span class="line">    turtle.circle(10.5*i,150)</span><br><span class="line">    turtle.end_fill()</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x,y)</span><br><span class="line">    turtle.setheading(til)</span><br><span class="line">    turtle.pensize(node/2+1)</span><br><span class="line"></span><br><span class="line">#定义生成树</span><br><span class="line">def draw(node,length,level,yu,button):</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    tu=cos(radians(turtle.heading()+5))/8+0.25</span><br><span class="line">    turtle.pencolor(tu*1.6,tu*1.2,tu*1.4)</span><br><span class="line">    turtle.pensize(node/1.2)</span><br><span class="line">    x=random.randint(0,10)  #生成随机数决定要画树枝还是飘落的叶子</span><br><span class="line"></span><br><span class="line">    if level==top and x&gt;6:</span><br><span class="line">        turtle.forward(length)</span><br><span class="line">        yu[level]=yu[level]-1</span><br><span class="line">        c=random.randint(2,10)</span><br><span class="line">        for i in range(1,c):</span><br><span class="line">            leaf(turtle.xcor(),turtle.ycor(),node)</span><br><span class="line">            if random.random()&gt;0.3:</span><br><span class="line">                turtle.penup()</span><br><span class="line">                #飘落</span><br><span class="line">                t1=turtle.heading()</span><br><span class="line">                an1=-40+random.random()*40</span><br><span class="line">                turtle.setheading(an1)</span><br><span class="line">                dis=int(800*random.random()*0.5+400*random.random()*0.3+200*random.random()*0.2)</span><br><span class="line">                turtle.forward(dis)</span><br><span class="line">                turtle.setheading(t1)</span><br><span class="line">                turtle.right(90)</span><br><span class="line">                #画叶子</span><br><span class="line">                leaf(turtle.xcor(),turtle.ycor(),node)</span><br><span class="line">                turtle.left(90)</span><br><span class="line">                t2=turtle.heading()</span><br><span class="line">                turtle.setheading(an1)</span><br><span class="line">                turtle.backward(dis)</span><br><span class="line">                turtle.setheading(t2)</span><br><span class="line">    elif level==top and x&lt;7:</span><br><span class="line">        turtle.penup()</span><br><span class="line">        turtle.forward(length)</span><br><span class="line">    elif level&gt;3 and x&gt;6:</span><br><span class="line">        turtle.pendown()</span><br><span class="line">        turtle.forward(length)</span><br><span class="line">        c=random.randint(4,6)</span><br><span class="line">        for i in range(3,c):</span><br><span class="line">            leaf(turtle.xcor(),turtle.ycor(),node)</span><br><span class="line">        leaf(turtle.xcor(),turtle.ycor(),node)</span><br><span class="line">        button=1</span><br><span class="line">    else:</span><br><span class="line">        turtle.forward(length)</span><br><span class="line">        yu[level]=yu[level]-1</span><br><span class="line">    if node&gt;0 and button==0:</span><br><span class="line">        right=random.random()*5+17</span><br><span class="line">        left=random.random()*20+19</span><br><span class="line">        child_length=length*(random.random()*0.25+0.7)</span><br><span class="line">        r=random.randint(0,1)</span><br><span class="line">        if r==1:</span><br><span class="line">            turtle.right(right)</span><br><span class="line">            level=level+1</span><br><span class="line">        else:</span><br><span class="line">            turtle.left(right)</span><br><span class="line">            level=level+1</span><br><span class="line">        draw(node-1,child_length,level,yu,button)</span><br><span class="line">        yu[level]=yu[level]+1</span><br><span class="line">        if yu[level]&gt;1:</span><br><span class="line">            if r==1:</span><br><span class="line">                turtle.left(right+left)</span><br><span class="line">                draw(node-1,child_length,level,yu,button)</span><br><span class="line">                turtle.right(left)</span><br><span class="line">                yu[level]=yu[level]-1</span><br><span class="line">            else:</span><br><span class="line">                turtle.right(right + left)</span><br><span class="line">                draw(node - 1, child_length, level, yu, button)</span><br><span class="line">                turtle.left(left)</span><br><span class="line">                yu[level] = yu[level] - 1</span><br><span class="line">        else:</span><br><span class="line">            if r==1:</span><br><span class="line">                turtle.left(right+left)</span><br><span class="line">                turtle.right(left)</span><br><span class="line">            else:</span><br><span class="line">                turtle.right(right+left)</span><br><span class="line">                turtle.left(left)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.backward(length)</span><br><span class="line"></span><br><span class="line">def clear_all():</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(0, 0)</span><br><span class="line">    turtle.color(&#x27;white&#x27;)</span><br><span class="line">    turtle.pensize(800)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.fd(300)</span><br><span class="line">    turtle.bk(600)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 重定位海龟的位置</span><br><span class="line">def go_to(x, y, state):</span><br><span class="line">    turtle.pendown() if state else turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_heart(size):</span><br><span class="line">    turtle.color(&#x27;red&#x27;, &#x27;pink&#x27;)</span><br><span class="line">    turtle.pensize(2)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.setheading(150)</span><br><span class="line">    turtle.begin_fill()</span><br><span class="line">    turtle.fd(size)</span><br><span class="line">    turtle.circle(size * -3.745, 45)</span><br><span class="line">    turtle.circle(size * -1.431, 165)</span><br><span class="line">    turtle.left(120)</span><br><span class="line">    turtle.circle(size * -1.431, 165)</span><br><span class="line">    turtle.circle(size * -3.745, 45)</span><br><span class="line">    turtle.fd(size)</span><br><span class="line">    turtle.end_fill()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 画出发射爱心的小人</span><br><span class="line">def draw_people(x, y):</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.pensize(2)</span><br><span class="line">    turtle.color(&#x27;black&#x27;)</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.circle(60, 360)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.setheading(90)</span><br><span class="line">    turtle.fd(75)</span><br><span class="line">    turtle.setheading(180)</span><br><span class="line">    turtle.fd(20)</span><br><span class="line">    turtle.pensize(4)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.circle(2, 360)</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.fd(40)</span><br><span class="line">    turtle.pensize(4)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.circle(-2, 360)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line">    turtle.setheading(-90)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.fd(20)</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.fd(35)</span><br><span class="line">    turtle.setheading(60)</span><br><span class="line">    turtle.fd(10)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line">    turtle.setheading(-90)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.fd(40)</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.fd(35)</span><br><span class="line">    turtle.setheading(-60)</span><br><span class="line">    turtle.fd(10)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line">    turtle.setheading(-90)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.fd(60)</span><br><span class="line">    turtle.setheading(-135)</span><br><span class="line">    turtle.fd(60)</span><br><span class="line">    turtle.bk(60)</span><br><span class="line">    turtle.setheading(-45)</span><br><span class="line">    turtle.fd(30)</span><br><span class="line">    turtle.setheading(-135)</span><br><span class="line">    turtle.fd(35)</span><br><span class="line">    turtle.penup()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 绘制文字</span><br><span class="line">def draw_text(text, t_color, font_size):</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-350, 0)</span><br><span class="line">    turtle.color(t_color)</span><br><span class="line">    turtle.write(text, font=(&#x27;宋体&#x27;, font_size, &#x27;normal&#x27;))</span><br><span class="line">    time.sleep(2)</span><br><span class="line">    clear_all()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 爱心发射</span><br><span class="line">def draw_():</span><br><span class="line">    turtle.speed(0)</span><br><span class="line">    draw_people(-250, 20)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-150, -30)</span><br><span class="line">    draw_heart(14)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-200, -200)</span><br><span class="line">    turtle.color(&#x27;pink&#x27;)</span><br><span class="line">    turtle.write(&#x27;Biu~&#x27;, font=(&#x27;宋体&#x27;, 60, &#x27;normal&#x27;))</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-40, -60)</span><br><span class="line">    draw_heart(25)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-28, -200)</span><br><span class="line">    turtle.color(&#x27;pink&#x27;)</span><br><span class="line">    turtle.write(&#x27;Biu~&#x27;, font=(&#x27;宋体&#x27;, 60, &#x27;normal&#x27;))</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(240, -100)</span><br><span class="line">    draw_heart(45)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(150, -200)</span><br><span class="line">    turtle.color(&#x27;pink&#x27;)</span><br><span class="line">    turtle.write(&#x27;七夕快乐~&#x27;, font=(&#x27;宋体&#x27;, 60, &#x27;normal&#x27;))</span><br><span class="line">    turtle.hideturtle()</span><br><span class="line">    time.sleep(3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    # 隐藏海龟</span><br><span class="line">    hideturtle()</span><br><span class="line">    turtle.setup(width=0.75,height=0.75)</span><br><span class="line"></span><br><span class="line">    draw_text(&quot;Are You Readly？&quot;, &quot;black&quot;, 60)</span><br><span class="line">    draw_text(&quot;接下来&quot;, &quot;skyblue&quot;, 60)</span><br><span class="line">    draw_text(&quot;大飞哥，七夕快乐！&quot;, &quot;pink&quot;, 80)</span><br><span class="line">    draw_()</span><br><span class="line">    # 使用mainloop防止窗口卡死</span><br><span class="line">    mainloop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#主函数</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    turtle.setup(width=0.75,height=0.75)             #屏幕占比</span><br><span class="line">    turtle.hideturtle()                            #隐藏</span><br><span class="line">    turtle.speed(0)                              #速度0-9</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.left(90)</span><br><span class="line">    turtle.backward(300)</span><br><span class="line">    top=9</span><br><span class="line">    yu=FR(top)</span><br><span class="line">    yu.remove(yu[0])</span><br><span class="line">    button=0</span><br><span class="line">    draw(9,120,0,yu,button)</span><br><span class="line">    turtle.write(&quot;    相爱相知!\n&quot;,font=(&quot;微软雅黑&quot;,14,&quot;normal&quot;))</span><br><span class="line">    turtle.write(&quot;  ~七夕快乐！&quot;,font=(&quot;微软雅黑&quot;,16,&quot;normal&quot;))</span><br><span class="line">    time.sleep(3)</span><br><span class="line">    turtle.clear()</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>Python运行就好啦，拿走不谢~~</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>blessing</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode连接远程机器上的docker</title>
    <url>/2022/08/02/ubuntuOS/vscode-docker-otherPC/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>vscode连接远程机器上的docker，应用太多了，此处是用win的PC连接远程ubuntuPC上运行的docker，docker是基于ubuntu20.04的博客部署系统~</p>
<p><strong>又一个利器~</strong><br><span id="more"></span></p>
<h2 id="1，前提"><a href="#1，前提" class="headerlink" title="1，前提"></a>1，前提</h2><p>宿主机连接本机上运行的docker，并用vscode连接，详见<a href="https://sophia-hxw.github.io/2022/08/01/ubuntuOS/vscode-docker-onePC/">宿主机上vscode连接本机docker</a></p>
<h2 id="2，docker上的ssh配置"><a href="#2，docker上的ssh配置" class="headerlink" title="2，docker上的ssh配置"></a>2，docker上的ssh配置</h2><p>在docker中修改sshd服务的配置文件，即<code>/etc/ssh/sshd_config</code>，将端口号的配置项去掉注释即可。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Port 22</span><br></pre></td></tr></table></figure><br>注意：docker使用的ssh默认的端口号22，映射到主机上的端口号是8022（详情见<a href="https://sophia-hxw.github.io/2022/08/01/ubuntuOS/vscode-docker-onePC/">宿主机上vscode连接本机docker</a>）</p>
<h2 id="3，远程主机-非docker-开放端口"><a href="#3，远程主机-非docker-开放端口" class="headerlink" title="3，远程主机(非docker)开放端口"></a>3，远程主机(非docker)开放端口</h2><p>此处主机上开放的端口是8022，也就是<code>docker run</code>时用来映射docker内22的端口，就不赘述了~<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo iptables -I INPUT -p tcp --dport 8022 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<h2 id="4，测试连接"><a href="#4，测试连接" class="headerlink" title="4，测试连接"></a>4，测试连接</h2><ul>
<li>配置免密访问<br>密码登录请自行测试，为了方便，本文采用了密钥对来免密访问。</li>
</ul>
<p>复制本机（我的路径是C:\Users\xinwen.ssh\id_rsa.pub）的公钥到docker的可信赖机器中（/root/.ssh/authorized_keys）。</p>
<ul>
<li>ssh连接测试<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 本机连接远程主机</span><br><span class="line">ssh xinwen@192.168.1.18</span><br><span class="line"></span><br><span class="line"># 本机连接远程主机上的docker</span><br><span class="line">ssh root@192.168.1.18 -p 8022</span><br></pre></td></tr></table></figure>
其中，192.168.1.18是远程主机的ip，xinwen是远程主机上的用户，8022是远程主机上开放给docker的端口，root是docker中部署了hexo博客系统的用户。</li>
</ul>
<h2 id="5，本机vscode上的插件"><a href="#5，本机vscode上的插件" class="headerlink" title="5，本机vscode上的插件"></a>5，本机vscode上的插件</h2><p>Remote-SSH</p>
<p>登录命令<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh root@192.168.1.18 -p 8022</span><br></pre></td></tr></table></figure></p>
<h2 id="6，总结"><a href="#6，总结" class="headerlink" title="6，总结"></a>6，总结</h2><p>docker(端口22)-&gt;远程主机(端口8022)-&gt;本机</p>
<p>本文中对应的系统：<br>ubuntu20.04-&gt;ubuntu20.04-&gt;windows</p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode连接本机docker</title>
    <url>/2022/08/01/ubuntuOS/vscode-docker-onePC/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>vscode连接docker方便开发，此处主要用在hexo的博客编写，很方便~</p>
<p><strong>安心码文了~</strong><br><span id="more"></span></p>
<h2 id="1，docker启动"><a href="#1，docker启动" class="headerlink" title="1，docker启动"></a>1，docker启动</h2><h3 id="1-1-blog镜像的配置"><a href="#1-1-blog镜像的配置" class="headerlink" title="1.1 blog镜像的配置"></a>1.1 blog镜像的配置</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM ubuntu:20.04</span><br><span class="line"></span><br><span class="line">WORKDIR /home</span><br><span class="line">LABEL maintainer=&quot;XINWEN&quot;</span><br><span class="line">RUN mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">RUN echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list </span><br><span class="line"></span><br><span class="line">ENV TZ=Asia/Shanghai</span><br><span class="line">RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get upgrade -y</span><br><span class="line"></span><br><span class="line">RUN DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends apt-utils \</span><br><span class="line">    &amp;&amp; apt-get -y install dialog</span><br><span class="line"></span><br><span class="line">RUN apt-get -y install dialog clang-format nfs-common \</span><br><span class="line">    &amp;&amp; apt-get -y install openssh-server \</span><br><span class="line">    &amp;&amp; apt-get -y install vim python3-pip git curl</span><br><span class="line"></span><br><span class="line">RUN pip3 install -i http://mirrors.aliyun.com/pypi/simple -U pip --trusted-host mirrors.aliyun.com \</span><br><span class="line">    &amp;&amp; pip3 config set global.index-url http://mirrors.aliyun.com/pypi/simple/ \</span><br><span class="line">    &amp;&amp; pip3 config set install.trusted-host mirrors.aliyun.com</span><br><span class="line"></span><br><span class="line">RUN apt-get install -y nodejs npm</span><br><span class="line"></span><br><span class="line">RUN node -v \</span><br><span class="line">    &amp;&amp; npm install n -g \</span><br><span class="line">    &amp;&amp; n stable \</span><br><span class="line">    &amp;&amp; hash -r \</span><br><span class="line">    &amp;&amp; cp /usr/local/bin/node /usr/bin/node \</span><br><span class="line">    &amp;&amp; node -v</span><br><span class="line"></span><br><span class="line">RUN mkdir /var/run/sshd</span><br><span class="line">EXPOSE 22</span><br><span class="line">CMD [&quot;/user/sbin/ssh&quot;,&quot;-D&quot;]</span><br><span class="line"></span><br><span class="line">ENTRYPOINT /bin/bash</span><br></pre></td></tr></table></figure>
<p>构建命令，在Dockerfile的同目录运行：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -f Dockerfile -t blog .</span><br></pre></td></tr></table></figure></p>
<h3 id="1-2-创建容器"><a href="#1-2-创建容器" class="headerlink" title="1.2 创建容器"></a>1.2 创建容器</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it --name=xinwen -p 4000:4000 -p 8022:22 -v /home/xinwen/xinwenblog:/xinwen blog</span><br></pre></td></tr></table></figure>
<h2 id="2，宿主机和docker中关于ssh的配置"><a href="#2，宿主机和docker中关于ssh的配置" class="headerlink" title="2，宿主机和docker中关于ssh的配置"></a>2，宿主机和docker中关于ssh的配置</h2><h3 id="2-1-docker允许远程登录"><a href="#2-1-docker允许远程登录" class="headerlink" title="2.1 docker允许远程登录"></a>2.1 docker允许远程登录</h3><p>在docker中修改sshd服务的配置文件，在<code>/etc/ssh/sshd_config</code>位置，将下面的配置项去掉注释即可。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PermitRootLogin yes</span><br><span class="line">PubkeyAuthentication yes    </span><br><span class="line">PasswordAuthentication yes</span><br></pre></td></tr></table></figure><br>保存后重启sshd服务，也就是：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ssh restart</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-宿主机查看docker的ip"><a href="#2-2-宿主机查看docker的ip" class="headerlink" title="2.2 宿主机查看docker的ip"></a>2.2 宿主机查看docker的ip</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker inspect xinwen</span><br></pre></td></tr></table></figure>
<p>在宿主机上运行，xinwen是docker别名。</p>
<h3 id="2-3-登录宿主机进行连接验证"><a href="#2-3-登录宿主机进行连接验证" class="headerlink" title="2.3 登录宿主机进行连接验证"></a>2.3 登录宿主机进行连接验证</h3><p>密码登录或者ssh免密登录方式都可以，二者择一即可。</p>
<ul>
<li><p>密码登录<br>修改docker的root用户密码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">passwd</span><br></pre></td></tr></table></figure>
</li>
<li><p>ssh免密登录<br>宿主机上生成自己的秘钥对，邮箱也相应更改为自己的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;hxinwen1218@sina.com&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>复制宿主机的公钥到docker中<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -i /home/xinwen/.ssh/id_rsa.pub root@172.17.0.2</span><br></pre></td></tr></table></figure><br>root是docker的用户，<code>172.17.0.2</code>是docker的ip，<code>/home/xinwen/.ssh/id_rsa.pub</code>是宿主机的公钥。</p>
<ul>
<li>验证登录<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh root@172.17.0.2</span><br></pre></td></tr></table></figure>
输出下面信息就成功了<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Welcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.15.0-41-generic x86_64)</span><br><span class="line"></span><br><span class="line"> * Documentation:  https://help.ubuntu.com</span><br><span class="line"> * Management:     https://landscape.canonical.com</span><br><span class="line"> * Support:        https://ubuntu.com/advantage</span><br><span class="line"></span><br><span class="line">This system has been minimized by removing packages and content that are</span><br><span class="line">not required on a system that users do not log into.</span><br><span class="line"></span><br><span class="line">To restore this content, you can run the &#x27;unminimize&#x27; command.</span><br><span class="line">Last login: Mon Aug  1 17:55:51 2022 from 172.17.0.1</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3，VSCODE插件"><a href="#3，VSCODE插件" class="headerlink" title="3，VSCODE插件"></a>3，VSCODE插件</h2><p>主要是两个插件<code>Docker</code>和<code>Remote-Containers</code></p>
<p>点击左下角”&gt;&lt;“标志，在顶部的输入下拉框选择<code>Attach to Running Container...</code>就能打开docker的home目录。</p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo上的next主题优化项</title>
    <url>/2022/08/01/ubuntuOS/hexo-next-theme/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>hexo上next的主题优化项，选择了一些个人的偏爱项，基本就是当前博客的展示效果了~</p>
<p><strong>安心码文了~</strong><br><span id="more"></span></p>
<h2 id="1，下载主题文件"><a href="#1，下载主题文件" class="headerlink" title="1，下载主题文件"></a>1，下载主题文件</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd hexo #本文blog的主目录的名称为hexo</span><br><span class="line"></span><br><span class="line">git clone https://github.com/theme-next/hexo-theme-next themes/next</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2，主题优化"><a href="#2，主题优化" class="headerlink" title="2，主题优化"></a>2，主题优化</h2><h3 id="2-1-显示本地图片"><a href="#2-1-显示本地图片" class="headerlink" title="2.1 显示本地图片"></a>2.1 显示本地图片</h3><ul>
<li><p>安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-marked</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改站点文件<code>_config.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 文章资源文件夹</span><br><span class="line">post_asset_folder: true</span><br><span class="line"># 以下内容需要添加</span><br><span class="line">marked:</span><br><span class="line">  prependRoot: true</span><br><span class="line">  postAsset: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>引用图片的方式<br>在文章的同级目录放置与文章同名的文件夹来存放需要在本文中展示的图片</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用这种方式引用图片</span><br><span class="line">![](image.jpg)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-2-图片放大预览功能"><a href="#2-2-图片放大预览功能" class="headerlink" title="2.2 图片放大预览功能"></a>2.2 图片放大预览功能</h3><ul>
<li>修改<code>next.yml</code>文件<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fancybox: true #添加图片放大预览功能</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-3-添加关于、标签、分类页面"><a href="#2-3-添加关于、标签、分类页面" class="headerlink" title="2.3 添加关于、标签、分类页面"></a>2.3 添加关于、标签、分类页面</h3><ul>
<li><p>修改主题配置文件<code>next.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 去除下面的注释</span><br><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  about: /about/ || fa fa-user</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br><span class="line">  categories: /categories/ || fa fa-th</span><br><span class="line">  archives: /archives/ || fa fa-archive</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new page &quot;about&quot;</span><br><span class="line">hexo new page &quot;tags&quot;</span><br><span class="line">hexo new page &quot;categories&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改文件和配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source\about\index.md</span><br><span class="line">source\tags\index.md</span><br><span class="line">source\categories\index.md</span><br></pre></td></tr></table></figure>
<p>相应文件内修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 关于</span><br><span class="line">type: &quot;about&quot;</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">title: 标签</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">title: 分类</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-4-添加搜索功能"><a href="#2-4-添加搜索功能" class="headerlink" title="2.4 添加搜索功能"></a>2.4 添加搜索功能</h3><ul>
<li><p>安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>
</li>
<li><p>站点文件<code>_config.yml</code>添加搜索</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-5-回到顶部按钮"><a href="#2-5-回到顶部按钮" class="headerlink" title="2.5 回到顶部按钮"></a>2.5 回到顶部按钮</h3><p>修改配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">back2top:</span><br><span class="line">  enable: true</span><br><span class="line">  # 将返回按钮设置在侧边栏</span><br><span class="line">  sidebar: false</span><br><span class="line">  # 按钮上显示进度百分比</span><br><span class="line">  scrollpercent: true</span><br></pre></td></tr></table></figure></p>
<h3 id="2-6-字数统计和预计阅读时间"><a href="#2-6-字数统计和预计阅读时间" class="headerlink" title="2.6 字数统计和预计阅读时间"></a>2.6 字数统计和预计阅读时间</h3><ul>
<li><p>安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-symbols-count-time --save</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改站点配置文件<code>_config.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  symbols: true  # 文章字数</span><br><span class="line">  time: true  # 阅读时长</span><br><span class="line">  total_symbols: true  # 所有文章总字数</span><br><span class="line">  total_time: true  # 所有文章阅读中时长</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改主题配置文件<code>next.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: true # 是否换行显示 字数统计 及 阅读时长</span><br><span class="line">  item_text_post: true  # 文章 字数统计 阅读时长 使用图标 还是 文本表示</span><br><span class="line">  item_text_total: true # 博客底部统计 字数统计 阅读时长 使用图标 还是 文本表示</span><br><span class="line">  awl: 4</span><br><span class="line">  wpm: 275</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-7-代码块复制和代码高亮"><a href="#2-7-代码块复制和代码高亮" class="headerlink" title="2.7 代码块复制和代码高亮"></a>2.7 代码块复制和代码高亮</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">codeblock:</span><br><span class="line">  # 代码高亮</span><br><span class="line">  highlight_theme: night bright</span><br><span class="line">  # 复制</span><br><span class="line">  copy_button:</span><br><span class="line">    enable: true</span><br><span class="line">    # 显示文本复制结果</span><br><span class="line">    show_result: true</span><br><span class="line">    # 可以选择的样式: default | flat | mac</span><br><span class="line">    style: mac</span><br></pre></td></tr></table></figure></p>
<h3 id="2-8-文章内链接文本样式"><a href="#2-8-文章内链接文本样式" class="headerlink" title="2.8 文章内链接文本样式"></a>2.8 文章内链接文本样式</h3><ul>
<li>在<code>hexo/source/_data</code>中新增样式文件<code>styles.styl</code><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 修改链接样式</span><br><span class="line">.post-body p a&#123;</span><br><span class="line">  color: #0593d3;</span><br><span class="line">  border-bottom: none;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    color: #ff106c;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">a, span.exturl &#123;</span><br><span class="line">  border-bottom: none;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    color: #ff106c;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>修改主题配置文件<code>next.yml</code>，去掉<code>styles.styl</code>的注释<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">style: source/_data/styles.styl</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-9-文章底部增加版权信息"><a href="#2-9-文章底部增加版权信息" class="headerlink" title="2.9 文章底部增加版权信息"></a>2.9 文章底部增加版权信息</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">creative_commons:</span><br><span class="line">  license: by-nc-sa</span><br><span class="line">  sidebar: false # 不显示在侧边栏</span><br><span class="line">  post: true</span><br><span class="line">  language:</span><br></pre></td></tr></table></figure></p>
<h3 id="2-10-文章底部tag图标"><a href="#2-10-文章底部tag图标" class="headerlink" title="2.10 文章底部tag图标"></a>2.10 文章底部tag图标</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tag_icon: true</span><br></pre></td></tr></table></figure></p>
<h3 id="2-11-侧边栏文章目录设置"><a href="#2-11-侧边栏文章目录设置" class="headerlink" title="2.11 侧边栏文章目录设置"></a>2.11 侧边栏文章目录设置</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  enable: true</span><br><span class="line">  # 自动编号</span><br><span class="line">  number: false</span><br><span class="line">  # 超出宽度跨行</span><br><span class="line">  wrap: true</span><br><span class="line">  # 展开所有</span><br><span class="line">  expand_all: true</span><br><span class="line">  # 最大标题深度</span><br><span class="line">  max_depth: 6</span><br></pre></td></tr></table></figure></p>
<h3 id="2-12-侧边社交链接"><a href="#2-12-侧边社交链接" class="headerlink" title="2.12 侧边社交链接"></a>2.12 侧边社交链接</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">social:</span><br><span class="line">  GitHub: https://github.com/sophia-hxw || fab fa-github</span><br><span class="line">  CSDN: https://blog.csdn.net/sophia_xw || crosshairs</span><br><span class="line">  E-Mail: mailto:xinwen618@gmail.com || fa fa-envelope</span><br></pre></td></tr></table></figure></p>
<h3 id="2-13-文章置顶"><a href="#2-13-文章置顶" class="headerlink" title="2.13 文章置顶"></a>2.13 文章置顶</h3><ul>
<li><p>安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-index-pin-top --save</span><br></pre></td></tr></table></figure>
<p>其中，hexo-generator-index是hexo默认的文章排序插件，hexo-generator-index-pin-top是替换掉默认的排序插件，且有置顶功能。</p>
</li>
<li><p>需要置顶的文章中添加置顶项</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: hexo上文章怎么置顶？</span><br><span class="line">date: 2022-06-22 23:28:09</span><br><span class="line">tags: </span><br><span class="line">- hexo</span><br><span class="line">categories:</span><br><span class="line">- ubuntuOS</span><br><span class="line">top: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加文章置顶标志<br>打开<code>/blog/themes/next/layout/_macro</code>目录下的<code>post.swig</code>文件，在<code>&lt;div class=&quot;post-meta&quot;&gt;</code>标签下添加下面内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% if post.top %&#125;</span><br><span class="line">  &lt;i class=&quot;fa fa-thumb-tack&quot;&gt;&lt;/i&gt;</span><br><span class="line">  &lt;font color=7D26CD&gt;置顶&lt;/font&gt;</span><br><span class="line">  &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-14-专业数学公式引擎"><a href="#2-14-专业数学公式引擎" class="headerlink" title="2.14 专业数学公式引擎"></a>2.14 专业数学公式引擎</h3><p>Hexo默认使用”hexo-renderer-marked”引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线<code>_</code>代表斜体，会被渲染引擎处理为<code>&lt;em&gt;</code>标签。</p>
<p>类似的语义冲突的符号还包括<code>&#39;*&#39;, &#39;&#123;&#39;, &#39;&#125;&#39;, &#39;\&#39;</code>等。</p>
<ul>
<li>安装依赖<br>更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<ul>
<li>修改新渲染引擎的歧义<br>然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为hexo-renderer-kramed引擎也有语义冲突的问题。</li>
</ul>
<p>接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//  escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">  escape: /^\\([`*\[\]()#$+\-.!_&gt;])/</span><br></pre></td></tr></table></figure>
<p>这一步是在原基础上取消了对\,{,}的转义(escape)。<br>同时把第20行的em变量也要做相应的修改。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//  em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">  em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><br></pre></td></tr></table></figure>
<p>再重新启动hexo，若没有解决问题就需要打开主题中mathjax的支持了</p>
<ul>
<li><p>修改主题配置文件<code>next.yml</code><br>进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>文章使用渲染工具<br>在文章的Front-matter里打开mathjax开关，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: index.html</span><br><span class="line">date: 2016-12-28 21:01:30</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">--</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-15-主题布局改为圆角"><a href="#2-15-主题布局改为圆角" class="headerlink" title="2.15 主题布局改为圆角"></a>2.15 主题布局改为圆角</h3><ul>
<li><p>在 <code>hexo/source/_data</code> 目录下新建 <code>variables.styl</code> 文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 圆角设置</span><br><span class="line">$border-radius-inner     = 20px 20px 20px 20px;</span><br><span class="line">$border-radius           = 20px;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改主题配置文件<code>next.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 去掉注释</span><br><span class="line">variable: source/_data/variables.styl</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-16-设置博客背景图片"><a href="#2-16-设置博客背景图片" class="headerlink" title="2.16 设置博客背景图片"></a>2.16 设置博客背景图片</h3><ul>
<li><p>背景图片位置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo/source/images </span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<code>hexo/source/_data/styles.styl</code>样式代码<br>// 设置背景图片<br>body {<br>  background:url(/images/background.jpg);<br>  background-repeat: no-repeat;<br>  background-attachment:fixed; //不重复<br>  background-size: cover;      //填充<br>  background-position:50% 50%;<br>}</p>
</li>
</ul>
<h3 id="2-17-设置博客文章透明度"><a href="#2-17-设置博客文章透明度" class="headerlink" title="2.17 设置博客文章透明度"></a>2.17 设置博客文章透明度</h3><p>修改<code>hexo/source/_data/styles.styl</code>样式代码<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//博客内容透明化</span><br><span class="line">//文章内容的透明度设置</span><br><span class="line">.content-wrap &#123;</span><br><span class="line">  opacity: 0.9;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//侧边框的透明度设置</span><br><span class="line">.sidebar &#123;</span><br><span class="line">  opacity: 0.9;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//菜单栏的透明度设置</span><br><span class="line">.header-inner &#123;</span><br><span class="line">  background: rgba(255,255,255,0.9);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//搜索框（local-search）的透明度设置</span><br><span class="line">.popup &#123;</span><br><span class="line">  opacity: 0.9;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="2-18-顶部阅读进度条"><a href="#2-18-顶部阅读进度条" class="headerlink" title="2.18 顶部阅读进度条"></a>2.18 顶部阅读进度条</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">reading_progress:</span><br><span class="line">  enable: true</span><br><span class="line">  # 显示在顶部</span><br><span class="line">  position: top</span><br><span class="line">  color: &quot;#06d633&quot;</span><br><span class="line">  height: 3px</span><br></pre></td></tr></table></figure></p>
<h3 id="2-19-文章阴影"><a href="#2-19-文章阴影" class="headerlink" title="2.19 文章阴影"></a>2.19 文章阴影</h3><p>在<code>hexo/source/_data/styles.styl</code>中添加样式代码<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 文章阴影</span><br><span class="line">.post &#123;</span><br><span class="line">   margin-top: 50px;</span><br><span class="line">   margin-bottom: 50px;</span><br><span class="line">   padding: 25px;</span><br><span class="line">   -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);</span><br><span class="line">   -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="2-20-修改分类页面样式"><a href="#2-20-修改分类页面样式" class="headerlink" title="2.20 修改分类页面样式"></a>2.20 修改分类页面样式</h3><p>添加<code>hexo/source/_data/styles.styl</code>样式代码<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 分类&amp;&amp;标签 页面样式</span><br><span class="line">// 分类&amp;&amp;标签 页面样式</span><br><span class="line">.post-block.page &#123;</span><br><span class="line">    margin-top: 40px;</span><br><span class="line">&#125;</span><br><span class="line">// 分类页面page</span><br><span class="line">.category-all-page &#123;</span><br><span class="line">    box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.4);</span><br><span class="line">    background-color: #797D7F;</span><br><span class="line">    padding: 20px 30px 60px 30px;</span><br><span class="line">    border-radius: 25px 25px 25px 25px;</span><br><span class="line">&#125;</span><br><span class="line">.category-all-title &#123;</span><br><span class="line">    font-family: Impact;</span><br><span class="line">    font-size: 24px;</span><br><span class="line">    color: aqua;</span><br><span class="line">&#125;</span><br><span class="line">.category-list &#123;</span><br><span class="line">    overflow: auto;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li &#123;</span><br><span class="line">    height: 100%;</span><br><span class="line">    float: left;</span><br><span class="line">    border-right: 3px solid #222;</span><br><span class="line">    padding: 0 20px;</span><br><span class="line">&#125;</span><br><span class="line">.category-all ul li &#123;</span><br><span class="line">    list-style: none!important;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li:last-child &#123;</span><br><span class="line">    border-right: none;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li a &#123;</span><br><span class="line">    font-size: 16px;</span><br><span class="line">    text-decoration: none;</span><br><span class="line">    color: aqua;</span><br><span class="line">    font-family: Helvetica, Verdana, sans-serif;</span><br><span class="line">    text-transform: uppercase;</span><br><span class="line">    -webkit-transition: all 0.5s ease;</span><br><span class="line">    -moz-transition: all 0.5s ease;</span><br><span class="line">    -o-transition: all 0.5s ease;</span><br><span class="line">    -ms-transition: all 0.5s ease;</span><br><span class="line">    transition: all 0.5s ease;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li a:hover &#123;</span><br><span class="line">    color: black;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li.active a &#123;</span><br><span class="line">    font-weight: bold;</span><br><span class="line">    color: black;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="3，总结"><a href="#3，总结" class="headerlink" title="3，总结"></a>3，总结</h2><p>以上是本博客中已经部署的样式，可以按需选用~</p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>PIFuHD model</title>
    <url>/2022/07/28/3D_reconstruction/PIFuHD/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>title: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization</p>
<p><a href="https://arxiv.org/abs/2004.00452">essay link</a></p>
<p><strong>PIFuHD是PIFu原作者的最新工作，发表于CVPR2020(oral)，论文主要是基于单张RGB实现3D人体重建，效果非常惊艳~</strong><br><span id="more"></span></p>
]]></content>
      <categories>
        <category>3DReconstruction</category>
      </categories>
      <tags>
        <tag>PIFuHD</tag>
      </tags>
  </entry>
  <entry>
    <title>XR, VR和AR的概念区别</title>
    <url>/2022/07/26/tools/xr-vr-ar/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>元宇宙与人机交互中的几个基本概念，比如XR，VR，AR等，你能正确理解么？</p>
<p><strong>元宇宙，新世界啦~</strong><br><span id="more"></span></p>
<h2 id="XR"><a href="#XR" class="headerlink" title="XR"></a>XR</h2><p>Extended Reality，拓展现实。</p>
<p>指通过计算机将现实与虚拟相结合，打造一个人机交互的虚拟环境，是VR，AR和MR等技术的统称。</p>
<h2 id="VR"><a href="#VR" class="headerlink" title="VR"></a>VR</h2><p>Virtual Reality，虚拟现实。</p>
<p>模拟一个虚拟世界，利用计算机生成一种模拟环境，使用户利用设备沉浸到该环境中，让人有种身临其境的感觉，强调用户与虚拟世界的实时交互，带来封闭式、沉浸式的虚拟世界体验，这个虚拟世界不是我们直接就能看到的，而是利用设备（如VR眼镜）才能看到，所以称为“虚拟现实”。</p>
<h2 id="AR"><a href="#AR" class="headerlink" title="AR"></a>AR</h2><p>Augmented Reality，增强现实。</p>
<p>一种将现实世界信息和虚拟世界信息“无缝”集成的新技术，它把原本在现实世界的一定时间范围内很难体验到的实体信息（视觉、听觉、味觉、触觉、嗅觉等信息）通过计算机等科学技术，模拟仿真后再叠加到现实世界，被人类感官所感知，从而达到了超越现实的感官体验，现实环境和虚拟的物体实时地叠加到了同一个画面或空间同时存在，从而实现对现实世界的“增强”，所以称之为“增强现实”。</p>
<h2 id="MR"><a href="#MR" class="headerlink" title="MR"></a>MR</h2><p>Mixed Reality，混合现实。</p>
<p>混合现实技术是虚拟现实技术的进一步发展，该技术通过在现实场景呈现虚拟场景信息，在现实世界、虚拟世界和用户之间搭起一个交互反馈的信息回路，以增强用户体验的真实感。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单地说，VR的画面都是假的，AR的画面是一半真的，一半假的。</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>XR</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之独立成分分析（note11）</title>
    <url>/2022/07/22/cs229/note11/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note11的主要内容：。。。</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>liquid学习资源</title>
    <url>/2022/07/20/tools/liquid-intro/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Liquid 是一门灵活、安全的模版语言，被用于许多不同环境中。Liquid 被创建之初是用在 Shopify 商店系统中的，后来也被广泛用于 Jekyll 网站中。随着时间的推移，Shopify 和 Jekyll 分别为 Liquid 添加了针对各自用途的对象（object）、标记（tag）和过滤器（filter）。目前最流行的 Liquid 版本包括 Liquid、Shopify Liquid 和 Jekyll Liquid。</p>
<p><a href="https://liquid.bootcss.com/basics/introduction/">websit</a></p>
<p><strong>学习新内容了，要加油哦~</strong><br><span id="more"></span></p>
<h2 id="1，简介"><a href="#1，简介" class="headerlink" title="1，简介"></a>1，简介</h2><p>对象告诉Liquid 在页面的哪个位置展示内容。对象和变量名由双花括号标识：$\{\{ … \}\}$。</p>
<p>标记创造了模板的逻辑和控制流。他们由单括号加百分号标识：$\{\% … \%\}$。<br>标记并不产生任何可见的文本输出。这意味着你可以用他们为变量赋值、创建条件和循环逻辑，并且不在页面上显示出任何 Liquid 逻辑代码。</p>
<p>标记的作用：</p>
<ul>
<li>控制流</li>
<li>迭代</li>
<li>变量赋值</li>
</ul>
<p>过滤器改变Liquid对象的输出。他们被用在输出上，通过一个 $|$ 符号分隔。</p>
<h2 id="2，操作符"><a href="#2，操作符" class="headerlink" title="2，操作符"></a>2，操作符</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">==</span><br><span class="line">!=</span><br><span class="line">&lt;</span><br><span class="line">&lt;=</span><br><span class="line">&gt;</span><br><span class="line">&gt;=</span><br><span class="line">or</span><br><span class="line">and</span><br></pre></td></tr></table></figure>
<p><strong>contains</strong>操作符作用：</p>
<ul>
<li>检查在一个字符串中是否存在某个子串。</li>
<li>用于检查一个字符串数组中是否存在某个字符串。</li>
</ul>
<p>Note: 只能用于搜索字符串。你不能将其用于从一个对象数组中检查是否存在某个对象。</p>
<h2 id="3，真值和假值"><a href="#3，真值和假值" class="headerlink" title="3，真值和假值"></a>3，真值和假值</h2>]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>liquid</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之感知器和大边界分类器（note6）</title>
    <url>/2022/07/12/cs229/note6/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note6的主要内容：感知器和大边界分类器</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="感知器和大边界分类器"><a href="#感知器和大边界分类器" class="headerlink" title="感知器和大边界分类器"></a>感知器和大边界分类器</h2><p>本章是讲义中关于学习理论的最后一部分，我们来介绍另一种机器学习模式。在之前的内容中，我们考虑的都是批量学习的情况，即给了我们训练样本集用于学习，然后用学习得到的假设 $h $来评估和判别测试数据。在本章，我们要讲一种新的机器学习模式：在线学习，这种情况下，我们的学习算法要在进行学习的同时给出预测。</p>
<p>学习算法会获得一个样本序列，其中内容为有次序的学习样本，$(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), …(x^{(m)},y^{(m)}) $。最开始获得的就是 $x^{(1)} $，然后需要预测 $y^{(1)} $。在完成了这个预测之后，再把 $y^{(1)} $ 的真实值告诉给算法（然后算法就利用这个信息来进行某种学习了）。接下来给算法提供 $x^{(2)} $，再让算法对 $y^{(2)} $ 进行预测，然后再把 $y^{(2)} $ 的真实值告诉给算法，这样算法就又能学习到一些信息了。这样的过程一直持续到最末尾的样本 $(x^{(m)},y^{(m)}) $。在这种在线学习的背景下，我们关心的是算法在此过程中出错的总次数。因此，这适合需要一边学系一边给出预测的应用情景。</p>
<p>接下来，我们将对感知器学习算法的在线学习误差给出一个约束。为了让后续的推导更容易，我们就用正负号来表征分类标签，即设 $y \in \{−1, 1\} $。<br>回忆一下感知器算法（在第二章中有讲到），其参数 $\theta \in \mathbb{R}^{n+1} $，该算法据下面的方程来给出预测： </p>
<script type="math/tex; mode=display">h_{\theta}(x) = g(\theta^{T}x)</script><p>其中：</p>
<script type="math/tex; mode=display">g(z) = \left \{\begin{array}{ll} 1 & if \quad z\geq 0 \\ -1 & if \quad z<0 \end{array} \right.</script><p>然后，给定一个训练样本 $(x, y) $，感知器学习规则就按照如下所示来进行更新。如果 $h_{\theta}(x) = y $，那么不改变参数。若二者相等关系不成立，则进行更新。</p>
<script type="math/tex; mode=display">\theta := \theta +yx</script><p>当感知器算法作为在线学习算法运行的时候，每次对样本给出错误判断的时候，则更新参数，下面的定理给出了这种情况下的在线学习误差的约束边界。要注意，下面的错误次数的约束边界与整个序列中样本的个数 $m $不具有特定的依赖关系，和输入特征的维度 $n $也无关。</p>
<p><strong>定理</strong> (Block, 1962, and Novikoff, 1962)。设有一个样本序列：$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots (x^{(m)}, y^{(m)}) $。假设对于所有的 $i $，都有 $||x^{(i)}|| ≤ \mathcal{D} $，更进一步存在一个单位长度向量 $u, \quad ||u||_{2} = 1 $ 对序列中的所有样本都满足 $y^{(i)}\cdot (u^{T} x^{(i)}) \geq \gamma $（例如，$u^{T} x^{(i)} \geq \gamma\quad if\quad y^{(i)} = 1 $, 而 $u^{T}x^{(i)} \leq −\gamma $，若 $y^{(i)} = −1 $，则 $u $就以一个宽度至少为 $\gamma $的边界分开了样本数据。）而此感知器算法针对这个序列给出错误预测的综述的上限为 $(\mathcal{D}/\gamma)^{2} $。</p>
<p><strong>证明</strong>感知器算法每次只针对出错的样本进行权重更新，设 $\theta^{(k)} $ 为犯了第 $k $个错误的时候的权重。则 $\theta^{(1)} =\vec{0}  $（因为初始权重为零），若第 $k $个错误发生在样本 $(x^{(i)}, y^{(i)}) $，则$g((x^{(i)})^{T}\theta^{(k)}) \neq y^{(i)} $，也就意味着：</p>
<script type="math/tex; mode=display">(x^{(i)})^{T}\theta^{(k)}y^{(i)} \leq 0</script><p>另外根据感知器算法的定义，我们知道 $\theta^{(k+1)} = \theta^{(k)} + y^{(i)}x^{(i)} $<br>然后就得到： </p>
<script type="math/tex; mode=display">\begin{array}{ll} (\theta^{(k+1)})^{T}u & = (\theta^{(k)})^{T}u+y^{(i)}(x^{(i)})^{T}u \\ & \geq (\theta^{(k)})^{T}u+\gamma \end{array}</script><p>利用一个简单的归纳法得到：</p>
<script type="math/tex; mode=display">(\theta^{(k+1)})^{T}u \geq k\gamma</script><p>同时根据感知器算法的定义能得到： </p>
<script type="math/tex; mode=display">\begin{array}{ll} ||\theta^{(k+1)}||^{2} & = ||\theta^{(k)}+y^{(i)}x^{(i)} ||^{2} \\ & = ||\theta^{(k)}||^{2}+||x^{(i)}||^{2}+2y^{(i)}(x^{(i)})^{T}\theta^{(i)}\\ & \leq ||\theta^{(k)}||^{2}+||x^{(i)}||^{2}\\ & \leq ||\theta^{(k)}||^{2}+\mathcal{D}^{2} \end{array}</script><p>所以有：</p>
<script type="math/tex; mode=display">||\theta^{(k+1)}||^{2}\leq k\mathcal{D}^{2}</script><p>把上面的两个式子结合起来：</p>
<script type="math/tex; mode=display">\begin{array}{ll}\sqrt{k}\mathcal{D} & \geq ||\theta^{(k+1)}|| \\ & \geq (\theta^{(k+1)})^{T}u \\ &\geq k\gamma  \end{array}</script><p>上面第二个不等式是基于 $u $是一个单位向量（$z^{T}u = ||z||\cdot||u|| cos\phi \leq ||z|| \cdot ||u|| $，其中的$\phi $ 是向量 $z $和向量 $u $的夹角）。结果则表明 $k \leq (\mathcal{D}/\gamma)^{2} $。因此，如果感知器犯了一个第 $k $个错误，则 $k ≤ (\mathcal{D}/\gamma )^{2} $。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之kmeans聚类算法（note7）</title>
    <url>/2022/07/11/cs229/note7/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note7的主要内容：kmeans算法</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="kmeans聚类算法"><a href="#kmeans聚类算法" class="headerlink" title="kmeans聚类算法"></a>kmeans聚类算法</h2><p>在聚类的问题中，我们得到了一组训练样本集 $\{x^{(1)}, \cdots, x^{(m)}\} $，然后想要把这些样本划分成若干个相关的“类群”。其中的 $x^{(i)}\in\mathbb{R}^{n}$，而并未给出分类标签 $y^{(i)} $。所以这就是一个无监督学习的问题了。</p>
<p>K 均值聚类算法如下所示：</p>
<ul>
<li>1, 随机初始化聚类中心 $μ_{1}, μ_{2},\cdots, μ_{k}\in\mathbb{R}^{n} $。 </li>
<li>2, 重复下列过程直到收敛:<br>  { <ul>
<li>对每个 $i $，设 $c^{(i)} := arg\mathop{min}\limits_{j}||x^{(i)}-u_{j} ||^{2} $</li>
<li>对每个 $j $，设 $u_{j} := \frac{\sum_{i=1}^{m}1\{c^{(i)} = j\}x^{(i)}}{\sum^{m}_{i=1}1\{c^{(i)} = j \}} $<br>} </li>
</ul>
</li>
</ul>
<p>在上面的算法中，$k $是我们这个算法的一个参数，也就是我们要分出来的群组个数；而聚类重心 $μ_{j} $表示的是我们对各个聚类的中心位置的当前猜测。在上面算法的第一步当中，需要初始化聚类中心，可以这样实现：随机选择 $k $个训练样本，然后设置聚类中心等于这 $k $ 个样本各自的值。（当然也还有其他的初始化方法。） </p>
<p>算法的第二步当中，循环体内重复执行两个步骤：   </p>
<ul>
<li>（i）将每个训练样本 $x^{(i)} $“分配”给距离最近的聚类重心 $μ_{j} $；</li>
<li>（ii）把每个聚类重心 $μ_{j} $ 移动到所分配的样本点的均值位置。下面的 图1 就展示了运行 k均值聚类算法的过程。</li>
</ul>
<p><img src="kmeans.png" alt="kmeans-algorithms"></p>
<p>图1：$k $均值聚类算法。图中的圆形点表示的是训练样本，交叉符号表示的是聚类重心。</p>
<ul>
<li>(a) 原始训练样本数据集。 </li>
<li>(b) 随机初始化的聚类重心（这里的初始化方法就跟我们上面说的不一样，并没有从训练样本中选择两个点）。</li>
<li>(c-f) 运行 $k $均值聚类算法中的两步迭代的示意图。在每一次迭代中，我们把每个训练样本分配给距其最近的聚类重心（用同样颜色标识出），然后把聚类重心移动到所分配的样本的均值位置。（用颜色区分效果最好了。）</li>
</ul>
<p>$k $均值聚类算法能保证收敛性么？可以的，至少在一定意义上能这么说。尤其是我们可以定义一个下面这样的函数作为失真函数：</p>
<script type="math/tex; mode=display">J(c,u) = \sum\limits_{i=1}^{m}||x^{(i)}-u_{c^{(i)}} ||^{2}</script><p>这样就可以用 $J $来衡量每个样本 $x^{(i)} $和对应的聚类重心 $μ_{c^{(i)}} $之间距离的平方和，明显能看出 $k $均值聚类算法正好就是对 $J $的坐标下降过程。尤其是内部的循环体中，$k $均值聚类算法重复对 $J $进行最小化，当 $u $固定的时候用 $c $来最小化 $J $，当 $c $固定的时候则用 $u $ 最小化 $J $。这样就保证了 $J $是单调降低的，它的值也就必然收敛。（通常这也表明了 $c $和 $u $也收敛。在理论上来讲，$k $均值可能会在几种不同的聚类之间摆动，也就是说某些组不同值的 $c $和 $u $对应有完全相同的 $J $值，不过在实践中这种情况几乎不会遇到。）</p>
<p>失真函数 $J $，是一个非凸函数，所以对 $J $进行坐标下降并不一定能够收敛到全局最小值，也就是说，$k $均值聚类算法可能只是局部最优的。通常除了这个问题之外，$k $均值聚类效果都不错，能给出很好的聚类。如果你担心陷入到某些比较差的局部最小值，通常可以多次运行 $k $均值距离（使用不同的随机值进行来对聚类重心 $μ_{j} $ 进行初始化）。然后从所有的不同聚类方案中，选择能提供最小失真 $J(c,μ) $ 的。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之学习理论（note4）</title>
    <url>/2022/07/08/cs229/note4/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note4的主要内容：偏差和方差权衡，假设集合有限/无限的学习理论</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="Part-VI-学习理论"><a href="#Part-VI-学习理论" class="headerlink" title="Part VI 学习理论"></a>Part VI 学习理论</h2><h3 id="1，偏差和方差的权衡"><a href="#1，偏差和方差的权衡" class="headerlink" title="1，偏差和方差的权衡"></a>1，偏差和方差的权衡</h3><p>在讲线性回归的时候，讨论过这样的问题：拟合数据的时候，选择线性的$y = \theta_{0} +\theta_{1}x $“简单”模型，还是选择多项式的$y=\theta_{0}+\theta_{1}x+···\theta_{5}x^{5} $这种“复杂”模型。如下图所示：</p>
<p><img src="bias-variance.png" alt="bias-variance"></p>
<p>最右侧图所示，用一个五次多项式来进行拟合，得到的并不是一个好模型。这个五次多项式对于训练集中的每一个 $x $都给出了非常好的预测的 $y $ 值，我们也不能指望这个模型能够对训练集之外的点给出靠谱的预测。换句话说，用这种高次多项式来对训练集进行学习，得到的模型根本不能扩展运用到其他数据上。一个模型的泛化误差正是那些不属于训练集的样本预测偏差。</p>
<p>最左边的线性拟合和最右边的高次多项式拟合都有非常大的泛化误差，这两个模型各自出的问题是很不一样的。如果 $y $ 和 $x $ 之间的关系不是线性的，那么即便我们有一个非常大规模的训练集，然后用来进行线性拟合，得到的线性模型都还是不能够准确捕捉到数据的分布。我们粗略地将一个模型的偏差（bias）定义为预测的泛化误差，即便我们要去拟合的对象是一个非常大的甚至是无限的训练数据集。这样的话，对于上面三幅图中所展示的那个情况来看，最左边的那个线性模型就具有特别大的偏差，可能是对数据欠拟合（也就是说，没有捕捉到数据所体现的结构特征）。</p>
<p>除了这个偏差之外，还有另外一个构成泛化误差的因素，也就是模型拟合过程的方差（variance）。在最右边的图中，使用了五次多项式进行了拟合，这样有很大的风险，很可能我们基于数据拟合出来的模型可能碰巧只适合于眼下这个小规模的有限的训练集，而并不能反映 $x $ 和 $y $ 之间更广泛的关系。例如，在实际中，可能我们选择的训练集中的房屋碰巧就是一些比平均价格要稍微贵一些的房屋，也可能有另外的一些比平均值要低一点的房屋，等等。通过对训练集拟合得到的这个“不太靠谱的”的模式，我们得到的可能也就是一个有很大泛化误差的模型。这样的话，我们就说这个模型的方差很大。</p>
<p>通常情况下，咱们需要在偏差（bias）和方差（variance）之间进行权衡妥协。如果我们的模型过于“简单”，而且参数非常少，那这样就可能会有很大的偏差（bias），而方差（variance）可能就很小；如果我们的模型过于“复杂”，有非常多的参数，那就可能反过来又特别大的方差（variance），而偏差（bias）就会小一些。在上面三种不同拟合的样例中，用二次函数来进行拟合得到的效果，明显是胜过一次线性拟合，也强于五次多项式拟合。</p>
<h3 id="2，预备知识"><a href="#2，预备知识" class="headerlink" title="2，预备知识"></a>2，预备知识</h3><p>这部分，开始进入到机器学习的理论，本章内容非常有趣，而且有启发性，还能帮助我们培养直觉，能得到在不同背景下如何最佳应用学习算法的经验规则。此外，我们还会探究一些问题：首先，上文我们刚刚谈论到的偏差和方差，能不能总结一下？这个问题还会引出关于模型选择的方法，这些方法可以在对一个训练集进行拟合的时候来帮助确定要用的多项式应该是几阶的。其次，在机器学习的过程中，我们真正关注的也就是泛化误差，不过绝大部分的学习算法都是将训练集和模型结合的。那么针对训练集的表现好坏程度，为何就能告诉我们泛化误差的信息呢？例如，我们能将训练集的误差和泛化误差联系起来么？第三个，也是最后一点，是否存在某些条件，又能否在这些条件下证明某些学习算法能够良好工作？</p>
<p>我们先来给出两个很简单又很有用的引理。</p>
<p>引理1  (联合约束，The union bound)。设 $A_{1}, A_{2}, …, A_{k}$  是 $K$个不同事件（但不一定互相独立），则有： 在概率论中，联合约束通常被当做是公理，实际上也很直观： $k $ 个事件同时发生的概率最多是 $k $ 个不同的事件每个都发生的概率的总和。</p>
<script type="math/tex; mode=display">P(A_{1}\bigcup\cdots\bigcup A_{k})\leq P(A_{1})+...+P(A_{k})</script><p>引理2  (Hoeffding 不等式) 。设 $Z1, … , Zm$ 是$m $ 个独立且服从伯努利分布的随机变量。例如：$P(Z_{i}=1)=\phi $ 而 $P(Z_{i}=0)=1−\phi $. 设$\hat{\phi}=\frac{1}{m}\sum_{i=1}^{m}Z_{i}$是这些随机变量的平均值，然后设任意的 $\gamma &gt; 0$ 为某一固定值，则有： </p>
<script type="math/tex; mode=display">P(|\phi - \hat{\phi}|>\gamma)\leq 2exp(-2\gamma^{2}m)</script><p>这个引理（也称为切尔诺夫约束）表明，如果我们我们从一个伯努利分布的随机变量中选取平均值 $\hat\phi$来作为对 $\phi$ 的估计值，那么只要 $m $ 足够大，我们偏移真实值很远的概率就比较小。另外一种表述方式是：如果你有一个硬币，抛起来落下人头朝上的概率是 $\phi$，如果你抛了 $m $次，然后计算人头朝上的比例，若 $m $ 非常大，那么这个比例的值，就是一个对 $\phi$ 的一个概率很好的估计。</p>
<p>基于上面这两个引理，我们就可以去证明在机器学习理论中一些很深刻和重要的结论了。</p>
<p>为了简化表述，我们先关注一下二分法分类，其中的标签简化为 $y \in \{0, 1\} $。然后我们即将讲到的所有内容也都会推广到其它问题中，例如回归问题以及多类别的分类问题等等。</p>
<p>假设我们有一个给定的训练集 $S = \{(x^{(i)},y^{(i)});i = 1,…,m\} $，其样本规模为 $m $，集合中的训练样本 $(x^{(i)},y^{(i)}) $ 是可以符合某概率分布 $\mathcal{D} $ 的独立且同分布的随机变量。设一个假设为 $h $，我们则用如下的方法定义训练误差（也成为学习理论中的经验风险或者经验误差 ）：</p>
<script type="math/tex; mode=display">\hat{\epsilon}(h) = \frac{1}{m}\sum\limits_{i=1}^{m}1\{h(x^{(i)})\neq y^{(i)} \}</script><p>这个值只是假设模型 $h $ 分类错误样本占据训练样本总数的分数。如果我们要特定指针对某个训练样本集合 $S$ 的经验误差 $\hat{\epsilon}(h) $，可以写作 $\hat{\epsilon}_{S}(h) $。然后我们就可以定义泛化误差（generalization error）为：</p>
<script type="math/tex; mode=display">\epsilon (h) = P_{(x,y)\sim \mathcal{D}}(h(x)\neq y)</script><p>经验误差 $\hat{\epsilon}(h) $ 的这个定义实际上也就相当于，基于分布 $D $ 给出的一个新的样本 $(x, y) $ ，假设模型 $h $ 对该样本分类错误的概率。 </p>
<p>要注意，这里我们有一个预先假设，也就是训练集的数据与要用来检验假设用的数据都服从同一个分布 $D $（这一假设存在于对泛化误差的定义中），这个假设通常也被认为是 PAC 假设之一。</p>
<p>考虑线性分类的情况，假设 $h_{\theta}(x) = 1\{θ^{T}x \geq 0\} $，拟合参数 $\theta$ 的合理方法是什么呢？一个思路就是可以使训练误差最小化，然后选择取最小值时候的$\theta$：</p>
<script type="math/tex; mode=display">\hat{\theta} = arg\mathop{min}\limits_{\theta}\hat{\epsilon}(h_{\theta})</script><p>我们把上面这个过程称之为经验风险最小化（empirical risk minimization，缩写为 ERM），而这种情况下通过学习算法得到的假设结果就是 $\hat{h} = h_{\hat{\theta}}$ 。我们把 ERM 看做为最“基础”的学习算法，在这一系列的讲义中我们主要关注的就是这种算法。（其他的例如逻辑回归等等算法也可以看作是对 ERM 的某种近似。）</p>
<p>在咱们关于机器学习理论的研究中，有一种做法很有用，就是把具体的参数化抽象出去，也把是否使用线性分选器之类的问题也抽象出去。我们把学学习算法所使用的假设类$\mathcal{H}$定义为所有分类器的集合。对于线性分类问题来说，$\mathcal{H} = \{ h_{\theta}: h_{\theta}(x) = 1{θ^{T}x \geq 0}, \theta \in \mathcal{R}^{n+1}\} $，是一个对 $X $进行分类的所有分类器的集合，其中所有分类边界为线性。更广泛来说，假设我们研究神经网络，那么可以设 $H $ 为能表示某些神经网络结构的所有分类器的集合。</p>
<p>现在就可以把经验风险最小化（ERM）看作是对函数类 $H $ 的最小化，其中由学习算法来选择假设（hypothesis）：</p>
<script type="math/tex; mode=display">\hat{h} = arg\mathop{min}_{h\in \mathcal{H}}\hat{\epsilon}(h)</script><h3 id="3，集合-mathcal-H-有限"><a href="#3，集合-mathcal-H-有限" class="headerlink" title="3，集合$\mathcal{H}$有限"></a>3，集合$\mathcal{H}$有限</h3><p>首先来考虑假设类有限的学习问题，其中假设类 $\mathcal{H} = {h_{1}, …, h_{k}} $，由 $k $ 个不同假设组成。因此，$\mathcal{H} $ 实际上就是由 $k $ 个从输入特征 $\mathcal{X} $ 映射到集合 $\{0, 1\} $ 的函数组成的集合，而经验风险最小化（ERM）就是从这样的 $k $ 个函数中选择训练误差最小的作为$\hat{h} $。</p>
<p>我们希望能够确保 $\hat{h} $ 的泛化误差，需要两个步骤：首先要表明$\hat{\epsilon}(h) $ 是对所有 $h $ 的 $\epsilon(h) $ 的一个可靠估计。其次就需要表明这个 $\hat{\epsilon}(h) $ 位于 $\hat{h}$ 泛化误差的上界。</p>
<p>任选一个固定的 $h_{i}\in\mathcal{H} $，假如有一个伯努利随机变量$\mathcal{Z} $，其分布下面会定义。 然后我们从 $\mathcal{D} $ 中取样 $(x, y) $，并设 $\mathcal{Z} = 1\{ h_{i}(x) \neq y\} $。也就是说，我们会选择一个样本，然后令 $\mathcal{Z} $ 指示 $h_{i} $ 是否对该样本进行了错误分类。类似地，我们还定义了一个 $\mathcal{Z}_{j} = 1\{ h_{i}(x^{(j)}) \neq y^{(j)}\} $。由于我们的训练样本都是从 $\mathcal{D} $ 中取来的独立随机变量，所以在此基础上构建的 $\mathcal{Z} $ 和 $\mathcal{Z}_{j} $ 也都服从相同的分布。</p>
<p>这样就能找到针对随机选取的训练样本进行错误分类的概率$\epsilon(h) $ — 正好就是 $\mathcal{Z} $ （以及 $\mathcal{Z}_{j} $ ）的期望值。然后，就可以把训练误差写成下面这种形式：</p>
<script type="math/tex; mode=display">\hat{\epsilon}(h_{i}) = \frac{1}{m}\sum\limits^{m}_{j=1}\mathcal{Z}_{j}</script><p>因此，$\hat{\epsilon}(h_{i}) $就正好是 $m $ 个随机变量 $Z_{j} $ 的平均值，而这个 $Z_{j} $ 是服从伯努利分布的独立随机变量，其均值就是$\epsilon (h_{i}) $。接下来，就可以使用 Hoeffding 不等式，得到下面的式子：</p>
<script type="math/tex; mode=display">P(|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|>\gamma)\leq 2exp(-2\gamma^{2}m)</script><p>这就表明，对于我们给定的某个固定的 $h_{i} $，假如训练样本的规模 $m $ 规模很大的时候，训练误差很接近泛化误差的概率是很高的。然而我们不仅仅满足于针对某一个特定的 $h_{i} $ 能保证 $\epsilon(h_{i}) $ 接近 $\hat{\epsilon}(h_{i}) $ 且接近的概率很高，还要证明同时针对所有的 $h\in\mathcal{H} $ 这个结论都成立。为了证明这个结论，我们设 $A_{i} $ 来表示事件 $|\epsilon(h_{i}) − \hat{\epsilon}(h_{i})| &gt; \gamma $，已经证明了，对于任意的固定的 $A_{i} $，都有 $P(A_{i}) \leq 2exp(−2\gamma^{2}m) $ 成立。接下来，使用联合约束（union bound），就可以得出下面的关系：</p>
<script type="math/tex; mode=display">\begin{array}{ll} P(\exists h\in\mathcal{H}.|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|>\gamma ) & = \quad P(A_{1}\cup\cdots\cup A_{k}) \\ & \leq \quad\sum\limits^{k}_{i=1}P(A_{i}) \\ & \leq\quad\sum\limits^{k}_{i=1}2exp(-2\gamma^{2}m)\\ & = \quad 2kexp(-2\gamma^{2}m) \end{array}</script><p>如果等式两边都用 1 来减去原始值，则不等关系改变为： </p>
<script type="math/tex; mode=display">\begin{array}{ll} P(\neg\exists h\in\mathcal{H}.|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|>\gamma) & = \quad P(\forall h\in\mathcal{H}.|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|\leq\gamma) \\ &\geq\quad 1-2kexp(-2\gamma^{2}m) \end{array}</script><p>如上所示，至少有 $1-2kexp(-2\gamma^{2}m)$ 的概率，能确保对于所有的 $h\in\mathcal{H} $，$\epsilon(h) $在 $\hat{\epsilon}(h) $附近的 $\gamma$ 范围内。这种结果就叫做一致收敛结果，因为这是一个针对所有的 $h \in\mathcal{H} $ 都同时成立的约束（与之相反的是只针对某一个 $h $ 才成立的情况）。</p>
<p>在上面的讨论中，我们涉及到的是针对某些 $m $ 和 $\gamma $ 的特定值，给定一个概率约束：对于某些 $h \in\mathcal{H} $ , 都有$|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|&gt;\gamma $。这里我们感兴趣的变量有三个：$m $, $\gamma $, 以及误差的概率；我们可以将其中的任意一个用另外两个来进行约束。</p>
<p>例如，我们可以提出下面这样的一个问题：给定一个 $\gamma$ 以及某个 $\delta &gt; 0 $，那么如果要保证训练误差处于泛化误差附近 $\gamma$ 的范围内的概率最小为 $1 – \delta$，那么 $m $ 应该要多大呢？可以设 $\delta = 2kexp(-2\gamma^{2}m) $ 然后解出来 $m $（自己给自己证明一下这样是对的吧！），然后我们就发现，如果有：</p>
<script type="math/tex; mode=display">m\geq \frac{1}{2\gamma^{2}}log\frac{2k}{\delta}</script><p>并且概率最小为 $1 – \delta $，就能保证对于所有的 $h\in\mathcal{H} $ 都有$|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|\leq\gamma) $。（反过来，这也表明，对于某些 $h\in\mathcal{H} $， $|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|&gt;\gamma) $的概率最大为$\delta$。）这种联合约束也说明了需要多少数量的训练样本才能对结果有所保证。是某些特定的方法或者算法所需要训练集的规模 $m $ 来实现一定程度的性能，这样的训练集规模 $m $ 也叫做此类算法的样本复杂度。</p>
<p>上面这个约束的关键特性在于要保证结果，所需的训练样本数量只有 $k $ 的对数级别，$k $ 即假设集合 $\mathcal{H} $ 中的假设个数。这个特性稍后会很重要。</p>
<p>同理，我们也可以将 $m $ 和 $\delta $ 设置为固定值，然后通过上面的等式对 $\gamma $ 进行求解，然后表明对于所有的 $h\in\mathcal{H}$ ，都有概率为 $1 – \delta $（这里还是要你自己去证明了，不过你相信这个是对的就好了。）。</p>
<p>现在，我们假设这个联合收敛成立，也就是说，对于所有的 $h\in\mathcal{H}$，都有 $|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|\leq\gamma $。我们的学习算法选择了 $\hat{h} = arg\mathop{min}_{h\in\mathcal{H}}\hat{\epsilon}(h) $，关于这种算法的泛化，我们能给出什么相关的证明呢？</p>
<p>将 $h^{\ast } = arg\mathop{min}_{h\in\mathcal{H}}\epsilon(h) $定义为 $\mathcal{H} $ 中最佳可能假设。这里要注意此处的 $h^{\ast } $ 是我们使用假设集合 $\mathcal{H} $ 所能找出的最佳假设，所以很自然地，我们就能理解可以用这个 $h^{\ast } $ 来进行性能对比了。则有：</p>
<script type="math/tex; mode=display">\begin{array}{ll} \epsilon(\hat{h}) & \leq\quad \hat{\epsilon}(\hat{h})+\gamma \\ & \leq\quad \hat{\epsilon}(h^{\ast })+\gamma \\ & \leq\quad \epsilon(h^{\ast })+2\gamma  \end{array}</script><p>上面的第一行用到了定理$|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|\leq\gamma $（可以通过上面的联合收敛假设来推出）。第二行用到的定理是 $\hat{h}$ 是选来用于得到最小 $\hat{\epsilon}(h) $，然后因此对于所有的 $h $ 都有 $\hat{\epsilon}(\hat{h})\leq\hat{\epsilon}(h) $，也就自然能推出$\hat{\epsilon}(\hat{h})\leq\hat{\epsilon}(h^{\ast }) $。第三行再次用到了上面的联合收敛假设，此假设表明 $\hat{\epsilon}(h^{\ast }) \leq \epsilon(h^{\ast })+\gamma $。所以，我们就能得出下面这样的结论：如果联合收敛成立，那么 $\hat{h}$ 的泛化误差最多也就与 $\mathcal{H} $ 中的最佳可能假设相差 $2\gamma $。</p>
<p>好了，咱们接下来就把上面这一大堆整理成一条定理（theorem）。</p>
<p><strong>定理</strong> 设 $|\mathcal{H}| = k $，$\mathcal{H} $中的元素个数为$k $，然后设 $m $ 和 $\delta $ 为任意的固定值。然后概率至少为 $1 − \delta $，则有：</p>
<script type="math/tex; mode=display">\epsilon(\hat{h})\leq (\mathop{min}\limits_{h\in\mathcal{H}}\epsilon(h))+2\sqrt{\frac{1}{2m}log\frac{2k}{\delta}}</script><p>上面这个可以通过令 $\gamma $ 等于平方根的形式，然后利用我们之前得到的概率至少为 $1 – \delta $ 的情况下联合收敛成立，接下来利用联合收敛能表明 $\epsilon(h)$最多比 $\epsilon(h^{∗ }) = \mathop{min}\limits_{h\in\mathcal{H}}\epsilon(h) $多  $2\gamma $（这个前面我们已经证明过了）。</p>
<p>这也对我们之前提到过的在模型选择的过程中在偏差/方差之间的权衡给出了定量方式。例如，加入我们有某个假设类 $\mathcal{H} $，然后考虑切换成某个更大规模的假设类 $\mathcal{H}\subseteq\mathcal{H}’ $。如果我们切换到了 $\mathcal{H}’ $ ，那么第一次的 $min_{h}\epsilon(h)$只可能降低（因为我们这次在一个更大规模的函数集合里面来选取最小值了）。因此，使用一个更大规模的假设类来进行学习，我们的学习算法的“偏差”只会降低。然而，如果 $k $ 值增大了，那么第二项的那个二倍平方根项也会增大。这一项的增大就会导致我们使用一个更大规模的假设的时候，“方差”就会增大。</p>
<p>通过保持 $\gamma $ 和 $\delta $ 为固定值，然后像上面一样求解 $m $，我们还能够得到下面的样本复杂度约束：</p>
<p><strong>推论（Corollary）</strong>：设 $ |\mathcal{H}| = k $，然后令 $\delta $,$\gamma $ 为任意的固定值。对于满足概率最少为 $1 − \delta $ 的 $\epsilon(\hat{h})\leq min_{h\in\mathcal{H}}\epsilon(h) + 2\gamma $，下面等式关系成立： </p>
<script type="math/tex; mode=display">\begin{array}{ll} m & \geq \frac{1}{2\gamma^{2}}log\frac{2k}{\delta} \\ & = O(\frac{1}{\gamma^{2}}log\frac{k}{\delta}) \end{array}</script><h3 id="4，集合-mathcal-H-无限"><a href="#4，集合-mathcal-H-无限" class="headerlink" title="4，集合$\mathcal{H}$无限"></a>4，集合$\mathcal{H}$无限</h3><p>我们已经对有限个假设类的情况证明了一些有用的定理。然而有很多的假设类都包含有无限个函数，其中包括用实数参数化的类（比如线性分类问题）。那针对这种无限个假设的情况，我们能证明出类似的结论么？ </p>
<p>我们先从一些看似不太“准确”论证的内容开始。当然也有更好的更通用的论证，但先从这种不太“准确”的内容出发，将有助于锻炼我们在此领域内的直觉。</p>
<p>若我们有一个假设集合 $\mathcal{H} $，使用 $d $ 个实数来进行参数化。由于我们使用计算机表述实数，而 IEEE 的双精度浮点数（ C 语言里面的 double 类型）使用了 64 bit 来表示一个浮点数，这就意味着如果我们在学习算法中使用双精度浮点数，那我们的算法就由 $64d $ 个 bit 来进行参数化。这样我们的这个假设类实际上包含的不同假设的个数最多为 $k = 264d $ 。结合上一节的最后一段那个推论（Corollary），我们就能发现，要保证 $\epsilon(\hat{h})\leq\epsilon(h^{\ast })+2\gamma $，同时还要保证概率至少为 $1 − \delta $ ，则需要训练样本规模 $m $ 满足 $m\geq O(\frac{1}{\gamma^{2}}log\frac{2^{64d}}{\delta}) = O(\frac{d}{\gamma^{2}}log\frac{1}{\delta}) = O_{\gamma,\delta}(d) $。（这里的 $\gamma,\delta$ 下标表示最后一个大$O$ 可能是一个依赖于$\gamma $和$\delta $的隐藏常数。）因此，所需的训练样本规模在模型参数中最多也就是线性的。</p>
<p>由于我们要依赖 64bit 浮点数，所以上面的论证还不能完全令人满意，但这个结论大致上是正确的。如果我们试图使训练误差最小化，那么为了使用具有 $d $ 个参数的假设类的学习效果“较好”，通常就需要 $d $ 的线性数量来确定训练样本规模。</p>
<p>（这里要注意的是，对于使用经验风险最小化的学习算法，上面这些结论已经被证明适用。因此，样本复杂度对 $d $ 的线性依赖性通常适用于大多数分类识别学习算法，但训练误差或者训练误差近似值的最小化，就未必适用于分类识别了。对很多的非 ERM 学习算法提供可靠的理论论证，仍然是目前很活跃的一个研究领域。）</p>
<p>前面的论证还有另外一部分让人不太满意，就是依赖于对 $\mathcal{H}$ 的参数化。根据直觉来看，这个参数化似乎应该不会有太大影响：我们已经把线性分类器写成了 $h_{\theta}(x) = 1\{\theta_{0} + \theta_{1}x_{1} + ···\theta_{n}x_{n} \geq 0\} $的形式，其中有 $n+1 $ 个参数 $\theta_{0},…,\theta_{n} $。但也可以写成 $h_{u,v}(x) = 1\{(u^{2}_{0} − v_{0}^{2}) + (u^{2}_{1} − v_{1}^{2})x_{1} + ··· (u^{2}_{n} − v_{n}^{2})x_{n} \geq 0\} $的形式，这样就有 $2n+2 $ 个参数 $u_{i}, v_{i} $了。然而这两种形式都定义了同样的一个 $\mathcal{H}：$ 一个 $n$ 维的线性分类器集合。</p>
<p>要推导出更让人满意的论证结果，我们需要再额外定义一些概念。</p>
<p>给定一个点的集合 $\mathcal{S} = \{x^{(i)}, …, x^{(d)}\} $（与训练样本集合无关），其中 $x^{(i)} \in \mathcal{X} $，如果 $\mathcal{H} $ 能够对 集合 $\mathcal{S} $ 实现任意的标签化，则称 $\mathcal{H} $  打散（shatter）了 $\mathcal{S} $。例如，对于任意的标签集合 $\{y^{(1)}, …, y^{(d)}\} $，都有类 $\mathcal{H} $ 中的某个函数 $h $ 满足 $h(x^{(i)}) = y^{(i)} $，其中 $i = 1, \cdots d $。</p>
<p>给定一个假设类 $\mathcal{H} $，我们定义其 VC维度，写作$VC(\mathcal{H}) $，这个值也就是能被 $\mathcal{H} $ 打散的最大的集合规模。（如果 $\mathcal{H} $ 能打散任意大的集合，那么 $ VC(H) = \infty $。）</p>
<p>例如，若一个集合由下图所示的三个点组成：<br><img src="exam-set.png" alt="exam-set"></p>
<p>那么二维线性分类器 $(h(x) = 1\{\theta_{0} +\theta_{1}x_{1} + \theta_{2}x_{2} \geq 0\}) $的集合 $\mathcal{H} $ 能否将上图所示的这个集合打散呢？答案是能。具体来看则如下图所示，以下八种分类情况中的任意一个，我们都能找到一种用能够实现 “零训练误差” 的线性分类器：<br><img src="pts-classify.png" alt="pts-classify"></p>
<p>此外，这也有可能表明，这个假设类 $\mathcal{H} $ 不能打散4 个点构成的集合。因此，$\mathcal{H} $ 可以打散的最大集合规模为 3，也就是说 $VC(\mathcal{H})= 3 $。</p>
<p>这里要注意，$\mathcal{H} $ 的 VC 维度为3，即便有某些 3 个点的集合不能被 $\mathcal{H} $ 打散。例如如果三个点都在一条直线上（如下图左侧的图所示），那就没办法能够用线性分类器来对这三个点的类别进行划分了（如下图右侧所示）。<br><img src="pts-not-classify.png" alt="pts-not-classify"></p>
<p>换个方式来说，在 VC 维 的定义之下，要保证 $VC(\mathcal{H}) $至少为 $\mathcal{D} $，只需要证明至少有一个规模为 $d $ 的集合能够被 $\mathcal{H} $ 打散就可以了。</p>
<p>这样就能够给出下面的定理了，该定理来自 Vapnik。</p>
<p><strong>定理</strong>：给定 $\mathcal{H} $，设 $d = VC(\mathcal{H}) $。然后对于所有的 $h\in\mathcal{H} $，都有至少为 $1−\delta $的概率使下面的关系成立：</p>
<script type="math/tex; mode=display">|\epsilon(h)-\hat{\epsilon}(h)|\leq O(\sqrt{\frac{d}{m}log\frac{m}{d}+\frac{1}{m}log\frac{1}{\delta}})</script><p>此外，有至少为 $1−\delta $ 的概率：</p>
<script type="math/tex; mode=display">\epsilon(\hat{h})\leq\epsilon(h^{\ast })+O(\sqrt{\frac{d}{m}log\frac{m}{d}+\frac{1}{m}log\frac{1}{\delta}})</script><p>换句话说，如果一个假设类有有限的 VC 维，那么只要训练样本规模 $m $ 增大，就能够保证联合收敛成立，和之前一样，这就能够让我们以 $\epsilon(h^{∗ }) $的形式来给 $\epsilon(h) $ 建立一个约束，此外还有下面的推论：</p>
<p><strong>推论</strong>（Corollary）：对于所有的 $h \in\mathcal{H} $成立的 $|\epsilon(h)-\hat{\epsilon}(h)|\leq\gamma $ （因此也有 $\epsilon(\hat{h})\leq\epsilon(h^{\ast })+2\gamma $），则有至少为 $1 – \delta $的概率，满足 $m = O_{\gamma,\delta}(d) $。</p>
<p>换个方式来说，要保证使用 $\mathcal{H}$ 训练的算法的学习效果“良好”，那么训练集样本规模 $m $ 需要与 $\mathcal{H} $ 的 VC 维度线性相关。这也表明，对于“绝大多数”假设类来说，（假设是“合理”参数化的）VC 维度也大概会和参数的个数线性相关。把这些综合到一起，我们就能得出这样的一个结论：<strong>对于一个试图将训练误差最小化的学习算法来说：训练样本个数通常与假设类 $\mathcal{H} $ 的参数个数线性相关</strong>。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之主成分分析（note10）</title>
    <url>/2022/07/06/cs229/note10/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note10的主要内容：PCA</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="PART-XI-主成分分析"><a href="#PART-XI-主成分分析" class="headerlink" title="PART XI 主成分分析"></a>PART XI 主成分分析</h2><p>之前介绍了因子分析，用$k $ 维子空间对 $x\in \mathcal{R}^{n} $ 进行近似建模，有 $k ≪ n $。具体来说，已知点 $x{(i)} $ ，在 $k $ 维仿射空间$\{\Lambda z + \mu; z \in \mathcal{R}^{k}\} $ 中生成某个 $z^{(i)} $ ，然后增加 $\Psi $-协方差噪音。因子分析是基于概率模型，然后参数估计使用了迭代期望最大化算法。</p>
<p>在本章讲义中，学习一种新的方法，主成分分析（Principal Components Analysis，缩写为 PCA），这个方法也是用来对数据近似所处的子空间进行判别。然而，主成分分析算法会更加直接，只需要进行一种特征向量计算，并且不需要再去使用期望最大化（EM）算法。</p>
<p>假如我们有一个数据集 ${x^{(i)}; i = 1, . . ., m} $，其中包括了 $m $ 种不同汽车的属性，例如最大速度，转弯半径等等。设其中每个 $i $ 都有 $x^{(i)}\in \mathcal{R}^{n} $，(且$n ≪ m $)。但对于两个不同的属性，例如 $x_{i} $ 和 $x_{j} $，对应着以英里每小时（mph）为单位的最高速度和以公里每小时（kph）为单位的最高速度，这两个属性应该是线性相关的，只在对 mph 和 kph 进行四舍五入时候会有引入一些微小差异。所以，这个数据实际上应该是近似处于一个 $n-1 $ 维的子空间中。我们如何自动检测和删除掉这一冗余（redundancy）呢？</p>
<p>举一个不那么麻烦的例子，设想有一个数据集，其中包含的是对一个无线电遥控直升机（radio-controlled helicopters）飞行员协会进行调查得到的数据，其中的 $x_{1}^{(i)} $ 指代的是飞行员 $i $ 的飞行技能，而 $x_{2}^{(i)} $ 指代的是飞行员对飞行的喜爱程度。无线电遥控直升机是很难操作的，只有那些非常投入，并且特别热爱飞行的学生，才能成为好的飞行员。所以，上面这两个属性 $x_{1} $ 和 $x_{2} $ 之间的相关性是非常强的。如下图，可以认为在数据中沿着对角线方向表征了一个人对飞行投入程度的内在“源动力（karma）”，只有少量的噪音脱离这个对角线方向。如下图所示，我们怎么来自动去计算出 $u_{1} $ 的方向呢？</p>
<p><img src="pilot-exam1.png" alt="pilot-skill"></p>
<p>接下来介绍主成分分析算法，但在运行 PCA 之前，我们首先要进行一些预处理（pre-process），正则化（normalize）数据的均值（mean）和方差（variance），如下所示：</p>
<ul>
<li>1，设$\mu = \frac{1}{m}\sum_{i=1}^{m}x^{(i)} $</li>
<li>2，$x^{(i)} = x^{(i)} − \mu $</li>
<li>3，设 $\sigma^{2}_{j} = \frac{1}{m}\sum_{i}(x^{(i)}_{j})^{2} $</li>
<li>4，$x^{(i)}_{j} = x^{(i)}_{j}/\sigma_{j} $ </li>
</ul>
<p>第（1-2）步把数据的平均值清零，然后可以省略掉所有有零均值的数据。第（3-4）步将每个坐标缩放，使之具有单位方差，这确保了不同属性的数据都在同样的“尺度”上来进行处理。例如，如果 $x_{1} $ 是汽车的最大速度，然后 $x_{2} $ 是汽车的座位数量，这样这个重新正则化（renormalization）就把不同的属性进行了缩放，然后这些不同属性就更具有对比性。如果对不同的属性有先验知识，就可以省略第（3-4）步。例如，如果每个数据点表示灰度图像中的每个数据点，而每个 $x_{j}^{(i)} $ 就从 $\{0, 1, . . . , 255\} $ 中取值，对应的也就是在图像 $i $ 中像素 $j $ 位置的灰度值。</p>
<p>正则化之后，对数据近似所处的方向，也就是“主要变化轴”$u $，该如何去计算呢？一种方法是找出一个单位向量$u $，使得数据投影在 $u $ 的方向上的时候，投影的数据的方差最大。</p>
<p>直观来看，在这个方向上，数据开始有一定规模的方差/信息量。我们要选择的是这样一个方向的单位向量 $u $，数据能近似投放到与单位向量 $u $ 一致的方向/子空间，并且尽可能多地保留上面的方差。</p>
<p>设下面的数据集，我们已经进行了正则化步骤：</p>
<p><img src="pts-exam1.png" alt="pts-exam1"></p>
<p>现在，加入我们选择的单位向量 $u $ ，下图中的圆点表示的就是原始数据在这条线上面的投影。</p>
<p><img src="pts-exam2.png" alt="pts-exam2"></p>
<p>可以看到，上面投影得到的数据依然较大的方差，而这些点距离零点也都比较远。如下图所示，选择另外一个方向的单位向量：</p>
<p><img src="pts-exam3.png" alt="pts-exam3"></p>
<p>上面这幅图的投影中的方差就明显小了很多，而且投影得到的点位置也距离原点更近很多。</p>
<p>我们希望能自动地选择出来如上面两幅图中第一幅那样的方向的单位向量 $u $。要对这个过程进行公式化（formalize），给定一个向量 $u $ 和一个点 $x $，$x $ 投影到 $u $ 上的投影长度就可以用 $x^{T}u $ 来得到。也就是说，如果 $x^{(i)} $ 是我们数据集中的一个点（上面几个图中画叉的 $x $ 点中的一个），那么这个点在 $u $ 上的投影（对应的是图中的圆点）就是从原点到  $x^{T}u $ 的距离。因此，要最大化投影的方差，就要找到一个能够将下面式子最大化的单位长度向量 $u $：</p>
<script type="math/tex; mode=display">\begin{array}{ll}\frac{1}{m}\sum\limits_{i=1}^{m}((x^{(i)})^{T}u)^{2} & = \quad \frac{1}{m}\sum\limits_{i=1}^{m}u^{T}x^{(i)}(x^{(i)})^{T}u\\
& = \quad u^{T}(\frac{1}{m}\sum\limits^{m}_{i=1}x^{(i)}(x^{(i)})^{T})u\end{array}</script><p>容易发现，要让上面的式子最大化，$||u||_{2} = 1 $ 给出了$\sum = \frac{1}{m}\sum^{m}_{i=1}x^{(i)}(x^{(i)})^{T}$的主特征向量，而这也正好就是数据的经验协方差矩阵（假设零均值）。</p>
<p>总结一下，如果我们要找一个 1 维子空间来近似数据，就要选择 $\sum $ 的主特征向量作为单位向量 $u $。更广义地理解，就是如果要将数据投影到一个 $k $ 维子空间$(k &lt; n) $，就应当选择 $\sum $ 的 $k $ 个特征向量来作为单位向量 $u_{1}, . . ., u_{k} $。这里的 $u_{i} $ 就成了数据的一组新的正交基。</p>
<p>然后，要使用这组正交基来表示 $x^{(i)} $，只需要计算对应的向量：</p>
<script type="math/tex; mode=display">y^{(i)} = \begin{bmatrix} u^{T}_{1}x^{(i)} \\ u^{T}_{2}x^{(i)} \\ \vdots \\u^{T}_{k}x^{(i)} \end{bmatrix}</script><p>$x^{(i)} \in \mathcal{R}^{n}$，向量 $y^{(i)} $就是对 $x^{(i)} $ 的近似表示。因此，主成分分析算法也被称为是一种维度降低算法，其中的单位向量 $u_{1},…,u_{k} $ 也就叫做数据集的前 $k $ 个主成分。</p>
<p><strong>Remark</strong>虽然仅当 $k = 1 $的情况下，可使用特征向量的特性，很明显，在所有可能的正交基中，选择的那一组就能使得$\sum_{i}||y^{(i)}||^{2}_{2}$取最大值。因此，我们对基向量的选择应当是尽可能保留原始数据的方差信息。</p>
<p>主成分分析算法也可以有另外一种推导方式：将数据投影到 $k $ 维子空间中，选择一组基向量，使得投影引起的近似误差最小。</p>
<p>主成分分析算法有很多应用；接下来给出若干样例。首先是压缩，用更低维度的 $y^{(i)} $ 来表示 $x^{(i)} $ ，这个用途很明显了。如果我们把高维度的数据降维到 $k = 2$ 或者 $3 $，那么就可以将 $y^{(i)} $ 进行可视化了。例如，如果我们把汽车数据降维到 2 维，那么就可以把压缩后的数据投影（例如这时候投影中的一二点可能就代表了骑车的类型），来看看哪些车彼此相似，以及这些车可以聚集成那些组。</p>
<p>另一个常用应用就是使用 $x^{(i)} $ 作为输入特征，进行监督学习算法之前降低数据维度的预处理步骤。除了有利于缓解计算性能压力之外，降低数据维度还可以降低假设类的复杂度，然后避免过拟合（例如，低维度的输入特征控件上的线性分类器会有更小的 VC 维度）。</p>
<p>最后，正如在遥控直升机飞行员样例，可以把 PCA 作为一种降噪算法。在那个例子中，算法从对遥控飞行技巧和热爱程度的带噪的衡量中估计了直观的“遥控飞行原动力”。同时，还能把这种思路用于人脸图像，得到的就是特征脸算法，其中每个点 $x^{(i)} \in \mathcal{R}^{100×100} $ 都是一个 $10000 $ 维的向量，每个坐标对应的是一个 $100x100 $ 的人脸图像中的一个像素灰度值。使用主特征分析算法，我们就可以用更低维度的 $y^{(i)} $ 来表示每个图像 $x^{(i)} $。在这个过程中，我们希望主成分能够保存有用的信息和面孔之间的系统变化，能捕获到一个人看上去的模样，而不是由于细微的光线变化、轻微的拍摄状况差别等而引起的图像中的“噪音”。然后通过降低维度计算 $||y^{(i)} − y^{(j)}||_{2} $ 来测量人脸 $i $ 和 $j $ 之间的距离，这样就能得到面部匹配和检索算法。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之正则化与模型选择（note5）</title>
    <url>/2022/07/04/cs229/note5/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note5的主要内容：模型选择，特征选择，贝叶斯估计</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="Part-VII-正则化和模型选择"><a href="#Part-VII-正则化和模型选择" class="headerlink" title="Part VII 正则化和模型选择"></a>Part VII 正则化和模型选择</h2><p>设想给一个机器学习的问题选择模型，例如，若选择多项式回归模型$h_{\theta}(x) = g(\theta_{0}+\theta_{1}x+\theta_{2}x^{2}…+\theta_{k}x^{k})$，然后$k$的取值。那怎么才能自动选择一个可在偏差（bias）和方差（variance）之间平衡的模型呢？或者换一个说法，怎么自动选出来一个带宽参数（bandwidth parameter） $\tau $ 用于局部加权回归（locally weighted regression），或者怎么自动选出一个参数 C 用于拉格朗日正则化的支持向量机算法（l1-regularized SVM）？</p>
<p>为了具体一些，假设个数有限的模型集合$M = {M_{1},…,M_{d}} $。例如，在上面提到的例子，$M_{i} $ 就是 $i $次多项式回归模型。换个说法就是，如果我们要从支持向量机算法（SVM）、神经网络算法（neural network）、逻辑回归算法（logistic regression）当中三选一，那么这里的 M 就应该都包含这些模型。</p>
<h3 id="1，交叉验证"><a href="#1，交叉验证" class="headerlink" title="1，交叉验证"></a>1，交叉验证</h3><p>假设有一个训练集$S $，了解了经验风险最小化（empirical risk minimization，缩写为 ERM），就像算法初始化一样，接下来通过ERM来进行模型选择：</p>
<ul>
<li>1，对训练集 $S $ 中的每一个模型$M_{i} $ 进行训练，得到相应的假设$h_{i} $。</li>
<li>2，从这些假设中选取训练误差最小的假设（hypotheses）。</li>
</ul>
<p><strong>这个算法是行不通的。</strong></p>
<p>若考虑多项式模型，要考虑多项式的阶（最高次项的次数），阶越高，对训练集 S 的拟合程度就越好，训练误差自然也就更小。然而，这个方法选出来的总是那种波动非常强（high-variance）的高次多项式模型（high-degree polynomial model），这种情况之前讨论过，通常都是很差的选择。</p>
<p>下面这个算法就更好一些，叫做保留交叉验证（hold-out cross validation），也叫做简单交叉验证（simple cross validation），步骤如下：</p>
<ul>
<li>1，随机拆分训练集 $S $ 成 $S_{train} $ (如可选 70% 的比例) 和 $S_{cv} $ (训练集中剩余的 30%用于验证)。这里的 $S_{cv} $ 就叫做保留交叉验证集。</li>
<li>2，只对集合 $S_{train} $ 中的每一个模型 Mi 进行训练，然后得到假设$h_{i} $。</li>
<li>3，筛选并输出验证集上有最小误差的 $\hat{\epsilon}S_{cv}(h_{i}) $假设$h_{i} $ 。</li>
</ul>
<p>这样通过在部分未进行训练的样本集 $S_{cv}$ 上进行测试，我们对每个假设 hi 的真实泛化误差（generalization error）就能得到相对更好的估计，然后就能选择出来一个最小估计泛化误差的假设了。通常可以选择 $\frac{1}{4} ~ \frac{1}{3} $ 的数据样本用来作为交叉验证集，30% 是一个很典型的选择。</p>
<p>还有另外一种备选方法，就是在第三步的时候，选择与最小估计经验误差 $\hat{\epsilon}S_{cv}(h_{i}) $ 对应的模型 $M_{i} $ ，然后在完整数据集 S 上用 $M_{i} $ 来再次训练。（这个思路通常都不错，但有一种情景例外，就是学习算法对初始条件和数据的扰动非常敏感的情况。在这样的方法中，适用于$S_{train} $ 的模型未必就能够同样适用于 $S_{cv} $，这样就最好还是放弃再训练的步骤。）</p>
<p>使用保留交叉验证集的一个弊端就是“浪费”了30% 左右的训练样本数据集，甚至即便对整个训练集重新训练模型，也无非是尝试在一个 0.7m 规模的训练样本集上试图寻找一个好的模型来解决一个机器学习问题，而并不是使用了全部的 m 个训练样本，因为我们每次都是在仅 0.7 m 规模样本上进行训练得到模型。当然了，如果数据非常充足，或者是很廉价的话，也可以用这种方法，而如果训练样本数据本身就很稀缺的话，那就最好用其他方法了。</p>
<p>下面就是一种这样的方法，名字叫k-折交叉验证(k-fold cross validation)，这样验证集数据规模都更小：</p>
<ul>
<li>1，随机将训练集 $S $ 切分成 $k $ 个不相交的子集，其中每一个子集的规模为 $\frac{m}{k} $ 个训练样本，把这些子集称为 $S_{1},…,S_{k} $。</li>
<li>2，对每个模型 $M_{i} $，我们都按照下面的步骤来进行评估：对$j = 1, …, k $，在 $ S_{1}···\bigcup S_{j−1} \bigcup S_{j+1}\bigcup ···S_{k} $上（也就是除了 $S_{j} $ 之外的其他所有数据）对模型 $M_{i} $ 进行训练，然后得到假设 $h_{ij} $ 。接下来针对 $S_{j} $ 使用假设 $h_{ij} $ 进行测试，得到经验误差 $\hat{\epsilon}S_{j}(h_{ij}) $，对其取平均值（也就是对所有的 $j $ 都计算然后取平均值），计算得到的值就当做是模型 $M_{i} $ 的估计泛化误差。</li>
<li>3，选择具有最小估计泛化误差的模型 $M_{i} $，然后在整个训练样本集 $S $ 上重新训练该模型，这样得到的假设就可以输出作为最终结果了。</li>
</ul>
<p>通常这里折叠的次数 $k $ 一般是 10，即 $k = 10 $。这样每次进行保留用于验证的数据块就只有 $\frac{1}{k} $ ，这就比之前的 30% 要小多了，当然这样一来这个过程也要比简单的保留交叉验证方法消耗更多算力成本，因为现在需要对每个模型都进行 k 次训练。</p>
<p>虽然通常选择都是设置 $k = 10 $，不过如果一些问题中数据量确实很匮乏，那有时候也可以走一点极端，设 $k = m $，这样是为了每次能够尽可能多地利用数据，尽可能少排除数据。这种情况下，我们需要在训练样本集 $S $ 中除了某一个样本外，在其他所有样本上进行训练，然后在保留出来的单独样本上进行检验。然后把计算出来的 $m = k $个误差放到一起求平均值，这样就得到了对一个模型的泛化误差的估计。这个方法有专门的名字；由于每次都保留了一个训练样本，所以这个方法就叫做弃一法交叉验证（leave-one-out cross validation）。</p>
<p>最后总结一下，咱们讲了不同版本的交叉验证，在上文中是用来作为选择模型的方法，实际上也可以更单纯地用来对一个具体的模型或者算法进行评估。例如，如果你已经实现了某种学习算法，然后想要估计一下算法的性能表现（或者是你创造了一种新的学习算法，然后希望在技术论文中报告你的算法在不同测试集上的表现），交叉验证都是个很好的解决方法。</p>
<h3 id="2，特征选择"><a href="#2，特征选择" class="headerlink" title="2，特征选择"></a>2，特征选择</h3><p>模型选择的重要阶段就是特征选择，设想你面对一个监督学习问题，其中特征值的数量 $n $ 特别大（甚至可能比训练样本集规模还大，即$n &gt;&gt; m $），然而可能只有小部分的特征是与学习任务“相关”的。即便是针对 $n $ 个输入特征值使用一个简单的线性分类器，假设类的 VC 维也依然能达到 O(n)，就很有过拟合的潜在风险，除非训练样本集也足够巨大。</p>
<p>在这样的背景下，就可以用特征选择算法来降低特征值的数目。假设有 $n $ 个特征，那么就有 $2^{n} $ 种可能的特征子集（因为 n 个特征中的任意一个都可以被某个特征子集包含或者排除），因此特征选择就可以看做是对 $2^{n} $ 个可能的模型进行选择。对于特别大的 $n $，要是彻底枚举和对比全部 $2^{n} $ 种模型，成本就太高了，所以通常的做法都是使用启发式搜索过程（heuristic search procedure）来找到一个好的特征子集。下面的搜索过程叫做向前搜索（forward search）：</p>
<ul>
<li>1，初始化集合  $ \mathcal{F} = \emptyset $. </li>
<li>2，循环下面的过程<br>  {<br>  (a) 对于 $ i =1, …, n $ ，如果 $ i\notin \mathcal{F} $, 则令 $\mathcal{F}_{i} = \mathcal{F} \bigcup \{i\} $，然后使用某种交叉验证来评估特征  $\mathcal{F}_{i} $。（也就是说，仅仅使用  $\mathcal{F}_{i} $ 当中的特征来训练你的学习算法，然后估计一下泛化误差。）<br>  (b) 将  $\mathcal{F} $ 设为步骤 (a) 中的最佳特征子集。<br>  } </li>
<li>3，整个搜索过程中筛选出来了最佳特征子集，将其输出。</li>
</ul>
<p>上述算法最外层的循环体可以在 $\mathcal{F} = \{1, … , n\} $  为全部特征时终止，或者也可以在 $|\mathcal{F}| $ 超过某个预设阈值时终止（当算法要用到的特征数量达到了最大值）。</p>
<p>这个算法本质是模型特征选择包装器的一个实例，算法本身就是将学习算法进行“打包”的过程，然后重复调用这个学习算法来评估算法对不同特征子集的效果。除了向前搜索，还有其他的搜索过程，例如逆向搜索，从 $\mathcal{F} = \{1, …, n\} $ ，即全部特征开始，然后重复，每次删减一个特征，直到 $\mathcal{F} $ 为空集，即 $ \mathcal{F} = \emptyset $ 时终止。</p>
<p>这种包装器特征选择算法通常效果不错，不过对算力开销也很大，尤其是要对学习算法进行多次调用。实际上，完整的向前搜索（是 $\mathcal{F} $ 从空集开始，到最终达到整个样本集规模，即 $\mathcal{F} = \{1, …, n\} $ 终止），要对学习算法调用约 $O(n^{2}) $ 次。</p>
<p><strong>过滤器特征选择算法</strong>（Filter feature selection methods）给出的特征子集选择方法更具有启发性，而且在算力上的开销成本也更低。思路是，计算一个简单的分值 $S(i)$，用来衡量每个特征 $x_{i} $  对分类标签 $y $ 的信息量。然后，只需找到最大信息量分值 $S(i) $ 的一组，选择使用其中的 $k $ 个特征。</p>
<p>怎么去定义用于衡量信息量的分值 $S(i) $ 呢？一种思路是使用 $x_{i} $ 和 $y $ 之间相关系数的值（或其绝对值），这可以在训练样本数据中算出。这样我们选出的就是与分类标签$y $关系最密切的特征值。实践中，通常（尤其当特征 $x_{i} $ 为离散值时）选择 $x_{i} $ 和 $y $ 的互信息（mutual information）来作为 $S(i) $，缩写为 $MI(x_{i}, y) $。</p>
<script type="math/tex; mode=display">MI(x_{i},y) = \sum_{x_{i}\in\{0,1\}}\sum_{y\in\{0,1\}}p(x_{i},y)log\frac{p(x_{i},y)}{p(x_{i})p(y)}</script><p>上式中，假设了 $x_{i} $ 和 $y $ 都已经二值化，更广泛的情况下总和将会超过变量的范围。$p(x_{i},y)$， $p(x_{i}) $和 $p(y) $ 的概率都可以根据它们在训练集上的经验分布而推测得到。</p>
<p>要对这个信息量$x_{i}$的作用有一个更直观的印象，也可以将互信息表达成 KL 散度（Kullback-Leibler divergence，也称 KL 距离，常用来衡量两个概率分布的距离）：</p>
<script type="math/tex; mode=display">MI(x_{i},y) = KL(p(x_{i},y)||p(x_{i})p(y))</script><p>在下一节当中，会更多描述 KL ，这里比较通俗地说，这个概念对 $p(x_{i},y) $ 和 $p(xi)p(y)$ 的概率分布差异大小给出一个衡量。如果 $x_{i} $ 和 $y $ 是两个独立的随机变量，那么必然有 $p(x_{i}, y) = p(x_{i})p(y) $，而两个分布之间的 KL 散度就应该是 $0 $。这也符合下面这种很自然的认识：如果 $x_{i} $ 和 $y $ 相互独立，那么 $x_{i} $ 很明显对 $y $ 是“完全无信息量”的，因此对应的信息量分值 $S(i) $ 就应该很小。与之相反地，如果 $x_{i} $ 对 $y $ “有很大的信息量”，那么这两者的互信息$MI(x_{i},y) $ 就应该很大。</p>
<p>最后一个细节：现在已经根据信息量分值 $S(i) $ 的高低来对特征组合进行了排序，那么要如何选择特征个数 $k$ 呢？一个标准办法就是使用交叉验证从可能的不同 $k $ 值中进行筛选。例如，在对文本分类使用朴素贝叶斯方法），这个问题中的词汇规模$n $ 通常都会特别大，使用交叉验证的方法来选择特征子集，一般都能提高分类器精度。</p>
<h3 id="3，贝叶斯估计和正则化"><a href="#3，贝叶斯估计和正则化" class="headerlink" title="3，贝叶斯估计和正则化"></a>3，贝叶斯估计和正则化</h3><p>在本章，我们要讲一下我们“军火库”中的另外一种工具，用于我们对抗过拟合。</p>
<p>在本章的开头部分，我们谈到了使用最大似然来进行参数拟合，然后根据下面的式子来选择参数：</p>
<script type="math/tex; mode=display">\theta_{ML} = arg\mathop{max}\limits_{\theta}\prod\limits_{i=1}^{m}p(y^{(i)}|x^{(i)}; \theta)</script><p>在随后的讨论中，我们将$\theta$视为一个未知参数，概率统计中将$\theta $视为未知常值，并不是随机的，但恰好是未知的，我们的工作就是提出统计处理流程（如最大似然）来尝试估计这个参数。</p>
<p>另外一种解决参数估计的方法是贝叶斯估计，$\theta$ 当做是未知的随机变量，先给定一个在 $\theta $ 上的先验分布$p(\theta) $，这个分布表示关于参数的“预先判断”。给定一个训练集合 $S = \{(x^{(i)},y^{(i)})\}^{m}_{i=1} $，当对一个新的 $x $ 进行预测的时候，可以计算在参数上的后验分布（posterior distribution）：</p>
<script type="math/tex; mode=display">\begin{array}{ll}p(\theta|S) & = \quad \frac{p(S|\theta)p(\theta)}{p(S)}\\
& = \quad \frac{(\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta))p(\theta)}{\int_{\theta}(\prod_{i=1}^{m}p(y^{(i)}|x^{(i)}, \theta)p(\theta))d\theta}\end{array}</script><p>上式中，$p(y^{(i)}|x^{(i)},\theta) $ 是机器学习问题中的模型。例如，如果是贝叶斯逻辑回归，可能就会选择 $p(y^{(i)}|x^{(i)}, \theta) = h_{\theta}(x^{(i)})^{y(i)}(1−h_{\theta}(x^{(i)}))(1−y^{(i)}) $，其中 $h_{\theta}(x^{(i)}) = 1/(1 + exp(−\theta^{T}x^{(i)}))^{3} $。</p>
<p>若有一个新的测试样本 $x $，需要进行预测，可以使用$\theta$ 上的后验分布来计算分类标签上的后验分布：</p>
<script type="math/tex; mode=display">p(y|x, \theta) = \int_{\theta}p(y|x,\theta)p(\theta|S)d\theta</script><p>上面等式中，$p(\theta|S) $ 前面介绍过，如果目标是要根据给定的 $x $ 来预测对应的 $y $ 值，那就可以输出：</p>
<script type="math/tex; mode=display">E[y|x,S] = \int_{y}yp(y|x,S)dy</script><p>简单概述这个过程，可认为是一种“完全贝叶斯”预测，其中预测是通过计算相对于 $\theta$ 上的后验概率 $p(\theta|S) $ 的平均值而得出的。但是，这个后验分布的计算通常是比较困难的，需要对 $\theta$ 进行积分，而 $\theta$ 通常是高维度的，这通常是不能以闭合形式来实现的。</p>
<p>因此在实际应用中，我们都是用一个与 $\theta$ 后验分布近似的分布来替代。常用的一个近似是把对 $\theta$ 的后验分布替换为一个单点估计。对 $\theta$的最大后验估计（MAP，maximum a posteriori estimate）为：</p>
<script type="math/tex; mode=display">\theta_{MAP} = arg\mathop{max}\limits_{\theta}\prod\limits_{i=1}^{m}p(y^{(i)}|x^{(i)},\theta)p(\theta)</script><p>注意到了么，这个式子基本和对 $\theta$ 的最大似然估计（ML,maximum likelihood estimate）形式一样，除了末尾多了一个先验概率分布 $p(\theta)$。</p>
<p>实际应用里面，对先验概率分布 $p(\theta) $ 的常见选择是假设 $\theta \sim N(0 ,\tau^{2}I) $。使用这样的一个先验概率分布，拟合出来的参数 $θ_{MAP} $ 将比最大似然估计得到的参数范数更小。在实践中，贝叶斯最大后验估计（Bayesian MAP estimate）对比参数的最大似然估计（ML estimate of the parameters），前者就更易于避免过拟合。例如，贝叶斯逻辑回归就是一种非常有效率的文本分类算法，即便在文本分类中参数规模 $n $ 通常是远远大于样本规模 $m $ 的，即 $n ≫ m $。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229的lecture note1</title>
    <url>/2022/06/22/cs229/note1/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note1的主要内容：监督学习概览，线性回归机器概率解释，逻辑回归…..</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="PART-II-分类和逻辑回归"><a href="#PART-II-分类和逻辑回归" class="headerlink" title="PART II 分类和逻辑回归"></a>PART II 分类和逻辑回归</h2><p>接下来讨论分类的问题，其实本质和回归问题很像，只是待预测的$y$值值域为个数有限的离散值的集合。</p>
<p>首先来看二分类问题，$y$只有两个取值，0 或者 1（此处讨论的问题也可以拓展到多类的情况）。</p>
<p>例如，假如要建立一个垃圾邮件筛选器，那么就可以用 $x^{i}$ 表示一个邮件中的若干特征，邮件是垃圾邮件时$y=1$，称为正类别（positive class），否则 $y=0$，被称为负类别，有的情况下也分别表示成 $“+”$ 和 $“-”$ 。</p>
<p>对于给定的一个 $x^{i}$，对应的$y^{i}$ 也称为训练样本的标签（label）。</p>
<h3 id="5-逻辑回归"><a href="#5-逻辑回归" class="headerlink" title="5, 逻辑回归"></a>5, 逻辑回归</h3><p>忽略$y$取值于离散集合后，也可以按照前面线性回归的算法来由$x$预测$y$，但是这样构建的例子性能和表现都会比较差。而且，直观来看，$y\in\{0,1\}$，当$\quad h_{\theta}(x)&gt;1 \quad or\quad h_{\theta}(x)&lt;0\quad$都没有意义。</p>
<p>所以，换一个假设函数$h_{\theta}(x)$</p>
<script type="math/tex; mode=display">h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}</script><p>其中，$g(z) = \frac{1}{1+e^{-z}}$称为逻辑函数或者sigmoid函数，下图是$g(z)$的函数图像<br><img src="sigmoid-func.png" alt="逻辑函数"></p>
<ul>
<li>$g(z)\rightarrow 1 \quad if \quad z\rightarrow\infty$，同时，$g(z)\rightarrow0 \quad if \quad z\rightarrow -\infty$；</li>
<li>$g(z)\in(0,1)\quad and \quad h(x)\in(0,1)$；</li>
<li>$g(z)$在$(0,1)$上光滑递增；</li>
<li>$g(z)$还有些其他的性质会在后面讲到。</li>
</ul>
<p>前面，约定$x_{0}=1$后，重写$h_{\theta}(x)=\theta^{T}x=\theta_{0}+\sum^{n}_{j=1}\theta_{j}x_{j}$。此处我们选定$g$来作为估计函数，先讨论sigmoid导数的某些好用的特性：</p>
<script type="math/tex; mode=display">g^{'}(z) = g(z)(1-g(z))</script><p>自行补充推导过程。</p>
<p>那么，给定逻辑回归模型后，怎么去拟合一个合适的$\theta$呢？</p>
<p>之前已经证明过，在一系列假设的前提下，最小二乘法回归可以通过最大似然估计来推出。接下来就给分类模型做一系列的统计学假设，然后用最大似然法来拟合参数吧。</p>
<p>首先假设</p>
<script type="math/tex; mode=display">p(y|x;\theta) = (h_{\theta}(x))^{y}(1-h_{\theta}(x))^{1-y}</script><p>假设$m$个训练样本是独立生成，那么可以将带参数的似然函数写做：</p>
<script type="math/tex; mode=display">\begin{array}{ll} L(\theta) & = \quad p(\vec{y}|X;\theta)\\ & = \quad \prod^{m}_{i=1}p(y^{i}|x^{i};\theta)\\ & = \quad \prod^{m}_{i=1}(h_{\theta}(x^{i}))^{y^{i}}(1-h_{\theta}(x^{i}))^{1-y^{i}}\end{array}</script><p>类似常规处理方法，取对数之后更方便计算</p>
<script type="math/tex; mode=display">\begin{array}{ll}  l(\theta) & = \quad log L(\theta) \\ & = \quad \sum^{m}_{i=1}y^{i}logh(x^{i})+(1-y^{i})log(1-h(x^{i})) \end{array}</script><p>怎么最大化似然函数呢？与线性回归中用到的求导方法类似，咱们这次就是用梯度上升法（gradient ascent）。仍然用向量的形式来对参数进行更新，也就是：</p>
<script type="math/tex; mode=display">\theta := \theta + \alpha\nabla_{\theta}l(\theta)</script><p>因为是求极大值，所以注意下式子中是+号而不是-号，用一个样本$(x,y)$来推导随机梯度法的导数公式：</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial\theta_{j}}l(\theta) = (y-h_{\theta}(x))x_{j}</script><p><strong>请自行推导哦</strong></p>
<p>所以，随机梯度上升的更新公式为：</p>
<script type="math/tex; mode=display">\theta_{j} := \theta_{j}+\alpha(y^{i}-h_{\theta}(x^{i}))x_{j}^{i}</script><p>跟之前的 LMS 更新规则相对比，形式上挺相似的，不过这并不是同一个算法，因为这里的$h_{\theta}(x^{i})$现在定义成了一个$\theta^{T}x^{i}$的非线性函数。尽管如此，面对不同的学习问题使用了不同的算法，却得到了看上去一样的更新规则，这个还是有点让人吃惊。这是一个巧合么，还是背后有更深层次的原因呢？在我们学到了 GLM 广义线性模型时就会得到答案了。</p>
<h3 id="6，题外话：感知学习算法"><a href="#6，题外话：感知学习算法" class="headerlink" title="6，题外话：感知学习算法"></a>6，题外话：感知学习算法</h3><p>现在岔开一下话题，简要地聊另一个算法，这个算法的历史很有趣，并且之后在讲学习理论时还要讲到它。</p>
<p>设想一下，对逻辑回归方法修改一下，强制它输出的值只能是0或1。要实现这个目的，很自然就应该把函数 $g$ 的定义修改一下，改成一个阈值函数（threshold function）</p>
<script type="math/tex; mode=display">g(z) = \left\{\begin{array}{ll} 1 & if\quad z\geq0\\ 0 & if\quad z<0 \end{array}\right.</script><p>然后，令$h_{\theta}(x)=g(\theta^{T}x)$，使用上面定义的阈值函数$g$，然后更新规则即为：</p>
<script type="math/tex; mode=display">\theta_{j} := \theta_{j}+\alpha(y^{i}-h_{\theta}(x^{i}))x^{i}_{j}</script><p>这就是感知学习算法。</p>
<p>在 1960 年代，“感知器（perceptron）”被认为是对大脑中单个神经元工作方法的一个粗略建模，这个简单的算法也是我们后续在本课程中讲学习理论时的起点。</p>
<p>但一定要注意，虽然这个感知学习算法可能看上去跟之前讲的其他算法挺相似，但实际上这是一个和逻辑回归、最小二乘线性回归等算法在本质上完全不同的算法。</p>
<p>尤其重要的是，很难对感知器的预测赋予有意义的概率解释，也很难作为一种最大似然估计算法来推出感知器学习算法。</p>
<h3 id="7，最大化-l-theta-的另一种算法"><a href="#7，最大化-l-theta-的另一种算法" class="headerlink" title="7，最大化$l(\theta)$的另一种算法"></a>7，最大化$l(\theta)$的另一种算法</h3><p>回到$g(z)$为sigmoid函数时的逻辑回归算法，重新讨论下最大化$l(\theta)$的算法。</p>
<p>首先来考虑牛顿法来求方程零点，如有函数$f: R-&gt;R$，求参数$\theta$使得$f(\theta)=0$，此处的$\theta\in R$ 是实数。</p>
<p>牛顿法的更新规则是：</p>
<script type="math/tex; mode=display">\theta := \theta-\frac{f(\theta)}{f'(\theta)}</script><p>这个方法可以通过一个很自然的解释，把它理解成用一个线性函数来对函数 $f$ 进行逼近，这条直线是 $f$ 的切线，而猜测值是 $\theta$，求解的方法就是找到线性方程等于零的点，把这一个零点作为 $\theta$ 值设置为下一次猜测，然后依次类推。</p>
<p><img src="newtons method.png" alt="newtons_method"></p>
<p>在最左边的图里面，可以看到函数 $f$ 和 $y=0$ 的图像，想要找一个 $\theta$ 来让 $f(\theta)=0$，这时候发现这个 $\theta$ 值大概在 1.3 左右。加入咱们猜测的初始值设定为 $\theta =4.5$，牛顿法就是在 $\theta =4.5$ 这个位置画一条切线（中间的图）。这样就给出了下一个 $\theta$  猜测值的位置，也就是这个切线的零点，大概是2.8。最右面的图中的是再运行一次这个迭代产生的结果，这时候 θ 大概是1.8。就这样几次迭代之后，很快就能接近 $\theta =1.3$。</p>
<p>牛顿方法给出了求解$f(\theta)=0$，怎么用它来最大化损失函数$l$呢？$l$取最大值的点应该是导数$l’(\theta)$的第一个零点。所以，令$f(\theta)=l’(\theta)$，可以用同样的算法来最大化$l$，能得到如下更新规则：</p>
<script type="math/tex; mode=display">\theta := \theta-\frac{f'(\theta)}{f''(\theta)}</script><p>思考下：如果是最小化函数而不是最大化函数呢？应该是怎样的更新规则？</p>
<p>近期介绍的逻辑回归中，$\theta$是向量，所以需要将牛顿方法一般化。多维空间中的牛顿方法（也叫Newton-Raphson方法）更新规则为：</p>
<script type="math/tex; mode=display">\theta := \theta - H^{-1}\nabla_{\theta}l(\theta)</script><p>此处的$\nabla_{\theta}l(\theta)$是$l(\theta)$关于$\theta_{i}$的导数，$H$是$n$维的矩阵（加上偏置项是$n+1$维矩阵），叫Hessian矩阵，表达式：</p>
<script type="math/tex; mode=display">H_{ij}=\frac{\partial^{2}l(\theta)}{\partial\theta_{i}\partial\theta_{j}}</script><p>牛顿方法比（批量）梯度下降法更快收敛，更少的迭代次数就能获得极小。但是，一次牛顿迭代比梯度下降的计算量大很多，需要求Hessian矩阵及其逆矩阵，如果$n$不大的情况下，牛顿法明显有更快的优势。当用牛顿方法来解决逻辑回归的似然函数的最大化问题时，通常也把求解过程叫做Fisher scoring。</p>
<h2 id="PART-III-广义线性模型（GLM）"><a href="#PART-III-广义线性模型（GLM）" class="headerlink" title="PART III 广义线性模型（GLM）"></a>PART III 广义线性模型（GLM）</h2><p>目前位置，我们讨论了一个回归($y|x;\theta\sim\mathcal{N}(u,\sigma^{2})$)和一个分类($y|x;\theta\sim Bernoulli(\phi)$)案例，$u$和$ \phi $是定义在$x$和$ \theta $上的函数。在本节，我们会发现这两种方法都是一个更广泛使用的模型的特例，这种更广泛使用的模型就叫做广义线性模型。我们还会讲一下广义线性模型中的其他模型是如何推出的，以及如何应用到其他的分类和回归问题上。</p>
<h3 id="指数簇"><a href="#指数簇" class="headerlink" title="指数簇"></a>指数簇</h3><p>在学习 GLMs 之前，我们要先定义一下指数组分布。如果一个分布能用下面的方式来写出来，我们就说这类分布属于指数族：</p>
<script type="math/tex; mode=display">p(y;\eta) = b(y)exp(\eta^{T}T(y)-a(\eta))</script><p>上面的式子中，$\eta$叫做此分布的自然参数（也叫典范参数），$T(y)$ 叫做充分统计量，我们目前用的这些分布中通常$T(y) = y$；而 $a(\eta)$ 是一个对数分割函数，$e^{-a(\eta)}$这个量本质上扮演了归一化常数的角色，也就是确保 $p(y;\eta)$的总和等于1</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>shopify-cli在ubuntu20.04上搭建使用环境</title>
    <url>/2022/06/17/ubuntuOS/shopify-cli/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在本地构建shopify中主题和app的开发环境，在ubuntu和docker环境中构建的主要步骤~</p>
<p>主题涉及到liquid、css和html，前端可能相对更熟悉。</p>
<p><strong>先尝试主题的构建，再逐步到app开发~</strong><br><span id="more"></span></p>
<h2 id="1，基础docker"><a href="#1，基础docker" class="headerlink" title="1，基础docker"></a>1，基础docker</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it --name=shopify -p 9292:9292 -p 3456:3456 -v C:\Coding\shopifyStoreTheme:/home ubuntu20.04</span><br></pre></td></tr></table></figure>
<p>9292是本地预览的端口，3456是在关联到远端shopify店铺时的信息交互端口</p>
<h2 id="2，shopify-cli环境配置"><a href="#2，shopify-cli环境配置" class="headerlink" title="2，shopify-cli环境配置"></a>2，shopify-cli环境配置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br><span class="line"></span><br><span class="line"># 安装ruby</span><br><span class="line">apt-get install -y ruby-full</span><br><span class="line"></span><br><span class="line"># 验证，ruby --version和gem --version可查看版本号</span><br><span class="line"></span><br><span class="line"># 更改ruby的源到国内</span><br><span class="line">gem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/</span><br><span class="line"></span><br><span class="line"># 更新缓存</span><br><span class="line">gem sources -u</span><br><span class="line"></span><br><span class="line"># shopify-cli是ruby的依赖管理器gem的一个依赖包，类似nodejs对于npm一样。</span><br><span class="line">gem install shopify-cli</span><br><span class="line"></span><br><span class="line"># 需要这个组件</span><br><span class="line">apt-get install -y git</span><br></pre></td></tr></table></figure>
<h2 id="3，shopify-cli命令"><a href="#3，shopify-cli命令" class="headerlink" title="3，shopify cli命令"></a>3，shopify cli命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用shopify help &lt;command&gt; 显示有关特定命令的详细信息。</span><br><span class="line"> </span><br><span class="line"># 本地预览</span><br><span class="line">shopify theme serve --host=0.0.0.0 --port=9292</span><br></pre></td></tr></table></figure>
<p>Note：默认是120.0.0.1:9292的访问地址，但是因为在docker中，所以host改成了0.0.0.0，端口不变</p>
<h2 id="4，参考文献"><a href="#4，参考文献" class="headerlink" title="4，参考文献"></a>4，参考文献</h2><p><a href="https://shopify.dev/themes">帮助文档</a></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>shopify-cli</tag>
      </tags>
  </entry>
  <entry>
    <title>jetson性能对比</title>
    <url>/2022/06/09/cv_engineering/jetson-performance/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>yolo, mask rcnn和其他图像分类算法在常用jetson硬件上的表现，仅供参考哦~</p>
<p><strong>工程上选模型可做参考~</strong><br><span id="more"></span></p>
<h2 id="1，yolo系列"><a href="#1，yolo系列" class="headerlink" title="1，yolo系列"></a>1，yolo系列</h2><div class="table-container">
<table>
<thead>
<tr>
<th>network</th>
<th>device</th>
<th>activation</th>
<th>precision</th>
<th>batch</th>
<th>DLA</th>
<th>framework</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>yolov3</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>24ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>18ms</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>30ms</td>
</tr>
<tr>
<td>-</td>
<td>TX2</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>99ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>90ms(22.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>58ms(14.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>TX2</td>
<td>leaky</td>
<td>fp16</td>
<td>32</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>2930ms(91.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>32</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>440ms(13.75ms each)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>leaky</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>104ms(26ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>20ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>12.5ms</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>leaky</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>20ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>66ms(16.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>36ms(9ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>32</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>256ms(8ms each)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>leaky</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>64ms(16ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>52ms(13ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>10ms</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>relu</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>17ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>30ms(7.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>58ms(14.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>54ms(13.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>1050ti</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>45ms(11.25ms each)</td>
</tr>
<tr>
<td>yolov3-tiny</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>5ms</td>
</tr>
<tr>
<td>yolo-resnet</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>14ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>44ms(11ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>12ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>39ms(10ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>30ms(7.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>fp16</td>
<td>4</td>
<td>yes</td>
<td>TRT5.1.6</td>
<td>68ms(17ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>22ms(5.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>1050ti</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>24ms(6ms each)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="2，mask-rcnn模型"><a href="#2，mask-rcnn模型" class="headerlink" title="2，mask rcnn模型"></a>2，mask rcnn模型</h2><div class="table-container">
<table>
<thead>
<tr>
<th>device</th>
<th>input shape</th>
<th>precision</th>
<th>batch</th>
<th>framework</th>
<th>pure enqueue time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1050ti</td>
<td>1024x1024</td>
<td>fp32</td>
<td>1</td>
<td>TRT7.0</td>
<td>364ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>-</td>
<td>-</td>
<td>140ms</td>
</tr>
<tr>
<td>Xavier</td>
<td>-</td>
<td>fp16</td>
<td>-</td>
<td>TRT7.1</td>
<td>136ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>-</td>
<td>-</td>
<td>103ms</td>
</tr>
<tr>
<td>NX</td>
<td>-</td>
<td>fp32</td>
<td>-</td>
<td>-</td>
<td>871ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>fp16</td>
<td>-</td>
<td>-</td>
<td>239ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>-</td>
<td>-</td>
<td>165ms</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3，常用图像分类模型"><a href="#3，常用图像分类模型" class="headerlink" title="3，常用图像分类模型"></a>3，常用图像分类模型</h2><div class="table-container">
<table>
<thead>
<tr>
<th>network</th>
<th>device</th>
<th>precision</th>
<th>batch</th>
<th>DLA</th>
<th>framework</th>
<th>pure enqueue time</th>
</tr>
</thead>
<tbody>
<tr>
<td>google net</td>
<td>apex</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>1.5ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>3.5ms(avg 0.9ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>5.5ms(avg 0.7ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>17.5ms(avg 0.55ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>64ms(avg 0.5ms)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>half</td>
<td>1</td>
<td>-</td>
<td>TRT7.1</td>
<td>3ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>2ms</td>
</tr>
<tr>
<td>-</td>
<td>tx2</td>
<td>half</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>5.2ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>118ms(avg 3.7ms)</td>
</tr>
<tr>
<td>-</td>
<td>1050ti</td>
<td>float32</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>3.5ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>11ms(avg 2.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>16ms(avg 2ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>61ms(avg 1.9ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>236ms(avg 1.84ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>1.5ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>4.5ms(avg 1.1ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>6ms(avg 0.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>24ms(avg 0.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>90ms(avg 0.7ms)</td>
</tr>
<tr>
<td>resnet50</td>
<td>apex</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>2.2ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>4.3ms(avg 1.1ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>7.5ms(avg 0.9ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>25ms(avg 0.8ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>94.5ms(avg 0.74ms)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>half</td>
<td>1</td>
<td>-</td>
<td>TRT7.1</td>
<td>6ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>103ms(avg 3.2ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>3.8ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>64ms(avg 2ms)</td>
</tr>
<tr>
<td>-</td>
<td>tx2</td>
<td>half</td>
<td>1</td>
<td>-</td>
<td>TRT5.1.6</td>
<td>13ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>320ms(avg 10ms)</td>
</tr>
<tr>
<td>-</td>
<td>1050ti</td>
<td>float32</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>8ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>23ms(avg 5.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>38ms(avg 4.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>133ms(avg 4ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>510ms(avg 4ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>3ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>8ms(avg 2ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>14ms(avg 1.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>44ms(avg 1.4ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>167ms(avg 1.3ms)</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>jetson</tag>
      </tags>
  </entry>
  <entry>
    <title>人脸识别的算法演变</title>
    <url>/2022/06/08/facerecog/face-recognition-algorithms/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>人脸识别的算法演变之路，从传统方法到CNN，再到各种loss和其他的优化方向等等~</p>
<p><strong>抓紧时间看论文啊，会不断更新的~</strong><br><span id="more"></span></p>
<h2 id="1，一些传统思路"><a href="#1，一些传统思路" class="headerlink" title="1，一些传统思路"></a>1，一些传统思路</h2><h3 id="Eigenface"><a href="#Eigenface" class="headerlink" title="Eigenface"></a>Eigenface</h3><h3 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h3><h3 id="LBP-Joint-Bayes"><a href="#LBP-Joint-Bayes" class="headerlink" title="LBP+Joint Bayes"></a>LBP+Joint Bayes</h3><h2 id="2，CNN基线"><a href="#2，CNN基线" class="headerlink" title="2，CNN基线"></a>2，CNN基线</h2><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><h2 id="3，CNN和各种Loss"><a href="#3，CNN和各种Loss" class="headerlink" title="3，CNN和各种Loss"></a>3，CNN和各种Loss</h2><h3 id="Pairwise"><a href="#Pairwise" class="headerlink" title="Pairwise"></a>Pairwise</h3><h3 id="Triplet"><a href="#Triplet" class="headerlink" title="Triplet"></a>Triplet</h3><h3 id="DeepFace"><a href="#DeepFace" class="headerlink" title="DeepFace"></a>DeepFace</h3><h3 id="DeepID1"><a href="#DeepID1" class="headerlink" title="DeepID1"></a>DeepID1</h3><h3 id="DeepID2"><a href="#DeepID2" class="headerlink" title="DeepID2"></a>DeepID2</h3><h3 id="DeepID2-1"><a href="#DeepID2-1" class="headerlink" title="DeepID2+"></a>DeepID2+</h3><h3 id="DeepID3"><a href="#DeepID3" class="headerlink" title="DeepID3"></a>DeepID3</h3><h3 id="FaceNet"><a href="#FaceNet" class="headerlink" title="FaceNet"></a>FaceNet</h3><h3 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h3><h2 id="归一化等"><a href="#归一化等" class="headerlink" title="归一化等"></a>归一化等</h2><h3 id="Lsoftmax"><a href="#Lsoftmax" class="headerlink" title="Lsoftmax"></a>Lsoftmax</h3><h3 id="Asoftmax"><a href="#Asoftmax" class="headerlink" title="Asoftmax"></a>Asoftmax</h3><h3 id="NormFace-Coco-Loss"><a href="#NormFace-Coco-Loss" class="headerlink" title="NormFace/Coco Loss"></a>NormFace/Coco Loss</h3><h3 id="Feature-Incay"><a href="#Feature-Incay" class="headerlink" title="Feature Incay"></a>Feature Incay</h3><h3 id="AMSoftmax-CosFace"><a href="#AMSoftmax-CosFace" class="headerlink" title="AMSoftmax/CosFace"></a>AMSoftmax/CosFace</h3><h3 id="Arcface-InsightFace"><a href="#Arcface-InsightFace" class="headerlink" title="Arcface/InsightFace"></a>Arcface/InsightFace</h3>]]></content>
      <categories>
        <category>facerecog</category>
      </categories>
      <tags>
        <tag>face recognition</tag>
      </tags>
  </entry>
  <entry>
    <title>UI设计资源</title>
    <url>/2022/06/02/tools/UI-source/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>设计师资源，日常APP或者网页设计中用得上的，基本免费，当然也有消费的项目~</p>
<p><strong>物美价廉~</strong><br><span id="more"></span></p>
<h2 id="1-最大的免费图标库-Flaticon"><a href="#1-最大的免费图标库-Flaticon" class="headerlink" title="1, 最大的免费图标库-Flaticon"></a>1, 最大的免费图标库-Flaticon</h2><p><a href="https://www.flaticon.com/">official link</a></p>
<p>英文关键字搜索，常用图标格式，矢量图或像素图</p>
<h2 id="2-阿里巴巴矢量图标库-iconfont"><a href="#2-阿里巴巴矢量图标库-iconfont" class="headerlink" title="2, 阿里巴巴矢量图标库-iconfont"></a>2, 阿里巴巴矢量图标库-iconfont</h2><p><a href="https://www.iconfont.cn/">official link</a></p>
<p><strong>哈哈，我的最爱~</strong><br>中英文，拼音啥的都可以搜索</p>
<p>多种图标格式，矢量和像素图</p>
<h2 id="3-Easyicon"><a href="#3-Easyicon" class="headerlink" title="3, Easyicon"></a>3, Easyicon</h2><p><a href="https://www.easyicon.net/">official link</a></p>
<p>多种图标格式，可转换第三方图标为微软或mac图标</p>
<h2 id="4-扁平化APP图标库-iconsDB"><a href="#4-扁平化APP图标库-iconsDB" class="headerlink" title="4, 扁平化APP图标库-iconsDB"></a>4, 扁平化APP图标库-iconsDB</h2><p><a href="https://www.iconsdb.com/">official link</a></p>
<p>大部分支持个人和商业用途</p>
<h2 id="5-爱看图标网-IconPng"><a href="#5-爱看图标网-IconPng" class="headerlink" title="5, 爱看图标网-IconPng"></a>5, 爱看图标网-IconPng</h2><p><a href="http://www.iconpng.com/">official link</a></p>
<h2 id="6-图标设计综合网站-IconDeposit"><a href="#6-图标设计综合网站-IconDeposit" class="headerlink" title="6, 图标设计综合网站-IconDeposit"></a>6, 图标设计综合网站-IconDeposit</h2><p><a href="https://www.icondeposit.com/">official link</a></p>
<h2 id="7-社区爱-icons8"><a href="#7-社区爱-icons8" class="headerlink" title="7, 社区爱-icons8"></a>7, 社区爱-icons8</h2><p><a href="https://icons8.cn/">official link</a></p>
<h2 id="8-iconfinder"><a href="#8-iconfinder" class="headerlink" title="8, iconfinder"></a>8, iconfinder</h2><p><a href="https://www.iconfinder.com/">official link</a></p>
<h2 id="9-axure商城"><a href="#9-axure商城" class="headerlink" title="9, axure商城"></a>9, axure商城</h2><p><a href="https://www.axureshop.com/">official link</a></p>
<p>很多原型，各个业务领域，高保真、低保真都有</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>UI source</tag>
      </tags>
  </entry>
  <entry>
    <title>useful pip source in China</title>
    <url>/2022/05/20/python/pip-source-in-China/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>python安装模块常用的是pip，但是source在国外，如果没有好用的梯子，那就换成国内的镜像源吧，本文基本内容如下：</p>
<ul>
<li>国内的常用pip源；</li>
<li>两种pip应用国内源的方式；</li>
<li>清理pip的缓存；</li>
</ul>
<span id="more"></span>
<h2 id="1，国内源"><a href="#1，国内源" class="headerlink" title="1，国内源"></a>1，国内源</h2><ul>
<li>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></li>
<li>阿里云：<a href="http://mirrors.aliyun.com/pypi/simple">http://mirrors.aliyun.com/pypi/simple</a></li>
<li>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple">https://pypi.mirrors.ustc.edu.cn/simple</a></li>
<li>华中理工大学：<a href="http://pypi.hustunique.com">http://pypi.hustunique.com</a></li>
<li>山东理工大学：<a href="http://pypi.sdutlinux.org">http://pypi.sdutlinux.org</a></li>
<li>豆瓣：<a href="http://pypi.douban.com/simple">http://pypi.douban.com/simple</a></li>
</ul>
<h2 id="2，临时使用"><a href="#2，临时使用" class="headerlink" title="2，临时使用"></a>2，临时使用</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install -i http://mirrors.aliyun.com/pypi/simple/</span><br><span class="line">or</span><br><span class="line">pip3 install -i http://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure>
<h2 id="3，永久使用"><a href="#3，永久使用" class="headerlink" title="3，永久使用"></a>3，永久使用</h2><ul>
<li>linux<br>修改或增加文件<code>~/.pip/pip.conf</code>, 内容为：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host=mirrors.aliyun.com</span><br></pre></td></tr></table></figure></li>
<li>windows<br>文件路径：<code>C:\Users\WQP\pip\pip.ini</code></li>
</ul>
<h2 id="4，清理pip缓存"><a href="#4，清理pip缓存" class="headerlink" title="4，清理pip缓存"></a>4，清理pip缓存</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># linux</span><br><span class="line">~/.cache/pip</span><br><span class="line"># win</span><br><span class="line">%LocalAppData%\pip\Cache</span><br><span class="line"># OS X</span><br><span class="line">~/Library/Caches/pip</span><br></pre></td></tr></table></figure>
<h2 id="5，pip包下载"><a href="#5，pip包下载" class="headerlink" title="5，pip包下载"></a>5，pip包下载</h2><p><a href="https://pypi.org/">pypi</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pip source</tag>
      </tags>
  </entry>
  <entry>
    <title>vmware中的ubuntu突然没有网络</title>
    <url>/2022/05/20/ubuntuOS/vmware-ubuntu1804-wlan-error/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在vmware虚拟机中使用ubuntu系统时，有次突然出现没有了网络，几番查找后，找到了解决办法，但是至今不晓得问题的成因，尴尬了~~尽管如此，先把问题解决了再说吧^_^</p>
<p><strong>come on, girl~~</strong><br><span id="more"></span></p>
<h2 id="1，首先查看Ubuntu的网络设置"><a href="#1，首先查看Ubuntu的网络设置" class="headerlink" title="1，首先查看Ubuntu的网络设置"></a>1，首先查看Ubuntu的网络设置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /var/lib/NetworkManager</span><br><span class="line">sudo cat NetworkManager.state</span><br></pre></td></tr></table></figure>
<p>如果NetworkingEnabled=false，那么进入下一步，否则查找其他原因</p>
<h2 id="2，解决方案"><a href="#2，解决方案" class="headerlink" title="2，解决方案"></a>2，解决方案</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 关闭网络</span><br><span class="line">sudo service network-manager stop</span><br><span class="line"># 用vi或者gedit修改NetworkManager.state</span><br><span class="line">NetworkingEnabled=true</span><br><span class="line"># 重新开启网络</span><br><span class="line">sudo service network-manager start</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>ubuntu wlan error</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu中解决PL2303的驱动问题</title>
    <url>/2022/05/20/hardware/ubuntu-PL2303/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个问题出现在，win10的PC+VMWare的ubuntu+hi3559a环境搭建时ubuntu系统出现的驱动问题，若有类似情况，亦可用下文方案解决。</p>
<span id="more"></span>
<h2 id="1，详细步骤"><a href="#1，详细步骤" class="headerlink" title="1，详细步骤"></a>1，详细步骤</h2><ul>
<li><p>文件复制</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /lib/modules/4.2.0-27-generic/kernel/drivers/usb/serial/pl2303.ko /usr/src/linux-headers-4.2.0-27-generic/drivers/usb/serial</span><br></pre></td></tr></table></figure>
<p>内核不同，可能路径不同</p>
</li>
<li><p>安装命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ modprobe usbserial</span><br><span class="line">$ modprobe pl2303</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证<br>输入<code>lsmod | grep usbserial</code>可以看到<code>usbserial</code>信息说明安装成功；<br>输入<code>dmesg | tail</code>可以看到<code>usb pl2303</code>等信息亦说明安装成功；</p>
</li>
</ul>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>ubuntu PL2303</tag>
      </tags>
  </entry>
  <entry>
    <title>边缘提取方法</title>
    <url>/2022/05/19/cv_engineering/edge-extract/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Chen Mingming老师组的三个边缘提取的模型，还有部分数据集，效果不错~</p>
<p><strong>用边缘特征做过一些图像应用哦</strong><br><span id="more"></span></p>
<h2 id="1，边缘提取hed"><a href="#1，边缘提取hed" class="headerlink" title="1，边缘提取hed"></a>1，边缘提取hed</h2><p><a href="https://github.com/s9xie/hed">github link</a></p>
<ul>
<li>作者<br>南开大学程明明</li>
<li>对应文章   <ul>
<li>author = {“Xie, Saining and Tu, Zhuowen”},</li>
<li>Title = {Holistically-Nested Edge Detection},</li>
<li>Booktitle = “Proceedings of IEEE International Conference on Computer Vision”,</li>
<li>Year = {2015},</li>
</ul>
</li>
<li>结果<br>BSDS benchmark数据集：<br>ODS=.790 and OIS=.808<br>BSD500 数据集：<br>ODS F-score of .790<br>NYU Depth 数据集：<br>ODS F-score of .746<br>速度： (0.4s per image)</li>
</ul>
<h2 id="2，边缘提取rcf"><a href="#2，边缘提取rcf" class="headerlink" title="2，边缘提取rcf"></a>2，边缘提取rcf</h2><p><a href="https://github.com/meteorshowers/RCF-pytorch">pytorch版本：github</a><br><a href="https://github.com/yun-liu/rcf">caffe版本：github</a></p>
<ul>
<li>作者<br>南开大学程明明</li>
<li>文章<br>title={Richer Convolutional Features for Edge Detection},<br>author={Liu, Yun and Cheng, Ming-Ming and Hu, Xiaowei and Bian, Jia-Wang and Zhang, Le and Bai, Xiang and Tang, Jinhui},<br>journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},<br>year={2019}</li>
<li>特性<br>Using VGG16 network；<br>BSDS500 benchmark 数据集：<br>ODS F-measure of 0.811， speed (8 FPS)<br>fast version of RCF 快速版本：<br>ODS F-measure of 0.806 with 30 FPS</li>
</ul>
<h2 id="3，边缘提取dfi"><a href="#3，边缘提取dfi" class="headerlink" title="3，边缘提取dfi"></a>3，边缘提取dfi</h2><p><a href="https://github.com/backseason/DFI">github link</a></p>
<ul>
<li>作者<br>南开大学程明明</li>
<li>文章<br>title={Dynamic Feature Integration for Simultaneous Detection of Salient Object, Edge and Skeleton},<br>author={Jiang-Jiang Liu and Qibin Hou and Ming-Ming Cheng},<br>journal={IEEE Transactions on Image Processing},<br>year={2020}</li>
</ul>
]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>edge extract</tag>
      </tags>
  </entry>
  <entry>
    <title>hi3559a开发记录</title>
    <url>/2022/05/19/hardware/hi3559a-coding/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>hisi视觉模型开发，主要是检测任务，三个类别，本文包含项目开发的基本流程，包括模型训练，模型转化以及hisi官方提供的关于yolov3的源码及其调用分析。</p>
<span id="more"></span>
<h2 id="1，模型训练"><a href="#1，模型训练" class="headerlink" title="1，模型训练"></a>1，模型训练</h2><h3 id="1-1-运行环境"><a href="#1-1-运行环境" class="headerlink" title="1.1 运行环境"></a>1.1 运行环境</h3><p>在caffe的基础docker中操作的，方便后续其他视觉框架转到caffe的模型，其实训练框架用的是darknet。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull bvlc/caffe</span><br></pre></td></tr></table></figure></p>
<h3 id="1-2-使用darknet进行训练"><a href="#1-2-使用darknet进行训练" class="headerlink" title="1.2 使用darknet进行训练"></a>1.2 使用darknet进行训练</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd darknet</span><br></pre></td></tr></table></figure>
<ul>
<li>编译darknet<br>  <a href="https://www.cnblogs.com/gocodinginmyway/p/13747221.html">darket环境搭建参考</a></li>
<li>根据kmeans算法生成对应数据集和对应尺寸的anchors，并修改cfg<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./darknet detector calc_anchors data/visdrone.data -num_of_clusters 6 -width 416 -height 416</span><br></pre></td></tr></table></figure></li>
<li>训练yolo-fastest<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./darknet detector train ./data/visdrone.data ./cfg/yolo-fastest.cfg yolo-fastest.conv.109</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="1-3-其他训练命令"><a href="#1-3-其他训练命令" class="headerlink" title="1.3 其他训练命令"></a>1.3 其他训练命令</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./darknet detector train ./data/visdrone.data ./cfg/yolo-fastest-xl.cfg yolo-fastest-xl.conv.109</span><br><span class="line">./darknet detector train ./data/visdrone.data ./cfg/yolov4-visdrone.cfg yolov4.conv.137</span><br><span class="line">./darknet detector train ./data/visdrone.data ./cfg/MobileNetV2-yolov3-lite.cfg MobileNetV2--Lite.conv.57</span><br></pre></td></tr></table></figure>
<h2 id="2，模型转化"><a href="#2，模型转化" class="headerlink" title="2，模型转化"></a>2，模型转化</h2><h3 id="2-1-转化为caffe模型"><a href="#2-1-转化为caffe模型" class="headerlink" title="2.1 转化为caffe模型"></a>2.1 转化为caffe模型</h3><p><a href="https://github.com/ChenYingpeng/darknet2caffe">yolov4转caffe_link</a><br><a href="https://github.com/dog-qiuqiu/MobileNet-Yolo/tree/master/darknet2caffe">yolofastest转caffe_link</a><br><a href="./darknet2caffe-hi3559a.markdown">自训练的yolov4模型darknet转caffe</a></p>
<h3 id="2-2-仿真模型转化"><a href="#2-2-仿真模型转化" class="headerlink" title="2.2 仿真模型转化"></a>2.2 仿真模型转化</h3><ul>
<li><p>仿真模型转化参数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[prototxt_file] ./mark_prototxt/yolov4-visdrone-2class_mark_nnie_20210219113115.prototxt</span><br><span class="line">[caffemodel_file] D:\BaiduNetdiskDownload\yolov4-visdrone-2class-shortcut16\yolov4-visdrone-2class.caffemodel</span><br><span class="line">[batch_num] 1</span><br><span class="line">[net_type] 0</span><br><span class="line">[sparse_rate] 0</span><br><span class="line">[compile_mode] 0</span><br><span class="line">[is_simulation] 1</span><br><span class="line">[log_level] 2</span><br><span class="line">[instruction_name] ./../data/detection/yolov4/inst/yolov4_func</span><br><span class="line">[RGB_order] BGR</span><br><span class="line">[data_scale] 0.0039062</span><br><span class="line">[internal_stride] 16</span><br><span class="line">[image_list] ./../data/detection/yolov4/image_ref_list.txt</span><br><span class="line">[image_type] 1</span><br><span class="line">[mean_file] null</span><br><span class="line">[norm_type] 3</span><br></pre></td></tr></table></figure>
</li>
<li><p>仿真模型转化输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Start [RuyiStudio Wk NNIE Mapper] [D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_func.cfg] sample_simulator (2021-02-19 11:32:18)</span><br><span class="line">Mapper Version 1.1.3.0_B010 (NNIE_1.1) 1905091707159355</span><br><span class="line"></span><br><span class="line">begin net parsing....</span><br><span class="line"></span><br><span class="line">end net parsing</span><br><span class="line"></span><br><span class="line">begin prev optimizing....</span><br><span class="line"></span><br><span class="line">end prev optimizing....</span><br><span class="line"></span><br><span class="line">begin net quantalizing(GPU)....</span><br><span class="line"></span><br><span class="line">end quantalizing</span><br><span class="line"></span><br><span class="line">begin POST optimizing....</span><br><span class="line"></span><br><span class="line">end POST optimizing</span><br><span class="line"></span><br><span class="line">begin NNIE[0] mem allocation....</span><br><span class="line"></span><br><span class="line">.end NNIE[0] memory allocating</span><br><span class="line"></span><br><span class="line">begin NNIE[0] instruction generating....</span><br><span class="line"></span><br><span class="line">..............end NNIE[0] instruction generating</span><br><span class="line"></span><br><span class="line">begin lbs binary code generating....</span><br><span class="line"></span><br><span class="line">end lbs binary code generating</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">===============D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_func.cfg Successfully!===============</span><br><span class="line"></span><br><span class="line">End [RuyiStudio Wk NNIE Mapper] [D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_func.cfg] sample_simulator (2021-02-19 11:34:58)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-3-板子模型转化"><a href="#2-3-板子模型转化" class="headerlink" title="2.3 板子模型转化"></a>2.3 板子模型转化</h3><ul>
<li><p>板子模型转化参数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[prototxt_file] ./mark_prototxt/yolov4-visdrone-2class_mark_nnie_20210219114447.prototxt</span><br><span class="line">[caffemodel_file] D:\BaiduNetdiskDownload\yolov4-visdrone-2class-shortcut16\yolov4-visdrone-2class.caffemodel</span><br><span class="line">[batch_num] 1</span><br><span class="line">[net_type] 0</span><br><span class="line">[sparse_rate] 0</span><br><span class="line">[compile_mode] 0</span><br><span class="line">[is_simulation] 0</span><br><span class="line">[log_level] 2</span><br><span class="line">[instruction_name] ./../data/detection/yolov4/inst/yolov4__inst</span><br><span class="line">[RGB_order] BGR</span><br><span class="line">[data_scale] 0.0039062</span><br><span class="line">[internal_stride] 16</span><br><span class="line">[image_list] ./../data/detection/yolov4/image_ref_list.txt</span><br><span class="line">[image_type] 1</span><br><span class="line">[mean_file] null</span><br><span class="line">[norm_type] 3</span><br></pre></td></tr></table></figure>
</li>
<li><p>板子转化输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Start [RuyiStudio Wk NNIE Mapper] [D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_inst.cfg] sample_simulator (2021-02-19 11:45:53)</span><br><span class="line">Mapper Version 1.1.3.0_B010 (NNIE_1.1) 1905091707159355</span><br><span class="line"></span><br><span class="line">begin net parsing....</span><br><span class="line"></span><br><span class="line">end net parsing</span><br><span class="line"></span><br><span class="line">begin prev optimizing....</span><br><span class="line"></span><br><span class="line">end prev optimizing....</span><br><span class="line"></span><br><span class="line">begin net quantalizing(GPU)....</span><br><span class="line"></span><br><span class="line">end quantalizing</span><br><span class="line"></span><br><span class="line">begin optimizing....</span><br><span class="line"></span><br><span class="line">end optimizing</span><br><span class="line"></span><br><span class="line">begin NNIE[0] mem allocation....</span><br><span class="line"></span><br><span class="line">end NNIE[0] memory allocating</span><br><span class="line"></span><br><span class="line">begin NNIE[0] instruction generating....</span><br><span class="line"></span><br><span class="line">...............end NNIE[0] instruction generating</span><br><span class="line"></span><br><span class="line">begin parameter compressing....</span><br><span class="line"></span><br><span class="line">end parameter compressing</span><br><span class="line"></span><br><span class="line">begin compress index generating....</span><br><span class="line"></span><br><span class="line">end compress index generating</span><br><span class="line"></span><br><span class="line">begin binary code generating....</span><br><span class="line"></span><br><span class="line">.end binary code generating</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">===============D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_inst.cfg Successfully!===============</span><br><span class="line"></span><br><span class="line">End [RuyiStudio Wk NNIE Mapper] [D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_inst.cfg] sample_simulator (2021-02-19 11:48:43)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3，hisi源码调用分析【yolov3为例】"><a href="#3，hisi源码调用分析【yolov3为例】" class="headerlink" title="3，hisi源码调用分析【yolov3为例】"></a>3，hisi源码调用分析【yolov3为例】</h2><p>函数入口源文件，<code>sample_nnie_main.c</code>   </p>
<p>选择<code>Yolov3</code>，会进入<code>sammple_nnie.c</code> 中的<code>SAMPLE_SVP_NNIE_Yolov3</code>，其中可以配置加载模型与图片文件的顺序；<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HI_CHAR *pcSrcFile = &quot;./data/nnie_image/rgb_planar/dog_bike_car_416x416.bgr&quot;;</span><br><span class="line">HI_CHAR *pcModelName = &quot;./data/nnie_model/detection/inst_yolov3_cycle.wk&quot;;</span><br></pre></td></tr></table></figure></p>
<p>而后分别调用：</p>
<ul>
<li>SAMPLE_COMM_SVP_CheckSysInit();</li>
<li>SAMPLE_COMM_SVP_NNIE_LoadModel();</li>
<li>SAMPLE_SVP_NNIE_Yolov3_ParamInit();</li>
<li>SAMPLE_SVP_NNIE_FillSrcData();</li>
<li>SAMPLE_SVP_NNIE_Forward();</li>
<li>SAMPLE_SVP_NNIE_Yolov3_GetResult;</li>
<li>SAMPLE_SVP_NNIE_Detection_PrintResult()<br>依次是系统初始化、NNie加载模型、参数初始化、读取数据、前向推导、获取结果、结果打印。<br>至此，yolov3例程的函数调用基本完成</li>
</ul>
<h2 id="4，关于yolov4的说明"><a href="#4，关于yolov4的说明" class="headerlink" title="4，关于yolov4的说明"></a>4，关于yolov4的说明</h2><p>yolov4的原版模型中，在转仿真或者板子模型时，不支持mish激活函数，有两种解决思路：</p>
<ul>
<li><p>用leakyReLU替换mish【本次交付模型方案】<br>相对于mish作为激活函数，leakyReLU在准确性上略有降低，但是在效率上有优势，因此更加适用于yolov4在嵌入式中的运行；   </p>
</li>
<li><p>数学公式替换<br>caffe中定义好了6种常用的激活函数：ReLu、Sigmod、Tanh、Absval、Power、BNll<br>mish表达式：Mish = x*tanh(ln(1+e^x))<br>BNLL表达式：f(x) = log(1+exp(x))<br>Tanh表达式<br>caffe中eltwise层有PROD类型操作可计算基于元素的乘法   </p>
</li>
</ul>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>hi3559a cv model</tag>
      </tags>
  </entry>
  <entry>
    <title>rcnn，yolo和ssd系列模型</title>
    <url>/2022/05/19/detection/fasterrcnn-yolo-ssd/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>简述目标检测算法早期三个比较出名的系列~</p>
<p><strong>太灌水了，质量要提高啊~</strong><br><span id="more"></span></p>
<h2 id="1，rcnn系列"><a href="#1，rcnn系列" class="headerlink" title="1，rcnn系列"></a>1，rcnn系列</h2><p>针对之前RCNN系列selective search的方法导致算法没有实时性，所以faster rcnn提出RPN网络来取代之前的方法，可以理解为fasterrcnn=fast rcnn+rpn网络，且rpn网络和fast rcnn的分类，回归网络共用特征提取层，这样使得引入RPN网络不会增加太多计算量。整体流程为先使用RPN网络找出可能存在object的区域，再将这些区域送入fast rcnn中进一步定位和分类。所以faster rcnn是典型的Two stage算法。因为faster rcnn中包含了两次定位，所以其精度一般高于YOLO和SSD算法，所以速度一般慢于YOLO和SSD。</p>
<h2 id="2，yolo系列"><a href="#2，yolo系列" class="headerlink" title="2，yolo系列"></a>2，yolo系列</h2><p>YOLO算法的特点是将检测问题转换成回归问题，即YOLO直接通过回归一次既产生坐标，又产生每种类别的概率。YOLO中将每张图分成7*7的网格，每个网格默认可能属于2个object，即在一张图片上提取98个region proposal，相比于faster rcnn使用Anchor机制提取20k个anchor再从中提取最终的300个region proposal，所以faster rcnn的精度比YOLO要高，但是由于需要处理更多region proposal，所以faster rcnn的速度要比YOLO慢。</p>
<h2 id="3，ssd系列"><a href="#3，ssd系列" class="headerlink" title="3，ssd系列"></a>3，ssd系列</h2><p>SSD相比于faster rcnn使用了多层网络特征，而不仅仅使用最后一层feature map。SSD还借鉴了YOLO算法中将检测任务转换为回归任务的思想，且SSD也借鉴了faster rcnn中的anchor机制，只是SSD的anchor不是每个位置的精调，而是类似于YOLO那样在feature map上分割出网格，在网格上产生anchor。但是SSD和YOLO不需要selective search步骤，所以SSD和YOLO同属于One-Stage算法。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>rcnn</tag>
        <tag>yolo</tag>
        <tag>ssd</tag>
      </tags>
  </entry>
  <entry>
    <title>deepstream sample test4</title>
    <url>/2022/05/19/tools/deepstream_sample_test4/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>准备系统学习下deepstream的相关内容，先跑下示例，是基于docker的。</p>
<p><strong>继续加油，还有很多内容哦~</strong><br><span id="more"></span></p>
<h2 id="1，deepstream的test4实例"><a href="#1，deepstream的test4实例" class="headerlink" title="1，deepstream的test4实例"></a>1，deepstream的test4实例</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-docker run -it -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY -v/home/hello/code:/home/code --name=kafkads nvcr.io/nvidia/deepstream:5.1-21.02-samples</span><br><span class="line"></span><br><span class="line">apt install python3-gi python3-gst-1.0 -y</span><br><span class="line">apt install python3-distutils</span><br><span class="line">cd /opt/nvidia/deepstream/deepstream-5.1/lib</span><br><span class="line">python3 setup.py install</span><br><span class="line"></span><br><span class="line">apt-get install libglib2.0 libglib2.0-dev</span><br><span class="line">apt-get install libjansson4  libjansson-dev</span><br><span class="line">apt-get install librdkafka1=0.11.3-1build1</span><br><span class="line"></span><br><span class="line">apt-get install libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev</span><br><span class="line">apt-get install libgstrtspserver-1.0-dev libx11-dev</span><br><span class="line"></span><br><span class="line">apt-get install nvidia-cuda-toolkit</span><br><span class="line"></span><br><span class="line">deepstream-test4-app -i /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.h264 -p /opt/nvidia/deepstream/deepstream-5.1/lib/libnvds_kafka_proto.so --conn-str=&quot;192.168.8.116;9092;test&quot; -c cfg_kafka.txt -s 0 --no-display</span><br><span class="line"></span><br><span class="line">python3 deepstream_test_4.py -i /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.h264 -p /opt/nvidia/deepstream/deepstream-5.1/lib/libnvds_kafka_proto.so --conn-str=&quot;192.168.8.116;9092;test&quot; -c cfg_kafka.txt -s &quot;0&quot; --no-display</span><br></pre></td></tr></table></figure>
<h2 id="2，报错"><a href="#2，报错" class="headerlink" title="2，报错"></a>2，报错</h2><ul>
<li><p>python3.6m的动态库    </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ImportError: libpython3.6m.so.1.0: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/weixin_43952432/article/details/100077912">解决方案参考</a></p>
</li>
<li><p>找不到cuda的runtime头文件   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fatal error: cuda_runtime.h: No such file or directory</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/seaun163/article/details/98962185">解决方案参考</a></p>
</li>
</ul>
<h2 id="3，参考文献"><a href="#3，参考文献" class="headerlink" title="3，参考文献"></a>3，参考文献</h2><p><a href="https://forums.developer.nvidia.com/t/cant-install-deepstream-5-1-on-rtx2070-dgpu/174842/7">nvidia论坛</a></p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>deepstream sample</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorrt parser的构建dockerfile</title>
    <url>/2022/05/19/cv_engineering/trt_parser.Dokcerfile/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>trtparser的自构建镜像，是基于官方的trtparser，自己添加了一些常用软件，特点如下：</p>
<ul>
<li>ubuntu换源；</li>
<li>构建pth-&gt;onnx-&gt;trt转模型需要的环境；</li>
<li>python3的常用包；</li>
</ul>
<p><strong>有dockerfile还真的方便~</strong></p>
<span id="more"></span>
<h2 id="dockerfile代码"><a href="#dockerfile代码" class="headerlink" title="dockerfile代码"></a>dockerfile代码</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM nvcr.io/nvidia/tensorrt:20.02-py3</span><br><span class="line"></span><br><span class="line">LABEL maintainer=&quot;XINWEN&quot;</span><br><span class="line">RUN mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">RUN echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; apt-get update \</span><br><span class="line">    &amp;&amp; apt-get -y install clang-format nfs-common \</span><br><span class="line">    &amp;&amp; apt-get -y install openssh-client </span><br><span class="line"></span><br><span class="line">RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py &amp;&amp; python3 get-pip.py --force-reinstall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ADD ./soft/* /softs/</span><br><span class="line">WORKDIR /softs</span><br><span class="line">RUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip \</span><br><span class="line">    &amp;&amp; pip3 install torch-1.3.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install Pillow-6.0.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install tensorboard-1.15.0-py3-none-any.whl \</span><br><span class="line">    &amp;&amp; pip3 install onnx-1.6.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install opencv_python-4.2.0.32-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install torchvision-0.4.1-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install onnxruntime-1.1.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; cd onnx-simplifier-0.2.2 &amp;&amp; python3 setup.py install \</span><br><span class="line"># RUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip \</span><br><span class="line">#     &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple torch==1.3.0 torchvision==0.4.1 \</span><br><span class="line">#     &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple onnx==1.6.0 onnxruntime==1.1.0 onnx-simplifier==0.2.2 \</span><br><span class="line">#     &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv_python==4.2.0 Pillow==6.0.0 \</span><br><span class="line">#     &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow_gpu==1.15.0 \</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pytest pyhamcrest pytest-cov black==19.3b0 isort==4.3.21 flake8 &#x27;pillow&lt;7.0.0&#x27; cython dvc</span><br><span class="line"></span><br><span class="line">WORKDIR /workspace</span><br><span class="line">RUN rm -rf /softs</span><br><span class="line"></span><br><span class="line"># RUN apt-get -y install isort clang-format nfs-common</span><br><span class="line">ENTRYPOINT /bin/bash</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>tensorrt parser</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorrt client的构建dockerfile</title>
    <url>/2022/05/19/cv_engineering/trt_client.Dockerfile/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>trtserver的docker镜像，官方的基础镜像，自己加了一些常用包，然后写了dockerfile来构建，基本内容如下：</p>
<ul>
<li>基础镜像是：tensorrtserver:20.02-py3-clientsdk</li>
<li>包含ubuntu换源；</li>
<li>pip3安装常用包，使用的是清华源；</li>
<li>提前下载了部分whl包到同dockerfile目录的</li>
</ul>
<p><strong>dockerfile加持后真的方便呀~</strong><br><span id="more"></span></p>
<h2 id="dockerfile源码"><a href="#dockerfile源码" class="headerlink" title="dockerfile源码"></a>dockerfile源码</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM nvcr.io/nvidia/tensorrtserver:20.02-py3-clientsdk</span><br><span class="line"></span><br><span class="line">LABEL maintainer=&quot;XINWEN&quot;</span><br><span class="line">RUN mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">RUN echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; apt-get update \</span><br><span class="line">    &amp;&amp; apt-get -y install clang-format nfs-common</span><br><span class="line"></span><br><span class="line">RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py &amp;&amp; python3 get-pip.py --force-reinstall</span><br><span class="line"></span><br><span class="line">ADD ./soft/* /softs/</span><br><span class="line">WORKDIR /softs</span><br><span class="line">RUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip \</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple bcolz \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple onnx==1.5.0 \</span><br><span class="line">    &amp;&amp; pip3 install torch-1.3.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install Pillow-6.0.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install opencv_python-4.2.0.32-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install torchvision-0.4.1-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    # &amp;&amp; pip3 install onnxruntime-1.1.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    # &amp;&amp; cd onnx-simplifier-0.2.2 &amp;&amp; python3 setup.py install \</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pytest pyhamcrest pytest-cov black==19.3b0 isort==4.3.21 flake8 &#x27;pillow&lt;7.0.0&#x27; cython dvc \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple scipy \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple sklearn \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib \</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pymongo</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pymysql</span><br><span class="line"></span><br><span class="line">WORKDIR /workspace</span><br><span class="line">RUN rm -rf /softs</span><br><span class="line"></span><br><span class="line">ENTRYPOINT /bin/bash</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>tensorrt client</tag>
      </tags>
  </entry>
  <entry>
    <title>makefile example--socket &amp; mysql</title>
    <url>/2022/05/19/cpp/makefile%20exam1-SOCKET%20mysql/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>用makefile的方式来编译socket服务端代码，包含常用makefile语法的使用~</p>
<p><strong>好久没这么用cpp了，耍起来啊~</strong><br><span id="more"></span></p>
<h2 id="1，socket服务端编译-用到mysql数据库"><a href="#1，socket服务端编译-用到mysql数据库" class="headerlink" title="1，socket服务端编译(用到mysql数据库)"></a>1，socket服务端编译(用到mysql数据库)</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#用变量定义文件目录</span><br><span class="line">INCL=-I/usr/local/mysql/include -I$(HOME)/incl</span><br><span class="line">#注意动态库或者静态库的写法</span><br><span class="line">LIB=-L/usr/local/mysql/lib -lmysqlclient -lmysqld -lmysqlservices -L$(HOME)/lib -lbanktest</span><br><span class="line">BINDIR=$(HOME)/bin</span><br><span class="line"></span><br><span class="line">.SUFFIXES: .cpp .c</span><br><span class="line"></span><br><span class="line">#后缀为cpp的文件怎么编译成.o</span><br><span class="line">.cpp.o:</span><br><span class="line">	g++ $&#123;INCL&#125; -c $&lt;</span><br><span class="line"></span><br><span class="line">#后缀为c的文件怎么编译成.o</span><br><span class="line">.c.o:</span><br><span class="line">	gcc $(INCL) -c $&lt;</span><br><span class="line"></span><br><span class="line">all: clean server</span><br><span class="line"></span><br><span class="line">server:server.o</span><br><span class="line">	@echo &quot;============开始编译============&quot;</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	@mv $@ $(BINDIR)</span><br><span class="line">	@echo &quot;============编译结束============&quot;</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">	@rm -f *.o</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>makefile mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>makefile example--des &amp; md5 &amp; base64</title>
    <url>/2022/05/19/cpp/makefile%20exam2-des%20md5%20base64/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>makefile编译代码，有其他需要链接的库，包含mysql，base64等等三方服务，详情看代码吧~</p>
<p><strong>写这个大概在五年之前了哦~</strong><br><span id="more"></span></p>
<h2 id="编译des-md5-base64密码服务"><a href="#编译des-md5-base64密码服务" class="headerlink" title="编译des md5 base64密码服务"></a>编译des md5 base64密码服务</h2><ul>
<li>all 规则示例   <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">INCL=-I/usr/local/mysql/include -I$(HOME)/incl</span><br><span class="line">LIB=-L/usr/local/mysql/lib -lmysqlclient -lmysqld -lmysqlservices</span><br><span class="line">BINDIR=$(HOME)/bin</span><br><span class="line">LIBDIR=$(HOME)/lib</span><br><span class="line"></span><br><span class="line">.SUFFIXES: .cpp .c</span><br><span class="line"></span><br><span class="line">.cpp.o:</span><br><span class="line">	g++ $&#123;INCL&#125; -c $&lt;</span><br><span class="line">.c.o:</span><br><span class="line">	gcc $(INCL) -c $&lt;</span><br><span class="line"></span><br><span class="line">#[NOTE]</span><br><span class="line">all: clean des md5 base64</span><br><span class="line"></span><br><span class="line">des:des.o main_des.o</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">md5:md5.o main_md5.o</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">base64test:base64.o main_base64.o</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">rsa:rsa.o main_rsa.o</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">libjiami.a:des.o md5.o base64.o</span><br><span class="line">	ar -r $@ $?</span><br><span class="line">	mv $@ $(LIBDIR)</span><br><span class="line"></span><br><span class="line">libdestest:main_des.o</span><br><span class="line">	gcc -o $@ $? $(LIB) -L$(HOME)/lib -ljiami</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">libtest.so:des.c md5.c base64.c</span><br><span class="line">	gcc -o $@ -fPIC -shared $?</span><br><span class="line">	mv $@ $(LIBDIR)</span><br><span class="line"></span><br><span class="line">libmd5test:main_md5.o</span><br><span class="line">	gcc -o $@ $? $(LIB) -L$(HOME)/lib -ltest</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">libbanktest.a:banktest.o banksql.o</span><br><span class="line">	ar -r $@ $?</span><br><span class="line">	mv $@ $(LIBDIR)</span><br><span class="line"></span><br><span class="line">banktest:banktest.o banksql.o</span><br><span class="line">	gcc -o $@ $? $(LIB) -L$(HOME)/lib -ltest</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">	rm -f *.o</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>makefile md5 base64</tag>
      </tags>
  </entry>
  <entry>
    <title>mongodb的构建docker</title>
    <url>/2022/05/19/tools/mongodb-docker/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>早期的博客，搬运过来的，貌似仍然能用，可以很方便的构建mongodb数据库~</p>
<p><strong>温故而知新，加油~</strong><br><span id="more"></span></p>
<h2 id="1，Docker搭建Mongodb"><a href="#1，Docker搭建Mongodb" class="headerlink" title="1，Docker搭建Mongodb"></a>1，Docker搭建Mongodb</h2><h3 id="1-1-基本搭建步骤"><a href="#1-1-基本搭建步骤" class="headerlink" title="1.1 基本搭建步骤"></a>1.1 基本搭建步骤</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 获取docker镜像</span><br><span class="line">docker pull mongo</span><br><span class="line"># 创建mongodb容器</span><br><span class="line">docker run --name  my-mongo  -p 27017:27017  -d mongo --auth</span><br><span class="line"># 如果加需要验证就加--auth，不需要验证，就去掉。默认mongodb是不使用用户认证</span><br><span class="line"># 进入容器设置用户</span><br><span class="line">docker exec -it 容器id /bin/bash</span><br><span class="line"># 用户设置命令</span><br><span class="line">mongo</span><br><span class="line">use admin</span><br><span class="line">db.createUser(&#123;user:&quot;root&quot;,pwd:&quot;root&quot;,roles:[&#123;role:&#x27;root&#x27;,db:&#x27;admin&#x27;&#125;]&#125;)   //创建用户,此用户创建成功,则后续操作都需要用户认证</span><br><span class="line">exit</span><br><span class="line"># 或者直接进入admin用户</span><br><span class="line">docker exec -it ly-mongo mongo admin</span><br><span class="line">db.createUser(&#123;user:&quot;root&quot;,pwd:&quot;root&quot;,roles:[&#123;role:&#x27;root&#x27;,db:&#x27;admin&#x27;&#125;]&#125;)   //创建用户,此用户创建成功,则后续操作都需要用户认证</span><br><span class="line">exit</span><br></pre></td></tr></table></figure>
<h3 id="1-2-测试"><a href="#1-2-测试" class="headerlink" title="1.2 测试"></a>1.2 测试</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mongo  宿主机ip/admin  -utest -p</span><br></pre></td></tr></table></figure>
<p>查看是否连接成功</p>
<h2 id="2，维护mongoDB"><a href="#2，维护mongoDB" class="headerlink" title="2，维护mongoDB"></a>2，维护mongoDB</h2><h3 id="2-1-指定MongoDB配置文件"><a href="#2-1-指定MongoDB配置文件" class="headerlink" title="2.1 指定MongoDB配置文件"></a>2.1 指定MongoDB配置文件</h3><p>当我们需要修改配置文件时，我们只需要在宿主机上创建一个mongodb.conf文件，并将该文件所在的文件夹映射到容器的/data/configdb文件夹中，同时，在容器的启动命令中添加—configsvr参数即可。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name some-mongo -d mongo --configsvr</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-数据持久化"><a href="#2-2-数据持久化" class="headerlink" title="2.2 数据持久化"></a>2.2 数据持久化</h3><p>在使用MongoDB的容器时，数据持久化有很多种方式，下面我们将描述一种推荐的方式:<br>在宿主机上创建一个数据存储目录，并将其映射到容器中的目录中。<br>这将数据库文件放在主机系统中的已知位置，并便于主机系统上的工具和应用程序访问文件。<br>缺点是用户需要确保目录存在，例如，主机系统上的目录权限和其他安全机制配置正确。<br>使用方法如下：<br>在宿主机中创建一个目录，例如/my/own/datadir。<br>如下命令启动容器：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name some-mongo -v /my/own/datadir:/data/db -d mongo:tag</span><br></pre></td></tr></table></figure></p>
<h3 id="2-3-数据库授权"><a href="#2-3-数据库授权" class="headerlink" title="2.3 数据库授权"></a>2.3 数据库授权</h3><p>默认情况下，Mongo数据库没有添加认证约束，也就是说任何人只要知道数据库服务的地址和端口，就可以正常访问数据库并对数据库进行增删改查。<br>为了增强数据库的安全性，我们需要对数据库添加授权认证。<br>添加方式如下：<br>在启动数据库容器命令中添加—auth参数。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name some-mongo -d mongo --auth</span><br></pre></td></tr></table></figure></p>
<p>使用exec命令进入命令行，并添加用户名和密码。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it some-mongo mongo admin</span><br><span class="line">db.createUser(&#123; user: &#x27;jsmith&#x27;, pwd: &#x27;some-initial-password&#x27;, roles: [ &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ] &#125;);</span><br></pre></td></tr></table></figure></p>
<h3 id="2-4-数据库备份"><a href="#2-4-数据库备份" class="headerlink" title="2.4 数据库备份"></a>2.4 数据库备份</h3><p>通常情况下，我们需要对数据库进行备份。<br>首先，我们需要将本地磁盘的某个文件夹映射到容器中的备份文件夹中：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name mongo -v /mnt/mongo/backup:/data/backup -d mongo</span><br></pre></td></tr></table></figure></p>
<p>数据库备份的方式如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec mongo sh -c &#x27;exec var=`date +%Y%m%d%H%M` &amp;amp;&amp;amp; mongodump -h localhost --port 27017 -u test -p test1 -d dbname -o /data/backup/$var_test1.dat&#x27;</span><br></pre></td></tr></table></figure></p>
<h2 id="3，推荐用法"><a href="#3，推荐用法" class="headerlink" title="3，推荐用法"></a>3，推荐用法</h2><p>执行如下命令拉取Mongo镜像：<br>docker pull mongo<br>创建Mongo专用的文件夹：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /mnt</span><br><span class="line">mkdir mongodb</span><br><span class="line">cd ./mongodb</span><br><span class="line">mkdir data</span><br><span class="line">mkdir backup</span><br></pre></td></tr></table></figure><br>执行如下命令启动MongoDB：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name mongo -p 27017:27017 -v /mnt/mongodb/data:/data/db -v /mnt/mongodb/backup:/data/backup -d mongo --auth</span><br></pre></td></tr></table></figure><br>接下来，我们需要进入容器的命令行去创建用户名和密码：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it mongo mongo admin</span><br><span class="line">db.createUser(&#123; user: &#x27;jsmith&#x27;, pwd: &#x27;password&#x27;, roles: [ &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ] &#125;);</span><br><span class="line">use test;</span><br><span class="line">db.createUser(&#123;user:&quot;testuser&quot;,pwd:&quot;testpass&quot;,roles:[&quot;readWrite&quot;]&#125;);</span><br><span class="line">db.auth(&quot;testuser&quot;,&quot;testpass&quot;)</span><br></pre></td></tr></table></figure><br>在运行一段时间以后，我们可以执行如下命令进行数据库备份：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec mongo sh -c &#x27;exec var=`date +%Y%m%d%H%M` &amp;amp;&amp;amp; mongodump -h localhost --port 27017 -u jsmith -p password -d dbname -o /data/backup/$var_test1.dat&#x27;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>mongodb docker</tag>
      </tags>
  </entry>
  <entry>
    <title>几个电视台的直播流</title>
    <url>/2022/05/19/cv_engineering/several-livestream/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>常用电视台的直播流，rtsp流，无意中检索到的，就保存了~</p>
<p><strong>网速好，可以直接看任意台哦</strong><br><span id="more"></span></p>
<h2 id="常用电视台的直播流"><a href="#常用电视台的直播流" class="headerlink" title="常用电视台的直播流"></a>常用电视台的直播流</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># cap = cv2.VideoCapture(&quot;rtmp://58.200.131.2:1935/livetv/hunantv&quot;) #湖南台</span><br><span class="line"># cap = cv2.VideoCapture(&quot;rtmp://58.200.131.2:1935/livetv/dftv&quot;) #东方台</span><br><span class="line"># cap = cv2.VideoCapture(&quot;rtmp://58.200.131.2:1935/livetv/gdtv&quot;) #广东台</span><br><span class="line"># cap = cv2.VideoCapture(&quot;rtmp://58.200.131.2:1935/livetv/gxtv&quot;) #广西台</span><br><span class="line"></span><br><span class="line">CCTV1高清：http://ivi.bupt.edu.cn/hls/cctv1hd.m3u8</span><br><span class="line">CCTV3高清：http://ivi.bupt.edu.cn/hls/cctv3hd.m3u8</span><br><span class="line">CCTV5高清：http://ivi.bupt.edu.cn/hls/cctv5hd.m3u8</span><br><span class="line">CCTV5+高清：http://ivi.bupt.edu.cn/hls/cctv5phd.m3u8</span><br><span class="line">CCTV6高清：http://ivi.bupt.edu.cn/hls/cctv6hd.m3u8</span><br><span class="line"></span><br><span class="line">CCTV-1综合:rtmp://58.200.131.2:1935/livetv/cctv1</span><br><span class="line">CCTV-2财经:rtmp://58.200.131.2:1935/livetv/cctv2</span><br><span class="line">CCTV-3综艺:rtmp://58.200.131.2:1935/livetv/cctv3</span><br><span class="line">CCTV-4中文国际:rtmp://58.200.131.2:1935/livetv/cctv4</span><br><span class="line">CCTV-5体育:rtmp://58.200.131.2:1935/livetv/cctv5</span><br><span class="line">CCTV-6电影:rtmp://58.200.131.2:1935/livetv/cctv6</span><br><span class="line">CCTV-7军事农业:rtmp://58.200.131.2:1935/livetv/cctv7</span><br><span class="line">CCTV-8电视剧:rtmp://58.200.131.2:1935/livetv/cctv8</span><br><span class="line">CCTV-9记录:rtmp://58.200.131.2:1935/livetv/cctv9</span><br><span class="line">CCTV-10科教:rtmp://58.200.131.2:1935/livetv/cctv10</span><br><span class="line">CCTV-11戏曲:rtmp://58.200.131.2:1935/livetv/cctv11</span><br><span class="line">CCTV-12社会与法:rtmp://58.200.131.2:1935/livetv/cctv12</span><br><span class="line">CCTV-13新闻:rtmp://58.200.131.2:1935/livetv/cctv13</span><br><span class="line">CCTV-14少儿:rtmp://58.200.131.2:1935/livetv/cctv14</span><br><span class="line">CCTV-15音乐:rtmp://58.200.131.2:1935/livetv/cctv15</span><br><span class="line">安徽卫视:rtmp://58.200.131.2:1935/livetv/ahtv</span><br><span class="line">兵团卫视:rtmp://58.200.131.2:1935/livetv/bttv</span><br><span class="line">重庆卫视:rtmp://58.200.131.2:1935/livetv/cqtv</span><br><span class="line">东方卫视:rtmp://58.200.131.2:1935/livetv/dftv</span><br><span class="line">东南卫视:rtmp://58.200.131.2:1935/livetv/dntv</span><br><span class="line">广东卫视:rtmp://58.200.131.2:1935/livetv/gdtv</span><br><span class="line">广西卫视:rtmp://58.200.131.2:1935/livetv/gxtv</span><br><span class="line">甘肃卫视:rtmp://58.200.131.2:1935/livetv/gstv</span><br><span class="line">贵州卫视:rtmp://58.200.131.2:1935/livetv/gztv</span><br><span class="line">湖北卫视:rtmp://58.200.131.2:1935/livetv/hbtv</span><br><span class="line">湖南卫视:rtmp://58.200.131.2:1935/livetv/hunantv</span><br><span class="line">河北卫视:rtmp://58.200.131.2:1935/livetv/hebtv</span><br><span class="line">河南卫视:rtmp://58.200.131.2:1935/livetv/hntv</span><br><span class="line">黑龙江卫视:rtmp://58.200.131.2:1935/livetv/hljtv</span><br><span class="line">江苏卫视:rtmp://58.200.131.2:1935/livetv/jstv</span><br><span class="line">江西卫视:rtmp://58.200.131.2:1935/livetv/jxtv</span><br><span class="line">吉林卫视:rtmp://58.200.131.2:1935/livetv/jltv</span><br><span class="line">辽宁卫视:rtmp://58.200.131.2:1935/livetv/lntv</span><br><span class="line">内蒙古卫视:rtmp://58.200.131.2:1935/livetv/nmtv</span><br><span class="line">宁夏卫视:rtmp://58.200.131.2:1935/livetv/nxtv</span><br><span class="line">青海卫视:rtmp://58.200.131.2:1935/livetv/qhtv</span><br><span class="line">四川卫视:rtmp://58.200.131.2:1935/livetv/sctv</span><br><span class="line">山东卫视:rtmp://58.200.131.2:1935/livetv/sdtv</span><br><span class="line">山西卫视:rtmp://58.200.131.2:1935/livetv/sxrtv</span><br><span class="line">陕西卫视:rtmp://58.200.131.2:1935/livetv/sxtv</span><br><span class="line">山东教育:rtmp://58.200.131.2:1935/livetv/sdetv</span><br><span class="line">中国教育-1:rtmp://58.200.131.2:1935/livetv/cetv1</span><br><span class="line">中国教育-3:rtmp://58.200.131.2:1935/livetv/cetv3</span><br><span class="line">中国教育-4:rtmp://58.200.131.2:1935/livetv/cetv4</span><br><span class="line">CCTV-第一剧场:rtmp://58.200.131.2:1935/livetv/dyjctv</span><br><span class="line">CCTV-国防军事:rtmp://58.200.131.2:1935/livetv/gfjstv</span><br><span class="line">CCTV-怀旧剧场:rtmp://58.200.131.2:1935/livetv/hjjctv</span><br><span class="line">CCTV-风云剧场:rtmp://58.200.131.2:1935/livetv/fyjctv</span><br><span class="line">CCTV-风云足球:rtmp://58.200.131.2:1935/livetv/fyzqtv</span><br><span class="line">CCTV-风云音乐:rtmp://58.200.131.2:1935/livetv/fyyytv</span><br><span class="line">CCTV-世界地理:rtmp://58.200.131.2:1935/livetv/sjdltv</span><br><span class="line">CCTV-1HD:rtmp://58.200.131.2:1935/livetv/cctv1hd</span><br><span class="line">CCTV-2HD:rtmp://58.200.131.2:1935/livetv/cctv2hd</span><br><span class="line">CCTV-3HD:rtmp://58.200.131.2:1935/livetv/cctv3hd</span><br><span class="line">CCTV-4HD:rtmp://58.200.131.2:1935/livetv/cctv4hd</span><br><span class="line">CCTV-5HD:rtmp://58.200.131.2:1935/livetv/cctv5hd</span><br><span class="line">CCTV5+HD:rtmp://58.200.131.2:1935/livetv/cctv5phd</span><br><span class="line">CCTV-6HD:rtmp://58.200.131.2:1935/livetv/cctv6hd</span><br><span class="line">CCTV-7HD:rtmp://58.200.131.2:1935/livetv/cctv7hd</span><br><span class="line">CCTV-8HD:rtmp://58.200.131.2:1935/livetv/cctv8hd</span><br><span class="line">CCTV-9HD:rtmp://58.200.131.2:1935/livetv/cctv9hd</span><br><span class="line">CCTV-10HD:rtmp://58.200.131.2:1935/livetv/cctv10hd</span><br><span class="line">CCTV-12HD:rtmp://58.200.131.2:1935/livetv/cctv12hd</span><br><span class="line">CCTV-14HD:rtmp://58.200.131.2:1935/livetv/cctv14hd</span><br><span class="line">CGTN-新闻:rtmp://58.200.131.2:1935/livetv/cctv16</span><br><span class="line">CETV-1:rtmp://58.200.131.2:1935/livetv/cetv1</span><br><span class="line">CETV-3:rtmp://58.200.131.2:1935/livetv/cetv3</span><br><span class="line">CETV-4:rtmp://58.200.131.2:1935/livetv/cetv4</span><br><span class="line">北京卫视高清:rtmp://58.200.131.2:1935/livetv/btv1hd</span><br><span class="line">北京影视高清:rtmp://58.200.131.2:1935/livetv/btv4hd</span><br><span class="line">北京体育高清:rtmp://58.200.131.2:1935/livetv/btv6hd</span><br><span class="line">北京新闻高清:rtmp://58.200.131.2:1935/livetv/btv9hd</span><br><span class="line">北京纪实高清:rtmp://58.200.131.2:1935/livetv/btv11hd</span><br><span class="line">北京卫视:rtmp://58.200.131.2:1935/livetv/btv1</span><br><span class="line">北京文艺:rtmp://58.200.131.2:1935/livetv/btv2</span><br><span class="line">北京科教:rtmp://58.200.131.2:1935/livetv/btv3</span><br><span class="line">北京影视:rtmp://58.200.131.2:1935/livetv/btv4</span><br><span class="line">北京财经:rtmp://58.200.131.2:1935/livetv/btv5</span><br><span class="line">北京体育:rtmp://58.200.131.2:1935/livetv/btv6</span><br><span class="line">北京生活:rtmp://58.200.131.2:1935/livetv/btv7</span><br><span class="line">北京青年:rtmp://58.200.131.2:1935/livetv/btv8</span><br><span class="line">北京新闻:rtmp://58.200.131.2:1935/livetv/btv9</span><br><span class="line">北京卡酷:rtmp://58.200.131.2:1935/livetv/btv10</span><br><span class="line">北京文艺高清:rtmp://58.200.131.2:1935/livetv/btv2hd</span><br><span class="line">安徽卫视高清:rtmp://58.200.131.2:1935/livetv/ahhd</span><br><span class="line">重庆卫视高清:rtmp://58.200.131.2:1935/livetv/cqhd</span><br><span class="line">东方卫视高清:rtmp://58.200.131.2:1935/livetv/dfhd</span><br><span class="line">天津卫视高清:rtmp://58.200.131.2:1935/livetv/tjhd</span><br><span class="line">东南卫视高清:rtmp://58.200.131.2:1935/livetv/dnhd</span><br><span class="line">江西卫视高清:rtmp://58.200.131.2:1935/livetv/jxhd</span><br><span class="line">河北卫视高清:rtmp://58.200.131.2:1935/livetv/hebhd</span><br><span class="line">湖南卫视高清:rtmp://58.200.131.2:1935/livetv/hunanhd</span><br><span class="line">湖北卫视高清:rtmp://58.200.131.2:1935/livetv/hbhd</span><br><span class="line">辽宁卫视高清:rtmp://58.200.131.2:1935/livetv/lnhd</span><br><span class="line">四川卫视高清:rtmp://58.200.131.2:1935/livetv/schd</span><br><span class="line">江苏卫视高清:rtmp://58.200.131.2:1935/livetv/jshd</span><br><span class="line">浙江卫视高清:rtmp://58.200.131.2:1935/livetv/zjhd</span><br><span class="line">山东卫视高清:rtmp://58.200.131.2:1935/livetv/sdhd</span><br><span class="line">广东卫视高清:rtmp://58.200.131.2:1935/livetv/gdhd</span><br><span class="line">深圳卫视高清:rtmp://58.200.131.2:1935/livetv/szhd</span><br><span class="line">黑龙江卫视高清:rtmp://58.200.131.2:1935/livetv/hljhd</span><br><span class="line">CHC高清电影:rtmp://58.200.131.2:1935/livetv/chchd</span><br><span class="line">上海纪实高清:rtmp://58.200.131.2:1935/livetv/docuchina</span><br><span class="line">金鹰纪实高清:rtmp://58.200.131.2:1935/livetv/gedocu</span><br><span class="line">全纪实高清:rtmp://58.200.131.2:1935/livetv/documentaryhd</span><br><span class="line">凤凰卫视中文台:rtmp://58.200.131.2:1935/livetv/fhzw</span><br><span class="line">凤凰卫视资讯台:rtmp://58.200.131.2:1935/livetv/fhzx</span><br><span class="line">凤凰卫视电影台:rtmp://58.200.131.2:1935/livetv/fhdy</span><br><span class="line">星空卫视:rtmp://58.200.131.2:1935/livetv/startv</span><br><span class="line">Star Sports:rtmp://58.200.131.2:1935/livetv/starsports</span><br><span class="line">Channel[V]:rtmp://58.200.131.2:1935/livetv/channelv</span><br><span class="line">探索频道:rtmp://58.200.131.2:1935/livetv/discovery</span><br><span class="line">国家地理频道:rtmp://58.200.131.2:1935/livetv/natlgeo</span><br><span class="line">CHC家庭影院:rtmp://58.200.131.2:1935/livetv/chctv</span><br><span class="line">CHC动作电影:rtmp://58.200.131.2:1935/livetv/chcatv</span><br><span class="line">美国电视频道:rtmp://media3.scctv.net/live/scctv_800</span><br><span class="line">香港财经:rtmp://202.69.69.180:443/webcast/bshdlive-pc</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>livestream</tag>
      </tags>
  </entry>
  <entry>
    <title>hi3559a and nfs</title>
    <url>/2022/05/19/hardware/hi3559a-nfs/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在进行板子调试的时候，用串口传输文件会比较繁琐，尤其是文件小而多时，所以此处采用了nfs服务，将PC机上的一个文件夹挂载到板子上共享存储服务。</p>
<p><strong>可以使劲、快速的板砖啦…啦啦啦啦…</strong><br><span id="more"></span></p>
<h2 id="1，PC上的安装和配置"><a href="#1，PC上的安装和配置" class="headerlink" title="1，PC上的安装和配置"></a>1，PC上的安装和配置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 安装</span><br><span class="line">sudo apt-get install nfs-kernel-server</span><br><span class="line"># 共享文件夹</span><br><span class="line">sudo mkdir /home/demo/hi3559a_share</span><br><span class="line"># 配置文件，打开```/etc/exports```文件，在最后加上：</span><br><span class="line">/home/demo/hi3559a_share *(rw,sync,no_subtree_check)</span><br></pre></td></tr></table></figure>
<p><strong>NOTE:参数说明找百度哦</strong><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 重启服务，上面的配置完成之后，重启服务：</span><br><span class="line">sudo /etc/init.d/rpcbind restart//重启 rpcbind</span><br><span class="line">sudo /etc/init.d/nfs-kernel-server restart //重启 NFS</span><br></pre></td></tr></table></figure></p>
<h2 id="2，开发板上的安装和配置"><a href="#2，开发板上的安装和配置" class="headerlink" title="2，开发板上的安装和配置"></a>2，开发板上的安装和配置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 安装客户端</span><br><span class="line">sudo apt-get install nfs-common</span><br><span class="line"># 挂载文件夹</span><br><span class="line">mkdirs -p /home/hihope/hi3559av100</span><br><span class="line">chmod 777 /home/hihope/hi3559av100</span><br></pre></td></tr></table></figure>
<h2 id="3，常用命令"><a href="#3，常用命令" class="headerlink" title="3，常用命令"></a>3，常用命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 挂载命令</span><br><span class="line">sudo mount -t nfs -o nolock,nfsvers=3,vers=3 192.168.86.105:/home/demo/hi3559a_share /home/hihope/hi3559av100</span><br><span class="line"># 卸载命令</span><br><span class="line">umount /home/hihope/hi3559av100</span><br><span class="line">df	//查看挂载信息</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>hi3559a nfs</tag>
      </tags>
  </entry>
  <entry>
    <title>hi3559a，windows和vmware&amp;ubuntu的开发环境配置</title>
    <url>/2022/05/19/hardware/hi3559a-windows-vmware-software/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="桥接模式"><a href="#桥接模式" class="headerlink" title="桥接模式"></a>桥接模式</h3><p>原理就找大佬们理解，也就不多说了。<br>我的理解是，PC和板子用网线连接，PC和虚拟机是逻辑连接，需要保证pc, ubuntu（vmware中的）和板子在同一个网段中。</p>
<h3 id="windows系统需要确认"><a href="#windows系统需要确认" class="headerlink" title="windows系统需要确认"></a>windows系统需要确认</h3><ul>
<li>关闭win10防火墙</li>
<li>确认网口和网线正常<br>  怎么确认就各显神通了啊~~~</li>
</ul>
<p><strong>然后就是干货了~</strong></p>
<span id="more"></span>
<h2 id="1，参数配置"><a href="#1，参数配置" class="headerlink" title="1，参数配置"></a>1，参数配置</h2><h3 id="1-1-板子配置"><a href="#1-1-板子配置" class="headerlink" title="1.1 板子配置"></a>1.1 板子配置</h3><ul>
<li>连接板子<br> 【Setup-&gt;General setup】     <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">端口：COM3</span><br><span class="line">语言：UTF-8</span><br><span class="line">界面语言：Simplified Chinese.lng</span><br></pre></td></tr></table></figure>
 【设置-&gt;串口】  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">端口: COM3</span><br><span class="line">比特率：115200</span><br><span class="line">数据位：8 bit</span><br><span class="line">校验位：none</span><br><span class="line">停止位：1 bit</span><br><span class="line">流量控制：none</span><br></pre></td></tr></table></figure></li>
<li><p>验证板子<br>板子usb或杜邦线转usb连接到台式机，上电或者重启可看到tera term端的输出</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Freeing unused kernel memory: 320K (ffffffc0047b0000 - ffffffc004800000)</span><br><span class="line"></span><br><span class="line">            _ _ _ _ _ _ _ _ _ _ _ _</span><br><span class="line">            \  _  _   _  _ _ ___</span><br><span class="line">            / /__/ \ |_/</span><br><span class="line">           / __   /  -  _ ___</span><br><span class="line">          / /  / /  / /</span><br><span class="line">  _ _ _ _/ /  /  \_/  \_ ______</span><br><span class="line">___________\___\__________________</span><br><span class="line"></span><br><span class="line">[RCS]: /etc/init.d/S00devs</span><br><span class="line">[RCS]: /etc/init.d/S01udev</span><br></pre></td></tr></table></figure>
</li>
<li><p>板子参数设置</p>
<ul>
<li>设置命令<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">setenv ITEMNAME ITEMVALUE   //ITEMNAME是设置项名称，ITEMVALUE是设置项值</span><br><span class="line">saveenv     //保存设置</span><br></pre></td></tr></table></figure></li>
<li>uboot中设置<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">setenv ipaddr 192.168.86.114</span><br><span class="line">setenv netmask 255.255.255.0</span><br><span class="line">setenv gatewayip 192.168.86.1</span><br><span class="line">setenv serverip 192.168.86.105</span><br></pre></td></tr></table></figure></li>
<li>板子中的系统文件（我的是/etc/init.d/S80…）<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ipaddr = 192.168.86.114</span><br><span class="line">netmask = 255.255.255.0</span><br><span class="line">gateway = 192.168.86.1</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="1-2-vmware"><a href="#1-2-vmware" class="headerlink" title="1.2 vmware"></a>1.2 vmware</h3><ul>
<li>soft install</li>
<li>OS install   <ul>
<li>ubuntu1804</li>
<li>network—-“www”<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IPv4 Method: Automatic(DHCP)</span><br></pre></td></tr></table></figure></li>
<li>network—-“Profile 1”<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IPv4 Method: Manual</span><br><span class="line">Addresses Address: 192.168.86.105</span><br><span class="line">Addresses Netmask: 255.255.255.0</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>增加桥接模式<br>  【编辑-&gt;虚拟网络编辑器-&gt;添加网络】</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">表格中</span><br><span class="line">名称：VMnet0</span><br><span class="line">类型：桥接模式</span><br><span class="line">外部连接：Intel(R) Ethernet ...（你自己的台式机网卡）</span><br><span class="line">“VMnet信息”中</span><br><span class="line">选择第一个，“桥接模式(将虚拟机直接连接到外部网络)”</span><br><span class="line">已桥接至(G):Intel(R) Ethernet Connection (2) I219-V</span><br></pre></td></tr></table></figure>
<p>  【虚拟机-&gt;设置-&gt;硬件-&gt;网络适配器-&gt;网络连接】</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">自定义(U):特定虚拟网络，下拉项选择上一步新增加的VMnet0</span><br></pre></td></tr></table></figure></li>
<li>桥接模式和联网模式的切换   <ul>
<li>关闭虚拟机的ubuntu系统   </li>
<li>虚拟机-&gt;设置-&gt;硬件-&gt;网络适配器-&gt;网络连接-&gt;NAT模式(N)：用于共享主机的IP地址</li>
</ul>
</li>
</ul>
<h3 id="1-3-PC设置"><a href="#1-3-PC设置" class="headerlink" title="1.3 PC设置"></a>1.3 PC设置</h3><p>网络和Internet-&gt;网络连接-&gt;以太网-&gt;属性(右键)-&gt;Internet协议版本4(TCP/IPv4)<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">勾选,&quot;使用下面的IP地址(S)&quot;</span><br><span class="line">IP地址: 192.168.86.189</span><br><span class="line">子网掩码k: 255.255.255.0</span><br></pre></td></tr></table></figure><br>确认即可</p>
<h2 id="2，互通验证"><a href="#2，互通验证" class="headerlink" title="2，互通验证"></a>2，互通验证</h2><ul>
<li><p>PC和vmware中ubuntu相互ping通</p>
</li>
<li><p>PC和板子相互ping通</p>
</li>
<li><p>vmware中ubuntu和板子相互ping通</p>
</li>
</ul>
<h2 id="3，关联资源"><a href="#3，关联资源" class="headerlink" title="3，关联资源"></a>3，关联资源</h2><p>参考连接：<a href="https://www.cnblogs.com/czjk/p/11699198.html">https://www.cnblogs.com/czjk/p/11699198.html</a></p>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>Hi3559a win10 vmware</tag>
      </tags>
  </entry>
  <entry>
    <title>anaconda安装，换源及构建虚拟环境</title>
    <url>/2022/05/17/python/anaconda-basic/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>anaconda的安装，换源和虚拟环境构建，对于python语言来说，anaconda是个不错的环境管理方案，可以隔离不同的python应用和包需求。</p>
<p><strong>工欲善其事必先利其器~</strong><br><span id="more"></span></p>
<h2 id="anaconda安装并换源："><a href="#anaconda安装并换源：" class="headerlink" title="anaconda安装并换源："></a>anaconda安装并换源：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bash Anaconda3-5.2.0-Linux-x86_64.sh</span><br><span class="line">vim ~/.bashrc</span><br><span class="line">export PATH=/home/XXX/anaconda3/bin:$PATH（XXX为自己的用户名）（在文件末尾处添加该语句）</span><br><span class="line">source ~/.bashrc</span><br><span class="line"></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line">之后vim ~/.condarc，把defaults删掉</span><br></pre></td></tr></table></figure>
<h2 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n exp38 python==3.8</span><br><span class="line">conda activate exp38</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>anaconda basic</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn的前向传播过程</title>
    <url>/2022/05/16/detection/forward-of-faster-rcnn/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>faster rcnn的前传过程和训练步骤</p>
<p><strong>经典的二阶段算法啊~</strong></p>
<span id="more"></span>
<h2 id="1，faster-rcnn前向过程"><a href="#1，faster-rcnn前向过程" class="headerlink" title="1，faster rcnn前向过程"></a>1，faster rcnn前向过程</h2><p>输入一张待检测图片-&gt;vgg16网络conv layers提取整张图片的特征，输出feature map分别输入到RPN和Fast RCNN网络开头-&gt;RPN网络得出region proposal，将这些候选框信息送入到Fast RCNN网络开头-&gt;利用候选框在之前送到的feature map提取特征，并通过ROI Pooling层得到规定大小的feature map-&gt;将这些feature map送入Fast RCNN网络中进行分类和回归坐标，最终得到需检测物体的坐标。</p>
<h2 id="2，faster-rcnn训练步骤"><a href="#2，faster-rcnn训练步骤" class="headerlink" title="2，faster rcnn训练步骤"></a>2，faster rcnn训练步骤</h2><ul>
<li>第一步，训练RPN，该网络用ImageNet预训练的模型初始化，并端到端微调，用于生成region proposal；</li>
<li>第二步，训练Fast R-CNN，由imageNet model初始化，利用第一步的RPN生成的region proposals作为输入数据，训练Fast R-CNN一个单独的检测网络，这时候两个网络还没有共享卷积层；</li>
<li>第三步，调优RPN，用第二步的fast-rcnn model初始化RPN再次进行训练，但固定共享的卷积层，并且只微调RPN独有的层，现在两个网络共享卷积层了；</li>
<li>第四步，调优Fast R-CNN,由第三步的RPN model初始化fast-RCNN网络，输入数据为第三步生成的proposals。保持共享的卷积层固定，微调Fast R-CNN的fc层。这样，两个网络共享相同的卷积层，构成一个统一的网络。</li>
</ul>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>faster rcnn forward</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn中的类别不均衡问题</title>
    <url>/2022/05/16/detection/class-imbalance-in-faster-rcnn/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>faster rcnn是17年前后He Kaiming应用比较广泛的大作之一，是典型的二阶段方法，其中有哪些比较好的处理类别不均衡的思想呢？可以从论文中窥探一二~</p>
<p><strong>多看论文啊~</strong><br><span id="more"></span></p>
<h2 id="1，论文中的解决方法"><a href="#1，论文中的解决方法" class="headerlink" title="1，论文中的解决方法"></a>1，论文中的解决方法</h2><ul>
<li><p>根据前景score的高低过滤得到可能是前景的exam，约1k~2k个，这样可过滤掉大部分简单负样本；</p>
</li>
<li><p>根据IoU的大小来调整正负样本比，比如1:3，这样可防止负样本过多；</p>
</li>
</ul>
<h2 id="2，Faster-RCNN怎么筛选正负anchor"><a href="#2，Faster-RCNN怎么筛选正负anchor" class="headerlink" title="2，Faster RCNN怎么筛选正负anchor"></a>2，Faster RCNN怎么筛选正负anchor</h2><ul>
<li>给两种锚点分配一个正标签<ul>
<li><ol>
<li>具有与实际边界框的重叠最高交并比（IoU）的锚点；</li>
</ol>
</li>
<li><ol>
<li>具有与实际边界框的重叠超过0.7 IoU的锚点。IoU比率低于0.3，</li>
</ol>
</li>
</ul>
</li>
<li>给非正面的锚点分配一个负标签。</li>
</ul>
<h2 id="3，衍生到通用类别不均衡问题"><a href="#3，衍生到通用类别不均衡问题" class="headerlink" title="3，衍生到通用类别不均衡问题"></a>3，衍生到通用类别不均衡问题</h2><p>限制正负样本比例为1:1，如果正样本不足，就用负样本补充，这种方法后面研究工作用的不多。通常针对类别不平衡问题可以从调整样本数或修改loss weight两方面去解决，常用的方法有OHEM、OHNM、class balanced loss和Focal loss。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>faster rcnn</tag>
        <tag>class imbalance</tag>
      </tags>
  </entry>
  <entry>
    <title>nms的原理和作用</title>
    <url>/2022/05/16/detection/-priciple-and-effects-of-nms/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>nms的原理，应用和发展方向</p>
<p><strong>尤其在anchor based中用得多~</strong><br><span id="more"></span></p>
<h2 id="1，作用和原理"><a href="#1，作用和原理" class="headerlink" title="1，作用和原理"></a>1，作用和原理</h2><ul>
<li><p>作用<br>本质是搜索局部极大值，抑制非极大值元素。</p>
</li>
<li><p>原理<br>NMS为非极大值抑制，用来抑制检测时冗余的框。</p>
</li>
</ul>
<h2 id="2，算法流程为"><a href="#2，算法流程为" class="headerlink" title="2，算法流程为"></a>2，算法流程为</h2><ul>
<li>1.对所有预测框的置信度降序排序</li>
<li>2.选出置信度最高的预测框，确认其为正确预测，并计算他与其他预测框的IOU </li>
<li>3.根据2中计算的IOU去除重叠度高的，IOU&gt;threshold阈值就删除 </li>
<li>4.剩下的预测框返回第1步，直到没有剩下的为止</li>
</ul>
<p><strong>注意：</strong><br>NMS一次处理一个类别，如果有N个类别，NMS就需要执行N次。</p>
<h2 id="3，待改进问题？"><a href="#3，待改进问题？" class="headerlink" title="3，待改进问题？"></a>3，待改进问题？</h2><p>假设两个目标靠的很近，则会识别成一个bbox，会有什么问题，怎么解决？</p>
<p>当两个目标靠的非常近时，置信度低的会被置信度高的框抑制掉，从而两个目标靠的非常近时会被识别成一个bbox。为了解决这个问题，可以使用softNMS。</p>
<p>softNMS基本思想：用稍低一点的分数来代替原有的分数，而不是直接置零）</p>
<h2 id="4，发展新动向？"><a href="#4，发展新动向？" class="headerlink" title="4，发展新动向？"></a>4，发展新动向？</h2>]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>nms</tag>
      </tags>
  </entry>
  <entry>
    <title>rpn的原理和作用</title>
    <url>/2022/05/16/detection/principle-and-effects-of-rpn/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>rpn原理和作用</p>
<span id="more"></span>
<h2 id="1，作用"><a href="#1，作用" class="headerlink" title="1，作用"></a>1，作用</h2><p>RPN专门用来提取候选框，一方面RPN耗时少，另一方面RPN可以很容易结合到Fast RCNN中，成为一个整体。</p>
<h2 id="2，实现细节"><a href="#2，实现细节" class="headerlink" title="2，实现细节"></a>2，实现细节</h2><p>一个特征图（Faster RCNN的公共Feature Map）经过sliding window处理，得到256维特征，对每个特征向量做两次全连接操作，一个得到2个分数，一个得到4个坐标{然后通过两次全连接得到结果2k个分数和4k个坐标[k指的是由锚点产生的K个框(K anchor boxes)]}</p>
<h2 id="3，anchor-box是怎么选取的？"><a href="#3，anchor-box是怎么选取的？" class="headerlink" title="3，anchor box是怎么选取的？"></a>3，anchor box是怎么选取的？</h2><p>滑窗的中心在原像素空间的映射点称为anchor，以此anchor为中心，生成k(paper中default k=9, 3 scales and 3 aspect ratios/不同尺寸和不同长宽比)个proposals。三个面积尺寸$(128^{2}，256^{2}，512^{2})$，然后在每个面积尺寸下，取三种不同的长宽比例（1:1,1:2,2:1）</p>
<h2 id="4，为什么提出anchor-box？"><a href="#4，为什么提出anchor-box？" class="headerlink" title="4，为什么提出anchor box？"></a>4，为什么提出anchor box？</h2><p>主要有两个原因：一个窗口只能检测一个目标、无法解决多尺度问题。<br>目前anchor box尺寸的选择主要有三种方式：人为经验选取、k-means聚类、作为超参数进行学习<br>为什么使用不同尺寸和不同长宽比？ 为了得到更大的交并比(IOU)。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>rpn</tag>
      </tags>
  </entry>
  <entry>
    <title>roi pooling</title>
    <url>/2022/05/16/detection/roi-pooling/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>roi pooling的基本原理和应用</p>
<p><strong>加油，夯实基础~</strong><br><span id="more"></span></p>
<h2 id="1，基本原理"><a href="#1，基本原理" class="headerlink" title="1，基本原理"></a>1，基本原理</h2><p>RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定$(w * h)$的矩形框，最大的好处就在于极大地提高了处理速度。</p>
<p><img src="roi_pooling.gif" alt="obj_detec"> </p>
<h2 id="2，实现"><a href="#2，实现" class="headerlink" title="2，实现"></a>2，实现</h2><ul>
<li>1，根据输入image，将ROI映射到feature map对应位置</li>
<li>2，将映射后的区域划分为相同大小的sections（数量与输出的维度相同）；</li>
<li>3，对每个sections进行max pooling操作；</li>
</ul>
<p>怎么做的映射?<br>映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”。</p>
<h2 id="3，优点"><a href="#3，优点" class="headerlink" title="3，优点"></a>3，优点</h2><ul>
<li>1.允许我们对CNN中的feature map进行reuse</li>
<li>2.可以显著加速training和testing速度；</li>
<li>3.允许end-to-end的形式训练目标检测系统。</li>
</ul>
<h2 id="4，缺点"><a href="#4，缺点" class="headerlink" title="4，缺点"></a>4，缺点</h2><p>由于 RoIPooling 采用的是最近邻插值（即INTER_NEAREST） ，在resize时，对于缩放后坐标不能刚好为整数的情况，采用了粗暴的舍去小数，相当于选取离目标点最近的点，损失一定的空间精度。</p>
<p>在这个过程中会有两次量化操作。对于一个region proposal，首先从原图经过全卷积网络到特征图，得到的候选框位置可能存在浮点数，进行取整操作从而出现第一次量化；其次，在ROI Pooling求取每个小网格的位置时也同样存在浮点数取整的情况。这两次量化的结果都使得候选框的位置会出现偏差，在论文里，作者把它总结为“不匹配问题（misalignment）。</p>
<p>经过上述两次量化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>roi pooling</tag>
      </tags>
  </entry>
  <entry>
    <title>roi align</title>
    <url>/2022/05/16/detection/roi-align/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>roi align产生的背景，原理和应用场景</p>
<p><strong>主要对比roi pooling</strong><br><span id="more"></span></p>
<h2 id="1，背景"><a href="#1，背景" class="headerlink" title="1，背景"></a>1，背景</h2><p>在mask rcnn中提出，是为了解决roi pooling的两次量化带来的位置精度损失问题。</p>
<h2 id="2，思路"><a href="#2，思路" class="headerlink" title="2，思路"></a>2，思路</h2><p>取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作。值得注意的是，在具体的算法操作上，ROI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，大致算法流程为：</p>
<ul>
<li>遍历每一个候选区域，保持浮点数边界不做量化。</li>
<li>将候选区域分割成k x k个单元，每个单元的边界也不做量化。</li>
<li>在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。</li>
</ul>
<h2 id="3，区别"><a href="#3，区别" class="headerlink" title="3，区别"></a>3，区别</h2><p>ROI Align舍去了近似像素取整数的量化方法，改用双线性插值的方法确定特征图坐标对应于原图中的像素位置.ROI Align很好地解决了ROI Pooling操作中两次量化造成的区域不匹配(mis-alignment)的问题。</p>
<h2 id="4，应用场景"><a href="#4，应用场景" class="headerlink" title="4，应用场景"></a>4，应用场景</h2><p>对于检测图片中大目标物体时，两种方案的差别不大，而如果是图片中有较多小目标物体需要检测，则优先选择RoiAlign，更精准些。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>roi align</tag>
      </tags>
  </entry>
  <entry>
    <title>yolo4中的数据增强</title>
    <url>/2022/05/16/detection/-data-augmentation-in-yolo4/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>常用数据增强方式和yolov4中使用的数据增强方式及其有效性。</p>
<p><strong>加油啊，还没总结完~</strong><br><span id="more"></span></p>
<h2 id="1，常用数据增强方式"><a href="#1，常用数据增强方式" class="headerlink" title="1，常用数据增强方式"></a>1，常用数据增强方式</h2><h3 id="1-1-畸变"><a href="#1-1-畸变" class="headerlink" title="1.1 畸变"></a>1.1 畸变</h3><ul>
<li>光照畸变</li>
<li>几何畸变</li>
</ul>
<h3 id="1-2-图像遮挡"><a href="#1-2-图像遮挡" class="headerlink" title="1.2 图像遮挡"></a>1.2 图像遮挡</h3><h4 id="1-2-1-随机擦除"><a href="#1-2-1-随机擦除" class="headerlink" title="1.2.1 随机擦除"></a>1.2.1 随机擦除</h4><ul>
<li>做法<br>选定图像一块区域，用随机像素或者平均像素来填充；</li>
<li>功能<br>防止模型记忆训练数据和过拟合；</li>
</ul>
<h4 id="1-2-2-cutout"><a href="#1-2-2-cutout" class="headerlink" title="1.2.2 cutout"></a>1.2.2 cutout</h4><ul>
<li>做法<br>训练中掩盖一个正方形区域，只对CNN第一层遮挡，填充使用的仍然是常数像素；</li>
<li>功能<br>防止过拟合；</li>
</ul>
<h4 id="1-2-3-Hide-and-Seek"><a href="#1-2-3-Hide-and-Seek" class="headerlink" title="1.2.3 Hide and Seek"></a>1.2.3 Hide and Seek</h4><ul>
<li>做法<br>将图像分割成s*s的patch，每个patch以一定概率隐藏</li>
<li>功能<br>让模型了解物体是什么样子，而不只是学习单个部分是什么样子</li>
</ul>
<h4 id="1-2-4-Grid-Mask"><a href="#1-2-4-Grid-Mask" class="headerlink" title="1.2.4 Grid Mask"></a>1.2.4 Grid Mask</h4><ul>
<li>做法<br>mask的网格，将图像隐藏其中</li>
<li>功能<br>让模型学习组成物体的做成部分</li>
</ul>
<h4 id="1-2-5-MixUp"><a href="#1-2-5-MixUp" class="headerlink" title="1.2.5 MixUp"></a>1.2.5 MixUp</h4><ul>
<li>做法<br>图像对及其标签的凸叠加</li>
<li>功能</li>
</ul>
<h2 id="2，yolov4数据增强"><a href="#2，yolov4数据增强" class="headerlink" title="2，yolov4数据增强"></a>2，yolov4数据增强</h2><h3 id="2-1-CutMix"><a href="#2-1-CutMix" class="headerlink" title="2.1 CutMix"></a>2.1 CutMix</h3><ul>
<li><p>做法</p>
</li>
<li><p>功能</p>
</li>
<li><p>问题思考？<br>  对比mixup和cutout，为什么cutmix会好？</p>
</li>
</ul>
<h3 id="2-2-Mosaic"><a href="#2-2-Mosaic" class="headerlink" title="2.2 Mosaic"></a>2.2 Mosaic</h3><ul>
<li><p>做法<br>把4张图片，通过随机缩放、随机裁减、随机排布的方式进行拼接</p>
</li>
<li><p>优点<br>学习到比正常尺寸小的物体；<br>可以“省”训练数据或迭代次数；<br>丰富了检测物体的背景和小目标，并且在计算Batch Normalization的时候一次会计算四张图片的数据，使得mini-batch大小不需要很大，一个GPU就可以达到比较好的效果；<br>丰富数据集：随机使用4张图片，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好；<br>减少GPU：直接计算4张图片的数据，使得Mini-batch大小并不需要很大，一个GPU就可以达到比较好的效果；</p>
</li>
<li><p>缺点<br>  如果我们的数据集本身就有很多的小目标，那么Mosaic数据增强会导致本来较小的目标变得更小，导致模型的泛化能力变差</p>
</li>
<li><p>问题思考</p>
</li>
</ul>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>data augmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>一阶段和两阶段目标检测网络</title>
    <url>/2022/05/16/detection/one-stage-vs-two-stage/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一阶段和二阶段目标检测算法</p>
<p><strong>总结不到位，请重来~</strong><br><span id="more"></span></p>
<h2 id="1，one-stage"><a href="#1，one-stage" class="headerlink" title="1，one stage"></a>1，one stage</h2><p>One-Stage检测算法，没有selective search产生region proposal的阶段，直接产生物体的类别概率和位置坐标，经过单次检测即可直接获得最终的检测结果。相比Two-Stage有更快的速度，但准确度低。代表网络有YOLO v1/v2/v3/9000,SSD,Retina-Net. （two-stage算法中的roi pooling会对目标做resize, 小目标的特征被放大，其特征轮廓也更为清晰，因此检测也更为准确）</p>
<h2 id="2，two-stage"><a href="#2，two-stage" class="headerlink" title="2，two stage"></a>2，two stage</h2><p>先由算法生成一系列作为样本的候选框，再通过卷积神经网络进行样本分类。</p>
<p>对于Two-stage的目标检测网络，主要通过一个卷积神经网络来完成目标检测过程，其提取的是CNN卷积特征，在训练网络时，其主要训练两个部分，第一步是训练RPN网络，第二步是训练目标区域检测的网络。网络的准确度高、速度相对One-stage慢。</p>
<p>Two-Stage检测算法将检测问题划分成两个阶段，首先是获取region proposal进行位置精修和分类阶段。相比于One-Stage,精度高，漏检率也低，但是速度较慢，代表网络有Fast rcnn，Faster rcnn，mask rcnn等。</p>
<h2 id="3，异同"><a href="#3，异同" class="headerlink" title="3，异同"></a>3，异同</h2><p>Two-Stage先对前景背景做了筛选，再进行回归，回归效果比较好，准度高但是相比较慢，One-Stage是直接对特征上的点进行直接回归，优点是速度快，因为用了多层特征图出框可能小目标效果比较好一点，缺点是因为正负样本失衡导致效果较差，要结合难例挖掘。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>one stage</tag>
        <tag>two stage</tag>
      </tags>
  </entry>
  <entry>
    <title>基于框和无框方法的目标检测</title>
    <url>/2022/05/16/detection/-anchor-based-vs-anchor-free/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>anchor based和anchor free两种方法对比，基本内容包含：</p>
<ul>
<li>各自定义</li>
<li>优缺点</li>
<li>应用场景</li>
<li>常用方法</li>
</ul>
<p><strong>一点一点加油~</strong><br><span id="more"></span></p>
<h2 id="1，定义"><a href="#1，定义" class="headerlink" title="1，定义"></a>1，定义</h2><h2 id="2，优缺点"><a href="#2，优缺点" class="headerlink" title="2，优缺点"></a>2，优缺点</h2><h3 id="2-1-anchor-base存在的问题"><a href="#2-1-anchor-base存在的问题" class="headerlink" title="2.1 anchor-base存在的问题"></a>2.1 anchor-base存在的问题</h3><ul>
<li>在训练时，size ratio和anchor number等超参很敏感，需仔细调参；</li>
<li>与锚点框相关超参 (scale、aspect ratio、IoU Threshold) 会较大影响最终预测效果；</li>
<li>预置的锚点大小、比例在检测差异较大物体时不够灵活，尤其是小目标物体，同时也限制了模型的泛化能力；</li>
<li>需要数量较多的anchor，但是大量的锚点会导致运算复杂度增大，产生的参数较多；</li>
<li>容易导致训练时negative与positive的比例失衡；</li>
</ul>
<h3 id="2-2-Anchor-free算法的优点"><a href="#2-2-Anchor-free算法的优点" class="headerlink" title="2.2 Anchor-free算法的优点"></a>2.2 Anchor-free算法的优点</h3><p>• 使用类似分割的思想来解决目标检测问题；<br>• 不需要调优与anchor相关的超参数；<br>• 避免大量计算GT boxes和anchor boxes 之间的IoU，使得训练过程占用内存更低。</p>
<h2 id="3，主要应用场景"><a href="#3，主要应用场景" class="headerlink" title="3，主要应用场景"></a>3，主要应用场景</h2><h2 id="4，常用代表性方法"><a href="#4，常用代表性方法" class="headerlink" title="4，常用代表性方法"></a>4，常用代表性方法</h2><h3 id="4-1-anchor-based"><a href="#4-1-anchor-based" class="headerlink" title="4.1 anchor based"></a>4.1 anchor based</h3><p>基于anchor-based的技术包括一个阶段和两个阶段的检测。  </p>
<p>一阶段的检测技术包括：</p>
<ul>
<li>SSD</li>
<li>DSSD</li>
<li>RetinaNet</li>
<li>RefineDet</li>
<li>YOLOV3</li>
<li>…</li>
</ul>
<p>二阶段技术包括：</p>
<ul>
<li>Faster-RCNN</li>
<li>R-FCN</li>
<li>FPN</li>
<li>Cascade R-CNN</li>
<li>SNIP</li>
<li>…</li>
</ul>
<p>一般的，两个阶段的目标检测会比一个阶段的精度要高，但一个阶段的算法速度会更快。</p>
<h3 id="4-2-anchor-free"><a href="#4-2-anchor-free" class="headerlink" title="4.2 anchor free"></a>4.2 anchor free</h3><p>anchor-free的技术包括基于Keypoint与Segmentation两类。</p>
<p>基于Keypoint技术包括</p>
<ul>
<li>CornerNet</li>
<li>CenterNet</li>
<li>CornerNet-Lite</li>
<li>…</li>
</ul>
<p>基于Segmentation的技术包括</p>
<ul>
<li>FSAF</li>
<li>FCOS</li>
<li>FoveaBox</li>
<li>…</li>
</ul>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>anchor based</tag>
        <tag>anchor free</tag>
      </tags>
  </entry>
  <entry>
    <title>小目标检测</title>
    <url>/2022/05/16/detection/tiny-obj-det/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>早期经典方法为何对小目标检测效果不好？小目标检测有哪几种方案？</p>
<p><strong>一步一步来~</strong><br><span id="more"></span></p>
<h2 id="1，faster-rcnn-yolo-ssd等对小目标为啥不work？"><a href="#1，faster-rcnn-yolo-ssd等对小目标为啥不work？" class="headerlink" title="1，faster rcnn, yolo, ssd等对小目标为啥不work？"></a>1，faster rcnn, yolo, ssd等对小目标为啥不work？</h2><p>SSD，YOLO等单阶段多尺度算法，小目标检测需要较高的分辨率，SSD对于高分辨的低层特征没有再利用，而这些层对于检测小目标很重要。按SSD的设计思想，其实SSD对小目标应该有比较好的效果，但是需要重新精细设计SSD中的default box，比如重新设计min_sizes参数，扩大小default box的数量来cover住小目标。但是随着default box数量的增加，网络速度也会降低。YOLO网络可以理解为是强行把图片分割成7*7个网格，每个网格预测2个目标，相当于只有98个anchor，所以不管是小目标，还是大目标，YOLO的表现都不是很理想，但是由于只需处理少量的anchor，所以YOLO的速度上有很大优势。</p>
<p>Faster rcnn系列对小目标检测效果不好的原因是faster rcnn只用卷积网络的最后一层，但是卷积网络的最后一层往往feature map太小，导致之后的检测和回归无法满足要求。甚至一些小目标在最后的卷积层上直接没有特征点了。所以导致faster rcnn对小目标检测表现较差。</p>
<p><strong>难点</strong><br>分辨率低，图像模糊，携带的信息少。</p>
<h2 id="2，可选解决方案"><a href="#2，可选解决方案" class="headerlink" title="2，可选解决方案"></a>2，可选解决方案</h2><h3 id="2-1-借鉴FPN的思想"><a href="#2-1-借鉴FPN的思想" class="headerlink" title="2.1 借鉴FPN的思想"></a>2.1 借鉴FPN的思想</h3><p>在FPN之前目标检测的大多数方法都是和分类一样，使用顶层的特征来进行处理。虽然这种方法只是用到了高层的语义信息，但是位置信息却没有得到，尤其在检测目标的过程中，位置信息是特别重要的，而位置信息又是主要在网络的低层。因此FPN采用了多尺度特征融合的方式，采用不同特征层特征融合之后的结果来做预测。</p>
<h3 id="2-2-要让输入的分布尽可能地接近模型预训练的分布"><a href="#2-2-要让输入的分布尽可能地接近模型预训练的分布" class="headerlink" title="2.2 要让输入的分布尽可能地接近模型预训练的分布"></a>2.2 要让输入的分布尽可能地接近模型预训练的分布</h3><p>先用ImageNet做预训练，之后使用原图上采样得到的图像来做微调，使用微调的模型来预测原图经过上采样的图像。该方法提升效果比较显著。</p>
<h3 id="2-3-多尺度"><a href="#2-3-多尺度" class="headerlink" title="2.3 多尺度"></a>2.3 多尺度</h3><p>采用多尺度输入训练方式来训练网络；</p>
<h3 id="2-4-借鉴Cascade-R-CNN的设计思路"><a href="#2-4-借鉴Cascade-R-CNN的设计思路" class="headerlink" title="2.4 借鉴Cascade R-CNN的设计思路"></a>2.4 借鉴Cascade R-CNN的设计思路</h3><p>优化目标检测中Two-Stage方法中的IOU阈值。检测中的IOU阈值对于样本的选取是至关重要的，如果IOU阈值过高，会导致正样本质量很高，但是数量会很少，会出现样本比例不平衡的影响；如果IOU阈值较低，样本数量就会增加，但是样本的质量也会下降。如何选取好的IOU，对于检测结果来说很重要。⑤采用分割代替检测方法，先分割，后回归bbox来检测微小目标。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>tiny objects det</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测网络小结</title>
    <url>/2022/05/16/detection/object-detection-models/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目标检测的经典网络，会不断更新哦~</p>
<p><strong>催就更~</strong><br><span id="more"></span></p>
<h2 id="目标检测经典网络"><a href="#目标检测经典网络" class="headerlink" title="目标检测经典网络"></a>目标检测经典网络</h2><p><img src="det_models.png" alt="obj_detec">   </p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>objext detection</tag>
      </tags>
  </entry>
  <entry>
    <title>传统目标检测小结</title>
    <url>/2022/05/16/detection/traditional-obj-det/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>传统目标检测的发展历程和优缺点</p>
<p><strong>有时间整理下HOG+SVM的原理吧~</strong><br><span id="more"></span></p>
<h2 id="1，主线"><a href="#1，主线" class="headerlink" title="1，主线"></a>1，主线</h2><p>区域选择-&gt;特征提取-&gt;分类器</p>
<h2 id="2，算法基本流程"><a href="#2，算法基本流程" class="headerlink" title="2，算法基本流程"></a>2，算法基本流程</h2><ul>
<li><ol>
<li>使用不同尺度的滑动窗口选定图像的某一区域为候选区域；</li>
</ol>
</li>
<li><ol>
<li>从对应的候选区域提取如Harr HOG LBP LTP等一类或者多类特征；</li>
</ol>
</li>
<li><ol>
<li>使用Adaboost SVM 等分类算法对对应的候选区域进行分类，判断是否属于待检测的目标。</li>
</ol>
</li>
</ul>
<h2 id="3，缺点"><a href="#3，缺点" class="headerlink" title="3，缺点"></a>3，缺点</h2><ul>
<li>1）基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余</li>
<li>2）手工设计的特征对于多样性的变化没有很好的鲁棒性</li>
</ul>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Nvidia GTX3090配置ubuntu20.04环境</title>
    <url>/2022/05/14/hardware/GTX3090-server-env/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>主机环境：DELL T7920图形工作站，2<em>银牌4214R，24核 2.4G 64G内存，2</em>1TB M.2固态，RTX3090，ubuntu2004系统，主要用于深度学习视觉方向的开发环境搭建。</p>
<span id="more"></span>
<h2 id="1，系统安装"><a href="#1，系统安装" class="headerlink" title="1，系统安装"></a>1，系统安装</h2><p><a href="https://github.com/sophia-hxw/uglyBlog/blob/main/ubuntu/UEFI%E5%92%8CLegacy.md">ubuntu2004安装</a></p>
<h2 id="2，显卡驱动"><a href="#2，显卡驱动" class="headerlink" title="2，显卡驱动"></a>2，显卡驱动</h2><h3 id="2-1-文件下载"><a href="#2-1-文件下载" class="headerlink" title="2.1 文件下载"></a>2.1 文件下载</h3><p><a href="https://www.nvidia.cn/Download/index.aspx">下载链接</a></p>
<h3 id="2-2-禁用nouveau"><a href="#2-2-禁用nouveau" class="headerlink" title="2.2 禁用nouveau"></a>2.2 禁用nouveau</h3><p>报错信息：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ERROR: The Nouveau kernel driver is currently in use by your system. This  driver is incompatible with the NVIDIA driver, and must be disabled before proceeding.</span><br><span class="line">Please consult the NVIDIA driver README and your Linux distribution&#x27;s documentation</span><br><span class="line">for details on how to correctly  disable the Nouveau kernel driver.</span><br></pre></td></tr></table></figure><br>如果出现上述报错，则需要禁用nouveau：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 首先打开或新建文件  </span><br><span class="line">sudo vim /etc/modprobe.d/blacklist-nouveau.conf</span><br><span class="line"># 在文件中添加如下内容</span><br><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line"># 执行命令使其生效并重启</span><br><span class="line">sudo update-initramfs -u</span><br><span class="line">sudo reboot</span><br><span class="line"># 查看禁用情况</span><br><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure></p>
<h3 id="2-3-驱动安装"><a href="#2-3-驱动安装" class="headerlink" title="2.3 驱动安装"></a>2.3 驱动安装</h3><p>显卡驱动：NVIDIA-Linux-x86_64-455.23.04.run<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo bash NVIDIA-Linux-...</span><br></pre></td></tr></table></figure><br>安装完成后用<code>nvidia-smi</code>可以查看显卡信息</p>
<h2 id="3，基本环境"><a href="#3，基本环境" class="headerlink" title="3，基本环境"></a>3，基本环境</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">py37或py38</span><br><span class="line">cuda11.0</span><br><span class="line">cudnn8.0.4</span><br><span class="line">tf2.5（tf-nightly）或 tf1.15.4</span><br><span class="line">pytorch1.7</span><br><span class="line">keras2.3</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>GTX3090</tag>
      </tags>
  </entry>
  <entry>
    <title>static IP of ubuntu20.04</title>
    <url>/2022/05/14/ubuntuOS/staticIP-of-ubuntu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自己搭建了一个简单的服务系统，就有了固定服务器ip的需求，方法不多说了，见下文~</p>
<p><strong>so easy~</strong><br><span id="more"></span></p>
<h2 id="1，详细步骤"><a href="#1，详细步骤" class="headerlink" title="1，详细步骤"></a>1，详细步骤</h2><p>系统版本：20.04<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip addr</span><br><span class="line"># 打开文件</span><br><span class="line">sudo vi /etc/netplan/01-netplan-manager-all.yaml</span><br><span class="line"># 添加内容：</span><br><span class="line">network:</span><br><span class="line">    ethernets:</span><br><span class="line">        ens33:#网卡名称，可能不一样</span><br><span class="line">            dhcp4: false</span><br><span class="line">            addresses: [192.168.1.18/24]</span><br><span class="line">            optional: true</span><br><span class="line">            gateway4: 192.168.1.1</span><br><span class="line">            nameservers:</span><br><span class="line">                addresses: [192.168.1.1,114.114.114.114]</span><br><span class="line">    version: 2</span><br><span class="line"># 使配置生效</span><br><span class="line">sudo netplan apply</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>static IP</tag>
      </tags>
  </entry>
  <entry>
    <title>backup and recovery of ubuntu OS</title>
    <url>/2022/05/14/ubuntuOS/ubuntu-backup-and-recovery/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ubuntu物理机系统，以前崩溃了就一直重装，然后各种软件挺头疼，想起在windows中屡试不爽的备份和换源，想在ubuntu系统中实践下，印象中，测试过两次，效果不太好，有待继续考证~~</p>
<p><strong>路漫漫其修远兮，壮士加油~~</strong><br><span id="more"></span></p>
<h2 id="1，系统备份"><a href="#1，系统备份" class="headerlink" title="1，系统备份"></a>1，系统备份</h2><h3 id="1-1-先清理"><a href="#1-1-先清理" class="headerlink" title="1.1 先清理"></a>1.1 先清理</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 清理旧版本的软件缓存</span><br><span class="line">sudo apt-get autoclean</span><br><span class="line"># 清理所有软件缓存</span><br><span class="line">sudo apt-get clean</span><br><span class="line"># 删除系统不再使用的孤立软件</span><br><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure>
<h3 id="1-2-优盘备份"><a href="#1-2-优盘备份" class="headerlink" title="1.2 优盘备份"></a>1.2 优盘备份</h3><ul>
<li>插入优盘，<code>df -h</code>查看优盘的位置，我的位置是<code>/media/tongtong/</code></li>
<li>切换到系统用户<code>sudo su</code></li>
<li>备份命令<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -cvpzf /media/tongtong/KINGSTON/ubuntu_backup@`date +%Y-%m+%d`.tar.gz --exclude=/proc --exclude=/media --exclude=/tmp --exclude=/home --exclude=/lost+found --exclude=/mnt --exclude=/run / </span><br></pre></td></tr></table></figure>
命令参数：<br>-c： 新建一个备份文档<br>-v： 显示详细信息<br>-p： 保存权限，并应用到所有文件<br>-z： 用gzip压缩备份文档，减小空间<br>-f： 指定备份文件的路径<br>–exclude： 排除指定目录，不进行备份   </li>
</ul>
<p><strong>请注意，如果没有把/home或者/boot目录单独分一个区，一定不要加–exclude=/home或–exclude=/boot参数！！！</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/proc：一个虚拟文件系统，系统运行的每一个进程都会自动在这个目录下面创建一个进程目录。既然是系统自动创建，也就没必要备份的必要了。</span><br><span class="line">/tmp：一个临时文件夹，系统的一些临时文件会放在这里。</span><br><span class="line">/lost+found：系统发生错误时（比如非法关机），可以在这里找回一些丢失文件。</span><br><span class="line">/media：多媒体挂载点，像u盘、移动硬盘、windons分区等都会自动挂载到这个目录下。</span><br><span class="line">/mnt：临时挂载点，你可以自己挂载一些文件系统到这里。</span><br><span class="line">/run：系统从启动以来产生的一些信息文件。</span><br><span class="line">/home：用户家目录，存放用户个人文件和应用程序。</span><br><span class="line">/boot：和系统启动相关的文件，像grub相关文件都放在这里，这个目录很重要！</span><br></pre></td></tr></table></figure>
<ul>
<li>备份<code>/root</code>目录<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -cvpzf /media/tongtong/KINGSTON/ubuntu_root_backup@`data +%Y-%m+%d`.tar.gz /root &gt;/dev/null</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>最后还要提一下就是，有可能备份到最后系统会提示”tar: 由于前次错误,将以上次的错误状态退出”，这个警告可以忽略，没什么影响的。</strong></p>
<h2 id="2，系统还原"><a href="#2，系统还原" class="headerlink" title="2，系统还原"></a>2，系统还原</h2><p><a href="https://blog.csdn.net/qq_35523593/article/details/78545530">reference_link</a></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>ubuntu backup</tag>
        <tag>ubuntu recovery</tag>
      </tags>
  </entry>
  <entry>
    <title>docker server on ubuntu20.04</title>
    <url>/2022/05/14/ubuntuOS/docker-server-of-ubuntu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ubuntu中docker server的搭建，也是常用的工程化工具了~</p>
<p><strong>go on~</strong><br><span id="more"></span></p>
<h2 id="1，安装步骤"><a href="#1，安装步骤" class="headerlink" title="1，安装步骤"></a>1，安装步骤</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 添加https源</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-common</span><br><span class="line"></span><br><span class="line">## 导入源仓库的 GPG key</span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line"></span><br><span class="line">## Docker APT源添加到系统</span><br><span class="line">sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;</span><br><span class="line"></span><br><span class="line">## 列出当前的docker版本</span><br><span class="line">sudo apt update</span><br><span class="line">apt list -a docker-ce</span><br><span class="line"></span><br><span class="line">## 安装特定版本docker</span><br><span class="line">sudo apt install docker-ce=&lt;VERSION&gt; docker-ce-cli=&lt;VERSION&gt; containerd.io</span><br><span class="line"></span><br><span class="line">## 阻止Docker自动更新</span><br><span class="line">sudo apt-mark hold docker-ce</span><br></pre></td></tr></table></figure>
<h2 id="2，报错解决"><a href="#2，报错解决" class="headerlink" title="2，报错解决"></a>2，报错解决</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post &quot;http://....&quot;: dial unix /var/run/docker.sock: connect: permission denied.</span><br></pre></td></tr></table></figure>
<p>用“给非root用于添加docker权限”来解决<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo groupadd docker     #添加docker用户组</span><br><span class="line">sudo gpasswd -a $USER docker     #将登陆用户加入到docker用户组中</span><br><span class="line">newgrp docker     #更新用户组</span><br><span class="line">docker ps    #测试docker命令是否可以使用sudo正常使用</span><br></pre></td></tr></table></figure></p>
<h2 id="3，docker镜像查询"><a href="#3，docker镜像查询" class="headerlink" title="3，docker镜像查询"></a>3，docker镜像查询</h2><p><a href="https://hub.docker.com/">official docker hub</a></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>macOS中brew安装</title>
    <url>/2022/05/14/macOS/brew-error-fix/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>macOS中常用brew来安装其他的软件，若brew命令找不到，那就需要自行安装了，详细步骤见下文哦~</p>
<p><strong>MacOS还是不太熟练，加油~</strong><br><span id="more"></span></p>
<h2 id="1，macOS-brew-install"><a href="#1，macOS-brew-install" class="headerlink" title="1，macOS brew install"></a>1，macOS brew install</h2><p>mac环境下，如何解决brew command not found错误，在终端下，执行以下命令，即可安装brew：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">% /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</span><br><span class="line"></span><br><span class="line">% echo &#x27;eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;&#x27; &gt;&gt; /Users/xinwen/.zprofile</span><br><span class="line"></span><br><span class="line">% eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;</span><br></pre></td></tr></table></figure><br>在终端环境下，brew —version 查看brew的版本，也可以验证brew是否安装成功</p>
]]></content>
      <categories>
        <category>macOS</category>
      </categories>
      <tags>
        <tag>M1 Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker on Windows10</title>
    <url>/2022/05/10/tools/win10-WSL2-docker/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>想在win10中使用docker desktop，所以采用了常用的WSL2组件的方式，亲测可用，用docker来配置一系列的开发环境还是比较省心的，若有定制的环境，还可以用dockerfile来构建，避免重复造轮子~</p>
<p><strong>速度杠杠的了~</strong><br><span id="more"></span></p>
<h2 id="1，windows环境"><a href="#1，windows环境" class="headerlink" title="1，windows环境"></a>1，windows环境</h2><p>win10内部版本高于19041，可在电脑属性中查看；<br>找到”启动或者关闭Windows功能”中打开“虚拟机平台”；</p>
<p>下载并安装<a href="https://docs.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package">Linux内核</a></p>
<p>在“启动或者关闭Windows功能”中确认打开“适用于 Linux 的 Windows 子系统”；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 重启系统并设置WSL 2 设置为默认版本</span><br><span class="line">wsl --set-default-version 2</span><br><span class="line"># 查看是不是WSL2</span><br><span class="line">wsl -l -v</span><br></pre></td></tr></table></figure>
<h2 id="2，安装Docker-Desktop-for-windows"><a href="#2，安装Docker-Desktop-for-windows" class="headerlink" title="2，安装Docker Desktop for windows"></a>2，安装Docker Desktop for windows</h2><p>下载<a href="https://www.docker.com/products/docker-desktop/">docker桌面版</a>并安装；</p>
<p>启动Docker Desktop for Windows，点击“设置”按钮，启用基于WSL2的引擎复选框（Use the WSL 2 based engine）；</p>
<h2 id="3，理论啥的"><a href="#3，理论啥的" class="headerlink" title="3，理论啥的"></a>3，理论啥的</h2><p>Docker Desktop for windows方式，其实质是利用docker的C/S架构，将windows模式下的docker对应docker.sock，docker客户端二进制和docker的数据目录挂载到WSL2里面的linux机器，在此linux机器下执行docker命令(docker命令为docker客户端)，实质为客户端通过 挂载的/var/run/docker.sock文件与windows里面的dockerd服务端进程通信。</p>
]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>Win10 docker</tag>
      </tags>
  </entry>
  <entry>
    <title>M1的MacOS系统恢复</title>
    <url>/2022/05/10/macOS/m1-MacOS-Recovery/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>macOS系统恢复出厂设置，2020款的MacPro，M1芯片的~</p>
<p><strong>M1的兼容性还有待各位大神填坑了~</strong><br><span id="more"></span></p>
<h2 id="1，Apple-Silicon-M1-Mac如何恢复出厂设置"><a href="#1，Apple-Silicon-M1-Mac如何恢复出厂设置" class="headerlink" title="1，Apple Silicon M1 Mac如何恢复出厂设置"></a>1，Apple Silicon M1 Mac如何恢复出厂设置</h2><ul>
<li>关闭计算机，然后按住电源按钮。</li>
<li>首次出现Apple徽标时，您会在其下方看到文本，让您知道继续按住它可以访问启动选项。</li>
<li>持续按住按钮约5秒钟，直到文本切换为“正在加载启动选项”。</li>
<li>接下来，单击选项&gt;继续。</li>
<li>选择具有管理员特权的用户，并在询问时输入帐户密码。</li>
</ul>
<h2 id="2，新的恢复工具"><a href="#2，新的恢复工具" class="headerlink" title="2，新的恢复工具"></a>2，新的恢复工具</h2><p>登录用户帐户后，您会看到部分恢复选项列表。</p>
<h3 id="2-1-从Time-Machine还原"><a href="#2-1-从Time-Machine还原" class="headerlink" title="2.1 从Time Machine还原"></a>2.1 从Time Machine还原</h3><p>如果要从以前的Time Machine备份还原Mac，请使用此选项。如果您丢失了许多文件，更改了设置或安装了导致Mac出现严重问题的应用程序，这将很有帮助。</p>
<h3 id="2-2-重新安装MacOS，具体步骤见后文"><a href="#2-2-重新安装MacOS，具体步骤见后文" class="headerlink" title="2.2 重新安装MacOS，具体步骤见后文"></a>2.2 重新安装MacOS，具体步骤见后文</h3><p>如果MacOS出现问题，可以尝试使用此选项重新安装最新版本的MacOS，而不删除任何文件或丢失任何数据。</p>
<h3 id="2-3-Safari"><a href="#2-3-Safari" class="headerlink" title="2.3 Safari"></a>2.3 Safari</h3><p>您可以使用Apple的浏览器搜索并解决如何修复Mac。</p>
<h3 id="2-4-磁盘实用程序"><a href="#2-4-磁盘实用程序" class="headerlink" title="2.4 磁盘实用程序"></a>2.4 磁盘实用程序</h3><p>用于修复，排除硬盘驱动器或对其进行故障排除的工具。</p>
<h2 id="3，擦除硬盘驱动器"><a href="#3，擦除硬盘驱动器" class="headerlink" title="3，擦除硬盘驱动器"></a>3，擦除硬盘驱动器</h2><p>要从硬盘驱动器中完全删除所有信息并重新安装MacOS，请打开“磁盘工具”，然后选择标有Macintosh HD的内部磁盘。单击“擦除”，然后按照提示进行操作。保留卷名称和格式，但作为参考，名称应为“ Macintosh HD”，格式应为AFPS。单击擦除。</p>
<h2 id="4，重新安装MacOS"><a href="#4，重新安装MacOS" class="headerlink" title="4，重新安装MacOS"></a>4，重新安装MacOS</h2><p>登录用户候，从选项列表中选择“重新安装MacOS ”。系统会要求您选择要安装的位置，该位置应为Macintosh HD（如果决定更改，则为硬盘驱动器的任何名称）。<br>然后，您的Mac将下载最新版本的MacOS，进行安装。</p>
]]></content>
      <categories>
        <category>macOS</category>
      </categories>
      <tags>
        <tag>M1 Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Legacy and UEFI mode on computers</title>
    <url>/2022/05/10/hardware/UEFI-and-Legacy-mode-of-PC/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>模式问题在安装系统时遇到比较多，老点的机器大多是MBR模式，以前用的光盘，优盘和恢复系统基本问题都不大。现在新的机器越来越多UEFI的模式了，所以装机时会有些需要注意的点，当然还有二者的混合模式，总而言之，这里只是简单的区别，想彻底了解的朋友还是自行查看更多资料哦~</p>
<span id="more"></span>
<h2 id="1，传统Legacy（MBR）模式"><a href="#1，传统Legacy（MBR）模式" class="headerlink" title="1，传统Legacy（MBR）模式"></a>1，传统Legacy（MBR）模式</h2><ul>
<li>如果机型相对比较旧，要么是Legacy启动模式，要么是Legacy+UEFI混合模式。</li>
<li>这种模式启动流程复杂，耗时长。</li>
<li>如果你是在Legacy模式下安装的系统，也只能在legacy模式下进系统。</li>
<li>Legacy兼容性很好，32位和64位都不在话下</li>
<li>最多只能支持4个主分区，而且只能控制2TB的分区，有一定限制</li>
</ul>
<h2 id="2，UEFI（GPT）模式"><a href="#2，UEFI（GPT）模式" class="headerlink" title="2，UEFI（GPT）模式"></a>2，UEFI（GPT）模式</h2><ul>
<li>Unified Extensible Firmware Interface   </li>
<li>如今大部分新机型的电脑基本都是采用UEFI启动模式。   </li>
<li>这是一种详细描述全新类型接口的标准。这种接口用于操作系统自动从预启动的操作环境，加载到一种操作系统上，从而使开机程序化繁为简，节省时间。   </li>
<li>在UEFI模式下安装的系统，只能用UEFI模式引导</li>
<li>UEFI只支持64位系统且磁盘分区必须为gpt模式</li>
<li>最多能够支持128个分区，最高能够支持18EB的容量</li>
<li>UEFI模式的安全引导功能，能够很好地防止病毒在引导开机时自行加载</li>
</ul>
<h2 id="3，Legacy-UEFI的混合模式"><a href="#3，Legacy-UEFI的混合模式" class="headerlink" title="3，Legacy+UEFI的混合模式"></a>3，Legacy+UEFI的混合模式</h2><h3 id="3-1-安装时报错"><a href="#3-1-安装时报错" class="headerlink" title="3.1 安装时报错"></a>3.1 安装时报错</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">No EFI system partition was found, the system will likely not be able to boot successfully...</span><br></pre></td></tr></table></figure>
<ul>
<li>解决办法<br>在手动分区时新建一个/boot/efi分区，然后将其格式设为efi，剩下的分区就照常设置就好</li>
</ul>
<h3 id="3-2-重装后系统启动报错"><a href="#3-2-重装后系统启动报错" class="headerlink" title="3.2 重装后系统启动报错"></a>3.2 重装后系统启动报错</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">No Boot Device Found. Press any key to reboot the machine.</span><br></pre></td></tr></table></figure>
<ul>
<li><p>问题来源<br>操作系统与当前BIOS模式不匹配。在UEFI模式下安装的系统，只能用UEFI模式引导；同理，如果你是在Legacy模式下安装的系统，也只能在legacy模式下进系统。如果更改过BIOS模式，可能会出现这种情况。</p>
</li>
<li><p>解决办法<br>正常情况下还是可以进入BIOS的，进去后更改Boot mode即可。<br>特殊情况：Default Boot Device Missing or Boot Failed且Boot Manager下无选择项。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>legacy and uefi</tag>
        <tag>PC bios</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo的主要搭建过程</title>
    <url>/2022/05/10/ubuntuOS/hexo-main/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个基本就是简单的搭建示例，对前端小白很友好，也够简单整洁，是喜欢的风格没跑了~</p>
<p><strong>安心码文了~</strong><br><span id="more"></span></p>
<h2 id="1，安装hexo"><a href="#1，安装hexo" class="headerlink" title="1，安装hexo"></a>1，安装hexo</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用 npm 一键安装 Hexo 博客程序</span><br><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo init      # 初始化</span><br><span class="line">npm install    # 安装组件</span><br><span class="line">hexo g   # 生成页面</span><br><span class="line">hexo s   # 启动预览</span><br></pre></td></tr></table></figure>
<p>访问 <a href="http://localhost:4000，出现">http://localhost:4000，出现</a> Hexo 默认页面，本地博客安装成功！</p>
<h2 id="2，连接github"><a href="#2，连接github" class="headerlink" title="2，连接github"></a>2，连接github</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;xinwen&quot;</span><br><span class="line">git config --global user.email &quot;hxinwen1218@sina.com&quot;</span><br><span class="line">ssh-keygen -t rsa -C &quot;hxinwen1218@sina.com&quot;    # 之后一路回车，大概率会在/root/.ssh中生成公钥和私钥</span><br></pre></td></tr></table></figure>
<p>登录github，进入Settings-&gt;SSH and GPG keys-&gt;New SSH key，title随意起，将刚才的公钥(id_rsa.pub)复制进去即可。</p>
<h2 id="3，新建github-Pages仓库"><a href="#3，新建github-Pages仓库" class="headerlink" title="3，新建github Pages仓库"></a>3，新建github Pages仓库</h2><p>新建仓库，注意Repository name，请务必为：用户名.github.io</p>
<p>博客地址：<a href="https://用户名.github.io">https://用户名.github.io</a></p>
<h2 id="4，部署hexo到Github-Pages"><a href="#4，部署hexo到Github-Pages" class="headerlink" title="4，部署hexo到Github Pages"></a>4，部署hexo到Github Pages</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 安装 hexo-deployer-git</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line"></span><br><span class="line"># 然后，修改 _config.yml 文件</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:用户名/用户名.github.io.git</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure>
<p>然后，运行<code>hexo d</code>即可部署到Github Pages.</p>
<h2 id="5，hexo其他信息"><a href="#5，hexo其他信息" class="headerlink" title="5，hexo其他信息"></a>5，hexo其他信息</h2><h3 id="5-1-hexo常用命令"><a href="#5-1-hexo常用命令" class="headerlink" title="5.1 hexo常用命令"></a>5.1 hexo常用命令</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new &quot;postName&quot; #新建文章</span><br><span class="line">hexo new page &quot;pageName&quot; #新建页面</span><br><span class="line">hexo generate #生成静态页面至public目录</span><br><span class="line">hexo server #开启预览访问端口（默认端口4000，&#x27;ctrl + c&#x27;关闭server）</span><br><span class="line">hexo deploy #部署到GitHub</span><br><span class="line">hexo help  # 查看帮助</span><br><span class="line">hexo version  #查看Hexo的版本</span><br></pre></td></tr></table></figure>
<h3 id="5-2-hexo命令缩写"><a href="#5-2-hexo命令缩写" class="headerlink" title="5.2 hexo命令缩写"></a>5.2 hexo命令缩写</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo n == hexo new</span><br><span class="line">hexo g == hexo generate</span><br><span class="line">hexo s == hexo server</span><br><span class="line">hexo d == hexo deploy</span><br></pre></td></tr></table></figure>
<h3 id="5-3-hexo组合命令"><a href="#5-3-hexo组合命令" class="headerlink" title="5.3 hexo组合命令"></a>5.3 hexo组合命令</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo s -g #生成并本地预览</span><br><span class="line">hexo d -g #生成并上传</span><br></pre></td></tr></table></figure>
<h2 id="6，hexo博客文件夹"><a href="#6，hexo博客文件夹" class="headerlink" title="6，hexo博客文件夹"></a>6，hexo博客文件夹</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">_config.yml         #网站的配置信息</span><br><span class="line">package.json        #应用程序的信息</span><br><span class="line">scaffolds           #模板文件夹</span><br><span class="line">source              #存放用户资源，markdown文档</span><br><span class="line">  _drafts</span><br><span class="line">  _posts</span><br><span class="line">themes              #主题文件夹</span><br><span class="line">public              #网站文件</span><br></pre></td></tr></table></figure>
<h2 id="7-参考文章"><a href="#7-参考文章" class="headerlink" title="7 参考文章"></a>7 参考文章</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/60578464#:~:text=%E4%BD%BF%E7%94%A8%20Hexo%2BGitHub%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%85%8D%E8%B4%B9%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B%EF%BC%88%E5%B0%8F%E7%99%BD%E5%90%91%EF%BC%89%201%20%E5%87%86%E5%A4%87%202%20%E8%BF%9E%E6%8E%A5%20Github....,Hexo%20%E5%88%B0%20GitHub%20Pages%206%20%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89....%207%20%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8">使用 Hexo+GitHub 搭建个人免费博客教程</a>    </li>
<li><a href="https://blog.csdn.net/qq_40540975/article/details/124489981">使用Hexo+github搭建个人博客</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/265077468">hexo博客中添加图片</a></li>
<li><a href="https://blog.csdn.net/tuckEnough/article/details/107383201">next主题美化</a></li>
<li><a href="https://github.com/theme-next/hexo-theme-next">next主题github</a></li>
</ul>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu20.04 install and update nodejs</title>
    <url>/2022/05/10/ubuntuOS/ubuntu20.04-install-and-update-nodejs/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在ubuntu系统中安装和更新nodejs，做什么用呢？在搭建自己的博客系统时用到，因为对ubuntu中常用的开发软件比较熟悉，哈哈~~</p>
<p><strong>喜欢自己搭建的博客，努力码文吧~~</strong><br><span id="more"></span></p>
<h2 id="构建和更新命令"><a href="#构建和更新命令" class="headerlink" title="构建和更新命令"></a>构建和更新命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># install on ubuntu20.04</span><br><span class="line">apt-get install -y nodejs npm</span><br><span class="line"># upgrade</span><br><span class="line">node -v</span><br><span class="line">npm install n -g</span><br><span class="line">n stable</span><br><span class="line">hash -r</span><br><span class="line"># 3, softlink update</span><br><span class="line">cp /usr/local/bin/node /usr/bin/node</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>nodejs &amp; npm</tag>
      </tags>
  </entry>
  <entry>
    <title>jekyll on ubuntu20.04</title>
    <url>/2022/05/10/ubuntuOS/jekyll-on-ubuntu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>还是在自建博客过程中发现的需求之一，看帖子说现在很多人用jekyll建博客，但是我这边在部署到github的时候出现了一堆问题，还是递归的出现…就是在解决问题时，会冒出另一个~所以果断转战到hexo了，下面是初步尝试环境的搭建~   </p>
<p>说明：在docker中，使用的都是root权限，所以一般用户自行添加sudo来运行命令</p>
<p><strong>还是很弱啊，要加油~</strong><br><span id="more"></span></p>
<h2 id="1，详细步骤"><a href="#1，详细步骤" class="headerlink" title="1，详细步骤"></a>1，详细步骤</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># jekyll依赖</span><br><span class="line">    apt-get install ruby-full build-essential zlib1g-dev</span><br><span class="line"># 环境变量</span><br><span class="line">    echo &#x27;# Install Ruby Gems to ~/gems&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">    echo &#x27;export GEM_HOME=&quot;$HOME/gems&quot;&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">    echo &#x27;export PATH=&quot;$HOME/gems/bin:$PATH&quot;&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">    source ~/.bashrc</span><br><span class="line"># 移除gem默认源，改成ruby-china源</span><br><span class="line">    gem sources -r https://rubygems.org/ -a https://gems.ruby-china.com/</span><br><span class="line"># 安装jekyll</span><br><span class="line">    gem install jekyll bundler</span><br><span class="line"># 使用Gemfile和Bundle的项目，可以做下面修改，就不用修改Gemfile的source</span><br><span class="line">    bundle config mirror.https://rubygems.org https://gems.ruby-china.com</span><br><span class="line"># 删除Bundle的一个镜像源</span><br><span class="line">    bundle config --delete &#x27;mirror.https://rubygems.org&#x27;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu20.04 change its source</title>
    <url>/2022/05/09/ubuntuOS/ubuntu20.04-change-its-source/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ubuntu官方源由于众所周知的原因，更新啊、安装软件啊都比较慢，甚至经常就卡住不动了，所以换源还是个很基础的操作滴，以前感觉“阿里云”好用，最近在ubuntu20.04上测试，莫名多了些问题，切换到“清华”源，丝滑般流畅啊~~</p>
<p><strong>相当于基础平A操作了，没有花样，就是好用~</strong></p>
<span id="more"></span>
<h2 id="1，ubuntu换源操作"><a href="#1，ubuntu换源操作" class="headerlink" title="1，ubuntu换源操作"></a>1，ubuntu换源操作</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /etc/apt/source.list /etc/apt/source.list.bak</span><br><span class="line">vim /etc/apt/source.list</span><br><span class="line"># 删除原有source，换成下面的源</span><br><span class="line">apt-get update</span><br><span class="line">apt-get upgrade</span><br></pre></td></tr></table></figure>
<h2 id="2，可用源"><a href="#2，可用源" class="headerlink" title="2，可用源"></a>2，可用源</h2><p>可自行选用，亲测可用：清华源<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#清华源</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#添加阿里源</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#中科大源</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line"> </span><br><span class="line">#163源</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>ubuntu source.list</tag>
      </tags>
  </entry>
</search>
