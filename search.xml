<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Transformers from an Optimization Perspective</title>
    <url>/2023/05/05/Transformer/Transformers%20from%20an%20Optimization%20perspective/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2>]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>A Mathematical Framework for Transformer Circuits</title>
    <url>/2023/04/27/Transformer/A%20Mathematical%20Framework%20for%20Transformer%20Circuits/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><a href="https://transformer-circuits.pub/2021/framework/index.html">link</a></p>
<p><strong>transformerd的数学框架？</strong><br><span id="more"></span></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Transformer语言模型在新兴产业中的使用越来越广泛，比如GPT-3，LaMDA，Codex，Meena，Gopher和其他类似模型。<br>然而，随着这些模型的扩展，它们的开放性和高容量为意想不到的甚至有害的行为创造了越来越大的范围。 即使在大型模型经过训练多年后，创建者和用户仍会定期发现他们以前没有意识到的模型功能（包括有问题的行为）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">【GPT-3】</span><br><span class="line">Language models are few-shot learners  [PDF]</span><br><span class="line">T.B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, others.</span><br><span class="line">arXiv preprint arXiv:2005.14165. 2020.</span><br><span class="line">【LaMDA】</span><br><span class="line">LaMDA: our breakthrough conversation technology  [link]</span><br><span class="line">E. Collins, Z. Ghahramani. 2021.</span><br><span class="line">【Codex】</span><br><span class="line">Evaluating large language models trained on code</span><br><span class="line">M. Chen, J. Tworek, H. Jun, Q. Yuan, H.P.d.O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, others.</span><br><span class="line">arXiv preprint arXiv:2107.03374. 2021.</span><br><span class="line">【Meena】</span><br><span class="line">Towards a human-like open-domain chatbot</span><br><span class="line">D. Adiwardana, M. Luong, D.R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, others.</span><br><span class="line">arXiv preprint arXiv:2001.09977. 2020.</span><br><span class="line">【Gopher】</span><br><span class="line">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher  [PDF]</span><br><span class="line">J.W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G.v.d. Driessche, L.A. Hendricks, M. Rauh, P. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X.L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C.d.M. d’Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D.d.L. Casas, A. Guy, C. Jones, J. Bradbury, M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, G. Irving.</span><br><span class="line">Preprint. 2021.</span><br></pre></td></tr></table></figure>
<p>解决这些问题的一种途径是寻求可解释性，试图对Transformer模型的详细计算进行逆向工程，类似于程序员将复杂的二进制文件反编译为人类可读的源代码。 如果这是可能的，它可能会提供一种更系统的方法来解释当前的安全问题、识别新的问题，甚至可能预测尚未构建的强大的未来模型的安全问题。 之前的一个项目，Distill Circuits, 曾尝试对视觉模型进行逆向工程，但到目前为止还没有类似的Tranformer或语言模型项目。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Thread: Circuits</span><br><span class="line">N. Cammarata, S. Carter, G. Goh, C. Olah, M. Petrov, L. Schubert, C. Voss, B. Egan, S.K. Lim.</span><br><span class="line">Distill. 2020.</span><br><span class="line">DOI: 10.23915/distill.00024</span><br></pre></td></tr></table></figure>
<p>在本文中，我们尝试采取原始、初级的步骤来对Transformer模型进行逆向工程，因为现代语言模型令人难以置信的复杂性和规模，我们发现从最简单的模型开始并从那里开始工作是最有成效的。 我们的目标是发现简单的算法模式、主题或框架，这些模式、主题或框架随后可以应用于更大、更复杂的模型。 具体来说，在本文中，我们将研究只有两层或更少层且只有注意力块的Transformer——这与像 GPT-3 这样的大型现代Transformer形成对比，GPT-3有 96 层，且注意力块与 MLP 块交替出现。</p>
<p>我们发现，通过以一种新的但在数学上等效的方式将Transformer的操作概念化，我们能够理解这些小模型，并对它们的内部运作方式获得重要的理解。 特别值得注意的是，我们发现我们称之为“感应头”的特定注意力头可以解释这些小模型中的上下文学习，并且这些头只在至少具有两个注意力层的模型中出现。 我们还介绍了对特定数据进行操作的一些示例。</p>
<p>在第一篇论文中，我们并未尝试将我们的见解应用于更大的模型，但在即将发表的一篇论文中，我们将证明我们理解Transformer的数学框架和感应头的概念仍然至少部分相关，对于更大、更真实的模型——尽管我们距离能够完全逆向工程这些模型还有很长的路要走。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="REVERSE-ENGINEERING-RESULTS"><a href="#REVERSE-ENGINEERING-RESULTS" class="headerlink" title="REVERSE ENGINEERING RESULTS"></a>REVERSE ENGINEERING RESULTS</h3><p>为了挑战Tranformer的逆向工程，我们先在其他算法和仅有注意力的模型上进行了尝试，有以下结论：</p>
<h3 id="CONCEPTUAL-TAKE-AWAYS"><a href="#CONCEPTUAL-TAKE-AWAYS" class="headerlink" title="CONCEPTUAL TAKE-AWAYS"></a>CONCEPTUAL TAKE-AWAYS</h3><h2 id="Transformer回顾"><a href="#Transformer回顾" class="headerlink" title="Transformer回顾"></a>Transformer回顾</h2>]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>Choose a Transformer-Fourier or Galerkin</title>
    <url>/2023/04/27/Transformer/Choose%20a%20Transformer-Fourier%20or%20Galerkin/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><a href="https://arxiv.org/pdf/2105.14995.pdf">paper link</a></p>
<p><strong>在算子学习的角度，将Transformer用在PDE的求解上</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>原文翻译</strong><br>在本文中，我们首次将 Attention Is All You Need [88] 中的自注意力机制应用于与偏微分方程相关的数据驱动算子学习问题。 努力解释启发式，并提高注意机制的功效。 通过在希尔伯特空间中使用算子逼近理论，首次证明了缩放点积注意力中的 softmax 归一化是充分的，但不是必需的。 在没有 softmax 的情况下，线性化 Transformer 变体的近似能力可以证明与 Petrov-Galerkin 投影分层相当，并且其估计与序列长度无关。 提出了一种模仿 Petrov-Galerkin 投影的新层归一化方案，以允许缩放通过注意力层传播，这有助于模型在使用非归一化数据的算子学习任务中实现显着的准确性。 最后，我们提出了三个算子学习实验，包括粘性 Burgers 方程、界面 Darcy 流和逆界面系数识别问题。 新提出的简单且基于注意力的算子学习器 Galerkin Transformer 与其 带softmax 归一化的相应模型相比，在训练成本和评估准确性方面都有显着改进。</p>
<p><strong>主要做了三个方面的事情：</strong></p>
<ul>
<li>尝试解释self-attention，提升其运算效率；</li>
<li>softmax的作用和存在的必要性；</li>
<li>层归一化的新替换方案；</li>
</ul>
<h2 id="一，介绍"><a href="#一，介绍" class="headerlink" title="一，介绍"></a>一，介绍</h2><p>偏微分方程 (PDE) 几乎来自每个多物理场和生物系统，从原子的相互作用到星系的合并，从细胞的形成到气候的变化。 几个世纪以来，科学家和工程师一直致力于逼近这些物理系统的控制PDE，计算机辅助模拟的出现为研究这些具有挑战性的问题提供了一种成本友好的方法。 传统方法，如有限元/差分法 [20、22]、谱方法 [12] 等，利用离散结构将无限维算子映射简化为有限维近似问题。 同时，在许多科学实践中，离散网格上可用的、PDE 控制现象的大量数据使现代黑盒模型（如物理信息神经网络（PINN）[71、62、49]）能够利用配置点上的测量 近似 PDE 解。</p>
<p>尽管如此，对于传统方法或数据驱动的函数学习器（如 PINN），给定 PDE，重点是逼近单个实例，例如，求解具有固定边界条件的某个系数的近似解，这个系数的微小变化会让数据驱动的函数学习器需要进行昂贵的再训练。 相比之下，算子学习者的目标是学习无限维函数空间之间的映射，这要困难得多但也有回报。 一个训练有素的算子学习者可以在没有重新训练或配置点的情况下评估许多实例，从而节省宝贵的资源，并且从长远来看将自己定位为更有效的方法。 数据驱动的分辨率不变算子学习是一个蓬勃发展的新研究方向 [60, 5, 56, 64, 90, 57, 61, 91, 37, 74]，开创性模型 DeepONet [60] 在架构上归因于运算符 [18] 的通用逼近定理。傅立叶神经算子 (FNO) [57]在某些基准测试中，展示了一种令人敬畏的最先进的性能，比 [100] 中的经典模型好几个数量级。</p>
<p>在监督学习下，算子学习者接受算子的输入函数及其对输入的响应作为目标的训练，由于两个函数都是在离散网格点上采样的，因此这是 seq2seq 问题的特例 [81]。 当前最先进的 seq2seq 模型是在 [88] 中首次引入的 Transformer。 作为 Transformer 的核心和灵魂，缩放点积注意力机制能够通过捕获远程交互信息来挖掘算子的隐藏结构。 受到 Transformers [50、19、75、84、96、97、95、59、76、66] 中许多富有洞察力的开创性工作的启发，我们以数学上深刻的方式对注意力机制进行了最低限度的修改，以更好地服务于算子学习。</p>
<p>在我们对缩放点积注意力在希尔伯特空间中的改编中，第一个也是最重要的变化是：没有 softmax 或其近似值。 在 vanilla attention [88] 中，矩阵乘法之后的 softmax 凸化了组合不同位置潜在表示的权重，这被认为是注意力机制正核解释中不可或缺的组成部分 [84]。 然而，softmax 全局作用于注意力矩阵的每一行的序列长度维度，进一步增加了经典 Transformer 中注意力的二次复杂度。 从理论上讲，与自然语言处理 (NLP) 传统中的“一行≈一词”不同，查询、键、值的列被视为离散网格上希尔伯特空间中的函数采样。 因此，去掉 softmax 允许我们验证离散的 Ladyzhenskaya–Babuška–Brezzi (LBB) 条件，这进一步证明了新提出的 Galerkin 类型的注意力可以明确地表示 Petrov-Galerkin 投影，并且这种近似能力是与序列长度无关（定理 4.3）。</p>
<p>在数值上，无 softmax 模型节省了宝贵的计算资源，在训练 FLOP 和内存消耗方面优于使用 softmax 的模型（第 5 节）。 然而在消融研究中，无 softmax 模型的训练变得不稳定（表 8），为了解决这个问题，提出了一种新的 Galerkin 投影类型层归一化方案，作为在 Petrov-Galerkin 解释（等式（40））证明中明确导出的归一化廉价对角替代方案。 由于现在可以通过编码器层传播可学习的缩放比例，因此具有这种新层归一化方案的基于注意力的算子学习器表现出对与 PDE 相关的某些物理特性（例如能量衰减）的更好理解。 结合其他受近似理论启发的技巧，包括投影矩阵的对角占优重新缩放初始化和位置编码的逐层丰富，各种算子学习任务的评估精度得到显着提高。</p>
<h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul>
<li><p>没有softmax的注意力<br>我们提出了一种新的简单自注意算子及其没有 softmax 归一化的线性变体。 提供了两种新的解释，以及证明与 Petrov-Galerkin 投影相当的线性变体的近似能力。</p>
</li>
<li><p>PDE的算子学习<br>我们将新提出的注意力算子与当前最先进的算子学习器傅立叶神经算子 (FNO) [57] 相结合，以显著提高其在 PDE 算子学习基准问题中的评估准确性。 此外，新模型能够根据传统方法或 FNO 无法完成的噪声测量恢复系数。</p>
</li>
<li><p>实验结果<br>我们提出了三个基准问题，以表明使用新提出的注意力机制的算子学习器在计算、内存效率以及准确性方面优于传统的 softmax 归一化。 用于重现我们结果的 PyTorch 代码可作为开源软件使用。</p>
</li>
</ul>
<h2 id="二，相关工作"><a href="#二，相关工作" class="headerlink" title="二，相关工作"></a>二，相关工作</h2><h3 id="PDE相关的算子学习"><a href="#PDE相关的算子学习" class="headerlink" title="PDE相关的算子学习"></a>PDE相关的算子学习</h3><p>在 [4, 5] 中，参数 PDE 的解算子的某些核形式是使用图神经网络来近似的。 另一个值得注意的并发方法是 DeepONet [60、61]。 [56] 通过利用多级网格结构进一步改进了内核方法。 [57] 提出了一种离散化不变的运算符学习器，以在某些基准问题中实现最先进的性能。 [90, 91] 提出了一种大致等同于附加注意力的 DeepONet，类似于 [7] 中的神经图灵机 (NMT)。 模型/降维与神经网络相结合是另一种流行的学习参数 PDE 解算子的方法 [10、64、55、24]。 深度卷积神经网络 (DCNN) 被广泛应用于学习具有固定离散化大小的解图 [1、9、40、36、35、100、86]。 最近，DCNN 已成功应用于各种反问题 [35, 47]，例如电阻抗层析成像 (EIT)。 据我们所知，对于一类具有随机界面几何形状的系数，还没有关于数据驱动方法的反界面系数识别的工作。</p>
<h3 id="attention机制和变体"><a href="#attention机制和变体" class="headerlink" title="attention机制和变体"></a>attention机制和变体</h3><p>除了 [88] 中开创性的缩放点积注意力之外，早期的 [7] 提出了一种基于附加内容的注意力，然而，由于多重非线性组合，梯度消失问题。 [25] 展示了在投影后移除 [7] 中的 softmax 归一化的第一个努力，然而，它在加性插值传播阶段之前仍然使用 Sigmoid 非线性，并且表现比它的 softmax 对应物差。 当前流行的将注意力线性化的方法利用特征映射的存在假设来近似 softmax 内核 [50、19、70]。 另一种类型的线性化利用矩阵乘积的低阶特性，使用各种方法，例如采样或投影 [73、11、79、92] 或快速多极分解 [65]。 [75] 中的猜想启发我们移除 softmax 整体。 [76] 首先提出了在没有 softmax 的情况下针对线性复杂度注意力的逆序列长度缩放归一化，但是，缩放归一化尚未在示例中得到广泛研究并且表现更差。</p>
<h3 id="Transformer的变量学习"><a href="#Transformer的变量学习" class="headerlink" title="Transformer的变量学习"></a>Transformer的变量学习</h3><p>[84] 中的内核解释启发我们使用 Galerkin 投影重新表述注意力。 [95，定理 2] 给出了去除 softmax 归一化以制定傅里叶型注意力的理论基础。 Nyström 近似 [97] 本质上承认注意力矩阵和积分核之间的相似性。 [96, 66, 59] 启发我们尝试不同的层归一化和重新缩放的对角占优初始化方案。 在我们的工作中反复使用位置编码来丰富潜在表示的做法可以追溯到 [2, 26]，最近，有助于 AlphaFold 2 [48] 的成功，因为如果 目标在坐标系和/或变换组中具有依赖性 ansatz，但难以明确量化。 其他关于调整注意机制以保存重要物理特性的研究在 [82, 31, 44] 中。</p>
<h2 id="三，PDE的算子学习"><a href="#三，PDE的算子学习" class="headerlink" title="三，PDE的算子学习"></a>三，PDE的算子学习</h2>]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>seven concepts in designing and analysising of algoritms</title>
    <url>/2023/04/25/math/seven_concepts_algorithms/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>design and analysis of algorithms can be broadly achieved with the help of seven concepts</p>
<p>《Numerical Methods for Algorithms Systems and Neural Networks》<br><a href="https://www.ifam.uni-hannover.de/en">reference link</a><br><a href="http://dx.doi.org/10.15488/11897">doi</a></p>
<p><strong>从七个要素来看算法的设计和分析……</strong><br><span id="more"></span></p>
<h2 id="算法定义"><a href="#算法定义" class="headerlink" title="算法定义"></a>算法定义</h2><p>算法是一系列指令，有一个对严谨数学问题的解决方案，其主要目的就是制定一个可以在计算机中实施的方案，可以进行所谓的数值模拟。 </p>
<p>直接求解方法对给定问题求解直到要求的舍入误差（例如高斯消除）。 迭代求解会逼近到一定精度（例如，用于求解线性方程系统的Richardson迭代、不动点迭代、梯度下降、牛顿法……）。 </p>
<p>算法在准确性、稳健性和效率方面各不相同。</p>
<h2 id="1-Approximation"><a href="#1-Approximation" class="headerlink" title="1, Approximation"></a>1, Approximation</h2><p>由于正如我们在上一节中刚刚了解到的那样，通常无法获得解析解，因此通过数值近似获得解。</p>
<h2 id="2-Convergence"><a href="#2-Convergence" class="headerlink" title="2, Convergence"></a>2, Convergence</h2><p>收敛是一个定性表达式，它告诉我们序列 $(a_n)_{n\in N}$ 的成员 $a_n$ 何时足够接近极限 $a$。 在数值数学中，这个极限通常是我们正在寻找的解决方案。</p>
<h2 id="3-Order-of-convergence"><a href="#3-Order-of-convergence" class="headerlink" title="3, Order of convergence"></a>3, Order of convergence</h2><p>在分析中，我们通常对收敛本身感兴趣，但在数值数学中，我们必须注意数值解具有足够精度所需的时间。 模拟时间越长，消耗的时间和能源（运行计算机的电力、服务器的空调等）就越多。 为了判断一个算法是否快，我们必须确定收敛的顺序。</p>
<h2 id="4-Error"><a href="#4-Error" class="headerlink" title="4, Error"></a>4, Error</h2><p>数值数学可以被认为是“误差数学”的分支。 </p>
<p>这是什么意思？ 并不是说数值建模错误、不准确或不精确！ 由于我们在最后几步之后切割序列或接受从我们的软件获得的足够准确的解决方案，我们需要说明这个数值解到精确解的近似程度。 换句话说，我们需要确定误差，而误差可能以各种形式出现。</p>
<h2 id="5-Error-Estimation"><a href="#5-Error-Estimation" class="headerlink" title="5, Error Estimation"></a>5, Error Estimation</h2><p>这是数值数学中最大的分支之一。 我们需要导出误差公式来判断数值模拟的结果，并衡量数值解与（未知）精确解在某个范数下的差异。</p>
<h2 id="6-Efficiency"><a href="#6-Efficiency" class="headerlink" title="6, Efficiency"></a>6, Efficiency</h2><p>一般来说，我们可以说，算法的收敛阶数越高，算法的效率就越高。 因此，我们可以更快地获得给定问题的数值解。 但是数值效率并不自动与资源有效计算相关。 例如，使用 MPI（消息传递接口）、硬件优化（CPU、GPU）、软件优化（以某种最佳方式排序 for 循环、算术评估等）开发并行代码可以进一步降低计算成本。</p>
<h2 id="7-Stability"><a href="#7-Stability" class="headerlink" title="7, Stability"></a>7, Stability</h2><p>最后，必须研究算法和实现在参数（模型、材料、数值）变化、边界条件、初始条件、不确定性方面的稳健性。 稳定性在最广泛的意义上与阿达玛的第三个条件相关。</p>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title>General datasets for deep learning in CV</title>
    <url>/2023/03/23/CNN/datasets/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通用检测任务的常用数据集，包含VOC，COCO，ImageNet，不断更新其他分支任务的数据集。</p>
<p><strong>big data is all you need</strong><br><span id="more"></span></p>
<h2 id="一，PASCAL-VOC"><a href="#一，PASCAL-VOC" class="headerlink" title="一，PASCAL VOC"></a>一，PASCAL VOC</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>由Mark Everingham (University ofLeeds)、Luc van Gool (ETHZ, Zurich)等人创立，有1.7W+张图片，分为20类。</p>
<p>PASCAL VOC挑战赛是计算机视觉竞赛的鼻祖，从2005年到2012年一共举办了8届，其任务涵盖：目标分类，目标检测，分割，人体部位，动作识别。</p>
<p>VOC图片集包括20个类别：人类;动物(鸟、猫、牛、狗、马、羊);交通工具(飞机、自行车、船、公共汽车、小轿车、摩托车、火车);室内(瓶子、椅子、餐桌、盆栽植物、沙发、电视)。</p>
<p><a href="http://host.robots.ox.ac.uk/pascal/VOC/">official download</a></p>
<h3 id="1-2-组织结构"><a href="#1-2-组织结构" class="headerlink" title="1.2 组织结构"></a>1.2 组织结构</h3><p>以 VOC 2007 为例，解压后的文件为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── Annotations 进行 detection 任务时的标签文件，xml 形式，文件名与图片名一一对应</span><br><span class="line">├── ImageSets 包含三个子文件夹 Layout、Main、Segmentation，其中 Main 存放的是分类和检测的数据集分割文件</span><br><span class="line">├── JPEGImages 存放 .jpg 格式的图片文件</span><br><span class="line">├── SegmentationClass 存放按照 class 分割的图片</span><br><span class="line">└── SegmentationObject 存放按照 object 分割的图片</span><br><span class="line"></span><br><span class="line">├── Main</span><br><span class="line">│   ├── train.txt 写着用于训练的图片名称， 共 2501 个</span><br><span class="line">│   ├── val.txt 写着用于验证的图片名称，共 2510 个</span><br><span class="line">│   ├── trainval.txt train与val的合集。共 5011 个</span><br><span class="line">│   ├── test.txt 写着用于测试的图片名称，共 4952 个</span><br></pre></td></tr></table></figure>
<h3 id="1-3-数据集xml文件标注格式"><a href="#1-3-数据集xml文件标注格式" class="headerlink" title="1.3 数据集xml文件标注格式"></a>1.3 数据集xml文件标注格式</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;annotation&gt;</span><br><span class="line">  &lt;folder&gt;17&lt;/folder&gt; # 图片所处文件夹</span><br><span class="line">  &lt;filename&gt;77258.bmp&lt;/filename&gt; # 图片名</span><br><span class="line">  &lt;path&gt;~/frcnn-image/61/ADAS/image/frcnn-image/17/77258.bmp&lt;/path&gt;</span><br><span class="line">  &lt;source&gt;  #图片来源相关信息</span><br><span class="line">    &lt;database&gt;Unknown&lt;/database&gt;  </span><br><span class="line">  &lt;/source&gt;</span><br><span class="line">  &lt;size&gt; #图片尺寸</span><br><span class="line">    &lt;width&gt;640&lt;/width&gt;</span><br><span class="line">    &lt;height&gt;480&lt;/height&gt;</span><br><span class="line">    &lt;depth&gt;3&lt;/depth&gt;</span><br><span class="line">  &lt;/size&gt;</span><br><span class="line">  &lt;segmented&gt;0&lt;/segmented&gt;  #是否有分割label</span><br><span class="line">  &lt;object&gt; 包含的物体</span><br><span class="line">    &lt;name&gt;car&lt;/name&gt;  #物体类别</span><br><span class="line">    &lt;pose&gt;Unspecified&lt;/pose&gt;  #物体的姿态</span><br><span class="line">    &lt;truncated&gt;0&lt;/truncated&gt;  #物体是否被部分遮挡（&gt;15%）</span><br><span class="line">    &lt;difficult&gt;0&lt;/difficult&gt;  #是否为难以辨识的物体， 主要指要结体背景才能判断出类别的物体。虽有标注， 但一般忽略这类物体</span><br><span class="line">    &lt;bndbox&gt;  #物体的bound box</span><br><span class="line">      &lt;xmin&gt;2&lt;/xmin&gt;     #左</span><br><span class="line">      &lt;ymin&gt;156&lt;/ymin&gt;   #上</span><br><span class="line">      &lt;xmax&gt;111&lt;/xmax&gt;   #右</span><br><span class="line">      &lt;ymax&gt;259&lt;/ymax&gt;   #下</span><br><span class="line">    &lt;/bndbox&gt;</span><br><span class="line">  &lt;/object&gt;</span><br><span class="line">&lt;/annotation&gt;</span><br></pre></td></tr></table></figure>
<h2 id="二，COCO"><a href="#二，COCO" class="headerlink" title="二，COCO"></a>二，COCO</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>全称是Microsoft Common Objects in Context，起源于微软于2014年出资标注的Microsoft COCO数据集，与ImageNet竞赛一样，被视为是计算机视觉领域最受关注和最权威的比赛之一。</p>
<p>COCO数据集是一个大型的、丰富的<strong>物体检测，分割和图像描述</strong>数据集。这个数据集以情景理解为目标，主要从复杂的日常场景中截取，图像中的目标通过精确的分割进行位置的标定。该数据集主要解决3个问题：<strong>目标检测</strong>，<strong>目标之间的上下文关系</strong>，<strong>目标的二维上的精确定位</strong>。COCO数据集有91类，虽然比ImageNet类别少，但是每一类的图像多，这有利于获得更多的每类中位于某种特定场景的能力，对比PASCAL VOC，其有更多类和图像。</p>
<p>COCO数据集包含20万个图像，其中80个类别（80个对象类别是stuff91类的子集，如果仅仅是做目标检测，基本只用80类即可。）中有超过50万个目标标注,它是最广泛公开的目标检测数据库，平均每个图像的目标数为7.2。</p>
<p><a href="http://cocodataset.org">website</a></p>
<h3 id="2-2-特点"><a href="#2-2-特点" class="headerlink" title="2.2 特点"></a>2.2 特点</h3><ul>
<li>目标集分割；</li>
<li>图像情景识别；</li>
<li>超像素分割；</li>
<li>330K图像（&gt; 200K标记）；</li>
<li>150万个对象实例；</li>
<li>80个对象类别；</li>
<li>91个stuff类别；</li>
<li>每张图片有5段情景描述；</li>
<li>有关键点的250,000人；</li>
</ul>
<p>三种标注类型：</p>
<ul>
<li>object instances（目标实例）</li>
<li>object keypoints（目标上的关键点）</li>
<li>image captions（看图说话）</li>
</ul>
<h3 id="2-3-组织结构"><a href="#2-3-组织结构" class="headerlink" title="2.3 组织结构"></a>2.3 组织结构</h3><p>以coco2017为例，解压后的文件为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── train2017 所有训练图像文件夹（118287张）</span><br><span class="line">├── val2017  所有验证图像文件夹（5000张）</span><br><span class="line">├── annotations 对应标注文件夹</span><br><span class="line">│   ├── instances_train2017.json 对应目标检测、分割任务的训练集标注文件</span><br><span class="line">│   ├── instances_val2017.json 对应目标检测、分割任务的验证集标注文件</span><br><span class="line">│   ├── captions_train2017.json 对应图像描述的训练集标注文件</span><br><span class="line">│   ├── captions_val2017.json 对应图像描述的验证集标注文件</span><br><span class="line">│   ├── person_keypoints_train2017.json 对应人体关键点检测的训练集标注文件</span><br><span class="line">│   ├── person_keypoints_val2017.json 对应人体关键点检测的验证集标注文件</span><br></pre></td></tr></table></figure>
<h3 id="2-4-标注文件格式"><a href="#2-4-标注文件格式" class="headerlink" title="2.4 标注文件格式"></a>2.4 标注文件格式</h3><p>以Object Instance为例，其json文件为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;info&quot;: info,               # dict</span><br><span class="line">    &quot;licenses&quot;: [license],      # list,内部是dict</span><br><span class="line">    &quot;images&quot;: [image],          # list,内部是dict</span><br><span class="line">    &quot;annotations&quot;: [annotation],# list,内部是dict</span><br><span class="line">    &quot;categories&quot;: [category]    # list,内部是dict</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">info&#123;                           # 数据集信息描述</span><br><span class="line">    &quot;year&quot;: int,                # 数据集年份</span><br><span class="line">    &quot;version&quot;: str,             # 数据集版本</span><br><span class="line">    &quot;description&quot;: str,         # 数据集描述</span><br><span class="line">    &quot;contributor&quot;: str,         # 数据集提供者</span><br><span class="line">    &quot;url&quot;: str,                 # 数据集下载链接</span><br><span class="line">    &quot;date_created&quot;: datetime,   # 数据集创建日期</span><br><span class="line">&#125;</span><br><span class="line">license&#123;</span><br><span class="line">    &quot;id&quot;: int,</span><br><span class="line">    &quot;name&quot;: str,</span><br><span class="line">    &quot;url&quot;: str,</span><br><span class="line">&#125; </span><br><span class="line">image&#123;      # images是一个list,存放所有图片(dict)信息。image是一个dict,存放单张图片信息 </span><br><span class="line">    &quot;id&quot;: int,                  # 图片的ID编号（每张图片ID唯一）</span><br><span class="line">    &quot;width&quot;: int,               # 图片宽</span><br><span class="line">    &quot;height&quot;: int,              # 图片高</span><br><span class="line">    &quot;file_name&quot;: str,           # 图片名字</span><br><span class="line">    &quot;license&quot;: int,             # 协议</span><br><span class="line">    &quot;flickr_url&quot;: str,          # flickr链接地址</span><br><span class="line">    &quot;coco_url&quot;: str,            # 网络连接地址</span><br><span class="line">    &quot;date_captured&quot;: datetime,  # 数据集获取日期</span><br><span class="line">&#125;</span><br><span class="line">annotation&#123; # annotations是一个list,存放所有标注(dict)信息。annotation是一个dict,存放单个目标标注信息。</span><br><span class="line">    &quot;id&quot;: int,                  # 目标对象ID（每个对象ID唯一），每张图片可能有多个目标</span><br><span class="line">    &quot;image_id&quot;: int,            # 对应图片ID</span><br><span class="line">    &quot;category_id&quot;: int,         # 对应类别ID，与categories中的ID对应</span><br><span class="line">    &quot;segmentation&quot;: RLE or [polygon],   # 实例分割，对象的边界点坐标[x1,y1,x2,y2,....,xn,yn]</span><br><span class="line">    &quot;area&quot;: float,              # 对象区域面积</span><br><span class="line">    &quot;bbox&quot;: [xmin,ymin,width,height], # 目标检测，对象定位边框[x,y,w,h]</span><br><span class="line">    &quot;iscrowd&quot;: 0 or 1,          # 表示是否是人群</span><br><span class="line">&#125;</span><br><span class="line">categories&#123;                     # 类别描述</span><br><span class="line">    &quot;id&quot;: int,                  # 类别对应的ID（0默认为背景）</span><br><span class="line">    &quot;name&quot;: str,                # 子类别名字</span><br><span class="line">    &quot;supercategory&quot;: str,       # 主类别名字</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="三，ImageNet"><a href="#三，ImageNet" class="headerlink" title="三，ImageNet"></a>三，ImageNet</h2><h3 id="3-1-简介"><a href="#3-1-简介" class="headerlink" title="3.1 简介"></a>3.1 简介</h3><p>ImageNet是目前深度学习图像领域应用得非常多的一个图像集，由斯坦福大学李飞飞创立，有1400W+张样例图片，分为27大类和2W+小类，只能用于非商业研究和教学使用。与ImageNet图像集相应的是著名的ILSVRC竞赛，各种新机器学习算法脱颖而出（AlexNet、ZFNet、GoogleNet、ResNet、…），图像识别率得以显著提高，在ILSVRC竞赛上一举成名是近几年来计算机视觉从业者的梦想。</p>
<p>对于如基于ImageNet的图像识别的结果评估，往往用到两个准确率的指标，一个是top-1准确率，一个是top-5准确率。Top-1准确率指的是输出概率中最大的那一个对应的是正确类别的概率；top-5准确率指的是输出概率中最大的5个对应的5个类别中包含了正确类别的概率。</p>
<p><a href="http://www.image-net.org/download-imageurls">official download</a></p>
<h3 id="3-2-特点"><a href="#3-2-特点" class="headerlink" title="3.2 特点"></a>3.2 特点</h3><ul>
<li>ImageNet拥有用于分类、定位和检测任务评估的数据。 </li>
<li>与分类数据类似，定位任务有1000个类别。准确率是根据最高五项检测结果计算出来的。 </li>
<li>所有图像中至少有一个边框。对200个目标的检测问题有470000个图像，平均每个图像有1.1个目标。</li>
</ul>
]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>datasets</tag>
      </tags>
  </entry>
  <entry>
    <title>General activate functions</title>
    <url>/2023/03/23/CNN/activation%20functions/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>激活函数汇总对比~</p>
<p><strong>随时加更中~</strong><br><span id="more"></span></p>
<h2 id="一，定义"><a href="#一，定义" class="headerlink" title="一，定义"></a>一，定义</h2><p>激活函数（又叫激励函数）是模型整个结构中的非线性扭曲力，神经网络的每层都会有一个激活函数。</p>
<p>激活函数的主要作用是提供网络的非线性建模能力，如果没有激活函数，那么神经网络只能表达线性映射，此刻即便是有再多的隐藏层，其整个网络跟单层神经网络也是等价的。因此可以说，只有加入了激活函数之后，深度神经网络才具备了分层的非线性的学习能力。</p>
<p>常用的激活函数：Sigmoid函数、tanh函数、Relu函数、Leaky ReLU函数、ELU (Exponential Linear Units) 函数、SoftMax函数、MaxOut函数。</p>
<p><strong>激活函数特点</strong></p>
<ul>
<li>可微性：因为优化方法是基于梯度的，这个性质是必须的</li>
<li>单调性：当激活函数是单调的时候，能够保证单层网络是凸函数</li>
<li>输出值的范围：激活函数的输出值的范围可以有限也可以无限。当输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更加显著；当输出值是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的learning rate</li>
<li>非饱和性：（饱和函数有Sigmoid、Tanh等，非饱和函数ReLU等）例如Sigmoid函数求导以后的值很小，两端的值接近为零在反向传播的时候，如果网络的层次过大便会发生梯度消失的问题，使得浅层的参数无法更新。</li>
<li>非线性：激活函数必须是非线性的。</li>
<li>计算简单：神经元都要经过激活运算的，在随着网络结构越来越庞大、参数量越来越多，激活函数如果计算量小就节约了大量的资源。</li>
</ul>
<h2 id="二，经典激活函数"><a href="#二，经典激活函数" class="headerlink" title="二，经典激活函数"></a>二，经典激活函数</h2><h3 id="1，sigmoid函数"><a href="#1，sigmoid函数" class="headerlink" title="1，sigmoid函数"></a>1，sigmoid函数</h3><p>Sigmoid 函数的图像看起来像一个 S 形曲线。</p>
<p><img src="sigmoid.jpg" alt="sigmoid">   </p>
<p>原函数：</p>
<script type="math/tex; mode=display">\mathrm{sigmoid}(x)= \frac{1}{1+e^{-x}}</script><p>导数：</p>
<script type="math/tex; mode=display">f^{'}(x)=f(x)(1-f(x))</script><p><strong>缺点</strong></p>
<ul>
<li>有梯度消失的风险；</li>
<li>函数输出不是以 0 为中心，这会降低权重的更新效率；</li>
<li>Sigmoid 函数执行指数运算，运行较慢。</li>
</ul>
<p><strong>适用场景</strong></p>
<ul>
<li>值域$(0,1)$，因此它对每个神经元的输出进行了归一化；</li>
<li>值域$(0,1)$，Sigmoid 非常合适将预测概率作为输出的模型</li>
<li>梯度平滑，避免<strong>跳跃</strong>的输出值；</li>
<li>函数是可微的。这意味着可以找到任意两个点的 sigmoid 曲线的斜率；</li>
</ul>
<h3 id="2，tanh函数"><a href="#2，tanh函数" class="headerlink" title="2，tanh函数"></a>2，tanh函数</h3><p>tanh 激活函数的图像也是 S 形，可以看做放大并平移的Logistic函数</p>
<p><img src="tanh.jpg" alt="tanh"></p>
<p>原函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathrm{Tanh}(x)&=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
    &=\frac{2}{1+e^{-2x}}-1 \\
    &=2\mathrm{sigmoid}(2x)-1
\end{align*}</script><p>导数：</p>
<script type="math/tex; mode=display">f^{'}(x)=1-f^2(x)</script><p><strong>优点</strong><br>值域$(-1,1)$，并且整个函数以 0 为中心，比 sigmoid 函数更好；</p>
<p><strong>缺点</strong><br>输入较大或较小时，输出几乎是平滑的并且梯度较小，这不利于权重更新</p>
<p>注意：在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。</p>
<h3 id="3，ReLU函数"><a href="#3，ReLU函数" class="headerlink" title="3，ReLU函数"></a>3，ReLU函数</h3><p><img src="ReLU.jpg" alt="ReLU"></p>
<p>原函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathrm{ReLU}(x)&=
    \begin{cases}
        x & x\geq 0\\
        0 & x<0
    \end{cases}\\
    &=max(0,x)
\end{align*}</script><p>导数：</p>
<script type="math/tex; mode=display">
f^{'}(x)=
    \begin{cases}
        1 & x\geq 0\\
        0 & x<0
    \end{cases}</script><p><strong>优点</strong></p>
<ul>
<li>当输入为正时，不存在梯度饱和问题。</li>
<li>计算速度快，因为 ReLU 函数中只存在线性关系。</li>
<li>ReLU函数的形式非常简洁，由两段线性函数组合起来后却是非线性的，看似简单，但ReLU的组合却可以逼近任何函数。</li>
<li>ReLU提出的最大作用是解决sigmoid函数导致的梯度消失问题的，ReLU有单侧抑制，会使一部分神经元的输出为0，这样就造成了网络的稀疏性，减少了参数的相互依存关系，缓解了过拟合问题，另外这也更符合生物神经元的特征。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>Dead ReLU 问题。当输入为负时，ReLU 完全失效，在正向传播过程中，这不是问题。但是在反向传播过程中，如果输入负数，则梯度将完全为零，sigmoid 函数和 tanh 函数也具有相同的问题；</li>
<li>可能会导致神经元死亡，权重无法更新的情况，这种死亡是不可逆转的。</li>
<li>ReLU 函数不是以 0 为中心的函数。</li>
</ul>
<p><strong>解释神经元死亡问题</strong><br>训练神经网络的时候，一旦学习率没有设置好，第一次更新权重的时候，输入是负值，那么这个含有ReLU的神经节点就会死亡，再也不会被激活。因为：ReLU的导数在x&gt;0的时候是1，在x&lt;=0的时候是0。如果x&lt;=0，那么ReLU的输出是0，那么反向传播中梯度也是0，权重就不会被更新，导致神经元不再学习。 也就是说，这个ReLU激活函数在训练中将不可逆转的死亡，导致了训练数据多样化的丢失。</p>
<p>在实际训练中，如果学习率设置的太高，可能会发现网络中40%的神经元都会死掉，且在整个训练集中这些神经元都不会被激活。所以，<strong>设置一个合适的较小的学习率</strong>，会降低这种情况的发生。为了解决神经元节点死亡的情况，有人提出了Leaky ReLU、P-ReLu、R-ReLU、ELU等激活函数。</p>
<h3 id="4，Leaky-RELU函数"><a href="#4，Leaky-RELU函数" class="headerlink" title="4，Leaky RELU函数"></a>4，Leaky RELU函数</h3><p>Leaky ReLU函数是一种专门设计用于解决神经元“死亡”问题的激活函数。</p>
<p><a href="http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">Rectified NonlinearitiesImprove Neural Network Acoustic Models </a></p>
<p><img src="LeakyReLU.jpg" alt="Leaky RELU"></p>
<p>原函数：</p>
<script type="math/tex; mode=display">
\begin{equation}
\mathrm{LeakyReLU}(x)= 
\left\{ 
    \begin{array}{ll}
    x & x>0 \\ 
    \alpha x & x\leq 0
    \end{array}
\right.
\end{equation}</script><p>导数：</p>
<script type="math/tex; mode=display">
\begin{equation}
f^{'}(x)= 
\left\{ 
    \begin{array}{ll}
    1 & x>0 \\ 
    \alpha & x\leq 0
    \end{array}
\right.
\end{equation}</script><p><strong>优点</strong></p>
<ul>
<li>神经元不会出现死亡的情况。</li>
<li>Leaky对于所有的输入，神经元不会饱和。</li>
<li>由于Leaky的线性、非饱和，在SGD中能够快速收敛。</li>
<li>计算速度要快很多，因为Leaky ReLU函数只有线性关系。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>Leaky ReLU函数中的α，需要通过先验知识人工赋值，通常取0.01。</li>
</ul>
<p><strong>为什么比ReLU好？</strong></p>
<ul>
<li>Leaky ReLU 通过把 x 的非常小的线性分量给予负输入（0.01x）来调整负值的零梯度问题；</li>
<li>Leaky ReLU 的函数范围是$\infty$（负无穷到正无穷）。</li>
</ul>
<p>从理论上讲，Leaky ReLU 具有 ReLU 的所有优点，而且 Dead ReLU 不会有任何问题，但在实际操作中，尚未完全证明 Leaky ReLU 总是比 ReLU 更好。</p>
<h3 id="5，ELU函数"><a href="#5，ELU函数" class="headerlink" title="5，ELU函数"></a>5，ELU函数</h3><p>paper：<a href="https://arxiv.org/abs/1511.07289v5">Fastand accurate deep network learning by exponential linear units (elus)</a></p>
<p>ELU 的提出也解决了 ReLU 的问题。与 ReLU 相比，ELU 有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。</p>
<p><img src="ELU.bmp" alt="ELU"></p>
<script type="math/tex; mode=display">
\begin{equation}
\mathrm{ELU}(x)=
\left\{
    \begin{array}{ll}
        x & x>0\\
        \alpha(e^x-1) & x\leq 0
    \end{array}
\right.
\end{equation}</script><p><strong>优点</strong></p>
<ul>
<li>没有 Dead ReLU 问题，输出的平均值接近 0，以 0 为中心；</li>
<li>通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习；</li>
<li>ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>计算强度更高，计算量较大</li>
</ul>
<p>显然，ELU 具有 ReLU 的所有优点，同样与 Leaky ReLU 类似，尽管理论上比 ReLU 要好，但目前在实践中没有充分的证据表明 ELU 总是比 ReLU 好。</p>
<h3 id="6，Softmax函数"><a href="#6，Softmax函数" class="headerlink" title="6，Softmax函数"></a>6，Softmax函数</h3><p>Softmax函数是用于<strong>多类分类问题</strong>的激活函数，在多类分类问题中，超过两个类标签则需要类成员关系。对于长度为 $K$ 的任意实向量，Softmax函数可以将其压缩为长度为 $K$，值在 $[0,1]$ 范围内，并且向量中元素的总和为1的实向量。</p>
<p>Softmax函数与正常的max函数不同：max函数仅输出最大值，但Softmax函数确保较小的值具有较小的概率，并且不会直接丢弃。Softmax函数的分母结合了原始输出值的所有因子，这意味着Softmax函数获得的各种概率彼此相关。</p>
<script type="math/tex; mode=display">\mathrm{Softmax}(x)=\frac{e^{x_i}}{\sum_ie^{x_i}}</script><p><strong>缺点</strong></p>
<ul>
<li>在零点不可微；</li>
<li>负输入的梯度为零，这意味着对于该区域的激活，权重不会在反向传播期间更新，因此会产生永不激活的死亡神经元。</li>
</ul>
<h3 id="7，Maxout函数"><a href="#7，Maxout函数" class="headerlink" title="7，Maxout函数"></a>7，Maxout函数</h3><p>在 Maxout 层，激活函数是输入的最大值，因此只有 2 个 Maxout 节点的多层感知机就可以拟合任意的凸函数。</p>
<p>单个 Maxout 节点可以解释为对一个实值函数进行分段线性近似 (PWL) ，其中函数图上任意两点之间的线段位于图（凸函数）的上方。</p>
<p>可以理解为是神经网络中的一层网络，类似于池化层、卷积层一样。我们也可以把Maxout函数看成是网络的激活函数层。</p>
<p>Maxout激活函数并不是一个固定的函数，不像Sigmod、Relu、Tanh等函数，是一个固定的函数方程.它是一个可学习的激活函数，因为我们 W 参数是学习变化的。它是一个分段线性函数</p>
<p>paper：<a href="https://arxiv.org/abs/1302.4389">Maxout Networks</a></p>
<script type="math/tex; mode=display">\mathrm{Maxout}(x)=max(w_ix_i+b_i)</script><p><strong>优点</strong><br>Maxout的拟合能力非常强，可以拟合任意的凸函数。Maxout具有ReLU的所有优点，线性、不饱和性。同时没有ReLU的一些缺点。如：神经元的死亡。</p>
<p><strong>缺点</strong><br>从上面的激活函数公式中可以看出，每个神经元中有两组(w,b)参数，那么参数量就增加了一倍，这就导致了整体参数的数量激增。</p>
<h3 id="8，swish函数"><a href="#8，swish函数" class="headerlink" title="8，swish函数"></a>8，swish函数</h3><p>Swish 的设计受到了 LSTM 和高速网络中 gating 的 sigmoid 函数使用的启发。我们使用相同的 gating 值来简化 gating 机制，这称为 self-gating。</p>
<p>self-gating 的优点在于它只需要简单的标量输入，而普通的 gating 则需要多个标量输入。这使得诸如 Swish 之类的 self-gated 激活函数能够轻松替换以单个标量为输入的激活函数（例如 ReLU），而无需更改隐藏容量或参数数量。Swish 具备无上界、有下界、平滑、非单调的特性。</p>
<p>paper：<a href="https://arxiv.org/abs/1710.05941">Searching for Activation Functions</a></p>
<p><img src="swish1.jpg" alt="swish original"><br><img src="swish2.jpg" alt="swish derivatives"></p>
<p>原函数：</p>
<script type="math/tex; mode=display">\mathrm{Swish}(x)=x*\mathrm{sigmoid}(\beta x)</script><ul>
<li>$\beta=0$ Swish激活函数变为线性函数$f(x)=\frac{x}{2} $；</li>
<li>$\beta=\infty$ Swish激活函数变为 $0$ 或 $x$，相当于Relu;</li>
<li>因此，Swish函数可以看作是介于线性函数与ReLU函数之间的平滑函数。</li>
</ul>
<p>导数：</p>
<script type="math/tex; mode=display">f^{'}(x)=\beta f(x)+\mathrm{sigmoid}(x)(1-\beta f(x))</script><p>优点：</p>
<ul>
<li>无界性：有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和；（同时，有界性也是有优势的，因为有界激活函数可以具有很强的正则化，并且较大的负输入问题也能解决）；</li>
<li>导数恒为正；</li>
<li>平滑度在优化和泛化中起了重要作用。</li>
</ul>
<h3 id="9，mish激活函数"><a href="#9，mish激活函数" class="headerlink" title="9，mish激活函数"></a>9，mish激活函数</h3><p>一种自正则的非单调神经激活函数，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。</p>
<p>paper:<a href="https://www.bmvc2020-conference.com/assets/papers/0928.pdf">Mish: A Self Regularized Non-Monotonic Neural Activation Function</a></p>
<p><img src="mish.jpg" alt="mish original"><br><img src="mish2.png" alt="mish derivatives"></p>
<script type="math/tex; mode=display">\mathrm{Mish} = x*tanh(ln(1+e^x))</script><p><strong>优点</strong></p>
<ul>
<li>无上限，没有上限，这样可以保证没有饱和区域，因此在训练过程中不会有梯度消失的问题，这个和relu后面的激活函数一样；</li>
<li>有下限，有下限的话能够保证具有一定的regularization effect，这对于神经网络训练来说是一个很好的特性；</li>
<li>非单调性，这个在swish里面也强调过，文章说这种特性能够使得很小的负input在保持负output的同时也能够提高表达能力和梯度流；</li>
<li>光滑性，这个主要是相比relu的，relu在0点处不光滑，会在实际优化中遇到一些求解的问题，当然这个应该还是和具体算法有关，大部分我们就把relu=0的时候梯度也变成0了，然后文章继续提到mish的光滑的特性使得在求解和模型泛化性方面相比其他激活函数表现要优良。</li>
</ul>
<h2 id="三，参考文献"><a href="#三，参考文献" class="headerlink" title="三，参考文献"></a>三，参考文献</h2><p><a href="https://ieeexplore.ieee.org/document/9577874">activate or not:learning customized activation</a><br><a href="https://zhuanlan.zhihu.com/p/85971385">常见的激活函数及其特点</a><br><a href="https://www.cnblogs.com/wj-1314/p/12015278.html">深度学习笔记——常用的激活（激励）函数</a></p>
<h2 id="四，常见问题"><a href="#四，常见问题" class="headerlink" title="四，常见问题"></a>四，常见问题</h2><h3 id="1，如何选择合适的激活函数"><a href="#1，如何选择合适的激活函数" class="headerlink" title="1，如何选择合适的激活函数"></a>1，如何选择合适的激活函数</h3><ul>
<li>通常来说，不能把各种激活函数串起来在一个网络中使用。</li>
<li>如果使用ReLU，那么一定要小心设置学习率(learning rate),并且要注意不要让网络中出现很多死亡神经元。如果死亡神经元过多的问题不好解决，可以试试Leaky ReLU、PReLU、或者Maxout。</li>
<li>尽量不要使用sigmoid激活函数，可以试试tanh。</li>
</ul>
<h3 id="2，相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？"><a href="#2，相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？" class="headerlink" title="2，相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？"></a>2，相比于sigmoid函数，tanh激活函数输出关于“零点”对称的好处是什么？</h3><p>对于sigmoid函数而言，其输出始终为正，这会导致在深度网络训练中模型的收敛速度变慢，因为在反向传播链式求导过程中，权重更新的效率会降低。</p>
<p>此外，sigmoid函数的输出均大于0，作为下层神经元的输入会导致下层输入不是0均值 的，随着网络的加深可能会使得原始数据的分布发生改变。而在深度学习的网络训练中， 经常需要将数据处理成零均值分布的情况，以提高收敛效率，因此tanh函数更加符合这个要求。</p>
<p>sigmoid函数的输出在[0,1]之间，比较适合用于二分类问题。</p>
<h3 id="3，为什么RNN中常用tanh函数作为激活函数而不是ReLU？"><a href="#3，为什么RNN中常用tanh函数作为激活函数而不是ReLU？" class="headerlink" title="3，为什么RNN中常用tanh函数作为激活函数而不是ReLU？"></a>3，为什么RNN中常用tanh函数作为激活函数而不是ReLU？</h3><p>RNN中将tanh函数作为激活函数本身就存在梯度消失的问题，而ReLU本就是为了克服梯度消失问题而生的。因为ReLU的导数只能为0或1，而导数为1的时候在RNN中很容易造成梯度爆炸问题。</p>
<h3 id="4，为什么会出现梯度爆炸的问题呢？"><a href="#4，为什么会出现梯度爆炸的问题呢？" class="headerlink" title="4，为什么会出现梯度爆炸的问题呢？"></a>4，为什么会出现梯度爆炸的问题呢？</h3><p>因为在RNN中，每个神经元在不同的时刻都共享一个参数W（这点与CNN不同，CNN中每一层都使用独立的参数Wi），因此在前向和反向传播中，每个神经元的输出都会作为下一个时刻本神经元的输入，从某种意义上来讲相当于对其参数矩阵W作了连乘，如果W中有其中一个特征值大于1，则多次累乘之后的结果将非常大，自然就产生了梯度爆炸的问题。</p>
<h3 id="5，那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？"><a href="#5，那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？" class="headerlink" title="5，那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？"></a>5，那为什么ReLU在CNN中不存在连乘的梯度爆炸问题呢？</h3><p>因为在CNN中，每一层都有不同的参数$W_i$，有的特征值大于1，有的小于1，在某种意义上可以理解为抵消了梯度爆炸的可能。</p>
<h3 id="6，什么是神经元“死亡”？"><a href="#6，什么是神经元“死亡”？" class="headerlink" title="6，什么是神经元“死亡”？"></a>6，什么是神经元“死亡”？</h3><p>Relu的输入值为负的时候，输出始终为0，其一阶导数也始终为0，这样会导致神经元不能更新参数，也就是神经元不学习了，这种现象叫做“Dead Neuron”。</p>
<h3 id="7，如何解决ReLU神经元“死亡”的问题？"><a href="#7，如何解决ReLU神经元“死亡”的问题？" class="headerlink" title="7，如何解决ReLU神经元“死亡”的问题？"></a>7，如何解决ReLU神经元“死亡”的问题？</h3><p>①采用Leaky ReLU等激活函数 ②设置较小的学习率进行训练 ③使用momentum优化算法动态调整学习率</p>
]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>activation</tag>
      </tags>
  </entry>
  <entry>
    <title>Unraveling Attention via Convex Duality-Analysis and Interpretations of Vision Transformers</title>
    <url>/2023/03/22/Transformer/Unraveling%20Attention%20via%20Convex%20Duality-Analysis%20and%20Interpretations%20of%20Vision%20Transformers/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：Unraveling Attention via Convex Duality-Analysis and Interpretations of Vision Transformers</p>
<p><a href="https://arxiv.org/abs/2205.08078">essay link</a></p>
<p><strong>通过凸对偶性解释Transformer网络~</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>使用自注意力或其替代方案的视觉Transformer已经在许多图像相关任务中展示了有潜力的结果，然而，注意力的基础归纳偏差尚不清楚。为了解决这个问题，本文从凸对偶的角度分析了注意力。对于非线性点积自注意力，以及 MLP 混合器和傅里叶神经算子 (FNO) 等替代机制，我们推导出等效的有限维凸问题，这些问题可解释并可求全局最优解。凸程序带来的块核范数正则化促使了潜在特征和token维度的低秩，特别是，我们展示了自注意力网络如何根据token的潜在相似性隐式地对其进行聚类。我们通过微调各种凸注意力头进行实验，采用的是 CIFAR-100 分类问题的预训练主干，结果表明与现有的 MLP 或线性头相比，注意力机制占优。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>Transformers 最近在语言和视觉的表示学习方面取得了巨大的成功，这主要是由于注意机制有效地混合了各层对token的表示以学习输入的语义。 在点积自注意力出现后，出现了几种有效的替代方法，可以很好地适应大型预训练任务的序列大小。然而，可学习的注意力归纳偏置没有得到很好的探索，对注意力归纳偏置的理论理解可以激发设计更高效的架构，并可以解释这些网络的泛化能力。</p>
<p>自注意力是首个视觉Transformer (ViT)中的基本构建块，它由两个线性函数的外积组成，后跟一个非线性函数以及与另一个线性函数的积，这使得它是非凸和不可解释的。理解注意力的一种方法是设计新的自注意力替代方案，其表现同样出色，这可能有助于解释其潜在机制。一组工作涉及基于多层感知器 (MLP) 的架构，tolstikhin2021mlp 等，而另一系列工作提出基于傅立叶的模型 lee2021fnet 等，其他人提出用矩阵分解 geng2021attention等 代替 self-attention。虽然所有这些作品都有吸引人的应用，利用了注意力结构的一般概念，但它们缺乏从优化角度对这些架构及微调后的应用做理论分析。</p>
<p>为了解决这个缺点，我们利用凸对偶性来分析具有 ReLU 激活的单个自注意力块。 由于自注意力会导致序列的平方复杂度，因此我们选择分析更高效的模块。 作为更高效模块的代表，我们专注于 MLP 混合器和傅立叶神经运算符（FNO）。 MLP 混合器（纯粹）在token和特征维度上使用 MLP 投影来混合token，相比之下，FNO 基于 2D 傅里叶变换来实现利用循环卷积混合token。</p>
<p>我们发现所有这三个分析模块都等同于有限维凸优化问题，表明“可证明地将它们优化到全局最优值”是有保障的。此外，我们对凸模型引起的偏差进行了观察，特别是，自注意力和 MLP-Mixer 模块的凸等价物类似于 MLP 的加权组合，但具有额外的自由度（例如，更高维度的归纳参数），以及将它们的各个子模块联系在一起以利用全局信息的独特块核范数正则化操作。相比之下，凸化的 FNO 混合器相当于循环卷积，而对 FNO 架构的轻微修改可以诱导出等效的分组卷积。我们在CIFAR-100 分类问题上进行Transformer的迁移学习，微调单个凸注意头来进行注意力头的实验测试和比较。我们观察到这些注意力模块的归纳偏差优于传统的凸模型。</p>
<p>本文的主要贡献总结如下：</p>
<ul>
<li>我们通过证明带线性（或 ReLU ）激活函数的自注意力、MLP-Mixer 和 FNO 模块与凸优化问题的等价性来保障其能求得到全局最优解。</li>
<li>通过分析这些等效的凸应用，本文为这些注意力模块的优化目标提供了可解释性。</li>
<li>实验验证了（凸）视觉 Transformer 在迁移学习任务中的表现优于基线凸方法。</li>
</ul>
<h3 id="1-1-相关工作"><a href="#1-1-相关工作" class="headerlink" title="1.1 相关工作"></a>1.1 相关工作</h3><p>这项工作主要与两条研究路线有关。</p>
<p><strong>自注意力的解释</strong><br>一种方法是通过实验观察注意力网络的特性来理解它们，例如，DINO 提出了一种针对 ViTs 的对比自监督学习方法。 据观察，学习到的注意力映射保留了图像的语义区域。 另一项工作比较了经过训练的 ViT 和 CNN 的跨层对齐，得出的结论是 ViT 在网络的各个层之间具有更统一的表示结构。 另一项工作使用深度泰勒分解方法来可视化输入图像中会引起特定 ViT 预测的部分。</p>
<p>另一种方法是分析注意力网络的表现力，一项工作将多头自注意力解释为贝叶斯推理，并提供了工具来决定使用多少个头，以及如何在不同的头中强化特征的区别表达。 其他分析表明，稀疏Transformer可以逼近任何函数，多头自注意力网络至少跟卷积网络的表达力相当，点积自注意力网络不是Lipschitz 连续的。</p>
<p><strong>凸神经网络</strong><br>从 pilanci2020neural 开始，已有大量工作证明各种 ReLU 及其变体的激活神经网络架构具有等效的凸优化问题。 这些包括双层卷积和矢量输出网络、更深层次的网络、具有批量标准化的网络和 Wasserstein GANs。最近的工作还展示了如何有效地优化这些等效凸网络的最简单形式，并结合额外的约束来增强对抗性鲁棒性。 然而，这些工作都没有分析过Transformer的构建块，而Transformer是许多最先进的视觉和语言处理任务中的主要方法。</p>
<h2 id="2，预备工作"><a href="#2，预备工作" class="headerlink" title="2，预备工作"></a>2，预备工作</h2><p>一般来说，我们分析监督学习问题，其中输入训练数据 $\{\mathbf{X}_i \in \mathbb{R}^{s \times d}\}_{i=1}^n$ 是嵌入层patch的结果，我们有相应的任意大小的标签 $\{\mathbf{Y}_i \in \mathbb{R}^{r \times c}\}_{i=1}^n$。 对于任意<strong>凸损失函数</strong> $\mathcal{L}(\cdot, \cdot)$，我们求解优化问题</p>
<script type="math/tex; mode=display">
p^\ast:=\min_{\theta}\sum\limits^n_{i=1}\mathcal{L}(f_{\theta}(\mathbf{X}_i),\mathbf{Y}_i)+\mathcal{R}(\theta) 
\tag{1}</script><p>$\theta$ 是可学习的参数，$f_{\theta}(\cdot)$ 是神经网络，$\mathcal{R}(\cdot)$ 是正则化器。 请注意，此公式包括了去噪和分类场景：在 $r = 1$ 的分类设置中，可以将全局平均池化吸收到凸损失 $\mathcal{L}$ 中，而如果 $r = s$，可以直接使用平方损失或其他凸损失函数，也可以使用这个公式来应用于监督学习和自监督学习。</p>
<p>在本文中，我们将 $(\cdot)_+ := \max \{0, \cdot\}$ 表示为 非线性的ReLU。 我们使用上标，比如 $\mathbf{A}^{(i_i, i_2)}$ 来表示矩阵块，用方括号，比如 $\mathbf{A}[i_1, i_2]$ 来表示矩阵的元素，其中参数指的是行（或行块）$i_1$ 和列（或列块）$i_2$。</p>
<h3 id="2-1-线性和-ReLU-MLPs-的隐式凸性"><a href="#2-1-线性和-ReLU-MLPs-的隐式凸性" class="headerlink" title="2.1 线性和 ReLU MLPs 的隐式凸性"></a>2.1 线性和 ReLU MLPs 的隐式凸性</h3><p>之前，已经证明标准的两层 ReLU MLP 等价于凸优化问题，简要描述相关背景，为本文中的大部分分析提供背景。 特别是，我们表示一个网络的隐藏层中有 $m$ 个神经元，权重衰减参数 $\beta &gt;0$ 和数据 $\mathbf{X} \in \mathbb{R} ^{n \times d}$, $\mathbf{Y} \in \mathbb{R}^{n \times c}$ 为:</p>
<script type="math/tex; mode=display">
p^\ast_{RMLP}:=\min_{\mathbf{w}_{2j},\mathbf{w}_{2j}}\mathcal{L}\bigg ( \sum\limits^m_{j=1}(\mathbf{X}\mathbf{w}_{1j})_+\mathbf{w}^\top_{2j},\mathbf{Y} \bigg )+\frac{\beta}{2}\sum\limits^m_{j=1}(\|\mathbf{w}_{1j}\|^2_2+\|\mathbf{w}_{2j}\|^2_2) 
\tag{2}</script><p>虽然这个问题如上所述是非凸的，但已经证明目标等同于等价凸优化问题的解决方案，并且两个问题的解决方案之间存在一对一的映射。 特别是，该分析利用了 <strong>hrperplane arrangements</strong>，枚举了 所有可能的非线性 ReLU 激活模式：</p>
<script type="math/tex; mode=display">
\mathcal{D}:=\{\mathrm{diag}(\mathbb{1}\{\mathbf{Xu}\geq 0\}):\mathbf{u}\in\mathcal{R}^d \} 
\tag{3}</script><p>集合 $\mathcal{D}$ 显然是有限的，其基函数的界为 $P := |\mathcal{D}| \leq 2r\left(\frac{e(n-1)}{r}\right)^r$，其中 $r := \mathrm{rank}(\mathbf{X})$。 通过凸对偶分析，我们可以通过枚举有限排列集 $\{\mathbf{D}_j\}_{j=1}^P$ 来表示一个等价的凸优化问题。 我们定义以下范数：</p>
<script type="math/tex; mode=display">
\begin{align*}
&\| \mathbf{Z} \|_{*,K}  :=\min_{t\geq 0}t\quad s.t.\mathbf{Z}\in t\mathcal{C} \\
&\mathcal{C} :=conv\{\mathbf{Z}=uv^\top\; : \; \mathbf{K}u\geq 0,\|\mathbf{Z}\|_\ast\leq 1,v\in\mathbb{R}^c \}
\end{align*}
\tag{4}</script><p>这个范数是一个准核范数，它与标准核范数的不同之处在于它所依赖的因式分解对其左因子施加了约束，在我们的例子中这将是一个仿射约束。 在凸 ReLU 神经网络中，选择 $\mathrm{K}$ 来强制存在 $\{\mathbf{u}_k, \mathbf{v}_k\}$ 这样 $\mathbf{Z} = \sum_k \mathbf{u}_k \mathbf{v}_k^\top$ 和 $\mathbf{D}_{j}\mathbf{X}\mathbf{Z} = \sum_k (\mathbf{X}\mathbf{u}_k)_+\mathbf{v}_k^\top$, 并惩罚 $\sum_k |\mathbf{u}_k|_2 |\mathbf{v}_k|_2$。</p>
<p>有了这个成立，可以证明</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{RMLP}:=\min_{\{\mathbf{z}_j\}^P_{j=1}}\mathcal{L}( \sum\limits^P_{j=1}(\mathbf{D}_j\mathbf{X}\mathbf{Z}_j,\mathbf{Y})+\beta\sum\limits^P_{j=1}\|\mathbf{Z}_j\|_{*,\mathbf{K}_j} \\
\mathbf{K}_j:=(2\mathbf{D}_j-\mathbf{I})\mathbf{X}
\end{align*}
\tag{5}\label{eq5}</script><p>双层 ReLU MLP 优化问题因此表示为具有核范数正则化约束的分片线性模型，这与双层线性激活 MLP 形成对比，后者的凸等价式为：</p>
<script type="math/tex; mode=display">
p^\ast_{RMLP}=\min_{\mathbf{z}}\mathcal{L}(\mathbf{X}\mathbf{Z},\mathbf{Y})+\beta\|\mathbf{Z}\|_\ast 
\tag{6}\label{eq6}</script><p>众所周知，这种核范数惩罚会诱导出低秩解决方案并出现在矩阵分解问题中。 还可以定义 gated ReLU 激活，其中 ReLU 门固定为 $\{\mathbf{h}_j\}_{j=1}^m$，</p>
<script type="math/tex; mode=display">
g(\mathbf{X}\mathbf{w}_{1j}):=\mathrm{diag}(1\{\mathbf{X}\mathbf{h}_j\geq 0\})(\mathbf{X}\mathbf{w}_{1j}) 
\tag{7}</script><p>然后，定义 $\{\mathbf{D}_j\}_{j=1}^m := \{\mathrm{diag}(1\{\mathbf{X}\mathbf{h}_j \geq 0\} )\}_{j=1}^m$, 相应的凸门控 ReLU 激活双层网络目标直接从 ReLU 和线性情况下得出，由下式给出</p>
<script type="math/tex; mode=display">
p^\ast_{RMLP}=\min_{\mathbf{z}^m_{j=1}}\mathcal{L}\bigg (\sum\limits^m_{j=1}\mathbf{D}_j\mathbf{X}\mathbf{Z}_j,\mathbf{Y}\bigg )+\beta\sum\limits^m_{j=1}\|\mathbf{Z}_j\|_\ast 
\tag{8}</script><p>我们注意到，对于线性和门控 ReLU 公式，凸权重的正则化成为了标准核范数，因为不再需要强制 ReLU 约束。已经证明，门控 ReLU 和 ReLU 网络之间存在小的近似差距，并且 ReLU 网络可以从门控 ReLU 问题的解决方案中形成。</p>
<p>在解决这些问题的有效算法方面，使用应用于凸线性和门控 ReLU 的加速近端梯度下降算法，可以在 $\mathcal{O}(1 /\sqrt{\epsilon})$ 迭代下获得 $\epsilon$ 的精度。对于凸 ReLU 公式，sahiner2020mathbftor 提出了适用于这种情况的凸 MLP 的 Frank-Wolfe 算法，对于 $\epsilon$ 精度，在一般情况下需要 $\mathcal{O}(1/\epsilon)$ 迭代。</p>
<p>在后续部分中，我们将通过类似的凸对偶技术演示具有线性和 ReLU 激活的常见视觉Transformer块如何与等效凸优化问题相关联。</p>
<h2 id="3，自注意力的隐式凸性"><a href="#3，自注意力的隐式凸性" class="headerlink" title="3，自注意力的隐式凸性"></a>3，自注意力的隐式凸性</h2><p>规范的 Vision Transformer (ViT) 使用自注意力和 MLP 作为其主干，特别是，自注意力网络的单个“头”由以下给出：</p>
<script type="math/tex; mode=display">
f_j(\mathbf{X}_i):=\sigma\bigg(\frac{\mathbf{X}_i\mathbf{Q}_j\mathbf{K}^\top_j\mathbf{X}^\top_i}{\sqrt{d}}\bigg)\mathbf{X}_i\mathbf{V}_j 
\tag{9}</script><p>其中 $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ 都是可学习的参数，$\sigma(\cdot)$ 通常（但不总是）表示非线性的 softmax。 在实践中，人们通常使用 $m$ 个注意力“头”，它们沿着特征维度连接在一起，然后是一个“通道混合”层，或者一个分类头：</p>
<script type="math/tex; mode=display">
\begin{align*}
f_{MHSA}(\mathbf{X}_i)& := [f_1(\mathbf{X}_i)\cdots f_m(\mathbf{X}_i)]\mathbf{W} \\
&=\sum\limits^m_{j=1}\sigma\bigg (\frac{\mathbf{X}_i\mathbf{Q}_j\mathbf{K}^\top_j\mathbf{X}^\top_i}{\sqrt{d}}\bigg) \mathbf{X}_i\mathbf{V}_j\mathbf{W}_j 
\end{align*}
\tag{10}</script><p>为了我们的分析，注意到 $\mathbf{Q}_j \mathbf{K}_j^\top$ 和 $\mathbf{V}_j\mathbf{W}_j$ 都可以表示为单个线性层，我们将多头自注意力网络建模为</p>
<script type="math/tex; mode=display">
f_{SA}(\mathbf{X}_i):=\sum\limits^m_{j=1}\sigma\bigg (\frac{\mathbf{X}_i\mathbf{W}_{1j}\mathbf{X}^\top_i}{\sqrt{d}}\bigg) \mathbf{X}_i\mathbf{W}_{2j} 
\tag{11}</script><p>然后我们定义多头自注意力训练问题如下所示：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{SA}:=&\min_{\mathbf{W}_{1j},\mathbf{W}_{2j}}\sum\limits^n_{i=1}\mathcal{L}(f_{SA}(\mathbf{X}_i),\mathbf{Y}_i)\\
&+\frac{\beta}{2}\sum\limits^m_{j=1}\|\mathbf{W}_{1j}\|^2_F+\|\mathbf{W}_{2j}\|^2_F 
\end{align*}
\tag{12}\label{eq12}</script><p>因此在公式中使用了通用的凸损失函数和标准权重衰减。 虽然当 $\sigma(\cdot)$ 表示 softmax 激活时直接凸分析是棘手的，但我们可以针对许多其他激活函数分析这种架构。 特别是，已经提出了具有线性和 ReLU 激活函数的自注意力，其性能与标准 softmax 激活网络相当。因此，我们将分析带线性、ReLU 和门控 ReLU 激活变体的多头自注意力模型。</p>
<p><strong>定理3.1</strong><br>对于带线性激活函数的多头自注意力网络的训练问题\eqref{eq12}，对于 $m \geq m^\ast$，其中 $m^\ast \leq \min\{d^2, dc\}$, 标准的非凸训练目标相当于一个凸优化问题，由下式给出</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{SA}:=&\min_{\mathbf{Z}\in\mathbb{R}^{d^2\times dc}}\sum\limits^n_{i=1}\mathcal{L}\bigg(\sum\limits^d_{k=1}\sum\limits^d_{\ell =1}\mathbf{G}_i[k,\ell ]\mathbf{X}_i\mathbf{Z}^{(k,\ell)},\mathbf{Y}_i \bigg)\\
&+\beta\|\mathbf{Z}\|_\ast
\end{align*}
\tag{13}\label{eq13}</script><p>其中，$\mathbf{G}_i:=\mathbf{X}^\top_i\mathbf{X}_i$，且 $\mathbf{Z}^{(k,\ell )}\in\mathbb{R}^{d\times c}$。</p>
<p>结果表明，线性激活自注意力模型由 Gram（特征相关）矩阵加权线性模型组成，核范数惩罚项将各个模型彼此组合。</p>
<p>还可以将凸模型视为一组具有加权核范数的线性模型，其中每个块 $\mathbf{Z}^{(k, \ell)}$ 具有相应的权重 $1/\mathbf{G}_i[k, \ell]$。因此，具有高相关性的特征将具有相应的较大范数的线性权重。我们注意到，当 $\beta = 0$ 时，线性自注意力模型 \eqref{eq13} 等价于线性两层 MLP \eqref{eq6}。</p>
<p>虽然通常 $\mathbf{Z}$ 上的核范数惩罚项在每个单独的线性模型 $\mathbf{Z}^{(k, \ell)}$ 上没有相应的范数，但以下结果总结了一个实例，核规范可以分解成更小的块。</p>
<p><strong>推论3.2</strong><br>假设 $\mathbf{X}_i$ 的某些特征与所有 $i$ 完全不相关，即 $\mathbf{G}_i$ 是对于所有 $i$ 的块对角线块 $\{\mathbf{ G}_i^{(b)} \in \mathbb{R}^{d_b \times d_b}\}_{b=1}^B$。 然后，凸问题 \eqref{eq13} 简化为以下凸问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{SA}:=&\min_{\mathbf{Z}^{(b)}}\sum\limits^n_{i=1}\mathcal{L}\bigg(\sum\limits^B_{b=1}\sum\limits^{d_b}_{k=1}\sum\limits^{d_B}_{\ell =1}\mathbf{G}_i^{(b)}[k,\ell ]\mathbf{X}_i\mathbf{Z}^{(b,k,\ell)},\mathbf{Y}_i \bigg)\\
&+\beta\|\mathbf{Z}^{(b)}\|_\ast,\mathbf{Z}^{(b)}\in\mathbb{R}^{d_bd\times d_bc}
\end{align*}
\tag{14}\label{eq14}</script><p>因此，这个推论表明，在不相关特征集的假设下，线性自注意力块在这些集上分离。 特别是，对应于 Gram 矩阵 $\mathbf{G}_i$ 中 $0$ 值的 $\mathbf{Z}$ 块将被设置为 $0$，从而消除不相关特征之间的相互作用。 这种现象如图1所示。</p>
<p>虽然这个线性模型为自注意力的基础提供了一个简单、优雅的解释，但我们也可以分析具有非线性的自注意力块。 因此，我们提供了 ReLU 激活自注意力的分析。</p>
<p><strong>定理3.3</strong><br>对于 ReLU 激活多头自注意力训练问题 \eqref{eq12}，我们定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{X}& :=
\begin{bmatrix}
\mathbf{X}_1\otimes\mathbf{X}_1\\
\cdots \\
\mathbf{X}_n\otimes\mathbf{X}_n
\end{bmatrix}\\
\{\mathbf{D_j}\}^P_{j=1}& :=\{\mathrm{diag} (1\{ \mathbf{X}\mathbf{u}_j\geq 0\}):\mathbf{u}_j\in\mathbb{R}^{d^2} \}
\end{align*}</script><p>其中 $P \leq 2r\left(\frac{e(n-1)}{r}\right)^r$ 和 $r := \mathrm{rank}(\mathbf{X})$。</p>
<p>那么，对于 $m \geq m^\ast$，且 $m^\ast \leq n\min\{d^2, dc\}$，标准的非凸训练等价于以下凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{SA}:=&\min_{\mathbf{Z}_j\in\mathbb{R}^{d^2\times dc}}\sum\limits^n_{i=1}\mathcal{L}\bigg(\sum\limits^P_{j=1}\sum\limits^d_{k=1}\sum\limits^d_{\ell =1}\mathbf{G}_{i,j}^{(k,\ell )}\mathbf{X}_i\mathbf{Z}_j^{(k,\ell)},\mathbf{Y}_i \bigg)\\
&+\beta\|\mathbf{Z}_j\|_{\ast,K_j},\mathbf{K}_j:=(2\mathbf{D}_j-\mathbf{I}_{ns^2})\mathbf{X}
\end{align*}
\tag{15}\label{eq15}</script><p>其中 $\mathbf{G}_{i,j} := (\mathbf{X}_i \otimes \mathbf{I}_{s})^\top \mathbf{D}_{j}^{( i)} (\mathbf{X}_i \otimes \mathbf{I}_{s})$, $\mathbf{G}_{i,j}^{(k, \ell)} \in \mathbb{R}^{s \times s}$ 和 $\mathbf{Z}_j^{(k, \ell)} \in \mathbb{R}^{d \times c}$。</p>
<p>有趣的是，虽然标准 ReLU MLP 的超平面排列仅取决于数据矩阵 $\mathbf{X}$，但对于自注意力网络，它们更复杂，而不是取决于 $\mathbf{X}_i \otimes \mathbf{X}_i$。这些超平面排列定义了约束核范数惩罚项的约束。人们可能会将 ReLU 激活自注意力模型视为两个模型的融合—，一种使用 $\mathbf{X}_i \otimes \mathbf{X}_i$ 生成超平面排列，另一种将 $\mathbf{X}_i$ 用于线性预测。因此，与线性自注意力情况不同，即使在 $\beta = 0$ 的情况下，ReLU 自注意力网络 \eqref{eq15} 也不等同于 ReLU MLP 模型 \eqref{eq5}。</p>
<p>此外，在 \eqref{eq13} 中的线性激活情况下，每个线性模型由 $\mathbf{G}_i$ 中的单项进行缩放，而在 ReLU 情况下，每个线性模型由对角矩阵 $\mathbf{G}_{i,j}^{(k, \ell)}$ 进行缩放，将来自 $\mathbf{X}_i$ 的二阶信息与由 ReLU 激活函数诱导的超平面排列联合起来。例如，人们可能会注意到区别项：</p>
<script type="math/tex; mode=display">\mathbf{G}^{(k,\ell )}_{i,j}=\sum\limits^s_{t=1}\mathbf{X}_i[t,k]\mathbf{X}_i[t,\ell ]\mathbf{D}^{(i,t)}_j \tag{16}</script><p>是对于对角线 $\mathbf{D}_{j}^{(i, t)} \in \{0, 1\}^{s \times s}$ 而言。 因此，$\mathbf{G}_{i,j}^{(k, \ell)}$ 可以看作是特征 $k$ 和 $\ell$ 之间的相关性，由对角线 $\{0, 1\}$ 进行加权。换句话说，一种“局部”相关性，其中局部性是通过 $\mathbf{D}_{j}^{(i, t)}$ 中的 $\{0, 1\}$ 值来体现。这种局部相关性对预测的每个token进行缩放，本质上是为未被 $\mathbf{D}_{j}^{(i, t)}$ 掩盖的token赋予权重。</p>
<h2 id="4，替换混合机制"><a href="#4，替换混合机制" class="headerlink" title="4，替换混合机制"></a>4，替换混合机制</h2><p>虽然自注意力是最初提出的用于视觉Transformer的token混合器，但还有许多其他替代方法已显示出产生类似结果，同时计算效率更高，我们在这里处理两个这样的架构。</p>
<h3 id="4-1-MLP-Mixer"><a href="#4-1-MLP-Mixer" class="headerlink" title="4.1 MLP Mixer"></a>4.1 MLP Mixer</h3><p>我们首先分析 MLP-Mixer 架构，这是一种替代自注意力网络的全 MLP 结构，在图像分类基准上具有竞争性能。 该提案很简单，—沿输入的一个维度应用 MLP，然后沿相反维度应用 MLP。 因此，这种 MLP-Mixer 架构的最简单形式可以写成：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{MM}&:=\min_{\mathbf{W}_{1j},\mathbf{W}_{2j}}\sum\limits^n_{i=1}\mathcal{L}(\sum\limits^m_{j=1}\sigma(\mathbf{W}_{1j}\mathbf{X}_i)\mathbf{W}_{2j},\mathbf{Y}_i)\\
&+\frac{\beta}{2}\sum\limits^m_{j=1}\|\mathbf{W}_{1j}\|^2_F+\|\mathbf{W}_{2j}\|^2_F
\end{align*}
\tag{17}\label{eq17}</script><p>其中 $\sigma$ 是激活函数。 虽然 tolstikhin2021mlp 使用 GeLU 的非线性，但我们分析了更简单的线性和 ReLU 对应激活函数，这对 MLP-Mixer 架构的底层结构提供了重要的见解。</p>
<p><strong>定理4.1</strong><br>对于线性激活MLP-Mixer的训练问题\eqref{eq17}，对于 $m \geq m^\ast$，其中 $m^\ast \leq \min\{s^2, dc\}$， 标准的非凸训练目标相当于一个下式的凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{MM}&:=\min_{\mathbf{Z}\in\mathbb{R}^{s^2\times dc}}\sum\limits^n_{i=1}\mathcal{L}([f_1(\mathbf{X}_i)\cdots f_c(\mathbf{X}_i)],\mathbf{Y}_i)\\
&+\beta\|\mathbf{Z}\|_\ast,f_p(\mathbf{X}_i):=\mathbf{Z}^{(p)}\text{vec}(\mathbf{X}_i)
\end{align*}
\tag{18}</script><p>其中 $\mathbf{Z}^{(p)} \in \mathbb{R}^{s \times sd}$ 对于 $p \in [c]$, 和 $\mathbf{Z} ^{(p, t)} \in \mathbb{R}^{s \times d}$ 对于 $t \in [s]$，以及</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{Z}^{(p)}=[\mathbf{Z}^{(p,1)}\cdots \mathbf{Z}^{(p,s)}]\\
\mathbf{Z}=
\begin{bmatrix}
\mathbf{Z}^{(1,1)}\cdots \mathbf{Z}^{(c,1)}\\
\cdots \\
\mathbf{Z}^{(1,s)}\cdots \mathbf{Z}^{(c,s)}
\end{bmatrix}
\end{align*}</script><p>我们可以将线性 MLP-Mixer 的拟合项与标准线性 MLP \eqref{eq6} 进行对比，其中网络输出的每一列 $k$ 为：</p>
<script type="math/tex; mode=display">
\mathbf{X}_i\mathbf{Z}^{(k)} = ({\mathbf{Z}^{(k)}}^\top \otimes \mathbf{I}_s)\text{vec}(\mathbf{X}_i)</script><p>其中 $\mathbf {Z}^{(k)} \in \mathbb{R}^d$。因此，与标准线性 MLP 相比，MLP-Mixer 为网络提供了 $s^2$ 量级多的自由度来拟合 $\mathbf{Y}_i$ 的每一列。这表明，与线性自注意力网络不同，即使 $\beta = 0$，线性 MLP-Mixer 模型也不等同于线性标准 MLP。人们可能会推测，与标准 MLP 相比，这种额外的隐式自由度允许类似 MLP 模型混合器更容易地适应复杂的分布。虽然从拟合项看来 $\mathbf{Y}_i$ 的每个输出类都是独立拟合的，但我们注意到这些输出通过 $\mathbf{Z}$ 上的核范数耦合在一起，这鼓励 $\{ \mathbf{Z}^{(k)}\}_{k=1}^c$ 彼此相似。</p>
<p>凸线性 MLP-Mixer 架构的另一种解释可以通过简单地将 $\mathbf{Z}$ 的列置换为 $\tilde{\mathbf{Z}}$ 来实现，这不会影响核范数，因此不会影响最优解。如果根据块 $\tilde{\mathbf{Z}}^{(t, k)} \in \mathbb{R}^{ s \times c},(t \in [s],k \in [d])$上 $\tilde{\mathbf{Z}}$ 的按列划分，凸优化问题也可以写成： </p>
<script type="math/tex; mode=display">
p^\ast_{MM}=\min_{\tilde{\mathbf{Z}}}\sum\limits^n_{i=1}\mathcal{L}\bigg (\sum\limits^s_{t=1}\sum\limits^d_{k=1}\mathbf{X}_i[t,k]\tilde{\mathbf{Z}}^{(t,k)},\mathbf{Y}_i  \bigg)+\beta\|\tilde{Z} \|_\ast 
\tag{19}\label{eq19}</script><p>在这里，与线性自注意力网络 \eqref{eq13} 的连接变得更加清晰，\eqref{eq13} 是线性模型的加权求和，其中权重对应于 Gram 矩阵项，而 \eqref{eq19} 是预测的加权求和，其中权重对应于数据矩阵项。 我们还注意到，在大多数网络中，通常 $s &lt; d$，因此与自注意力块相比，MLP-Mixer 块的求解复杂度较低，我们还可以将这些结果扩展到 ReLU 激活的 MLP-Mixers。</p>
<p><strong>定理4.2</strong><br>对于 ReLU 激活的 MLP-Mixer 训练问题 \eqref{eq17}，我们定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{X}& :=
\begin{bmatrix}
\mathbf{X}^\top_1\otimes\mathbf{I}_s\\
\mathbf{X}^\top_n\otimes\mathbf{I}_s
\end{bmatrix}\\
\{\mathbf{D}_j\}^P_{j=1}& :=\{\mathrm{diag}(1\{\mathbf{X}\mathbf{u}_j\geq 0 \}):\mathbf{u}_j\in\mathbb{R}^{s^2} \}
\end{align*}</script><p>其中 $P \leq 2r\left(\frac{e(n-1)}{r}\right)^r$ 和 $ r := \mathrm{rank}(\mathbf{X})$。然后，对于 $m \geq m^\ast$，其中 $m^\ast \leq n\min\{s^2, dc\}$，标准的非凸训练目标等价于以下凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  p^\ast_{MM}& =\min_{\mathbf{Z}_j\in\mathbb{R}^{s^2\times dc}}\sum\limits^n_{i=1}\mathcal{L}([f_1(\mathbf{X}_i)\cdots f_c(\mathbf{X}_i)],\mathbf{Y}_i)\\
  & + \beta\sum\limits^P_{j=1}\|\mathbf{Z}_j\|_{\ast,K_j},\mathbf{K}_j:=(2\mathbf{D}_j-\mathbf{I}_{nd})\mathbf{X}
\end{align*}
\tag{20}\label{eq20}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{align*}
    f_p(\mathbf{X_i}) &:= \sum_{j=1}^P \begin{bmatrix}  \mathbf{D}_{j}^{(i, 1)}\mathbf{Z}_j^{(p, 1)} \cdots   \mathbf{D}_{j}^{(i, d)}\mathbf{Z}_j^{(p, d)}\end{bmatrix}  \mathrm{vec}(\mathbf{X}_i)
\end{align*}</script><p>对于 $\mathbf{D}_{j}^{(i,k)} \in \mathbb{R}^{s \times s}$ 和 $\mathbf{Z}_j^{(p, k)} \in \mathbb{R}^{s \times s}$。</p>
<p>现在，与自注意力模型不同，超平面排列的有效数据矩阵是 $\mathbf{X}_i \otimes \mathbf{X}_i$，MLP-mixer 的排列使用 $\mathbf{X}_i^\top \otimes \mathbf{I}_s$，为分区数据提供额外的自由度，同时仍仅包含有关数据的一阶信息。 使用与 \eqref{eq19} 中相同的列置换技巧，可以将 \eqref{eq20} 写为</p>
<script type="math/tex; mode=display">
\begin{align*}
  p^\ast_{MM}& =\min_{\mathbf{\tilde{Z}}_j}\sum\limits^n_{i=1}\mathcal{L}\bigg (\sum\limits^P_{j=1}\sum\limits^s_{t=1}\sum\limits^d_{k=1}\mathbf{X}_i[t,k]\mathbf{D}_j^{(i,k)}\tilde{\mathbf{Z}}_j^{(t,k)},\mathbf{Y}_i\bigg )\\
  & + \beta\sum\limits^P_{j=1}\|\tilde{\mathbf{Z}}_j\|_{\ast,K_j}
\end{align*}
\tag{21}</script><p>现在我们再次清楚地看到与\eqref{eq15} ReLU 自注意力的差异，对角线排列由 $\mathbf{X}_i$ 而不是 Gram 矩阵加权， $\tilde{\mathbf{ Z}}_j^{(t, k)}$ 只是简单的预测，而不是线性模型的权重。</p>
<h3 id="4-2-Fourier神经算子"><a href="#4-2-Fourier神经算子" class="headerlink" title="4.2 Fourier神经算子"></a>4.2 Fourier神经算子</h3><p>与自注意力或类似 MLP 的注意力机制相比，还有一系列基于傅立叶的自注意力替代方案，最近在视觉任务中显示出前景。 我们介绍傅里叶神经算子 (FNO) ，其工作方式如下：</p>
<ul>
<li>i. 二维 DFT 首先应用于空间token； </li>
<li>ii. 每个token乘以自己的权重矩阵； </li>
<li>iii. 逆 DFT 将傅立叶token返回到原始（空间）域。</li>
</ul>
<p>以紧凑矩阵形式表示 FNO，请注意除了标准 MLP 权重 $\mathbf{W}_1 \in \mathbb{R}^{d \times m},\, \mathbf{W}_2 \in \mathbb{R}^{m \times c}$，FNO 块具有第三组权重 $\mathbf{L} \in \mathbb{R}^{s \times d \times d}$。 让我们定义傅里叶变换 $\mathbf{F} := \mathbf{F}_h \otimes \mathbf{F}_w$，它是 $h \times w$ 二维傅里叶变换的矢量化版本。 在傅立叶空间中将权重 $\mathbf{L}$ 写成为 $\mathbf{V}$ 更方便，其中二维傅立叶变换已应用于第一维，即 $\mathbf{V}[ :,i,j]$ 对应每个 $i,j$。</p>
<p>现在，定义 $\mathbf{V}^{(j)}=\mathbf{V}[j,:,:]$。 然后将 $\mathbf{F}\mathbf{X}_i$ 的每一行乘以 $d \times d$ 权重矩阵 $\mathbf{V}^{(j)}$，并转换回图像域如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
f_{FN}(\mathbf{X}_i):=\sigma\bigg ( \bigg ( \mathbf{F}^{-1}
    \begin{bmatrix}
        (\mathbf{F}\mathbf{X}_i)_1^\top\mathbf{V}^{(1)}\\
        \cdots \\
        (\mathbf{F}\mathbf{X}_i)_s^\top\mathbf{V}^{(s)}
    \end{bmatrix}
\bigg )\mathbf{W}_1\bigg )\mathbf{W}_2  
\end{align*}
\tag{22}\label{eq22}</script><p>这种表示可以大大简化。</p>
<p><strong>引理4.3</strong><br>对于权重 $\mathbf{W}_1 \in \mathbb{R}^{sd \times m}$, $\mathbf{W}_2 \in \mathbb{R}^{m \times c}$, FNO 块 \eqref{eq22} 可以等效地表示为</p>
<script type="math/tex; mode=display">
f_{FN}(\mathbf{X}_i)=\sum\limits^m_{j=1}\sigma(\mathrm{circ}(\mathbf{X}_i)\mathbf{w}_{1j})\mathbf{w}_{2j}^\top 
\tag{23}</script><p>其中 $\mathrm{circ}(\mathbf{X}_i) \in \mathbb{R}^{s \times sd}$ 表示由 $\mathbf {X}_i$ 沿着它的第一个维度的所有圆组成的矩阵。</p>
<p>因此，我们可以将 FNO 训练目标写为：</p>
<script type="math/tex; mode=display">
\begin{align*}
p^\ast_{FN}:=& \min_{\mathbf{w}_{1j},\mathbf{w}_{2j}}\sum\limits^n_{i=1}\mathcal{L}(f_{FN}(\mathbf{X}_i),\mathbf{Y}_i)\\
&+\frac{\beta}{2}\sum\limits^m_{j=1}\|\mathbf{w}_{1j}\|^2_2+\|\mathbf{w}_{2j}\|^2_2
\end{align*}
\tag{24}\label{eq24}</script><p>我们注意到 FNO 实际上非常类似于双层 CNN，其中第一层由具有完整循环填充的卷积层和全局卷积核组成。 与典型的卷积核通常很小并且卷积是局部的 CNN 不同，这里的卷积要大得多，这意味着参数比典型的 CNN 多得多。 之前已经通过凸对偶分析了类似的 CNN 架构。 因此，对于线性和 ReLU 激活，\eqref{eq24} 等价于凸优化问题。</p>
<p><strong>定理4.4</strong><br>对于线性激活FNO训练问题\eqref{eq24}，对于 $m \geq m^\ast$，其中 $m^\ast \leq \min\{sd, c\}$，标准非凸训练问题相当于一个凸优化问题，由下式给出：</p>
<script type="math/tex; mode=display">
p^\ast_{FN}=\min_{\mathbf{Z}\in\mathbb{R}^{sd\times c}}\sum\limits^n_{i=1}\mathcal{L}(\mathrm{circ}(\mathbf{X}_i)\mathbf{Z},\mathbf{Y}_i)+\beta\|\mathbf{Z}\|_\ast 
\tag{25}</script><p><strong>定理4.5</strong><br>对于 ReLU 激活的 FNO 训练问题 \eqref{eq24}，我们定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathbf{X}&:=
    \begin{bmatrix}
        \mathrm{circ}(\mathbf{X}_1)\\
        \cdots \\
        \mathrm{circ}(\mathbf{X}_n)
    \end{bmatrix}\\
    \{\mathrm{D}_j \}^P_{j=1}&:=\{\mathrm{diag}(1\{\mathrm{X}\mathrm{u}_j\geq 0 \}):\mathrm{u}_j\in\mathbb{R}^{sd} \}
\end{align*}</script><p>其中 $P \leq 2r\left(\frac{e(n-1)}{r}\right)^r$ 和 $r := \mathrm{rank}(\mathbf{X})$。 然后，对于 $m \geq m^\ast$，其中 $m^\ast \leq n\min\{sd, c\}$，标准非凸训练问题等价于凸优化问题，由下式给定：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p^\ast_{FN}=\min_{\mathbf{Z}_j\in\mathbb{R}^{sd\times c}}\sum\limits^n_{i=1}\mathcal{L}\bigg(\sum\limits^P_{j=1}\mathbf{D}_j^{(i)}\mathrm{circ}(\mathbf{X}_i)\mathbf{Z}_j,\mathbf{Y}_i)+\\
    \beta\sum\limits^P_{j=1}\|\mathbf{Z}_j\|_{\ast,K_j},\mathbf{K}_j:=(2\mathbf{D}_j-\mathbf{I}_{ns})\mathbf{X} 
\end{align*}
\tag{26}</script><h4 id="4-2-1-块对角FNO"><a href="#4-2-1-块对角FNO" class="headerlink" title="4.2.1 块对角FNO"></a>4.2.1 块对角FNO</h4><p>虽然 FNO 公式非常优雅，但它需要许多参数（每个token的 $d^2$ 个）。 因此，对自适应傅里叶神经算子 (AFNO) 的形式提出了修改。 一个重要的修改涉及强制token权重服从块对角线结构，与标准 FNO 相比，这显着提高了 AFNO 的训练和泛化能力，我们称这种架构为 B-FNO，归结为：</p>
<script type="math/tex; mode=display">
\begin{align*}
    f_{BFN}(\mathbf{X}_i)&:=\sigma\bigg (\mathbf{F}^{-1}
    \begin{bmatrix}
        (\mathbf{F}\mathbf{X}_i)^\top_1\mathbf{V}^{(1)}\\
        \cdots \\
        (\mathbf{F}\mathbf{X}_i)^\top_s\mathbf{V}^{(s)}
    \end{bmatrix}
    \bigg)\mathbf{W}_2\\
    \mathbf{L}^{(l)}&:=
    \begin{bmatrix}
        \mathbf{L}^{(l,1)} & &\\
        &\ddots \\
        &&\mathbf{L}^{(l,B)}
    \end{bmatrix}
    \in\mathbb{R}^{d\times m},l\in[s]\\
    \mathbf{W}_2&:=
    \begin{bmatrix}
        \mathbf{W}_2^{(1)}\\
        &\ddots \\
        && \mathbf{W}^B_2
    \end{bmatrix}\in\mathbb{R}^{m\times c}
\end{align*}
\tag{27}\label{eq27}</script><p><strong>引理4.6</strong><br>对于权重 $\mathbf{W}_{1b} \in \mathbb{R}^{sd/B \times m/B}$ 和 $\mathbf{W}_{2b} \in \mathbb{R}^{m/B \times c/B}$，假设 $\sigma$ 按元素操作，B-FNO 模型 \eqref{eq27} 可以等效地表示为：</p>
<script type="math/tex; mode=display">
\begin{align*}
    f_{BFN}(\mathbf{X}_i)&=[f^{(1)}_{BFN}(\mathbf{X}_i)\cdots f^{(B)}_{BFN}(\mathbf{X}_i)]\\
    f_{BFN}^{(b)}&=\sum\limits^m_{j=1}\sigma(\mathrm{circ}(\mathbf{X}_i^{(b)})\mathbf{w}_{1bj})\mathbf{w}^\top_{2bj}
\end{align*}
\tag{28}</script><p>其中 $\mathrm{circ}(\mathbf{X}_i^{(b)}) \in \mathbb{R}^{s \times sd/B}$ 是由所有 $s$ 来沿 $\mathbf{X}_i^{(b)} \in \mathbb{R}^{s \times d/B}$ 第一维的循环位移。</p>
<p>有趣的是，AFNO 的块对角线权重将 CNN 中的局部卷积与 全局，分组卷积( $B$ 组)进行对比。 我们因此定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p^\ast_{BFN}&:=\min_{\mathbf{W}_{1bj},\mathbf{W}_{2bj}}\sum\limits^n_{i=1}\mathcal{L}(f_{BFN}(\mathbf{X}_i),\mathbf{Y}_i)\\
    &+\frac{\beta}{2}\sum\limits^B_{b=1}\sum\limits^m_{j=1}\|\mathbf{w}_{1bj}\|^2_2+\|\mathbf{w}_{2bj}\|^2_2
\end{align*}
\tag{29}\label{eq29}</script><p><strong>定理4.7</strong><br>对于线性激活B-FNO训练问题\eqref{eq29}，对于 $m \geq m^\ast$ 和 $m^\ast \leq 1/B\min\{sd, c\}$，标准的非凸训练问题相当于一个凸优化问题，由下式给出：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p^\ast_{BFN}&=\min_{\mathbf{Z}_b}\sum\limits^n_{i=1}\mathcal{L}([f^{(1)}(\mathbf{X}_i)\cdots f^{(B)}(\mathbf{X}_i)],\mathbf{Y}_i)\\
    &+\beta\sum\limits^B_{b=1}\|\mathbf{Z}\|_\ast \\
    \mathbf{Z}_b & \in\mathbb{R}^{sd/B\times c/B},f^{(b)}:=\mathrm{circ}(\mathbf{X}_i^{(b)})\mathbf{Z}_b
\end{align*}
\tag{30}</script><p><strong>定理4.8</strong><br>前面给出了 ReLU 激活的 B-FNO 训练问题 \eqref{eq29}, 我们定义：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathbf{X}_b&:=
    \begin{bmatrix}
        \mathrm{circ}(\mathbf{X}_1^{(b)})\\
        \cdots \\
        \mathrm{circ}(\mathbf{X}_n^{(b)})
    \end{bmatrix}\\
    \{\mathbf{D}_{b,j} \}^{P_b}_{j=1}&:=\{\mathrm{diag}(1\{\mathbf{X}_b\mathbf{u}_j\geq 0 \}):\mathbf{u}_j\in\mathbb{R}^{sd/B} \}
\end{align*}</script><p>其中 $P_b \leq 2r_b\left(\frac{e(n-1)}{r_b}\right)^{r_b}$ 和 $r_b := \mathrm{rank}(\mathbf{X}_b )$。 那么，对于 $m \geq m^\ast$ 和 $m^\ast \leq n/B\min\{sd, c\}$，标准的非凸训练问题等价于以下凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p^\ast_{BFN}&=\min_{\mathbf{Z}_{b,j}}\sum\limits^n_{i=1}\mathcal{L}([f^{(1)}(\mathbf{X}_i)\cdots f^{(B)}(\mathbf{X}_i)],\mathbf{Y}_i) \\
    &+\beta\sum\limits^B_{b=1}\sum\limits^{P_b}_{j=1}\|\mathbf{Z}_{j,b}\|_{\ast,K_{b,j}}
\end{align*}
\tag{31}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathbf{Z}_{b,j} &\in \mathbb{R}^{sd/B \times c/B}, \, f^{(b)}(\mathbf{X}_i) := \sum_{j=1}^{P_b} \mathbf{D}_{b,j}\mathrm{circ}(\mathbf{X}_i^{(b)})\mathbf{Z}_{b,j} \\
    \mathbf{K}_{b,j} &:= (2\mathbf{D}_{b,j} - \mathbf{I}_{ns})\mathbf{X}_b.
\end{align*}</script><h2 id="5，数值结果"><a href="#5，数值结果" class="headerlink" title="5，数值结果"></a>5，数值结果</h2><p>在本节中，我们试图将本文分析的Transformer头的性能与基线凸优化方法进行比较，这种比较使我们能够在一个实际例子中说明这些新颖的头脑所带来的隐性偏置。特别是，我们考虑了训练这些凸头的单个新块以执行图像分类任务。这本质上是无需微调骨干网络的迁移学习，这在边缘计算和内存受限设置中可能是必须的。对于 few-shot 微调 transformer 任务，非凸优化在不同的随机初始化下不稳定，此外，仅微调网络的最后一层是一种常见做法，它在伪相关基准测试中表现非常好。</p>
<p>具体来说，我们试图对来自 CIFAR-100 数据集的图像进行分类，首先在 ImageNet-1k 数据集的 $224 \times 224$ 图像上使用 $16 \times 16$ 的块 ($ s=196$, $d=256$)。然后微调单凸头以对来自 CIFAR-100 的图像进行分类，同时保持预训练的骨干固定。</p>
<p>对于主干 gMLP 架构，我们在使用凸头进行训练之前，将特征维度减少到 $d=100$，并使用平均池作为预处理步骤。同样，为了提高计算效率，我们训练标准 ReLU 架构的门控 ReLU 变体，因为这些门控 ReLU 激活网络是不受约束的。对于 BFNO，我们选择 $B=5$。所有头部都使用相同的维度 $(d=100, s=196, c=100)$，我们选择ReLU头部中的神经元数量为 $m=100$，自注意力网络我们选择 $m=5$ 使参数计数大致相等。作为我们的基线，我们比较了一个简单的线性模型（即逻辑回归）和 MLP 的凸等价物，如第2.1节中所讨论的。</p>
<p>我们在表1中总结了结果，在这里，我们证明注意力变体优于标准凸 MLP 和线性基线。 这表明注意力结构的高阶信息和额外的自由度为困难的视觉任务提供了优势。 令人惊讶的是，对于自注意力、FNO 和 MLP-Mixer，线性和 ReLU 激活性能之间只有微小的差距，这表明这些架构的大部分优势在于它们的基本结构，而不是应用的非线性。 相反，对于 B-FNO，ReLU 和线性激活精度之间存在非常大的差距，这表明当应用组卷积时，这种非线性更为重要。 因此，这些凸化的架构为迁移学习的稳定和透明模型铺平了道路。</p>
<h2 id="6，总结"><a href="#6，总结" class="headerlink" title="6，总结"></a>6，总结</h2><p>我们证明了自注意力块和常见替代方案（如 MLP-Mixer、FNO 和 B-FNO）等价于线性和 ReLU 激活函数下的凸优化问题，这些等效的凸公式隐含地聚类相关特征，并在确保全局表示的情况下，用块核范数正则化器作为惩罚项。 对于未来的工作，仍然需要利用这些独特的正则化器的结构来为这些网络设计高效的近似求解器，可能会找到更快的求解器，例如 FISTA 或相关算法。 从长期的实际采用来看，未来的理论工作还需要对实践中经常使用的更深层次的网络进行分析。 通过选定凸公式，可以将这项工作用于设计新的网络架构。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>non-mercer</tag>
        <tag>convex duality</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines</title>
    <url>/2023/03/22/Transformer/Transformer%20and%20Infinite-Dimensional%20Non-Mercer%20Binary%20Kernel%20Machines/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：Transformer are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines</p>
<p><a href="https://arxiv.org/abs/2106.01506">essay link</a></p>
<p><strong>无限维non-mercer跟Transformer关联的文章，感觉这是最近五篇文章中最难的，主要泛函已经还给老师了，老天爷啊！</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>尽管它们在自然语言处理等核心 AI 领域无处不在，但Transformer等基于深度注意力的神经网络机制尚未得到充分理解。在本文中，我们提出了理解 Transformer 工作原理的新视角。</p>
<p>特别是，我们展示了作为 Transformer 操作核心的“点积注意力”可以表征为一对 Banach 空间上的核学习方法。尤其是，Transformer 的内核具有无限的特征维度。在此过程中，我们考虑将标准内核学习问题扩展到二进制设置，其中数据来自两个输入域，并且为每个跨域对定义了一个响应。</p>
<p>我们为这些具有非 Mercer（不定、不对称）内核的二进制内核机器证明了一个新的表示定理（这意味着学习的函数是再生内核 Banach 空间而不是 Hilbert 空间的元素），并且还证明了一个新的通用逼近定理表明 Transformer 计算可以学习任何二进制非 Mercer 再生内核 Banach 空间对。</p>
<p>我们在 Transformers 中用新内核进行实验，获得的结果表明标准 Transformer 内核的无限维度对其性能有部分影响。本文的结果为现代机器学习中一个非常重要但鲜为人知的模型提供了新的理论理解。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>自从 bahdanau_neural_2015 提出以来，所谓的神经注意力已经成为许多最先进的深度学习模型的支柱。在自然语言处理 (NLP) 中尤其如此，其中 Transformer 模型已经无处不在。这种无处不在的现象使得过去几年 NLP 的大部分突破都归功于为 Transformers 开发新的训练机制。</p>
<p>与大多数现代深度神经网络一样，对 Transformer 的理论理解已经落后于基于 Transformer 的 AI 任务（如 NLP）的性能改进速度。最近，一些作者注意到 Transformer 操作与深度学习理论中其他更容易理解的主题之间的关系，例如注意力和卷积之间的相似性以及多层 Transformer 中残差块的设计；另请参阅原始 Transformer 作者的官方参考代码库和最近的一些研究中对主要学习（全连接或注意）操作、元素非线性和归一化的重新排序用以研究更深层次的 Transformers 到归一化的“预规范”排序、学习操作、非线性，添加现代（“v2”）Resnets 的残差排序。</p>
<p>在本文中，我们提出了一个新的视角来理解 Transformer 的核心组件，即它的“点积注意力”操作。特别是，我们表明点积注意力可以表征为一类特定的内核方法。更具体地说，它是所谓的“非定”和“对称”的核方法，它们指的是经典类核的两个独立推广，不需要对称和（半）正定的经典假设确定性。事实上，我们在下面的定理2中表明，点积注意力可以学习任何不对称的不定核。</p>
<p>这种见解有几个有趣的含义。最直接的是，它为 Transformer 中一个更神秘的组件提供了一些理论依据。它还可能为应用数十年的经典核方法理论打开大门，以理解当今最重要的神经网络模型之一，这可能类似于数字信号处理工具如何广泛用于研究卷积神经网络。我们在本文的最后一点上迈出了第一步，提出了我们称为“二进制”内核机器的先前的内核方法泛化，它学习如何预测两个输入集中的元素对的不同值，类似于注意力模型。</p>
<p>本文的其余部分安排如下。Section2部分回顾了 Transformers 和经典内核方法的数学背景。Section3 节介绍了我们用来表征 Transformer 的再生内核 Banach 空间 (RKBS) 上的内核机器的定义。我们特别注意到，Transformer 可以描述为具有无限维特征的空间。Section4 开始了我们的理论结果，明确地描述了 Transformer 的再生内核，包括 Transformer 的内核特征映射的显式公式及其与先前内核的关系。Section5 部分讨论了作为内核学习器的Transformer，包括一个新的表示定理和将随机梯度下降训练的注意力网络表征为近似内核学习器。在第 Section6 节中，我们提供了经验证据，证明 Transformer 内核的无限维特征可能在某种程度上对模型的有效性负责。 Section7 部分给出结论，并总结了我们的工作。</p>
<h2 id="2，背景和相关工作"><a href="#2，背景和相关工作" class="headerlink" title="2，背景和相关工作"></a>2，背景和相关工作</h2><h3 id="2-1-Transformer神经网络模型"><a href="#2-1-Transformer神经网络模型" class="headerlink" title="2.1 Transformer神经网络模型"></a>2.1 Transformer神经网络模型</h3><p>Transformer 模型在自然语言处理等许多核心 AI 应用中无处不在。在这里，我们回顾了它的核心组件。假设我们有两组有序的向量，一组source元素 $\{s_1, s_2, \dots, s_S\},\;s_j \in \mathbb{R}^{d_s}$ 和一组target元素 $\{t_1, t_2, \dots, t_T\},\; t_i \in \mathbb{R}^{d_t}$。在其一般的形式中，构成 Transformer 模型主干的神经网络“注意力”操作是为每个 $t_i$， 计算源序列 $\{ s_j\}_{j=1}^s$ 的嵌入向量。通常，源集和目标集被认为是相同时，即 $s_i=t_i,\forall i$，这种注意力此时称为自注意力。</p>
<p>Transformer 中使用的特殊函数是所谓的“缩放点积”注意力，其形式为：</p>
<script type="math/tex; mode=display">
a_{ij}=\frac{(W^Qt_i)^T(W^Ks_j)}{\sqrt{d}},\alpha_{ij}=\frac{exp(a_{ij})}{\sum^S_{j=1}exp(a_{ij})}, t'_i=\sum\limits^S_{j=1}\alpha_{ij}W^Vs_j
\tag{1}\label{eq1}</script><p>其中 $W^V, W^K \in \mathbb{R}^{d_s \times d}$, 和 $W^Q \in \mathbb{R}^{d_t \times d}$ 是可学习的权重矩阵，通常称为分别为“value”、“key”和“query”权重矩阵。</p>
<p>通常具有独立参数矩阵的多个所谓的“注意力头”实现了方程(1)的几个并行计算，几个 $d$ 维头输出的笛卡尔积（向量级联）形成最终输出 $t’_i$。通常未归一化的 $a_{ij}$ 称为注意力分数或注意力对数，而归一化的 $\alpha_{ij}$ 称为注意力权重。</p>
<p>在本文中，我们将注意力限制在 方程(1) 中所示的注意力点积公式。其他几种注意力替代形式执行大致相同功能， 但Transformer 的点积公式是迄今为止最受欢迎的。</p>
<h3 id="2-2-核方法和生成核"><a href="#2-2-核方法和生成核" class="headerlink" title="2.2 核方法和生成核"></a>2.2 核方法和生成核</h3><p>核方法是一类经典而强大的机器学习方法，关键组成部分是核函数，它允许有效映射低维的输入数据，如用其他线性方案来解决此类问题（如分类或回归等）是不可能的，将低维输入数据映射到高维数据域或无限维嵌入域，这是有线性解决方案的。</p>
<p>给定两个非空集 $\mathcal{X}$ 和 $\mathcal{Y}$，核函数 $\kappa$ 是一个连续函数 $\kappa: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$。在接下来的几节中，我们将回顾经典的对称和（半）正定或 Mercer 内核，然后讨论更一般的形式。</p>
<h4 id="2-2-1-对称和半正定-Mercer-内核"><a href="#2-2-1-对称和半正定-Mercer-内核" class="headerlink" title="2.2.1 对称和半正定 (Mercer) 内核"></a>2.2.1 对称和半正定 (Mercer) 内核</h4><p>如果 $\mathcal{X}=\mathcal{Y}$，并且对于所有 $x_i, x_j \in \mathcal{X}=\mathcal{Y}$，特定内核 $\kappa$ 具有以下属性:</p>
<script type="math/tex; mode=display">
\begin{align*}
& \kappa (x_i,x_j) =\kappa (x_j,x_i)  \tag{2a}\label{eq2a}\\
c^T\mathbf{K}c\geq 0 \quad & \forall c\in\mathbb{R}^n;\quad i,j=1,\cdots n \tag{2b}\label{eq2b}
\end{align*}</script><p>其中 \eqref{eq2b} 中的 $\mathbf{K}$ 是 Gram 矩阵，定义为 $\mathbf{K}_{ij} = \kappa(x_i, x_j)$，那么 $\kappa$ 被称为 Mercer 内核。性质 \eqref{eq2a} 是对称性要求，而 \eqref{eq2b} 是（半）正定性的条件。</p>
<p>对于 Mercer 内核，众所周知，除其他事实外，</p>
<ul>
<li>(i) 我们可以在 $\mathcal{X}$ 上定义函数的 Hilbert 空间，表示为 $\mathcal{H}_\kappa$（称为再生核 Hilbert 空间，或 RKHS，与再生核 $\kappa$ 相关联）；</li>
<li>(ii) $\mathcal{H}_\kappa$ 对于每个 $x$ 都有一个（连续的）唯一元素 $\delta_x$，称为点评估泛函，具有性质 $f(x) = \delta_x(f) \quad \forall f \in \mathcal{H}_\kappa$；</li>
<li>(iii) $\kappa$ 具有所谓的复制属性，$\langle f,\kappa(x, \cdot)\rangle_{\mathcal{H}_\kappa} = f(x) \; \forall f \in \mathcal{H}_\kappa$，其中 $\langle\cdot，\cdot\rangle_{\mathcal{H}_\kappa}$ 是 $\mathcal{H}_\kappa$ 上的内积；</li>
<li>(iv) 我们可以定义一个“特征映射” $\Phi: \mathcal{X} \to \mathcal{F}_\mathcal{H}$，其中 $\mathcal{F}_\mathcal{H}$ 是另一个希尔伯特空间，有时也称为特征空间，$\kappa(x, y) = \langle\Phi(x), \Phi(y)\rangle_{\mathcal{F}_\mathcal{H}}$ （其中 $\langle\cdot, \cdot\rangle_{\mathcal{F}_\mathcal{H}}$ 是与 $\mathcal{F}_\mathcal{H}$ 关联的内积。</li>
</ul>
<p>最后一点指出了 RKHS 的内核技巧。</p>
<p>从机器学习和优化的角度来看，对称和正（半）定（PSD）内核是可取的，因为这些属性保证经验风险最小化内核学习问题（如支持向量机（SVM）、高斯过程等）是凸的。凸性为学习问题的易处理性和解决方案的最优性提供了有吸引力的保证。</p>
<h4 id="2-2-2-用非-Mercer-内核学习"><a href="#2-2-2-用非-Mercer-内核学习" class="headerlink" title="2.2.2 用非 Mercer 内核学习"></a>2.2.2 用非 Mercer 内核学习</h4><p>使用非 Mercer 内核或放松假设 \eqref{eq2a} 和 \eqref{eq2b} 的内核的学习方法已经研究了一段时间。工作 lin_study_2003等专注于使用对称但不正定的内核进行学习，即不满足 \eqref{eq2b}。自 schwartz_sous-espaces_1964 等以来，不定内核已被确定为所谓的再生内核 Krein 空间 (RKKS) 的再生内核。</p>
<p>在学习问题（例如具有不确定内核的 SVM）中替换 Mercer 内核使得优化问题通常是非凸的（因为内核 Gram 矩阵 $\mathcal{K}$ 不再总是 PSD）。一些使用不定内核学习的早期工作试图通过修改 Gram 矩阵的频谱来改善这个问题，使其再次变为 PSD。</p>
<p>最近，loosli_learning_2016 等提出了直接在 RKKS 中学习的优化程序。<br>他们报告说，与流行的 Mercer 核或光谱修改的不定核相比，使用不定核时在某些学习问题上的性能更好，这表明牺牲凸性可以根据经验提高性能。这个结论当然让人联想到深度神经网络的并发体验，由于其高度的非凸性，很难优化，但却提供了优于许多其他方法的性能。</p>
<p>另一项工作探索了内核方法​​在更一般的 Banach 空间中的学习应用，即再生内核 Banach 空间 (RKBS)。已经提出了各种结构作为 Banach 空间的再生核（取代 RKHS 的内积），包括半内积、通过傅立叶变换构造的正定双线性形式，以及其他。在这项工作中，我们考虑 RKBS 的内核可能既不是对称的也不是 PSD 的，接下来介绍这些空间的定义。</p>
<h2 id="3，一般再生内核-Banach-空间"><a href="#3，一般再生内核-Banach-空间" class="headerlink" title="3，一般再生内核 Banach 空间"></a>3，一般再生内核 Banach 空间</h2><p>最近，georgiev_construction_2014等 提出了 RKBS 及其再生内核的类似定义和构造，旨在包含先前的定义。在本文中，我们采用了定义的融合，并尝试使符号尽可能简单以满足我们的目的。</p>
<p><strong>定义1 再生内核 Banach 空间</strong><br>$\mathcal{X}$ 和 $\mathcal{Y}$ 是非空集合，核 $\kappa$ 是个度量函数，$\kappa:\mathcal{X} \times\mathcal{Y}\rightarrow\mathbb{R}$，$\mathcal{B}x$ 和 $\mathcal{B}y$ 是定义在$\mathcal{X}$ 和 $\mathcal{Y}$ 上带实数度量函数的Banach空间，定义 $\langle\cdot,\cdot\rangle:\mathcal{B}x\times\mathcal{B}y\rightarrow\mathbb{R}$ 是一个双线性非退化映射，使得：</p>
<script type="math/tex; mode=display">
\begin{align*}
\kappa(x,\cdot)\in\mathcal{B}y \quad & \forall x\in\mathcal{X}\tag{3a}\label{eq3a}\\
\langle f,\kappa(x,\cdot)\rangle_{\mathcal{B}x\times\mathcal{B}y}=f(x)\quad & \forall x\in\mathcal{X},f\in\mathcal{B}x \tag{3b}\label{eq3b}\\
\kappa(\cdot,y)\in\mathcal{B}x\quad & \forall y\in\mathcal{Y} \tag{3c}\label{eq3c}\\
\langle\kappa(\cdot,y),g\rangle_{\mathcal{B}x\times\mathcal{B}y}=g(y)\quad & \forall y\in\mathcal{Y},g\in\mathcal{B}y \tag{3d}\label{eq3d}
\end{align*}</script><p>那么，$\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 分别是 $\mathcal{X}$ 和 $\mathcal{Y}$ 上的一对再生核Banach空间（RKBS），$\kappa$ 是它们的再生核。</p>
<p>式 \eqref{eq3a} (或者\eqref{eq3c})的含义为，如果我们取 $\kappa$，两个变量 $x \in \mathcal{X}$ 和 $y \in \mathcal{Y}$ 的函数 , 并固定 $x$ (或者 $y$), 然后我们得到一个变量的函数。这个函数的一个变量必须是 $\mathcal{B}_\mathcal{Y}$ (或者 $\mathcal{B}_\mathcal{X}$) 的一个元素。<br>式子 \eqref{eq3b} 和 \eqref{eq3d} 是 $\kappa$ 再生属性。</p>
<p>出于我们的目的，扩展此定义以包含类似于 RKHS 的某些解释中针对特征使用的“特征映射”将很有用。</p>
<p><strong>定义2(RKBS 的特征映射)</strong><br>对于定义1中定义的一对 RKBS，假设存在映射 $\Phi_\mathcal{X}: \mathcal{X} \to \mathcal{F}_\mathcal{X}, \Phi_\mathcal{Y}: \mathcal{Y} \to \mathcal{F}_\mathcal{Y}$，其中 $\mathcal{F}_\mathcal{X}$ 和 $\mathcal{F}_\mathcal{Y}$ 是我们称为特征空间的 Banach 空间，以及一个非退化双线性映射 $\langle\cdot,\cdot\rangle_{\mathcal{F}_\mathcal{X} \times \mathcal{F}_\mathcal{Y}}: \mathcal{F}_\mathcal{X} \times \mathcal{F}_\mathcal{Y}\to \mathbb{R}$ 这样就有：</p>
<script type="math/tex; mode=display">
\kappa(x,y)=\langle\Phi_{\mathcal{X}(x)},\Phi_{\mathcal{Y}(y)}\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}}\quad \forall x\in\mathcal{X},y\in\mathcal{Y} 
\tag{4}\label{eq4}</script><p>在这种情况下，空格 $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 可以定义如下，可参考文献 xu_generalized_2019 。</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathcal{B}_{\mathcal{X}} & =\{f_v:\mathcal{X}\rightarrow\mathbb{R}:f_v(x)\triangleq\langle\Phi_{\mathcal{X}}(x),v\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}};v\in\mathcal{F}_{\mathcal{Y}},x\in\mathcal{X} \tag{5a}\label{eq5a}\\
\mathcal{B}_{\mathcal{Y}} & =\{g_u:\mathcal{Y}\rightarrow\mathbb{R}:g_u(y)\triangleq\langle u,\Phi_{\mathcal{Y}}(y)\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}};u\in\mathcal{F}_{\mathcal{X}},y\in\mathcal{Y} \tag{5b}\label{eq5b}
\end{align*}</script><p><strong>注意1</strong><br>我们简要讨论如何理解 \eqref{eq5a}和\eqref{eq5b} 给出的空间，以 \eqref{eq5a} 为例。它是一个变量 $x$ 的实值函数空间，其中该函数也由 $v$ 参数化。在 \eqref{eq5a} 中选取 $v \in \mathcal{F}_\mathcal{Y}$ 定义 $\mathcal{B}_\mathcal{X}$ 中的函数流形。这种具有固定 $v$ 的函数流形随函数 $\Phi_\mathcal{X}$ 而变化。通过取 $\Phi_\mathcal{X}(x)$ 和选定的 $v$ 的双线性积来定义在点 $x$ 处计算此流形中的函数 $f_v$。这也意味着我们可以结合 \eqref{eq4},\eqref{eq5a} 和 \eqref{eq5b}有：</p>
<script type="math/tex; mode=display">
\kappa(x,y)=\langle\Phi_{\mathcal{X}}(x),\Phi_{\mathcal{Y}}(y)\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}}=\langle f_{\Phi_{\mathcal{X}}(x)},g_{\Phi_{\mathcal{Y}}(y)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}} 
\tag{6}\label{eq6}</script><p>对所有的 $x\in \mathbb{X}, y\in\mathbb{Y}$</p>
<p><strong>注意2</strong><br>如果 $\Phi_\mathcal{X}(x)$ 和 $\Phi_\mathcal{Y}(y)$ 可以表示为实值可测函数的可数集，$\{\phi_\mathcal{X}(x)_\ell\}_{\ell \in \mathbb{N}}$ 和 $\{\phi_\mathcal{Y}(y)_\ell\}_{\ell \in \mathbb{N}}$ 对于 $(\phi_{\mathcal{X}})_\ell: \mathcal{X} \to \mathbb{R}$ 和 $(\phi_{\mathcal{Y}})_\ell: \mathcal{Y} \to \mathbb{R}$ （即 $\mathcal{F}_\mathcal{X}, \mathcal{F}_\mathcal{Y} \subset \prod_{\ell \in \mathbb{N}} \mathbb{R}$）； 而且 $\langle u,v\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}} = \sum_{\ell \in \mathbb{N}} u_\ell v_\ell$ 对 $u \in \mathcal{F}_\mathcal{X}, v \in \mathcal{F}_\mathcal{Y}$ 都成立。 那么我们从 lin_reproducing_2019 借用其“特征映射”构造对应于 xu_generalized_2019 的“广义 Mercer 内核”。</p>
<h2 id="4，作为-RKBS-内核的点积注意力"><a href="#4，作为-RKBS-内核的点积注意力" class="headerlink" title="4，作为 RKBS 内核的点积注意力"></a>4，作为 RKBS 内核的点积注意力</h2><p>我们现在正式陈述作为 RKBS 学习者的点积注意力公式。与 RKHS 非常相似，对于给定的内核及其关联的 RKBS 对，特征映射（以及双线性映射）不是唯一的。在下文中，我们基于其他内核的经典特征（例如 RBF 内核）展示了一个特征映射。</p>
<p><strong>命题1</strong><br>方程\eqref{eq1} 的（缩放）点积注意力计算是定义1和2意义上的 RKBS 的再生内核，输入集 $\mathcal{X}$ 和 $\mathcal{Y}$ 分别是目标元素 $\{t_i\}_{i=1}^T, \; t_i \in \mathbb{R}^{d_t}$ 和源元素 $\{s_j\}_{j=1}^S, s_j \in \mathbb{R}^{d_s}$ 所在的向量空间。那么特征映射为：</p>
<script type="math/tex; mode=display">
\begin{align*}
\Phi_{\mathcal{X}}(t) & =\sum\limits^{\infty}_{n=0}\sum\limits_{p_1+\cdots+p_d=n}\frac{\sqrt{\frac{n!}{p_1!\cdots p_d!}}\prod^d_{l=1}(q_\ell)^{p_\ell}}{d^{1/4}} \tag{7a}\label{eq7a}\\
\Phi_{\mathcal{Y}}(s) & =\sum\limits^{\infty}_{n=0}\sum\limits_{p_1+\cdots+p_d=n}\frac{\sqrt{\frac{n!}{p_1!\cdots p_d!}}\prod^d_{l=1}(k_\ell)^{p_\ell}}{d^{1/4}} \tag{7b}\label{eq7b}
\end{align*}</script><p>其中 $q_\ell$ 是 $q = W^Qt$ 的第 $\ell$ 个元素，$k_\ell$ 是 $k = W^Ks$ 的第 $\ell$ 个元素，其中 $W^Q \in \mathbb{R}^{d \times d_t}, W^K \in \mathbb{R}^{d \times d_s}$，在 $d \leq d_s$ 的条件下，而且 $rank(W^Q) = rank(W^K) = d$。双线性映射为 $\langle\Phi_\mathcal{X}(t),\Phi_\mathcal{Y}(s)\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}} = \Phi_\mathcal{X}(t) \cdot \Phi_\mathcal{Y}(s)$，巴拿赫空间定义为：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathcal{B}_{\mathcal{X}} & =\{f_k(t)=exp((W^Qt)^Tk/\sqrt{d});\; k\in\mathcal{F}_{\mathcal{Y}},t\in\mathcal{X} \} \tag{8a}\label{eq8a}\\
\mathcal{B}_{\mathcal{Y}} & =\{g_q(s)=exp(q^T(W^Ks)/\sqrt{d});\; q\in\mathcal{F}_{\mathcal{X}},s\in\mathcal{Y} \} \tag{8b}\label{eq8b}
\end{align*}</script><p>使用“幂级数query-key内核”相关的再生内核，定义如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
\kappa(t,s) & =\langle\Phi_{\mathcal{X}}(t),\Phi_{\mathcal{Y}}(s)\rangle_{\mathcal{F}_{\mathcal{X}}\times\mathcal{F}_{\mathcal{Y}}}\tag{}\\ 
&  =\langle f_{\Phi_{\mathcal{Y}}(s)},g_{\Phi_{\mathcal{X}}(t)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}}\tag{}\\
& =exp(\frac{(W^Qt)^T(W^Ks)}{\sqrt{d}}) \tag{9}\label{eq9}
\end{align*}</script><p>命题1的证明很简单，涉及通过将 \eqref{eq9} 中的两个无限级数相乘来验证 \eqref{eq7a}和\eqref{eq7b}，然后使用多项式定理和指数的泰勒展开。</p>
<p>在上面，当特别提到 Transformer 类型的模型而不是一般的 RKBS 时，我们使用 $t,s,q$ 和 $k$ 来表示 $x、y、u$ 和 $v$ ，分别绘制 RKBS 的元素与广泛使用的术语“目标”、“源”、“查询”和“键”之间的联系。</p>
<p>$W^Q$ 和 $W^K$ 的秩要求，意味着 $span(\{\Phi_\mathcal{X}(t), t \in \mathcal{X}\}) = \mathcal{F}_\mathcal{X}$ 和 $span(\{\Phi_\mathcal{Y}(s), s \in \mathcal{Y}\}) = \mathcal{F}_\mathcal{Y}$。这反过来意味着双线性映射是非退化的。</p>
<p><strong>注意3</strong><br>现在我们有了一对 RKBS 的例子，我们可以更具体地讨论注意1中的一些讨论。例如，验证 \eqref{eq8a}，当我们在 $\mathcal{F}_\mathcal{Y}$ 中选择 $k$ 时，在 $\mathcal{B}_\mathcal{X}$ 中定义了一个流形函数，其中 $k$ 是固定的 , 但 $W^Q$ 可以变化。类似地，选择 $q \in \mathcal{F}_\mathcal{X}$ 在 $\mathcal{B}_\mathcal{Y}$ 中定义流形。从 $\mathcal{F}_\mathcal{X}$ 和 $\mathcal{F}_\mathcal{Y}$ 中选择一个元素会将我们锁定到 $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 中的一个元素，这导致 \eqref{eq6} 中的相等性。</p>
<p><strong>注意4</strong><br>检查 \eqref{eq8a}-\eqref{eq9}，可以看到从 $\mathcal{F}_\mathcal{Y}$ 提取的元素参数化了 $\mathcal{B}_\mathcal{X}$ 的元素，如 \eqref{eq8a} 所示，是 $\Phi_\mathcal{Y}$ 的函数（反之亦然，对于 \eqref{eq8b}）。 这揭示了 Transformer 类型的注意力计算是针对 SVM 等应用程序考虑的 RKBS 泛化的确切机制，这些功能空间的其中之一被认为是固定的。</p>
<p><strong>注意5</strong><br>由于特征映射定义了 Banach 空间 \eqref{eq5a} 和 \eqref{eq5b}，学习到的参数 $W^Q$ 和 $W^K$ 意味着 Transformer 学习 RKBS 自身的参数表示。这与经典内核方法形成对比，在经典内核方法中，内核（以及再生空间）通常是固定的。事实上，在下面的定理2中，我们证明了 Transformer 架构（的变体）可以近似任何 RKBS 映射。</p>
<p><strong>注意6</strong><br>已知指数点积内核的对称版本是所谓的巴格曼空间的再生内核，它出现在量子力学中。</p>
<p><strong>注意7</strong><br>在命题1中值得注意的是，我们将点积注意力的内核定义为包括 softmax 运算的指数。因此，此操作的输出不是注意力分数 $a_{ij}$，而是非标准化的注意力权重 $\bar{\alpha}_{ij} = \alpha_{ij} \sum_j \alpha_{ij}$。将指数视为核运算的一部分表明，Transformer 的特征空间实际上是无限维的，就像 RBF 核被称为具有无限维特征空间一样。在第6节中，我们发现经验证据表明这种无限维度可能是 Transformer 有效性的部分原因。</p>
<h2 id="5，Transformer作为核学习者"><a href="#5，Transformer作为核学习者" class="headerlink" title="5，Transformer作为核学习者"></a>5，Transformer作为核学习者</h2><h3 id="5-1-二元-RKBS-学习问题及其表示定理"><a href="#5-1-二元-RKBS-学习问题及其表示定理" class="headerlink" title="5.1 二元 RKBS 学习问题及其表示定理"></a>5.1 二元 RKBS 学习问题及其表示定理</h3><p>大多数内核学习问题都采用经验风险最小化问题的形式。例如，如果我们有一个有限数据集 $(x_1, z_1), \dots, (x_n, z_n), x_i \in \mathcal{X}, z_i \in \mathbb{R}$ 的学习问题，并且想要学习一个函数 $f： \mathcal{X} \to \mathbb{R}$ 在 RKHS $\mathcal{H}_\mathcal{K}$ 中，学习问题可以写成：</p>
<script type="math/tex; mode=display">
f^\ast=\mathop{argmin}_{f\in\mathcal{H}_{\kappa}}\frac{1}{n}\sum\limits^n_{i=1}L(x_i,z_i,f(x_i))+\lambda R(\|f\|_{\mathcal{H}_{\kappa}}) 
\tag{10}\label{eq10}</script><p>其中 $L: \mathcal{X} \times \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ 是凸损失函数，$R: [0, \infty) \to \mathbb{R}$ 是严格递增的正则化函数，$\lambda$ 是比例常数。最近考虑在 RKBS 中学习的参考文献考虑了与 \eqref{eq10} 类似的问题，但 RKHS 中 $\mathcal{H}$ 替换为 RKBS。</p>
<p>然而，注意力的内核学习问题与 \eqref{eq10} 的不同之处在于，正如我们在上一节中讨论的那样，我们需要对每个参数对 $(t_i, s_j)$ 预测响应 $z_{ij}$（即注意力 logit）。这激发了在输入空间对上运行的经典内核学习问题类的泛化。我们现在讨论这种概括。</p>
<p><strong>定义3(二元核学习问题——正则化经验风险最小化)</strong></p>
<p>设 $\mathcal{X}$ 和 $\mathcal{Y}$ 为非空集，分别定义其上的RKBS $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 。设 $\langle\cdot,\cdot\rangle_{\mathcal{B}_\mathcal{X}\times\mathcal{B}_\mathcal{Y}}: \mathcal{B}_\mathcal{X} \times \mathcal{B}_\mathcal{Y} \to \mathbb{R}$ 是两个 RKBS 上的双线性映射。设 $\Phi_\mathcal{X}: \mathcal{X} \to \mathcal{F}_\mathcal{X}$，$\Phi_\mathcal{Y}: \mathcal{Y} \to \mathcal{F}_\mathcal{Y}$ 是固定特征映射，其属性为 $\langle\Phi_\mathcal{X}(x_i),\Phi_\mathcal{Y}(y_i)\rangle_{\mathcal{F}_\mathcal{X}\times\mathcal{F}_\mathcal{Y}} = \langle f_{\Phi_\mathcal{Y}(y)},g_{\Phi_\mathcal{X}(x)}\rangle_{\mathcal{B}_\mathcal{X}\times\mathcal{B}_\mathcal{Y}}$。若 $\{x_1, \dots, x_{n_x}\}, x_i \in \mathcal{X}$, $\{y_1, \dots, y_{n_y}\}, y_j \in \mathcal{Y}$, 且 $\{z_ {ij}\}_{i=1,\dots,n_x; \; j=1,\dots,n_y},\;z_{ij} \in \mathbb{R}$ 是有限数据集，其中为定义在 $(i,j)$ 的 $x_i$ 和 $y_j$ 数据对定义响应 $z_{ij}$。设 $L: \mathcal{X} \times \mathcal{Y} \times \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ 是固定 $(x_i, y_j, z_{i,j})$ 的凸损失函数，定义 $R_{\mathcal{X}}: [0, \infty) \to \mathbb{R}$ 和 $R_{\mathcal{Y}}: [0, \infty) \to \mathbb{R}$ 是凸的，且为严格增加正则化函数。</p>
<p>用于在一对 RKBS 上学习的二进制经验风险最小化核学习问题采用以下形式</p>
<script type="math/tex; mode=display">
\begin{align*}
f^\ast,g^\ast=\mathop{argmin}_{f\in\mathcal{B_{\mathcal{X}}},g\in\mathcal{B_{\mathcal{Y}}}}[\frac{1}{n_xn_y}\sum\limits_{i,j}L(x_i,y_j,z_{ij},\langle f_{\Phi_{\mathcal{Y}}(y_j)},g_{\Phi_{\mathcal{X}}(x_i)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}})\tag{}\\
+\lambda_{\mathcal{X}}R_{\mathcal{X}}(\|f\|_{\mathcal{B_{\mathcal{X}}}})+\lambda_{\mathcal{Y}}R_{\mathcal{Y}}(\|g\|_{\mathcal{B_{\mathcal{Y}}}})] \tag{11}\label{eq11}
\end{align*}</script><p>其中 $\lambda_\mathcal{X}$ 和 $\lambda_\mathcal{Y}$ 又是缩放常数。</p>
<p><strong>注意8</strong><br>在两个集合组成的成对数据上运行的二进制内核问题的想法并不是全新的：在协同过滤和张量内核方法等文献中都有先前的工作。我们的问题和结果在泛化到 Banach 而不是 Hilbert 空间方面是新的：正如 RKBS 文献中的前置工作指出的那样，RKBS 学习问题与 RKHS 学习问题的不同之处在于它们的额外非线性 和/或非凸性。因此，将二元学习问题扩展到 Banach 空间是由 Transformer 设置推动的，其中内核方法处于非线性和非凸深度神经网络的上下文中，而不是像 SVM 或矩阵完成那样的浅层学习器。有关更多讨论，请参阅附录A。</p>
<p>几乎所有经典的内核学习方法都能找到其形式由所谓的表示定理指定的解决方案。表示定理指出，再生核空间上正则化经验风险最小化问题的解可以表示为再生核对数据集评估的线性组合。因此，内核学习问题的经典解决方案简化为寻找该线性组合的系数。</p>
<p>fasshauer_solving_2015 等为 RKBS 学习问题提供了表示定理。然而，他们的定理只处理学习问题，其中数据点仅来自定义再生内核的集合之一（即，只有 $\mathcal{X}$ 而没有 $\mathcal{Y}$），这意味着寻求的解决方案是只有一个 Banach 空间（例如 $f: \mathcal{X} \to \mathbb{R}, f \in \mathcal{B}_\mathcal{X}$）。在这里，我们陈述并证明了定义3中提出的与 Transformers 更相关的二元情况的定理。</p>
<p><strong>定理1</strong><br>假设我们有一个形式为 \eqref{eq11} 的内核学习问题。设 $\kappa: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ 为 RKBS 上 $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 的再生内核，满足定义1 和 定义2。然后，给定 $\mathcal{B}_\mathcal{X}$ 和 $\mathcal{B}_\mathcal{Y}$ 的一些条件（参见附录B），正则化经验风险最小化问题 \eqref{eq11} 就有一个唯一的解法数据对 $(f^_, g^_)$，具有以下性质：</p>
<script type="math/tex; mode=display">
\iota(f^\ast)=\sum\limits^{n_x}_{i=1}\xi_i\kappa(x_i,\cdot);\quad \iota(g^\ast)=\sum\limits^{n_y}_{j=1}\zeta_j\kappa(\cdot,y_j) 
\tag{12}\label{eq12}</script><p>其中 $\iota(f)$ (或者 $\iota(g)$) 表示 $f$ (resp. $g$) 范数的 Gateaux 导数，约定 $\iota(0) \triangleq 0$，且有 $\xi_i,\zeta_j \in \mathbb{R}$。</p>
<p>证明见附录B。</p>
<h3 id="5-2-一个新的近似核学习问题和通用逼近定理"><a href="#5-2-一个新的近似核学习问题和通用逼近定理" class="headerlink" title="5.2 一个新的近似核学习问题和通用逼近定理"></a>5.2 一个新的近似核学习问题和通用逼近定理</h3><p>按照表示定理的建议，寻找内核学习问题（如 \eqref{eq10} 或 \eqref{eq11} 形式的 \eqref{eq12} 解决方案的缺点是它们很难扩展到大型数据集。众所周知，对于 RKHS 学习问题，找到与内核评估相乘的标量系数需要花费大约数据集大小三次方的时间，而查询模型需要线性时间。最流行的一类近似技术基于所谓的 Nystrom 方法，它构造核 Gram 矩阵的低阶近似并求解由该近似生成的问题。最近的一项工作已将 Nystrom 方法扩展到 RKKS 学习。</p>
<p>在本节中，我们将 Transformer 学习问题描述为一类新的近似核方法——可以称之为“蒸馏”方法。我们现在正式陈述这个想法。</p>
<p><strong>命题2（二进制核学习问题的参数化近似求解）</strong><br>考虑定义3中二进制内核学习问题的陈述，我们想找到解决方案对 $(f^\ast, g^\ast)$ 的近似值。特别地，我们会说我们想要一个近似值 $\hat \kappa: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ 这样就有：</p>
<script type="math/tex; mode=display">
\hat{\kappa}(x,y)\approx \langle f^\ast_{\Phi_{\mathcal{Y}}(y)},g^\ast_{\Phi_{\mathcal{X}}(x)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}} 
\tag{13}\label{eq13}</script><p>对于所有 $x \in \mathcal{X}$ 和 $y \in \mathcal{Y}$ 都成立。</p>
<p>比较 \eqref{eq13} 和 \eqref{eq6} 提出了一个解决方案：学习一个近似 $\kappa$ 的函数 $\hat \kappa$。特别是，\eqref{eq6} 建议学习特征映射的显式近似，即</p>
<script type="math/tex; mode=display">\hat{\kappa}(x,y)\approx\langle f^\ast_{\Phi_{\mathcal{Y}}(y)},g^\ast_{\Phi_{\mathcal{X}}(x)}\rangle_{\mathcal{B}_{\mathcal{X}}\times\mathcal{B}_{\mathcal{Y}}}</script><p>事实上，Transformer q-k映射正是这样做的。也就是说，虽然命题1中概述的 Transformer 核计算是有限维的，但它实际上可以近似潜在的无限维最优解 $(f^\ast, g^\ast)$，其特征在于定理1。下面证明这一事实。</p>
<p><strong>定理2</strong><br>设 $\mathcal{X} \subset \mathbb{R}^{d_t}$ 和 $\mathcal{Y} \subset \mathbb{R}^{d_s}$ 是紧致的； $t \in \mathcal{X}, s \in \mathcal{Y}$; 并设 $q_\ell: \mathcal{X} \to \mathbb{R}$ 和 $k_\ell: \mathcal{Y} \to \mathbb{R}$，且 $\ell=1, \dots, d$ 是两层神经网络，其中 $ m$ 是隐藏单元个数。然后，对于任何连续函数 $F: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ 和 $\epsilon &gt;0$，存在整数 $m, d &gt;0$ 使得：</p>
<script type="math/tex; mode=display">\bigg |F(t,s)-\sum^d_{l=1}q_l(t)k_l(s) \bigg |< \epsilon,\quad \forall t\in\mathcal{X},s\in\mathcal{Y}
\tag{14}\label{eq14}</script><p>证明详见附录C。</p>
<p>我们现在概述定理2与Transformer的关系。如果我们将两层神经网络 $\{q_\ell\}_{\ell=1}^d$ 和 $\{k_\ell\}_{\ell=1}^d$ 的输出连接成$d$ 维向量 $q: \mathbb{R}^{d_t} \to \mathbb{R}^d$ 和 $k: \mathbb{R}^{d_s} \to \mathbb{R}^d$，然后点积 $q(t)^Tk(s)$ 由 \eqref{eq14} 中的和表示，可以逼近 $\mathcal{X} \times \mathcal{Y}$ 上的任何实值连续函数。减去通用逼近定理应用中的常见警告（即，在实践中，输出元素共享隐藏单元而不是独立单元），这个点积正是注意力的对数 $a_{ij}$ 的计算结果，即 $F(t,s) \approx \log \kappa(t,s)$ 对于 \eqref{eq14} 中的 $F$ 和 \eqref{eq9} 中的 $\kappa$ 逼近直到一个比例常数 $\sqrt{d}$。</p>
<p>由于注意力对数和 Transformers 中使用的求幂q-k内核之间的指数映射是一对一的映射，如果我们取 $F(t, s) = \log \langle f^\ast_{\Phi_\mathcal{Y}(s)},g^\ast_{\Phi_\mathcal{X}(t)}\rangle_{\mathcal{B_{\mathcal{X}}\times\mathcal{B_{\mathcal{Y}}}}}$，那么我们可以使用 Transformer 的点积注意力很好地逼近任意 RKBS 中的最优解。</p>
<p>基于注意力的深度神经网络的核心思想是通过随机梯度下降学习 $q_\ell$ 和 $k_\ell$ 的参数表示。与传统的基于表示定理的学习函数不同，基于注意力的内核机器（如深度Transformer）的训练时间（通常，但没有保证）随数据集大小呈亚立方体缩放，而无论数据集大小如何，评估时间都保持不变。</p>
<h2 id="6，取幂的点积对Transformer来说是必不可少的吗？"><a href="#6，取幂的点积对Transformer来说是必不可少的吗？" class="headerlink" title="6，取幂的点积对Transformer来说是必不可少的吗？"></a>6，取幂的点积对Transformer来说是必不可少的吗？</h2><p>我们研究了具有经典内核机器中使用的多个内核 Transformer 的修改，训练了两个标准的机器翻译数据集和两个标准的情感分类任务。对于机器翻译，IWSLT14 DE-EN 是一个相对较小的数据集，而 WMT14 EN-FR 是一个相当大的数据集。对于情感分类，考虑 SST-2 和 SST-5。保留标准的非对称查询和关键特征映射，即 $q = W^Qt$ 和 $k = W^Ks$，并且只修改内核 $\kappa : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}_{\geq 0}$。下面，$\tau &gt;0$ 和 $\gamma \in \mathbb{R}$ 是每个头学习的标量。</p>
<p>我们感兴趣的内核是：</p>
<ul>
<li>(缩放)取幂点积 (EDP)，$\kappa(q,t) = \exp(q^Tk/\sqrt{d})$，即标准 Transformer 内核；</li>
<li>径向基函数 (RBF) 内核，$\kappa(q,t) = \exp(|-{\tau}/\sqrt{d} (q - k)|^2_2)$，其中 $| \cdot |_2$ 是标准的 2-范数。众所周知，RBF 内核是取幂点积的规范化版本，规范化使其具有平移不变性；</li>
<li>vanilla L2 距离，$\kappa(q,t) = {\tau}/\sqrt{d} | q - k |_2$;</li>
<li>交集内核的指数版本，$\kappa(q,t) = \exp(\sum_{\ell=1}^d \min( q_\ell, k_\ell ))$。交集内核的对称版本在计算机视觉应用的内核机器中很流行，通常被描述为具有关联的 RKHS，它是函数空间 $L^2$ 的子空间（即，在具有连续函数的特征空间的意义上，它是无限维的，与 EDP和RBF的无限维无限级数相反）；</li>
<li>一个二次多项式内核，$\kappa(q,t) = ( 1/\sqrt{d} q^Tk + \gamma)^2$。</li>
</ul>
<p>附录D中提供了完整的实施细节。</p>
<p>机器翻译的结果显示在表1中，几个结果脱颖而出。首先，据称具有无限维特征空间的取幂点积、RBF 和取幂交集核确实比具有低维特征映射的核（如二次核）表现更好。事实上，RBF 和 EDP 内核的性能大致相同，这表明深度 Transformer 可能不需要使 RBF 内核优于经典内核机器中的 EDP 的平移不变性。有趣的是，在 IWSLT14 DE-EN 上，（非正统的）取幂交集内核与 EDP 和 RBF 内核的性能大致相同，但在 WMT14 EN-FR 上稍差。如前所述，EDP 和 RBF 核具有无穷级数的特征空间，而交集核对应于连续函数的特征空间。在这两个数据集上，二次核比最好的无限维核表现稍差，而 L2 距离表现明显更差。</p>
<p>情绪分类的结果显示在表2中。与机器翻译实验不同，无限维内核在此任务上并不严格优于有限维内核。事实上，这里明显的输家是取幂的交集内核，而在机器翻译中表现最差的 L2 距离与表现最好的内核的标准差不超过一个标准差。然而，值得注意的是，情感分类测试准确度的差异意味着不可能在此任务上选择具有统计显着性的“最佳”。内核间的小变化可能与这个问题的相对简单性（以及数据集的相对较小）与机器翻译有关：也许不需要无限维的特征空间来在更简单的上获得 Transformer 级别的性能问题。</p>
<p>值得注意的是，取幂的点积内核（同样是标准的 Transformer 内核）始终保持高性能。这可能是他们所享有的通用近似属性的实际用途的实验证据（参见定理2）。</p>
<p>内核之间相对较小但具有统计显着性的性能差异让人联想到神经网络的激活函数（ReLU、ELU 等）的相同现象。此外，与 SST 情感分析任务的小得多的性能差异相比，机器翻译的内核间性能差异很大，表明不同复杂性问题的不同表征需求。总的来说，这些结果表明内核选择可能是 Transformer 网络的一个额外设计参数。</p>
<h2 id="7，结论"><a href="#7，结论" class="headerlink" title="7，结论"></a>7，结论</h2><p>在本文中，我们将经典内核方法与最先进的 Transformer 网络联系起来。除了对开发新的 RKBS 表示定理和其他核理论的理论兴趣之外，我们对什么可能使 Transformers 起作用有了新的认识。</p>
<p>我们的实验结果表明，Transformer 内核的无限维数使其成为应用中的一个很好的选择，类似于 RBF 内核如何成为标准选择，例如支持向量机。我们的工作还揭示了 Transformer 研究的新途径。例如，我们的实验结果表明，Transformer 内核的选择与神经网络设计中的激活函数具有相似的设计选择。</p>
<p>新的开放研究问题包括：</p>
<ul>
<li>(1) 指数点积是否应该始终是首选，或者不同的内核是否更适合不同的任务（参见 GELU 最近如何在 Transformers 中替代 ReLU 变得非常流行）；</li>
<li>(2) 用于结构化预测的向量值内核与例如多个注意力头之间的任何关系；</li>
<li>(3) 将 Transformer 类型的深度内核学习器扩展到非欧几里德数据（使用，例如，图形内核或流形上的内核）。</li>
</ul>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>non-mercer</tag>
      </tags>
  </entry>
  <entry>
    <title>Lifted Proximal Operator Machines</title>
    <url>/2023/03/22/Transformer/Lifted%20Proximal%20Operator%20Machines/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>title: Lifted Proximal Operator Machines</p>
<p><a href="https://arxiv.org/abs/1811.01501">essay link</a></p>
<p><strong>提升近端算子机，将非凸问题进行凸化的利器，已用于分析DNN的网络和训练策略等~</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一种用于训练前馈神经网络的新优化方法，通过将激活函数重写为等效的近端算子，然后将近端算子以正则项添加到目标函数来近似前馈神经网络，因此称本文提出的方法为提升近端算子机（LPOM）。 </p>
<p>LPOM 在所有权重或者激活层中都是块多凸的，这允许我们使用块坐标下降并行更新逐层权重和激活。最值得注意的是，我们只使用激活函数本身，而不是它的导数，从而避免基于梯度方法中的梯度消失或爆炸问题。所以我们的方法适用于各种非递减 Lipschitz 连续激活函数，而且可以是饱和的和不可微的。 LPOM 不比逐层激活需要更多的辅助变量，因此与 SGD方法使用的内存量大致相同。我们也证明了逐层更新权重和激活的收敛性，数据集 MNIST 和 CIFAR-10 上的实验证明了 LPOM 的优势。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>前馈深度神经网络 (DNN) 是完全级联的连接层，没有反馈连接。在最近年，随着硬件和数据集规模的进步，前馈 DNN 已成为许多任务的标准，例如图像识别, 语音识别，自然语言理解，并作为Go游戏学习系统的块。</p>
<p>几十年来，训练 DNN 是通过优化一个高度非凸且嵌套的网络权重函数来完成的。训练 DNN 的主要方法是随机梯度下降(SGD)，其有效性由DNN 在各种领域的实际应用中已经得到证明。最近，SGD 的许多变体已被提出，它使用自适应学习率和动量项，例如，Nesterov momentum，AdaGrad，RMSProp 和 Adam。 SGD 及其变体使用少量训练样本来估计全局梯度，使得每次迭代计算复杂度小。此外，估计的梯度是有噪声的，有助于鞍点逃逸。然而，他们也有一些缺点。一个主要问题是梯度消失或爆炸问题，即很多层的梯度指数级的减少或增加。这会导致缓慢或不稳定的收敛，尤其是在非常深的网络中更甚。这个缺陷可以通过使用非饱和激活函数来消除，例如整流线性单元 (ReLU) ，以及网络结构的修改，例如 ResNet。但是，根本问题依然存在。此外，他们无法直接处理不可微的激活函数（例如，二值化神经网络） 并且不允许跨层的并行权重更新。有关更多SGD限制的讨论请参考taylor2016training。</p>
<p>SGD 的缺点激发了对训练 DNN 的替代方法研究。最近在训练一个前馈神经网络被表述为带约束的优化问题，其中网络激活作为辅助变量引入，网络配置由分层约束保证。它打破了嵌套函数之间的依赖关系，转变为等式约束，因此可使用许多标准优化方法。许多工作研究了这种方法，不同之处在于如何处理等式约束。carreira2014distributed 将等式约束近似为二次正则项，交替优化网络权重和激活。zeng2018global 提出每层多一个辅助变量块，也通过二次正则项近似等式约束。受乘法器交替方向法启发(ADMM)，taylor2016training等使用了增广拉格朗日方法获得等式约束的精确增强。然而，这两种方法涉及拉格朗日乘数和非线性约束，因此对内存的要求更高，更难优化。ReLU 激活函数等同于一个简单的带约束的凸优化问题，受这一事实的启发，zhang2017convergent 放宽了非线性约束作为正则项，对网络架构和 ReLU 激活函数进行编码。因此，非线性约束不复存在。然而，他们的方法仅限于 ReLU 函数，不适用于其他激活函数。沿着这个思路，askari2018lifted 考虑了更多复杂的凸优化问题并讨论了几种非递减激活函数。然而，他们的方法对权重和激活的更新仍然限于 ReLU 函数。所以他们的方法不能胜过 SGD，只能服务于为 SGD 生成良好的初始化。其实我们已经发现了他们的公式不正确（参见“LPOM 的优势”小节）。</p>
<p>本文做出了以下贡献：</p>
<ul>
<li>我们提出了一种新的公式来训练前馈 DNN，我们称之为提升近端算子机 (LPOM)。 LPOM 是块多凸的，即当剩余的权重和激活是固定时，权重和激活的问题是凸的。相比之下，几乎现有所有的 DNN 训练方法都没有这样的性质。这大大方便了DNN的训练。</li>
<li>相应地，我们应用块坐标下降 (BCD) 来求解 LPOM，其中逐层权重和激活可以并行更新。最值得注意的是，逐层权重或激活的更新仅利用激活函数本身，而不是其导数，从而避免了基于梯度的训练方法中的梯度消失或爆炸问题。此外，LPOM 不需要比逐层激活更多的辅助变量，因此其内存成本接近 SGD。我们进一步证明更新逐层权重或激活的迭代是收敛的。</li>
<li>由于只有激活函数本身参与计算，LPOM 能够处理一般的非递减 Lipschitz 连续激活函数，可以是饱和的（例如 sigmoid 和 tanh）和不可微的（例如 ReLU 和 leaky ReLU）。因此 LPOM 成功地克服了使用大多数现有激活函数时的计算困难。</li>
</ul>
<p>我们在全连接的 DNN 上实施 LPOM 并在基准数据集MNIST 和 CIFAR-10 中对其进行测试，并获得了满意的结果。在卷积神经网络 (CNN)中，因为我们还没有重新制定池化和跳跃连接，将 LPOM 在 CNN 上的实施留作未来的工作。请注意，现有基于非梯度的方法也首先关注全连接的 DNN。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>在标准的前馈神经网络中，执行分类任务时训练 $n$ 层神经网络的优化问题可以写做：</p>
<script type="math/tex; mode=display">
\min_{W^i}\ell(\phi(W^{n-1}\phi(\cdots \phi(W^2\phi(W^1X^1)))),L) 
\tag{1}\label{eq1}</script><p>其中 $X^1 \in \mathbb{R}^{n_1\times m}$ 是批训练样本，$L \in \mathbb{R}^{c\times m}$ 表示对应标签，$n_1$ 是训练样本的维度，$m$ 是批量大小，$c$ 是类的数量，$\{W^i\}_{i=1}^{n-1}$ 是要学习的权重，为简单起见，省略了偏差，$\phi(\cdot)$ 是逐元素激活函数（例如，sigmoid、tanh 和 ReLU)，而 ${\ell}(\cdot,\cdot)$ 是损失函数（例如，最小二乘误差或交叉熵误差）。这里的神经网络被定义为嵌套函数，其中第一层神经网络的函数是$\phi(W^1X^1)$，第 $i$ 层($i=2,\cdots,n$) 函数的形式为 $\phi(W^iX)$，并且 $X$ 是第 $(i-1)$ 层函数的输出。优化 \eqref{eq1} 的常用方法是 SGD，即计算梯度，而网络的所有权重更新使用反向传播，具体通过梯度下降来更新权重。</p>
<p>通过引入逐层激活作为辅助变量块，神经网络的训练可以等价地公式化为等式约束优化问题:</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i \},\{X^i\} }\ell(X^n,L)\\
  &s.t. X^i=\phi(W^{i-1}X^{i-1}),i=2,3\cdots n
\end{align*}
\tag{2}\label{eq2}</script><p>其中 $X^i$ 是第 $i$ 层的激活，其他层符号与 \eqref{eq1} 中的相同。</p>
<p>问题 \eqref{eq2} 中的约束确保辅助变量 $\{X^i\}_{i=2}^{n}$ 完全匹配网络。 与问题\eqref{eq1}相比，问题\eqref{eq2}是有限制的。 但由于目标函数没有嵌套，因此更简单，这样的等价表达可能会带来更灵活的优化方法。 注意当使用 SGD 解决问题\eqref{eq1}时，它<br>实际上也是隐含地对问题\eqref{eq2}求解，但需要记录激活 $\{X^i\}_{i=2}^{n}$ 以便计算梯度。</p>
<p>受二次正则方法的启发，carreira2014distributed 提出了辅助坐标（MAC）的方法来求解问题\eqref{eq2}。 MAC使用<br>二次正则项近似等式约束，并试图求解以下问题：</p>
<script type="math/tex; mode=display">
\min_{\{W^i\},\{X^i\}}\ell(X^n,L)+\frac{\mu}{2}\sum\limits^n_{i=2}\|X^i-\phi(W^{i-1}X^{i-1})\|^2_F \tag{3}</script><p>其中 $\mu&gt;0$ 是控制约束权重的常数，$|\cdot|_F$ 是 Frobenius 范数。zeng2018global 用新的辅助变量解耦了 \eqref{eq2} 中的非线性激活函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i \},\{X^i\},\{U^i\}}\ell(X^n,L)\\
  &s.t. U^i=W^{i-1}X^{i-1},X^i=\phi(U^i),i=2,3\cdots n
\end{align*}
\tag{4}\label{eq4}</script><p>这称为 3-splitting 公式。相应地，问题\eqref{eq2} 是 2-splitting 公式。 对问题\eqref{eq4}同样适用 MAC 方法，而不是直接求解，可得出他们优化了下面的问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\},\{U^i\}}\ell(X^n,L)\\
  &+\frac{\mu}{2}\sum\limits^n_{i=2}(\|U^i-W^{i-1}X^{i-1}\|^2_F+\|X^i-\phi(U^i)\|^2_F)
\end{align*}
\tag{5}</script><p>他们采用 BCD 方法来解决上述问题。</p>
<p>taylor2016training 也考虑求解问题\eqref{eq4}。 受 ADMM启发，他们在输出层添加了拉格朗日乘子以实现对输出层的等式约束，产生下列问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\},\{U^i\},M}\ell(U^n,L)+\frac{\beta}{2}\|U^n-W^{n-1}X^{n-1}+M\|^2_F\\
  &+\sum\limits^{n-1}_{i=2}\frac{\mu_i}{2}(\|U^i-W^{i-1}X^{i-1}\|^2_F+\|X^i-\phi(U^i)\|^2_F)
\end{align*}
\tag{6}\label{eq6}</script><p>其中 $M$ 是拉格朗日乘子，$\beta&gt;0$ 和 $\mu_i&gt;0$是常数。 注意输出层上没有激活函数。 所以 \eqref{eq6} 是自适应启发式的 ADMM 算法。 zhang2016efficient 采用了类似的技术，但使用了不同的变量拆分方案：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\},\{U^i\}}\ell(X^n,L) \\
  &s.t. U^{i-1}=X^{i-1},X^i=\phi(W^{i-1}U^{i-1}),i=2,3\cdots n
\end{align*}
\tag{7}\label{eq7}</script><p>尽管有非线性等式约束，但 ADMM 并不旨在处理该类问题，他们为\eqref{eq7}中的每个约束添加了一个拉格朗日乘数。然后增强的拉格朗日问题如下所示：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\},\{U^i\},\{A^i\},\{B^i\}}\ell(X^n,L)\\
  &+\frac{\mu}{2}\sum\limits^n_{i=2}(\|U^{i-1}-X^{i-1}+A^{i-1}\|^2_F\\
  &+\|X^i-\phi(W^{i-1}U^{i-1})+B^{i-1}\|^2_F)
\end{align*}
\tag{8}</script><p>其中 $A^i$ 和 $B^i$ 是拉格朗日乘子。</p>
<p>与原始应用正则方法和 ADMM 不同，zhang2017convergent 将 ReLU 激活函数解释为一个简单的平滑凸优化问题。即，问题\eqref{eq2} 中的等式约束使用 ReLU 激活函数可以改写为凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  X^i&=\phi(W^{i-1}X^{i-1})\\
  &=max(W^{i-1}X^{i-1},\textbf{0})\\
  &=\mathop{argmin}_{U^i\geq 0}\|U^i-W^{i-1}X^{i-1}\|^2_F
\end{align*}
\tag{9}</script><p>其中 $\textbf{0}$ 是具有适当大小的零矩阵。基于这个观察，他们用以下方式近似激活函数为 ReLU 的问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\}}\ell(X^n,L)+\sum\limits^n_{i=2}\frac{\mu_i}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F\\
  &s.t.X^i\geq \textbf{0},i=2,3\cdots n
\end{align*}
\tag{10}\label{eq10}</script><p>其中正则项对网络结构和激活函数都进行了编码。与基于 MAC 和 ADMM 的方法不同，它确实不包括非线性激活。 此外，\eqref{eq10}的主要优势是该问题是块多凸的，即， 每个变量块变化时其余的块是固定的。 他们提出了一种新的 BCD 方法来求解这个问题，而且经验性的证明了在Caffe框架下基于 SGD 方法ADMM方法的优先级。askari2018lifted 等继承了同样的想法，通过引入更复杂的凸最小化问题，他们可以处理更一般的激活函数，例如 sigmoid、leaky ReLU 和正弦函数等。</p>
<h2 id="提升近端算子"><a href="#提升近端算子" class="headerlink" title="提升近端算子"></a>提升近端算子</h2><p>在本节中，我们描述了 LPOM 的基本思想及其相对于现有 DNN 训练方法的优势。跟 zhang2017convergent 和 askari2018lifted 一样，LPOM 首先将\eqref{eq2} 中的激活函数表示为凸最小化问题。 但是，我们希望这种表示不应仅限于特定激活函数，而且我们希望\eqref{eq2}中的等式约束满足LPOM的KKT条件。</p>
<h3 id="用近端算子进行重构"><a href="#用近端算子进行重构" class="headerlink" title="用近端算子进行重构"></a>用近端算子进行重构</h3><p>我们假设激活函数 $\phi$ 是非递减的，那么$\phi^{-1}(x)=\{y|x=\phi(y)\}$是一个凸集，$\phi^{-1}(x)$ 是 $\{y\}$ 单例当且仅当 $\phi$ 在 $\phi(y)$ 处是严格增加的。 我们要构建一个目标函数 $h(x,y)$，由 $y$ 参数化，使得它的最小值正好是 $x=\phi(y)$。 因此，我们可以通过最小化 $h(x,y)$ 替换约束 $x=\phi(y)$ ，可以将约束作为正则添加到DNN损失中。</p>
<p>优化问题更新变量的基本操作有两种：梯度更新和近端算子。 我们正在构造的优化问题和近端算子如下所示：</p>
<script type="math/tex; mode=display">
\textrm{prox}_f(y)=\mathop{argmin}_xf(x)+\frac{1}{2}(x-y)^2 
\tag{11}\label{eq11}</script><p>我们考虑使用近端算子来构造优化问题。 定义</p>
<script type="math/tex; mode=display">f(x)=\int^x_0(\phi^{-1}(y)-y)dy</script><p>注意 $f(x)$ 定义明确，即使 $\phi^{-1}(y)$ 对于某些介于 0 和 $x$ 之间的 $y$ 来说不是唯一的。 无论如何，$\phi^{-1}$、$f$ 和 $g$（稍后定义）不会显式用于我们的计算。 很容易证明问题\eqref{eq11}的优化条件是 $0\in (\phi^{-1}(x)-x) + (x-y)$。 所以\eqref{eq11} 的解正好是 $x=\phi(y)$。</p>
<p>请注意 $f(x)$ 是单位变量函数。 对于矩阵 $X=(X_{kl})$，我们定义$f(X)=(f(X_{kl}))$。 然后<br>以下最小化问题的优化性条件：</p>
<script type="math/tex; mode=display">
\mathop{argmin}_{X^i}\mathbf{1}^\top f(X^i)\mathbf{1}+\frac{1}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F \tag{12}\label{eq12}</script><p>其中 $\mathbf{1}$ 是全为1的列向量，是</p>
<script type="math/tex; mode=display">
\mathbf{0}\in\phi^{-1}(X^i)-W^{i-1}X^{i-1} \tag{13}</script><p>其中 $\phi^{-1}(X^i)$ 也是按元素定义的。 所以<br>\eqref{eq12} 的优化解是</p>
<script type="math/tex; mode=display">X^i=\phi(W^{i-1}X^{i-1}) \tag{14}\label{eq14}</script><p>这正是问题\eqref{eq2} 中的约束。 所以我们自然地将问题\eqref{eq2} 近似为：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\}}\ell(X^n,L)\\
  &+\sum\limits^n_{i=2}\mu_i\bigg (\mathbf{1}^\top  f(X^i)\mathbf{1}+\frac{1}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F \bigg)
\end{align*}
\tag{15}</script><p>然而，$\{X^i\}_{i=2}^{n-1}$ 的优化条件是：</p>
<script type="math/tex; mode=display">
\begin{align*}
  \mathbf{0}\in &\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})\\
  &+\mu_{i+1}(W^i)^\top (W^iX^i-X^{i+1}),i=2\cdots, n-1
\end{align*}
\tag{16}\label{eq16}</script><p>我们可以清楚地看到问题\eqref{eq2}的等式约束\eqref{eq14}不满足以上条件。</p>
<p>为了使等式约束 \eqref{eq14}满足逼近问题的最优性条件，我们需要将 \eqref{eq16} 修改为</p>
<script type="math/tex; mode=display">
\begin{align*}
  \mathbf{0}\in &\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})\\
  &+\mu_{i+1}(W^i)^\top (\phi(W^iX^i)-X^{i+1}),i=2\cdots, n-1
\end{align*}
\tag{17}\label{eq17}</script><p>这对应于以下问题：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{\{W^i\},\{X^i\}}\ell(X^n,L)+\sum\limits^n_{i=2}\mu_i\bigg (\mathbf{1}^\top f(X^i)\mathbf{1}\\
  &+\mathbf{1}^\top g(W^{i-1}X^{i-1})\mathbf{1} +\frac{1}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F \bigg)
\end{align*}
\tag{18}\label{eq18}</script><p>其中</p>
<script type="math/tex; mode=display">\int^x_0(\phi(y)-y)dy</script><p>$g(X)$ 是在矩阵 $X$ 逐元素定义的， $f(x)$ 和 $g(x)$的一些有代表性的激活函数显示在表1。 \eqref{eq18} 是我们提出的 LPOM ，其中强调下 $g$ 的引入非常重要且不明显。</p>
<h3 id="LPOM优势"><a href="#LPOM优势" class="headerlink" title="LPOM优势"></a>LPOM优势</h3><p>将 \eqref{eq18} 中 LPOM 的目标函数表示为 $F(W,X)$。 那么我们有下面的定理：</p>
<p><strong>定理1</strong><br>假设 $\ell(X^n, L)$ 在 $X^n$ 中是凸的并且 $\phi$ 是<br>非递减的。 那么 $F(W,X)$ 是块多凸，也就是，如果所有其他变量块都是固定的，每个 $X^i$ 和 $W^i$ 都是凸的。</p>
<p><strong>证明：</strong><br>$F(W,X)$ 可以简化为</p>
<script type="math/tex; mode=display">
\begin{align*}
  &F(W,X)=\ell(X^n,L)+\sum\limits^n_{i=2}\mu_i\bigg (\mathbf{1}^\top \tilde{f}(X^i)\mathbf{1}\\
  &+\mathbf{1}^\top \tilde{g}(W^{i-1}X^{i-1})\mathbf{1} +\langle X^i,W^{i-1}X^{i-1}\rangle \bigg)
\end{align*}
\tag{19}</script><p>其中 $\tilde{f}(x)=\int_0^x \phi^{-1}(y) dy$ 和 $\tilde{g}(x)=\int_0^x \phi(y) dy$。 因为 $\phi$ 和 $\phi^{-1}$ 是非递减的，所以 $\tilde{f}(x)$ 和 $\tilde{g}(x)$ 是凸的。 很容易验证 $\mathbf{1}^\top\tilde{g}(W^{i-1}!X^{i-1})\mathbf{1}$ 当 $W^{i-1}$ 固定时在 $X^{i-1}$ 中是凸的，当 $X^{i-1}$ 固定时在 $W^{i-1}$ 中是凸的。$F(W,X)$ 中的剩余项 $\langle X^{i},W^{i-1}X^{i-1}\rangle$ 当其他两个块固定时，在一个块中是线性的。证明完成。</p>
<p>由于子问题的凸性，定理1允许使用高效的 BCD 算法求解 LPOM，并保证可以得到更新 $X^i$ 和 $W^i$ 的最佳解决方案。 相反，正则项方法和基于 ADMM 的方法中的子问题都是非凸的。</p>
<p>与基于 ADMM 的方法相比，LPOM 除了 $\{X^i\}_{i=2}^{n}$ 不需要拉格朗日乘子和更多的辅助变量。 此外，我们还设计了精致的算法，这样求解 LPOM 也无需额外的变量。 所以 LPOM 的变量数比基于 ADMM 的方法少，因此大大节省了内存。实际上，它的内存成本接近于 SGD。</p>
<p>与正则项方法相比，LPOM 的优化条件更简单。 例如，LPOM 中的 $\{X^i\}_{i=2}^{n-1}$ 和 $\{W^i\}_{i=1}^{n-1}$ 优化条件是 \eqref{eq17} 和</p>
<script type="math/tex; mode=display">
(\phi(W^iX^i)-X^{i+1})(X^i)^\top\!=0,\; i=1,2\cdots n-1 
\tag{20}</script><p>而那些用于 MAC 的优化条件是</p>
<script type="math/tex; mode=display">
\begin{align*}
  &(X^i-\phi(W^{i-1}X^{i-1}))\\
  &+(W^i)^\top[(\phi(W^iX^i)-X^{i+1})\circ\phi^{'}(W^iX^i)]=\mathbf{0}\\
  &i=2,\cdots n-1
\end{align*}
\tag{21}</script><p>和</p>
<script type="math/tex; mode=display">
\begin{align*}
  [(\phi(W^iX^i)-X^{i+1})\circ\phi^{'}(W^iX^i)](X^i)^\top=\mathbf{0},i=1,\cdots n-1
\end{align*}
\tag{22}</script><p>其中 $\circ$ 表示逐元素乘法。 我们可以看到 MAC 的优化条件有额外的 $\phi’(W^iX^i)$，这是非线性的。zeng2018global 的优化条件可以在补充材料中找到，他们也还有一个额外的 $\phi’(U^i)$。 这可能意味着 MAC 和 zeng2018global 的解集更复杂，更大。 所以LPOM  可能更容易找到良好解决方案。</p>
<p>与凸优化重构方法相比，LPOM 可处理更一般的激活函数。 注意 zhang2017convergent 只考虑了 ReLU。 虽然 askari2018lifted 声称他们的公式可以处理一般的激活函数，其求解方法还是仅限于 ReLU。 此外 askari2018lifted 关于$\{X^i\}_{i=2}^{n-1}$ 和 $\{W^i\}_{i=1}^{n-1}$的优化条件推导是有误的，也就是：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathrm{0}\in&\mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})-\mu_{i+1}(W^i)^\top X^{i+1}\\
&i=2,\dots n-1
\end{align*}</script><p>相应地，$X^{i+1}(X^i)^\top=\mathbf{0},\,i=1,\cdots,n-1,$，很明显，等式约束\eqref{eq14} 不满足以上条件。 此外，不知何故 askari2018lifted 不管激活函数是什么，都添加了额外的约束 $X^i\geq \mathbf{0}$，所以他们的重构不能很好地近似原始 DNN \eqref{eq2}，这可能解释为什么 askari2018lifted 得不到好的结果。实际上，它们只能为 SGD 提供良好的初始化。</p>
<p>与基于梯度的方法（例如 SGD）相比，LPOM 可以使用任何非递减 Lipschitz 连续激活而没有数值求解，包括饱和（例如，sigmoid 和 tanh）和不可微分的（例如，ReLU 和 leaky ReLU），并且可以逐层并行更新权重和激活。 相反，基于梯度的方法只能使用有限的激活函数，例如 ReLU， leaky ReLU 和 softplus，以避免梯度消失或爆炸问题，并且在计算时无法并行化梯度和激活。</p>
<h2 id="求解LPOM"><a href="#求解LPOM" class="headerlink" title="求解LPOM"></a>求解LPOM</h2><p>多亏了块多凸性性质(定理1)，LPOM可以用BCD求解。 即，我们通过固定所有其他变量块来更新 $X^i$ 或 $W^i$。可以使用小批量训练数据来进行优化问题的求解，解决 LPOM 的整个算法总结在算法1，下面我们给出更多的细节。</p>
<h3 id="更新-X-i-n-i-2"><a href="#更新-X-i-n-i-2" class="headerlink" title="更新$\{X^i\}^n_{i=2}$"></a>更新$\{X^i\}^n_{i=2}$</h3><p>我们先介绍串行更新的方法 $\{X^i\}_{i=2}^n$，将 $\{X^i\}_{i=2}^{n}$ 从 $i=2$ 接连不断地更新到 $n$，就像 DNN 的前馈过程一样。 当 $i=2,\cdots,n-1$ 时，在 $\{W^i\}_{i=1}^{n-1}$ 和 $\{X^j\}_{j=2,j\neq i}^n$ 固定时，问题 \eqref{eq18}可以化为：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\min_{X^i}\mu_i\bigg(\mathbf{1}^\top f(X^i)\mathbf{1}+\frac{1}{2}\|X^i-W^{i-1}X^{i-1}\|^2_F \bigg)\\
  &+\mu_{i+1}\bigg(\mathbf{1}^\top g(W^iX^i)\mathbf{1}+\frac{1}{2}\|X^{i+1}-W^iX^i\|^2_F \bigg)
\end{align*}
\tag{23}</script><p>优化条件是：</p>
<script type="math/tex; mode=display">
\begin{align*}
  &\mathbf{0}\in \mu_i(\phi^{-1}(X^i)-W^{i-1}X^{i-1})\\
  &+\mu_{i+1}((W^i)^\top(\phi(W^iX^i)-X^{i+1}))
\end{align*}
\tag{24}</script><p>所以用下面迭代方法更新 $X^i$ 直到收敛：</p>
<script type="math/tex; mode=display">
X^{i,t+1}=\phi\bigg(W^{i-1}X^{i-1}-\frac{\mu_{i=1}}{\mu_i}(W^i)^\top(\phi(W^iX^{i,t})-X^{i+1}) \bigg) 
\tag{25}</script><p>其中上标 $t$ 为迭代次序。 收敛分析如下：</p>
<p><strong>定理2</strong><br>假设 $|\phi’(x)|\leq\gamma$，如果 $\rho&lt;1$，则迭代是收敛的并且收敛速率是线性的，其中</p>
<script type="math/tex; mode=display">\rho=\frac{\mu_{i+1}}{\mu_i}\gamma^2\sqrt{\|
 |(W^i)^\top||W^i| \|_1\| |(W^i)^\top| |W^i|\|_\infty}</script><p>证明可以在补充材料中找到。 在上面式子中，$|A|$ 是一个矩阵，其元素是 $A$ 的绝对值，$|\cdot|_1$ 和 $|\cdot|_{\infty}$ 分别是矩阵 1-范数（largest absolute column sum）和矩阵 $\infty$-范数（largest absolute row sum）。</p>
<p>当考虑 $X^n$ 时，问题\eqref{eq18} 简化为</p>
<script type="math/tex; mode=display">
\min_{X^n}\ell(X^n,L)+\mu_n\bigg(\mathbf{1}^\top f(X^i)\mathbf{1}+\frac{1}{2}\|X^n-W^{n-1}X^{n-1}\|^2_F \bigg) 
\tag{26}</script><p>优化条件是：</p>
<script type="math/tex; mode=display">
\mathbf{0}\in\frac{\partial\ell(X^n,L)}{\partial X^n}+\mu_n(\phi^{-1}(X^n)-W^{n-1}X^{n-1}) 
\tag{27}</script><p>所以用下面迭代来更新 $X^n$ 直到收敛</p>
<script type="math/tex; mode=display">
X^{n,t+1}=\phi\bigg(W^{n-1}X^{n-1}-\frac{1}{\mu_n}\frac{\partial\ell(X^{n,t},L)}{\partial X^n} \bigg) 
\tag{28}</script><p>收敛分析如下所示：</p>
<p><strong>定理3</strong><br>假设 $|\phi’(x)|\leq\gamma$ 和 $\bigg|\big(\frac{\partial^2\ell(X,L)}{\partial X_{kl}\partial X_{pq}}\big)\bigg|_1\leq\eta$。如果 $\tau &lt; 1$，则迭代收敛，收敛率为线性，其中 $\tau=\frac{\gamma\eta}{\mu_n}$。</p>
<p>证明也可以在补充材料中找到。 如果 ${\ell}(X^n,L)$ 是最小二乘误差，即 ${\ell}(X^n,L)=\frac{1}{2}|X^n-L|_F^2$，然后 $\bigg|\bigg|\big(\frac{\partial^2\ell(X,L)}{\partial X_{kl}\partial X_{pq}}\big)\bigg|\bigg|_1 = 1 $。 所以我们得到 $\mu_n&gt;\gamma$。</p>
<p>上面的串行更新程序可以很容易地更改为并行更新：每个 $X^i$ 都使用最新的其他 $X^j, j\neq i$ 的信息来更新。</p>
<h3 id="更新-W-i-n-1-i-1"><a href="#更新-W-i-n-1-i-1" class="headerlink" title="更新$\{W^i\}^{n-1}_{i=1}$"></a>更新$\{W^i\}^{n-1}_{i=1}$</h3><p>$\{W^i\}_{i=1}^{n-1}$ 可以完全并行更新，当 $\{X^i\}_{i=2}^{n}$ 是固定的，问题 \eqref{eq18}化为：</p>
<script type="math/tex; mode=display">
\begin{align*}
  \min_{W^i}\mathbf{1}^\top g(W^iX^i)\mathbf{1}+\frac{1}{2}\|W^iX^i-X^{i+1}\|^2_F\\
  i=1\cdots n-1 
\end{align*}
\tag{29}\label{eq29}</script><p>上述问题可以并行求解。 \eqref{eq29} 可以写做：</p>
<script type="math/tex; mode=display">
\min_{W^i}\mathbf{1}^\top\tilde{g}(W^iX^i)\mathbf{1}-\langle X^{i+1},W^iX^i \rangle\tag{30}
\label{eq30}</script><p>其中，如前所述 $\tilde{g}(x)=\int_0^x \phi(y)dy$。假设 $\phi(x)$ 是 $\beta$-Lipschitz 连续的，这对于几乎所有使用的激活函数都是成立的。 然后 $\tilde{g}(x)$ 是 $\beta$-平滑的：</p>
<script type="math/tex; mode=display">|\tilde{g}^{'}(x)-\tilde{g}^{'}(y)|=|\phi(x)-\phi(y)|\leq\beta|x-y| \tag{31}</script><p>问题 \eqref{eq30} 可以通过APG的局部线性化 $\hat{g}(W)=\tilde{g}(WX)$ 求解。 然而，$\hat{g}(W)$ 梯度的Lipschitz 常数，即 $\beta|X|_2^2$，可能非常大，因此收敛可能很慢。 下面我们提出一个 APG 的改进版本，专为解决问题\eqref{eq30} 而设计的高效算法。</p>
<p>考虑以下问题：</p>
<script type="math/tex; mode=display">
\min_xF(x)\equiv\varphi(Ax)+h(x) 
\tag{32}\label{eq32}</script><p>其中 $\varphi(y)$ 和 $h(x)$ 都是凸的。 而且，$\varphi(y)$ 是 $L_\varphi$-平滑的：$|\nabla\varphi(x)-\nabla \varphi(y)| \leq L_\varphi|x-y|,\forall x,y.$ 我们假设下面的问题：</p>
<script type="math/tex; mode=display">
x_{k+1}=\mathop{argmin}_x\langle\nabla\varphi(Ay_k),A(x-y_k) \rangle +\frac{L_\varphi}{2}\|A(x-y_k)\|^2+h(x)
\tag{33}\label{eq33}</script><p>对任何给定的 $y_k$ 很容易求解，我们提出用算法2求解 \eqref{eq32}，那么我们有下面的定理：</p>
<p><strong>定理4</strong><br>如果我们使用算法2来求解问题\eqref{eq32}，则收敛速度至少为 $O(k^{-2})$：</p>
<script type="math/tex; mode=display">
F(x_k)\!-\!F(x^*)\!+\!\frac{L_\varphi}{2}\|z_k\|^2\!\leq\!\frac{4}{k^2}\!\left(\!F(x_1)\!-\!F(x^*)\!+\!\frac{L_\varphi}{2}\|z_1\|^2\!\right)</script><p>其中，<br>$z_k=A[\theta_{k-1}x_{k-1}-x_k+(1-\theta_{k-1})x^<em>]$，且 $x^</em>$ 是问题\eqref{eq32}的任意优化解。</p>
<p>证明也可以在补充材料中找到。</p>
<p>通过用问题\eqref{eq30}实例化问题\eqref{eq32}，子问题\eqref{eq33} 变为：</p>
<script type="math/tex; mode=display">
\begin{align*}
  W^{i,t+1}&=\mathop{argmin}_W\langle\phi(Y^{i,t}X^i),(W-Y^{i,t})X^i\rangle\\
  &+\frac{\beta}{2}\|(W-Y^{i,t})X^i||^2_F-\langle X^{i+1}-WX^i\rangle
\end{align*}
\tag{34}</script><p>这是一个最小二乘问题，解是：</p>
<script type="math/tex; mode=display">
W^{i,t+1}=Y^{i,t}-\frac{1}{\beta}(\phi(Y^{i,t}X^i)-X^{i+1})(X^i)^{+} 
\tag{35}</script><p>其中 $(X^i)^{+}$ 是 $X^i$ 的伪逆，$Y^{i,t}$ 是算法2中的 $y_k$。</p>
<p>如果 $\phi(x)$ 严格递增且递增率 $\frac{\phi(y)-\phi(x)}{y-x}\, (y\neq x)$ 的下界为 $\alpha&gt;0$，那么 $\tilde{g}(x)$ 是强凸的，并且收敛是线性的，这里我们省略详情。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在本节中，我们通过与 SGD 和两种基于非梯度的方法askari2018lifted 及 taylor2016training 进行比较来评估 LPOM。 其他基于非梯度的方法不为分类任务训练完全连接的前馈神经网络（例如，使用跳跃连​​接，训练自动编码器，以及学习哈希等)，所以我们不能将它们包括在内进行比较。为简单起见，我们使用最小二乘损失函数和 ReLU 激活函数（使用 ReLU 的另一个原因是它可以产生更高的精度，尽管 LPOM 可以没有数值困难的用其他激活函数参与计算）除非另有说明。与 askari2018lifted 不同，我们不对权重 $\{W^i\}_{i=1}^{n-1}$ 使用任何正则化。我们对 LPOM 和 SGD 使用相同的输入和随机运行初始化。我们用 LPOM 的 MATLAB 实现而不优化代码，使用基于 Caffe 的 SGD 求解器。对于 Caffe 求解器，我们修改demo代码，仔细调参实现最好的精度。对于 askari2018lifted 和 taylor2016training，我们引用他们的论文的结果。</p>
<h3 id="与SGD比较"><a href="#与SGD比较" class="headerlink" title="与SGD比较"></a>与SGD比较</h3><p>我们对两个数据集进行实验，即 MNIST 和 CIFAR-10。对于 MNIST 数据集，我们使用 $28\times28=784$ 个原始像素作为输入，它包括 60,000 张训练图像和 10,000 张测试图像，不使用预处理或数据增强。LPOM和SGD，在每个epoch中都用所有训练样本运行一次。性能取决于网络结构的选择。与 zeng2018global 一样，我们实现了一个784-2048-2048-2048-10 前馈神经网络。对于 LPOM，我们只需在 \eqref{eq18} 中设置 $\mu_i=20$。我们在 LPOM 和 SGD 上都运行 100 个周期的 ，固定批量大小为 100。训练和测试精度如图1(a) 和 (b)，可以看到两种方法的训练精度都约等于 $100\%$。然而，LPOM 的测试准确性略优于 SGD（$98.2\%$ vs. $98.0\%$）。</p>
<p>对于 CIFAR-10 数据集，在 zeng2018global 中我们实现 3072-4000-1000-4000-10 前馈神经网络。我们通过分别减去训练数据集中红色、绿色和蓝色通道的均值来标准化彩色图像，不使用预处理或数据扩充。对于 LPOM，我们设置 \eqref{eq18} 中的 $\mu_i=100$。在 LPOM 和 SGD 上运行 100 个 epochs，批量大小为 100。训练和测试准确度如图1 (c) 和 (d) 所示，可以看到SGD和LPOM的训练精度是约等于 $100\%$。但是，LPOM 的测试精度优于SGD（$52.5\%$ 对 $47.5\%$）。</p>
<h3 id="与其他非梯度方法比较"><a href="#与其他非梯度方法比较" class="headerlink" title="与其他非梯度方法比较"></a>与其他非梯度方法比较</h3><p>我们用 MNIST 数据集上相同结构的网络与 askari2018lifted 中的结果进行比较。在实际计算中askari2018lifted 只用了 ReLU 激活函数，与 askari2018lifted 一样，我们在 LPOM 上运行 17 个 epochs，固定批量大小为 100，设置 $\mu_i=20$。使用 60,000 张训练图像和 10,000 张测试图像，不要使用预处理或数据增强。这两种方法的测试精度示于表2，可以看到带 ReLU 的 LPOM 表现很大差距的优于askari2018lifted 的方法，这符合我们在“LPOM 的优点”小节的描述。</p>
<p>按照 taylor2016training 中数据集和网络架构的设置，我们在 SVHN 数据集上测试 LPOM，设置 $\mu_i=20$。 SGD、taylor2016training 和 LPOM 的测试精度如表3所示。可以看到 LPOM 优于 SGD 和 taylor2016training。<br>如 taylor2016training 中所述，他们基于 ADMM 的方法和 SGD 的测试精度分别约为 $96.5\%$ 和 $95.0\%$。然而，LPOM 可以在相同的设置下达到 $98.3\%$ 的测试精度。<br>这进一步验证了LPOM的优势。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这项工作中，我们提出了 LPOM 来训练全连接前馈神经网络，使用近端运算符 LPOM 将神经网络转化为一个新的分块多凸模型，转换适用于一般非递减 Lipschitz 连续激活函数。我们自然地提出了块坐标下降算法，保障每个子问题收敛的情况下求解。 LPOM 可以并行解决，相比分层激活，无需更多辅助变量。 实验结果表明 LPOM 在完全连接的神经网络比 SGD，askari2018lifted 和 taylor2016training 效果更好。未来的工作包括将 LPOM 扩展到训练卷积和递归神经网络并应用 LPOM 到网络压缩。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>LPOM</tag>
      </tags>
  </entry>
  <entry>
    <title>Convexifying Transformers</title>
    <url>/2023/03/22/Transformer/Convexifying%20Transformers/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：Convexifying Transformers: Improving optimization and understanding of transformer networks</p>
<p><a href="https://arxiv.org/abs/2211.11052">essay link</a></p>
<p><strong>凸优化的角度理解和优化Transformer网络~</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>了解Transformer网络成功背后的基本机制仍然是深度学习中一个悬而未决的问题，尽管它们的出色表现主要归功于自我注意机制，但文献仍然缺乏对这些网络的可靠分析和对它们所学函数的解释。为此，我们研究了注意力Transformer网络的训练问题，并引入了一种新颖的凸分析方法来提高对这些网络的理解和优化。特别是，我们首先引入了自注意力机制的凸替代方案，并用我们的凸注意力重新表述了Transformer网络的正则化训练问题。然后，我们将重构为一个可解释且更易于优化的凸优化问题。此外，作为我们凸分析的副产品，我们揭示了一种隐式正则化机制，它促进了token之间的稀疏性。因此，我们不仅改进了注意力或称Transformer网络的优化，而且还提供了对它们学到的函数的理论理解，并且通过几个数值实验证明了我们理论的有效性。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>vaswani2017attention 提出的 Transformer 网络已成为各种任务中的主导架构，尤其是自然语言处理 (NLP)，因为它们具有非凡的泛化特性和从海量数据中学习的高能力。 尽管有大量关于Transformer网络有效性的经验证据，但由于其高度非线性和非凸结构，揭示其成功背后的潜在理论原因仍然是一个悬而未决的研究问题。</p>
<p>大量研究侧重于通过实证研究分析Transformer网络的某些组件，例如 liu2021analyzingattention 等研究了注意力机制对 transformer 网络的影响。尽管这些研究一致认为注意力是 Transformer 的重要组成部分，但它们也提出了一些有关可解释性和优化的问题。特别是，voita2019analyzing 证明可以删除大多数注意力头而不影响网络性能，这是网络中大量冗余的一个指标。 attentionacrossNLP 提供了一组经验证据表明某些 NLP 任务可能不需要注意力。此外，2021dong_attention 透露，虽然注意力是Transformer网络的核心，但在没有全连接 (FCN) 层和跳跃连接的情况下训练注意力网络极具挑战性，因为没有它们，网络输出会迅速退化。类似地，takase2022layer 讨论了层归一化和跳跃连接对Transformer网络的重要性，因此即使改变它们的位置也可能显着影响Transformer网络的性能。然而，仍然缺乏对这些问题背后的潜在因素的可靠理论分析，这可能是由于Transformer网络的高度复杂和非凸结构。</p>
<p>一系列论文还侧重于设计自注意机制的新替代方案，这些替代方案表现相似，并可能为整体模型提供进一步的解释。一组工作利用基于多层感知器的架构，如tolstikhin2021mlp等，而另一组论文提出基于傅里叶的模型如lee2021fnet等，21adaptive等还提出用矩阵分解 geng2021attention 代替 self-attention 机制。尽管这些工作成功地应用于某些应用，但它们缺乏从优化角度进行扎实的理论分析和理解。最近，sahiner2022convex 尝试通过完全改变自注意力机制的结构并移除 FC 层，通过凸对偶分析 transformer 网络。即使那样，它们也未能为Transformer提供可靠的实际意义，因为它们的公式极具挑战性且在实践中难以解决。</p>
<p>最近，另一项研究侧重于理解 transformer 网络训练过程中出现的结构和模式 如power2022grokking等。 特别是，grokking 现象首先由 power2022grokking 在特定算法任务（例如模除法运算）中观察到。 具体来说，grokking 指的是验证或测试准确性突然过渡到完美泛化，并且这种泛化发生在完美训练准确性点之后。 这个有趣的行为与深度学习模型训练中早期停止的常见做法相矛盾，并且肯定需要进一步了解为什么会出现这种现象。</p>
<p>为了解决与标准Transformer网络相关的问题，在本文中，我们开发了一个凸优化视角来训练、分析和理解Transformer网络。 特别是，我们首先提出了自注意力机制的凸替代方案，然后在结果模型上开发了我们的凸分析框架，如图1所示。</p>
<h3 id="1-1-贡献"><a href="#1-1-贡献" class="headerlink" title="1.1 贡献"></a>1.1 贡献</h3><p>本文贡献如下：</p>
<ul>
<li>我们提出了标准自注意力机制的替代公式，并用它研究了注意力Transformer网络的正则化训练问题。</li>
<li>如图1所示，我们使用提出的注意层凸化了注意Transformer网络的正则化训练问题，因此能够找到全局最优解而不需要任何非凸优化启发式，例如层归一化和跳跃连接。</li>
<li>我们还将我们的凸分析框架应用于各种架构，例如，有或没有 FCN 层的网络。因此，我们能够解释在整个训练过程中每个组件对学习模型的影响。</li>
<li>我们揭示了一种由注意力机制引起的隐式正则化机制，之后进一步将这种正则化描述为跨token的稀疏性诱导因素。</li>
<li>我们通过各种实验结果证明了凸重构的有效性。我们还表明，我们的重新表述显着减轻了最近论文 power2022grokking等，其研究中指出的 grokking 现象。</li>
</ul>
<h3 id="1-2-记号"><a href="#1-2-记号" class="headerlink" title="1.2 记号"></a>1.2 记号</h3><p>我们分别使用小写和大写粗体字母表示向量和矩阵，用下标表示向量或矩阵的某个列元素。 例如，$w_{jk}$ 表示矩阵 $\matrix{W}$ 的第 $jk$ 项，用 $\mathbf{I}_k$ 表示大小为 $k \times k$ 的单位矩阵，使用 $\mathbf{0}$（或 $\mathbf{1}$）表示具有适当尺寸的零（或1）向量或者矩阵，还使用 $[n]$ 表示范围从 $1$ 到 $n$ 的整数集，将 Euclidean 和 Frobenius 范数分别表示为 $|\cdot|_2$ 和 $|\cdot |_{F}$，还使用 $\mathbb{1}[x\geq0]$ 来表示 0-1 值指示函数，在表1中提供了在整篇论文中使用的更多符号。</p>
<h2 id="2，Transformer网络"><a href="#2，Transformer网络" class="headerlink" title="2，Transformer网络"></a>2，Transformer网络</h2><p>给定一个数据样本（或句子）$\mathbf{X} \in \mathbb{R}^{h \times d}$ 作为具有嵌入维度 $d$ 的 $h$ token序列，我们将键（key）、查询（query）和值（value）矩阵定义为：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{Q} & =\mathbf{X}\mathbf{W}_q,\quad \mathbf{W}_q\in\mathbb{R}^{d\times d} \\
\mathbf{K} & =\mathbf{X}\mathbf{W}_k,\quad \mathbf{W}_k\in\mathbb{R}^{d\times d} \\
\mathbf{V} & =\mathbf{X}\mathbf{W}_v,\quad \mathbf{W}_v\in\mathbb{R}^{d\times d}
\end{align*}</script><p>它们是自注意力机制的主要组成部分。 然后，一个基本上是自注意力堆叠的单个Transformer块、残差连接、层归一化和逐点前馈连接可以表示如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbf{A}_{s,j} &= \text{softmax}(\mathbf{Q}\mathbf{K}^\top\mathbf{V})\\
\mathbf{A}_o &= \mathbf{A}_s\mathbf{W}_o, \mathbf{W}_o\in\mathbb{R}^{d\times d}\\
\mathbf{X}_A &= LayerNorm(\mathbf{A}_o)+\mathbf{X}' \\
\mathbf{X}_B &= \sigma(\mathbf{X}_A\mathbf{W}_1)\mathbf{W}_2 
\end{align*}
\tag{1}\label{eq1}</script><p>其中 $\sigma(\cdot)$ 表示 FCN 层的激活函数，尽管跳跃连接、层归一化和 FCN 在Transformer块中也起着至关重要的作用，但这些网络的成功主要归功于自注意力部分，表示为 $\mathbf{A}_o$ 。 因此，在下一节中，我们首先研究简化的transformer网络的训练问题，网络输出直接为 $\mathbf{A}_o$。 然后，我们将推导扩展到具有 FCN 层的Transformer网络。</p>
<h2 id="3，仅注意力网络"><a href="#3，仅注意力网络" class="headerlink" title="3，仅注意力网络"></a>3，仅注意力网络</h2><p>我们首先考虑一个简化的Transformer网络，它只有一个自注意层，将输入序列 $\mathbf{X}\in \mathbb{R}^{n \times d}$ 映射到 $c$ 维输出序列 $\hat{\mathbf{Y}} \in \mathbb{R}^{n \times c}$ ，即：</p>
<script type="math/tex; mode=display">
\hat{\mathbf{Y}} = \text{softmax}(\mathbf{X}\mathbf{W}_q\mathbf{W}^\top_k\mathbf{X}^\top)\mathbf{X}\mathbf{W}_v\mathbf{W}_o 
\tag{2}\label{eq2}</script><p>我们也称模型 \eqref{eq2} 为仅注意力网络。这是一个有意义的模型，已应用于各种任务，包括机器翻译、语言建模、图像字幕和对象识别。</p>
<p>接下来我们考虑一个带有任意凸损失函数的标准回归框架。 给定训练集 $\{\mathbf{X}_i, \mathbf{Y}_i\}_{i=1}^N$，其中 $\mathbf{X}_i \in \mathbb{R}^{n \times d}$ 和 $\mathbf{Y}_i \in \mathbb{R}^{n \times c}$ 分别表示输入序列和标签输出，\eqref{eq2} 中仅注意力网络的权重衰减正则化训练问题如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{W_q,W_k,W_v,W_o}\sum\limits^N_{i=1}\mathcal{L}(\mathrm{softmax}(\mathbf{X}_i\mathbf{W}_q\mathbf{W}^\top_k\mathbf{X}^\top_i)\mathbf{X}\mathbf{W}_v\mathbf{W}_o,\mathbf{Y}_i)\\
    +\frac{\beta}{2}\sum\limits_{\ast\in\{ q,k,v,o\}}\|\mathbf{W}_\ast\|^2_F
\end{align*}
\tag{3}\label{eq3}</script><p>其中 $\mathcal{L}(\cdot)$ 是任意凸损失函数，包括平方损失和交叉熵，$\beta &gt;0$ 是正则化系数。</p>
<p>尽管 \eqref{eq2} 中的注意力模型在各种 NLP 任务中非常强大，例如，自然语言推理、神经机器翻译和文本分类，\eqref{eq3} 中相应的训练问题是一项极具挑战性的优化任务，需要对各种非凸优化启发式进行充分训练。 为了解决这些问题，在接下来的部分中，我们首先通过用替代凸层替换注意力部分来重新制定训练问题，然后将重新制定的训练问题转换为可解释的凸优化问题，从而实现全局优化网络参数。</p>
<h3 id="3-1-凸注意力层"><a href="#3-1-凸注意力层" class="headerlink" title="3.1 凸注意力层"></a>3.1 凸注意力层</h3><p>我们首先注意到，由于 $\text{softmax}(\cdot)$ 操作是高度非线性和非凸的，因此 \eqref{eq3} 中的训练问题是一个具有挑战性的非凸优化问题。 因此，人们可能无法充分训练注意力网络并在训练结束时获得微不足道的模型。 例如，dong_attention 表明注意力网络在整个训练过程中可能会退化，并且输出会收敛到秩为 1 的矩阵。 因此，他们无法学习基础任务。</p>
<p>为了避免与 \eqref{eq2} 中的非凸公式相关的问题，我们首先用更简单但有效的替代方法替换 $\text{softmax}$ 操作。 特别是，由于  $\text{softmax}$ 将其输入矩阵的行转换为概率分布，因此可以将其放宽为具有单位单纯形约束的线性运算，如下所示</p>
<script type="math/tex; mode=display">
\forall \mathbf{U}\in\mathbb{R}^{n\times n},\exists \mathbf{W}\in\Delta s.t. \quad \text{softmax}(\mathbf{U})\mathbf{X}=\mathbf{W}\mathbf{X}</script><p>其中 $\Delta := \{\mathbf{W} \in \mathbb{R}^{n \times n}: \mathbf{w}_i\geq 0, \matrix{1}^\top\mathbf{w}_i=1 , \forall i \in [ n]\}$ 表示约束的凸集，也称为单位单纯形约束。 因此，我们在不扰乱其结构的情况下简化和凸化了注意力机制。 基于这一观察，\eqref{eq3} 可以重新表述如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{W_1\in\Delta \\ W_2\in\mathbb{R}^{d\times d},\; W_3\in\mathbb{R}^{d\times c}}\sum\limits^N_{i=1}\mathcal{L}(\mathbf{W}_1\mathbf{X}_i\mathbf{W}_2\mathbf{W}_3,\mathbf{Y}_i)+\frac{\beta}{2}\big(\|\mathbf{W}_2\|^2_F+\|\mathbf{W}_3\|^2_F\big) 
\end{align*}
\tag{4}\label{eq4}</script><p>请注意，上面的模型使用单个头部注意力模型，因此，由于其表达能力不足，可能不具有实际意义。 因此，我们在 \eqref{eq4} 中引入 head 的概念如下：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{W_{1j}\in\Delta \\ W_{2j}\in\mathbb{R}^{d\times d},\; W_{3j}\in\mathbb{R}^{d\times c}}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}\mathbf{W}_{1j}^\top\mathbf{X}_i\mathbf{W}_{2j}\mathbf{W}_{3j},\mathbf{Y}_i\bigg)\\
    +\frac{\beta}{2}\bigg(\sum\limits^h_{j=1}\|\mathbf{W}_{2j}\|^2_F+\|\mathbf{W}_{3j}\|^2_F\bigg)
\end{align*}
\tag{5}\label{eq5}</script><p>现在，我们已准备好将凸分析工具应用于 \eqref{eq5}，详见下一节。</p>
<h3 id="3-2-凸优化应用到仅注意力网络"><a href="#3-2-凸优化应用到仅注意力网络" class="headerlink" title="3.2 凸优化应用到仅注意力网络"></a>3.2 凸优化应用到仅注意力网络</h3><p>作为热身，让我们考虑标量输出预测问题，目标是一维的，即 $y_i \in \mathbb{R}$。 然后，\eqref{eq5} 简化为以下优化问题</p>
<script type="math/tex; mode=display">
\min_{\mathbf{w}_{1j}\in\Delta \\ \mathbf{w}_{2j}\in\mathbb{R}^d,\; w_{3j}\in\mathbb{R}}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}\mathbf{w}_{1j}^\top\mathbf{X}_i\mathbf{w}_{2j}w_{3j},y_i\bigg)+\frac{\beta}{2}\sum\limits^h_{j=1}\big(\|\mathbf{w}_{2j}\|^2_F+(w_{3j})^2 \big) 
\tag{6}\label{eq6}</script><p>接下来，我们首先在参数 $\mathbf{w}_{2j}$ 和 $w_{3j}$ 之间应用重缩放，使得 \eqref{eq6} 可以描述为 $\ell_1$ 正则优化问题。</p>
<p><strong>引理1</strong><br>\eqref{eq6} 中的问题等同于下面的 $\ell_1$ 正则化训练问题：</p>
<script type="math/tex; mode=display">
\min_{\mathbf{w}_{1j}\in\Delta \\ \|\mathbf{w}_{2j}\|_2\leq 1,\; w_{3j}\in\mathbb{R}}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}\mathbf{w}_{1j}^\top\mathbf{X}_i\mathbf{w}_{2j}w_{3j},y_i\bigg)+\beta\|\mathbf{w}\|_1 
\tag{7}</script><p>基于引理1中的等价公式，下一个定理引入了等价于 \eqref{eq6} 的凸优化问题。</p>
<p><strong>定理1</strong><br>非凸优化问题 \eqref{eq6} 可以等效地转换为以下凸优化问题：</p>
<script type="math/tex; mode=display">
\min_{\mathbf{Z}\in\mathbb{R}^{n\times d}}\frac{1}{2}\sum\limits^N_{i=1}\mathcal{L}(trace(\mathbf{Z}^\top\mathbf{X}_i),y_i)+\beta\sum\limits^n_{k=1}\|\mathbf{z}_k\|_2 
\tag{8}\label{eq8}</script><p>请注意，\eqref{eq8} 中的等效凸模型需要单个参数矩阵 $\matrix{Z} \in \mathbb{R}^{n \times d}$，其中每一行是相应token的注意力分数。我们还注意到 \eqref{eq8} 中的正则化，即参数矩阵 $\matrix{Z}$ 行的 $\ell_2$ 范数之和，是一种特定类型的正则化，也称为组 $\ell_1$ 或 Lasso，由 bakin1999adaptive 引入并可促进跨参数的组稀疏性。在我们的例子中，组稀疏度跨越token索引 $k$，因此，可以将 \eqref{eq8} 中的模型解释为稀疏线性模型，其中稀疏性跨token。换句话说，\eqref{eq8} 可以解释为一个模型，它试图使用尽可能少的token来拟合训练标签 $\{y_i\}_{i=1}^N$。</p>
<p>与 \eqref{eq6} 中表示为 $\mathbf{w}_{1j} \in \Delta$的非负注意力得分不同，凸参数 $\matrix{Z} \in \mathbb{R}^{ n \times d}$ 不需要任何约束 . 因此，可以直接应用标准训练算法，如 SGD 和 Adam 来训练凸问题 \eqref{eq8}。 此外，可以从 \eqref{eq8} 的解中恢复 \eqref{eq6} 的一组最佳参数，如以下结果所示。</p>
<p><strong>命题1</strong><br>在求解 \eqref{eq8} 中的凸优化问题后，可以恢复 \eqref{eq6} 中的非凸优化问题的最优解，表示为 $\{\mathbf{w}_{1j}^<em>,\mathbf{w}_{2j}^</em>,w_{3j}^*\}_{j=1}^h$,如下：</p>
<script type="math/tex; mode=display">
\mathbf{w}^*_{1j}=\mathbf{e}_j,\; \mathbf{w}^*_{2j}=\frac{\mathbf{z}_j}{\sqrt{\|\mathbf{z}_j\|_2}},\;\mathbf{w}^*_{3j}=\sqrt{\|\mathbf{z}_j\|_2},\;\forall j\in [h]</script><p>其中 $\mathbf{e}_j \in \mathbb{R}^{n}$ 是第 $j$ 个普通基向量， $\mathbf{z}_j \in \mathbb{R}^{d}$ 是 $\matrix{Z}$矩阵的第 $j$ 行，我们假设由于 \eqref{eq8} 中的稀疏诱导正则化，$\matrix{Z}$ 的 $n$ 行中有 $h$ 非零行。</p>
<p>命题1证明了 \eqref{eq6} 中的非凸公式的参数与 \eqref{eq8} 中凸公式的参数之间存在一对一的映射关系。 因此，无需解决具有挑战性的非凸优化问题 \eqref{eq6} ，该问题还需要对多种启发式优化才能进行充分训练。 相反，可以解决凸问题 \eqref{eq8}，然后使用命题1中的映射来获得 \eqref{eq6} 的最优解。</p>
<h3 id="3-3-拓展到多维输出"><a href="#3-3-拓展到多维输出" class="headerlink" title="3.3 拓展到多维输出"></a>3.3 拓展到多维输出</h3><p>在上一节中，我们考虑了目标变量为标量的问题，即 $y_i \in \mathbb{R}$。 然而，对于某些问题，例如多类分类，目标变量可以是多维的，因此，我们现在将分析扩展到多个向量输出的问题，如下所示</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{\mathbf{w}_{1j}\in\Delta \\ \mathbf{w}_{2j}\in\mathbb{R}^d,\; \mathbf{w}_{3j}\in\mathbb{R}^c}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}\mathbf{w}_{1j}^\top\mathbf{X}_i\mathbf{w}_{2j}\mathbf{w}_{3j},\mathbf{y}_i\bigg)\\
    +\frac{\beta}{2}\bigg(\sum\limits^h_{j=1}\|\mathbf{w}_{2j}\|^2_2+\|\mathbf{w}_{3j}\|^2_1\bigg) 
\end{align*}
\tag{9}\label{eq9}</script><p>其中 $\mathbf{y}_i \in \mathbb{R}^{c}$ 和 $c$ 表示输出类别的数量。 请注意，这里我们在 $\mathbf{w}_{3j}$ 上应用 $\ell_1^2$范数，但这不会影响网络在实践中的性能。 然后，按照相同的推导在下一个结果中产生凸规划。</p>
<p><strong>定理2</strong><br>非凸优化问题 \eqref{eq9} 等价于以下凸优化问题</p>
<script type="math/tex; mode=display">
\min_{\mathbf{Z}_l\in\mathbb{R}^{n\times d}}\sum\limits^N_{i=1}\sum\limits^c_{l=1}\mathcal{L}(trace(\mathbf{Z}^\top_l\mathbf{X}_i),y_{il})+\beta\sum\limits^c_{l=1}\sum\limits^n_{k=1}\|\mathbf{z}_{lk}\|_2 
\tag{10}\label{eq10}</script><p>定理2表明等效的凸模型在输出索引 $l$ 上变得可分离，即，而不是 \eqref{eq8} 中的单个参数矩阵，这里我们有 $c$ 个参数矩阵由于，因为在非凸模型 \eqref{eq9} 中有 $c$ 个输出（详见表2）。 这也说明了网络中输出的数量直接控制了等效凸公式的超参数化水平。</p>
<h3 id="3-4-带FCN层的注意力网络"><a href="#3-4-带FCN层的注意力网络" class="headerlink" title="3.4 带FCN层的注意力网络"></a>3.4 带FCN层的注意力网络</h3><p>尽管 \eqref{eq5} 中的模型在各种应用中表现出有趣的特性，但它基本上是token矩阵 $\mathbf{X}$ 的线性函数。 因此，它很可能会遇到性能不足的问题，尤其是对于 NLP 中的一些具有挑战性的问题。 一系列论文 dong_attention 等也通过广泛的经验证据证实了 FCN 的重要性。 因此，在本节中，我们在 \eqref{eq5} 中将一个 FCN 层添加到我们的注意力模型中，并为这个新模型推导出一个等效的凸公式。</p>
<p>在这里，我们考虑以下优化问题</p>
<script type="math/tex; mode=display">
\begin{align*}
    \min_{\mathbf{w}_{1j}\in\Delta \\ \mathbf{w}_{2j},\mathbf{w}_{3j}\in\mathbb{R}^c}\sum\limits^N_{i=1}\mathcal{L}\bigg(\sigma\bigg(\sum\limits^h_{j=1}\mathbf{w}_{1j}^\top\mathbf{X}_i\mathbf{w}_{2j}\bigg)\mathbf{w}_{3j},\mathbf{y}_i\bigg)\\
    +\frac{\beta}{2}\bigg(\sum\limits^h_{j=1}\|\mathbf{w}_{2j}\|^2_2+\|\mathbf{w}_{3j}\|^2_1\bigg) 
\end{align*}
\tag{11}\label{eq11}</script><p>其中 $\sigma(\cdot)$ 是激活函数。</p>
<p><strong>定理3</strong><br>具有gated ReLU 激活的非凸优化问题 \eqref{eq11} 等价于以下凸优化问题：</p>
<script type="math/tex; mode=display">
\min_{\mathbf{Z}_{jl}\in\mathbb{R}^{n\times d}}\sum\limits^N_{i=1}\sum\limits^c_{l=1}\mathcal{L}\bigg(\sum\limits^h_{j=1}1_{ij}trace(\mathbf{Z}^\top_{jl}\mathbf{X}_i),y_{il}\bigg)+\beta\sum\limits^c_{l=1}\sum\limits^h_{j=1}\sum\limits^n_{k=1}\|\mathbf{z}_{jlk}\|_2 
\tag{12}\label{eq12}</script><p>其中 $1_{ij} := 1\{\mathbf{u}_{1j}^\top \mathbf{X}_i \mathbf{u}_{2j} \geq 0\}$ 表示gated ReLU 激活的指示函数，这里 $\{\mathbf{u}_{1j},\mathbf{u}_{2j}\}_{j=1}^h$ 是固定向量，可以随机选择。</p>
<p>定理3意味着引入激活函数会进一步增加等效凸公式的超参数化水平。 准确地说，\eqref{eq12} 的参数比 \eqref{eq10} 多 $h$ 倍，如表2所示。</p>
<h2 id="4，数值实验"><a href="#4，数值实验" class="headerlink" title="4，数值实验"></a>4，数值实验</h2><p>在本节中，我们将展示实验结果，以证实我们在前几节中的理论。</p>
<p><strong>BERT中的师生设置</strong><br>我们首先考虑在 Hugging Face 库中使用预训练 BERT 模型的师生设置，即 bert-base-uncased。特别是，我们从 glue 数据集的 mrpc 子集中获取数据，传递给预训练的 BERT 模型，并将输入和输出激活保存在特定层中。然后，我们训练仅注意模型，即标准非凸自注意 \eqref{eq3}、替代非凸注意 \eqref{eq9} 和凸 \eqref{eq10}，从头开始分别前后使用这些激活函数，用前面得到的数据作为我们的训练数据集。</p>
<p>本节中的所有实验都是使用 Google Colab 上的单个 GPU 执行的，还使用相同的正则化系数 $\beta$ 和优化器，即 Adam，并通过对两种算法的验证数据集执行网格搜索来调整学习率和正则化系数。但是，请注意，我们没有对所有实验中的凸模型使用任何非凸优化启发式方法，例如层归一化和跳过连接。在图2中，我们使用从预训练 BERT 模型的第六层提取的数据绘制了目标值（即训练损失 + 正则化项）和测试损失，以秒为时间单位。我们观察到，凸训练方法实现的目标值几乎比标准的非凸训练小一个数量级，后者可能停留在局部最小值，这种训练也能增强泛化能力，即我们的凸训练方法比标准的非凸训练获得更低的测试损失。为了理解每个模型学习的函数，我们还分析了图3中的注意力图。在这里，标准的非凸训练无法学习底层模型并输出跨token的统一注意力图。然而，凸训练输出了一个与地面真实注意力图非常相似的注意力图，因此我们成功地学习了训练数据中的结构。因此，这些实验清楚地说明了凸训练方法在训练和测试中的有效性。</p>
<p><strong>算法数据集和 Grokking</strong><br>受 power2022grokking 中观察到的 grokking 现象的启发，我们接下来使用 \eqref{eq1} 中的自我注意机制，在算法数据集上，验证凸训练方法对标准Transformer网络的有效性。特别是，我们使用在 power2022grokking 中相同的设置，并使用 $\mod 97$ 和 $\mod 15$ 评估模块化除法运算的性能，一直训练框架直至达到 $99\%$ 的测试精度。</p>
<p>在图4中，我们首先复制了 power2022grokking 中的结果，并确认这里确实出现了 grokking 现象，即非凸曲线（图4a中紫色）在 $10^3$ 左右时达到 $100 \%$ 的训练精度，但是图4b中需要超过 $10^5$ 次迭代才能达到完美泛化。我们还比较了非凸和凸训练方法，凸训练方法比图 4b 中的非凸训练方法收敛到完美泛化精度要快 10 倍。此外，凸模型在图 4c 中产生的测试损失也显着降低，这意味着它对测试预测具有更高的置信度，因此比标准非凸训练更稳健。</p>
<p>我们注意到在上一节中，我们理论上只分析了单个注意力Transformer块。然而，由于深度或层数（表示为$L$）的良性影响已经在深度学习文献中得到了经验证明，我们还建议将我们的凸模型扩展到更深的设置。我们在 \eqref{eq12} 中堆叠凸Transformer层以获得任意深度的网络。在图5 中，我们比较了双层Transformer网络与一层网络的性能。在这里，我们观察到虽然增加一层可以显着改善凸模型，尤其是在优化速度方面，但它无法对非凸模型产生任何明显的差异。此外，我们在 $\mod 15$ 操作上运行算法，由于样本数量较少，这基本上是更具挑战性的任务。在这种情况下，如图6 所示，单层模型无法完美地学习底层任务，但我们的凸模型在测试精度和测试损失方面明显更好。通过将层数增加到四层，我们使两个模型都能达到完美的泛化精度。我们的深度模型比非凸模型更快地达到这个水平并且产生更低的测试损失。</p>
<p>接下来，我们根据经验分析我们的凸模型和标准非凸模型上的 grokking 现象。 为此，我们绘制了图7a 中每个实验达到 $99\%$ 测试准确度所需的迭代次数。 请注意，这里我们不包括 $\mod 15$ 案例的单层模型结果，因为在该案例中两个模型都未能实现完美泛化。 图7a 清楚地表明，我们的凸训练方法比标准非凸训练更快地收敛到 $99\%$ 的准确度水平。 因此，我们还减轻了 grokking 现象的影响，如图7b 所示，我们根据迭代次数量化了 grokking 的数量。 基于这个实验，我们还推测 grokking 现象主要归因于标准Transformer模型的高度非线性和非凸结构。</p>
<h2 id="5，结论"><a href="#5，结论" class="headerlink" title="5，结论"></a>5，结论</h2><p>在本文中，我们研究了注意力Transformer网络的正则化训练问题，并开发了一个凸分析框架来训练这些网络。 特别是，我们首先提出了自注意力机制的凸替代方案，然后将这种替代注意力机制的训练问题重新表述为凸优化问题。 由于我们的凸重构，我们全局优化网络参数而不需要任何类型的非凸优化启发式。 此外，我们重构的训练框架学到的函数是透明和可解释的。 更重要的是，重构的问题揭示了数据中跨token的稀疏性诱导正则化机制，这也更清楚地说明了结果函数的结构及其泛化属性。 然后，我们通过几个数值实验，凭经验验证了我们的凸训练方法相对于标准非凸训练的有效性。</p>
<p>我们还注意到，通过凸优化理论的视角分析Transformer网络是极其重要的，因为它可能会大大改善对这些网络的理解和优化。 然而，由于网络模型固有的非凸结构，这也非常具有挑战性。 据我们所知，本文是朝着这个方向迈出的第一步，因此存在一些局限性，希望在未来的工作中消除这些局限性。 具体来说，在本文中，我们主要关注凸分析的理论方面，并在一些小规模问题实例上对理论进行了实证验证。 我们希望后续论文能对我们的理论进行全面、大规模的实证验证。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>convexify</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention is all you need</title>
    <url>/2023/03/22/Transformer/Attention%20is%20all%20you%20need/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：Attention is all you need</p>
<p><a href="https://arxiv.org/abs/1706.03762">essay link</a></p>
<p><strong>Transformer的开山鼻祖~</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>显性序列转换模型基于包括编码器和解码器的复杂递归或卷积神经网络，性能最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构，即 Transformer，它完全基于注意力机制，完全摒弃了循环和卷积。对两项机器翻译任务的实验表明，这些模型在质量上更胜一筹，同时可并行化程度更高，并且需要的训练时间明显减少。我们的模型在 WMT 2014 英德翻译任务中达到了 28.4 BLEU，比现有的最佳结果（包括集成）提高了超过 2 BLEU。在 WMT 2014 英法翻译任务中，我们的模型在八个 GPU 上训练 3.5 天后建立了一个新的单模型，其最先进的 BLEU 分数为 41.8，训练成本约占当时最好模型成本的一小部分。。我们通过将 Transformer 成功应用于具有大量和有限训练数据的英语选区解析，证明 Transformer 可以很好地泛化到其他任务。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>循环神经网络，特别是长短期记忆和门控循环神经网络，已被牢固确立为序列建模和转换问题（如语言建模和机器翻译）的最先进方法。 此后，许多工作继续涌现，努力推动循环语言模型和编码器-解码器架构的边界。</p>
<p>循环模型通常沿着输入、输出序列的符号位置进行因子计算，将位置在计算时间内对齐到步骤中，它们生成一系列隐藏状态 $h_t$，作为前一个隐藏状态 $h_{t-1}$ 和位置 $t$ 的输入函数。这种固有的顺序性质排除了训练样本中的并行化，这在较长的序列长度下变得至关重要，因为内存限制限制了跨样本的批处理。最近的工作通过分解技巧和条件计算显著提高了计算效率，同时后者还提高了模型性能。 然而，顺序计算的基本限制仍然存在。</p>
<p>注意机制已成为序列建模和转换模型的各种任务中不可或缺的一部分，允许依赖性建模而不考虑它们在输入或输出序列中的距离。 然而，在大部分情况（除少数情况外）下，这种注意力机制都与循环网络结合使用。</p>
<p>本文提出了 Transformer，这是一种避免重复出现的模型架构，而是完全依赖注意力机制来绘制输入和输出之间的全局依赖关系。 Transformer 允许显著提高并行化，并且在八个 P100 GPU 上经过短短 12 小时的训练后，可以达到翻译质量的新水平。</p>
<h2 id="2，背景"><a href="#2，背景" class="headerlink" title="2，背景"></a>2，背景</h2><p>减少顺序计算的目标也构成了扩展神经 GPU 、ByteNet  和 ConvS2S 的基础，所有这些都使用卷积神经网络作为基本构建块，并行计算输入和输出所有位置的隐藏表示。在这些模型中，将来自任意两个输入或输出位置信号相关联所需的操作数量随着位置之间的距离而增加，ConvS2S 呈线性增长，By​​teNet 呈对数增长，这使得学习远距离位置之间的依赖关系变得更加困难。在 Transformer 中，这被减少到恒定数量的操作，尽管由于平均注意力加权位置而导致有效分辨率降低的代价，我们用 Multi-Head Attention 抵消了这种效果，如3.2部分所述.</p>
<p>自注意力，有时称为内部注意力，是一种将单个序列的不同位置相关联以计算序列表示的注意力机制。自注意力已成功用于各种任务，包括阅读理解、抽象摘要、文本蕴含和学习与任务无关的句子表示。</p>
<p>端到端记忆网络基于循环注意机制而不是序列对齐循环，并且已被证明在简单语言问答和语言建模任务上表现良好。</p>
<p>然而，据我们所知，Transformer 是第一个完全依靠自注意力来计算其输入和输出表示而不使用序列对齐 RNN 或卷积的转换模型。在接下来的部分中，我们将描述 Transformer，激发自注意力并讨论它相对于 neural_gpu 等模型的优势。</p>
<h2 id="3，模型结构"><a href="#3，模型结构" class="headerlink" title="3，模型结构"></a>3，模型结构</h2><p>大多数有竞争力的神经序列转换模型都具有编码器-解码器结构， 这里，编码器将符号表示的输入序列 $(x_1, …, x_n)$ 映射到连续表示的序列 $\mathbf{z} = (z_1, …, z_n)$。 给定 $\mathbf{z}$，解码器随后逐个生成符号输出序列 $(y_1,…,y_m)$。 在每个步骤中，模型都是自回归的，在生成下一个时将之前生成的符号作为附加输入使用。</p>
<p>Transformer 遵循这种整体架构，为编码器和解码器使用堆叠式自注意力和逐点全连接层，分别如图1的左半部分和右半部分所示。</p>
<h3 id="3-1-编码器和解码器的堆叠"><a href="#3-1-编码器和解码器的堆叠" class="headerlink" title="3.1 编码器和解码器的堆叠"></a>3.1 编码器和解码器的堆叠</h3><p><strong>编码器</strong><br>编码器由 $N=6$ 个相同的层堆叠而成，每一层都有两个子层。第一个是多头自注意力机制，第二个是简单的、按位置的全连接前馈网络。我们在两个子层中都使用了一个残差连接，然后是层归一化，即每个子层的输出为 $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$，其中 $\mathrm{Sublayer}(x)$ 是子层实现的函数层本身。为了促成这些残差连接，模型中的所有子层以及嵌入层都产生维度 $d_{model}=512$ 的输出。</p>
<p><strong>解码器</strong><br>解码器也由 $N=6$ 个相同的层堆叠而成。除了每个编码器层中的两个子层之外，解码器还插入了第三个子层，它对编码器堆栈的输出执行多头注意力。与编码器类似，我们在每个子层上使用残差连接，然后进行层归一化，还修改了解码器堆栈中的自我注意子层，以阻止当前位置关注后续位置信息。这种掩码与输出嵌入偏移一个位置的事实相结合，确保了对位置 $i$ 的预测只能依赖于位置小于 $i$ 的已知输出。</p>
<h3 id="3-2-注意力"><a href="#3-2-注意力" class="headerlink" title="3.2 注意力"></a>3.2 注意力</h3><p>注意力函数可以描述为将 $q$ 和 一组 $k-v$ 对映射到输出，其中 $q$、$k$、$v$ 和 $output$ 都是向量，$output$ 计算为 $v$ 的加权和，其中分配给每个 $v$ 的权重由 $q$ 与相应的 $k$ 的兼容性函数计算。</p>
<h4 id="3-2-1-归一化点积注意力"><a href="#3-2-1-归一化点积注意力" class="headerlink" title="3.2.1 归一化点积注意力"></a>3.2.1 归一化点积注意力</h4><p>将本文的特殊注意力称为“归一化点积注意力”（图2），包含 $q$，$k$ 的输入维度为 $d_k$， $v$ 的维度为 $d_v$。 计算 $q$ 与 $k$ 的点积，逐个除以 $\sqrt{d_k}$，然后应用 softmax 函数来获得 $v$ 的权重。</p>
<p>在实践中，我们同时计算一组 $q$ 的注意力函数，并将它们打包到一个矩阵 $Q$ 中，$k$ 和 $v$ 也一起打包到矩阵 $K$ 和 $V$ 中，我们将输出矩阵计算为：</p>
<script type="math/tex; mode=display">
\begin{align*}
   \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\end{align*}
\tag{1}</script><p>两种最常用的注意力函数是加法注意力和点积（乘法）注意力。点积注意力与我们的算法相同，除了 $\frac{1}{\sqrt{d_k}}$ 的归一化系数。 加法注意力使用具有单个隐藏层的前馈网络计算兼容性函数。 虽然两者在理论上的复杂性相似，但点积注意力在实践中速度更快且空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</p>
<p>虽然对于较小 $d_k$ 的情况，这两种机制的表现相似，但加法注意力在 $d_k$ 较大时因无需缩放而优于点积注意力。 我们怀疑对于较大的 $d_k$ 值，点积的幅度会变大，将 softmax 函数推入梯度极小的区域。 为了抵消这种影响，我们将点积缩放 $\frac{1}{\sqrt{d_k}}$。</p>
<h4 id="3-2-2-多头注意力"><a href="#3-2-2-多头注意力" class="headerlink" title="3.2.2 多头注意力"></a>3.2.2 多头注意力</h4><p>与使用 $d_{model}$ 维 $k$、$v$、$q$ 执行单个注意力头不同，我们发现通过可学习的线性投影分别在 $k$、$v$、$q$ 上执行  $h$ 次是有益的，相应投影后的向量维度为 $d_k$，$d_k $ 和 $d_v$ 。然后，在 $q$ 、 $k$ 和 $v$ 的每个投影版本上，我们并行执行注意力函数，产生 $d_v$ 维输出值，这些值之后被连接起来并再次投影，产生最终值，如图2所示。</p>
<p>多头注意力允许模型共同关注来自不同位置、不同表示子空间的信息，对于单个注意力头，均值化可以抑制这种情况。</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathrm{MultiHead}(Q, K, V) &= \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O\\
    \text{where}\;\mathrm{head_i} &= \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)
\end{align*}</script><p>其中投影指的是参数矩阵 $W^Q_i \in \mathbb{R}^{d_{model} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{model} \times d_k}$, $W ^V_i \in \mathbb{R}^{d_{model} \times d_v}$ 和 $W^O \in \mathbb{R}^{hd_v \times d_{model}}$。</p>
<p>本文中，使用了 $h=8$ 个并行注意力层或头，对于其中的每一个，都使用 $d_k=d_v=d_{model}/h=64$，由于每个头的维数减少，总计算成本与具有全维的单头注意力相似。</p>
<h4 id="3-2-3-本文模型中注意力的应用"><a href="#3-2-3-本文模型中注意力的应用" class="headerlink" title="3.2.3 本文模型中注意力的应用"></a>3.2.3 本文模型中注意力的应用</h4><p>Transformer 以三种不同的方式使用多头注意力：</p>
<ul>
<li><p>在编码解码器注意力的网络层中，$q$ 来自于前面的解码器层，记忆 $k$ 和 $v$ 来自于编码器的输出，这允许解码器中的每个位置都参与输入序列中的位置，模仿了序列到序列模型中典型的编码器-解码器注意力机制。</p>
</li>
<li><p>编码器包含自注意力层，在自注意力层中，所有的 $k$ 、 $v$ 和 $q$ 都来自同一个地方，在这种情况下，前一层的输出在编码器中，编码器中的每个位置都可以关注编码器上一层中的所有位置。</p>
</li>
<li><p>类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的左向信息流，以保持自回归特性，通过屏蔽掉（设置为 $-\infty$）softmax 输入中对应于非法连接的所有值，在缩放点积注意力内部实现这一点。见图2。</p>
</li>
</ul>
<h3 id="3-3-基于位置的前馈网络"><a href="#3-3-基于位置的前馈网络" class="headerlink" title="3.3 基于位置的前馈网络"></a>3.3 基于位置的前馈网络</h3><p>除了注意力子层之外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，它相同地应用于每个对应位置。 这由两个线性变换组成，中间有一个 ReLU 激活。</p>
<script type="math/tex; mode=display">
\begin{align*}
   \mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2
\end{align*}
\tag{2}</script><p>虽然线性变换在不同位置是相同的，但它们在层与层之间使用不同的参数。 另一种描述方式是使用内核大小为 1 的两个卷积，输入和输出的维数为 $d_{model}=512$，内层的维数为 $d_{ff}=2048$。</p>
<h3 id="3-4-Embeddings-和-Softmax"><a href="#3-4-Embeddings-和-Softmax" class="headerlink" title="3.4 Embeddings 和 Softmax"></a>3.4 Embeddings 和 Softmax</h3><p>与其他序列转换模型类似，我们使用学习嵌入将输入标记和输出标记转换为维度 $d_{model}$ 的向量，还使用常用学习中的线性变换和 softmax 函数将解码器输出转换为预测的下一个标记概率。在我们的模型中，我们在两个嵌入层和 pre-softmax 线性变换之间共享权重矩阵，在嵌入层中，我们将这些权重乘以 $\sqrt{d_{model}}$。</p>
<h3 id="3-5-位置编码"><a href="#3-5-位置编码" class="headerlink" title="3.5 位置编码"></a>3.5 位置编码</h3><p>由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于标记在序列中的相对或绝对位置信息。为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入嵌入中，位置编码与嵌入具有相同的维度 $d_{model}$，因此可以将两者相加。位置编码有很多选择，可参见 JonasFaceNet2017等。</p>
<p>在这项工作中，我们使用不同频率的正弦和余弦函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
    PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{model}}) \\
    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{model}})
\end{align*}</script><p>其中 $pos$ 是位置，$i$ 是维度，也就是说，位置编码的每个维度对应一个正弦曲线，波长形成从 $2\pi$ 到 $10000 \cdot 2\pi$ 的几何级数。 我们选择这个函数是因为我们假设它可以让模型很容易地学习相对位置信息，因为对于任何固定偏移 $k$，$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性函数。</p>
<p>我们还尝试使用学习的位置嵌入代替，发现这两个版本产生几乎相同的结果（参见表3），选择正弦版本是因为它可以让模型推断出比训练期间遇到的序列长度更长的序列长度。</p>
<h2 id="4，为什么要自注意力"><a href="#4，为什么要自注意力" class="headerlink" title="4，为什么要自注意力"></a>4，为什么要自注意力</h2><p>在本节中，我们将自注意力层的各个方面与其他模型比较，这些模型通常用于将一个可变长度符号表示序列 $(x_1, …, x_n)$ 映射到另一个等长序列 $(z_1 , …, z_n)$, 其中 $x_i, z_i \in \mathbb{R}^d$, 例如典型序列转换编码器或解码器中的隐藏层，为了激励我们使用自注意力，我们考虑了三个必要条件。</p>
<p>一个是每层的总计算复杂度。另一个是可以并行化的计算量，用所需的最小顺序操作数来衡量。</p>
<p>第三个是网络中远程依赖关系之间的路径长度。学习长程依赖性是许多序列转换任务中的关键挑战，影响学习这种依赖性能力的一个关键因素是前向和后向信号在网络中必须经过的路径长度。输入和输出序列中任何位置组合之间的路径越短，就越容易学习远程依赖。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。</p>
<p>如表1中第3列所述，自注意力层将所有位置与恒定数量的顺序执行操作连接起来，而循环层需要 $O(n)$ 个顺序操作。<br>就计算复杂度而言，当序列长度 $n$ 小于表示维度 $d$ 时，自注意力层比循环层更快，这是句子表示中最常见的情况，通常在最先进的机器翻译模型中出现，例如词片和字节对表示。为了提高涉及非常长序列任务的计算性能，可以将自注意力限制为仅考虑以相应输出位置为中心的输入序列中大小为 $r$ 的邻域，会将最大路径长度增加到 $O(n/r)$，我们计划在未来的工作中进一步研究这种方法。</p>
<p>内核宽度 $k &lt; n$ 的单个卷积层不会连接所有输入和输出位置对，为了增加网络中任意两个位置之间的最长路径，在连续内核的情况下需要一堆 $O(n/k)$ 卷积层，或者在空洞卷积的情况下需要 $O(log_k(n))$。卷积层通常比递归层复杂 $k$ 倍，但是可分离卷积大大降低了复杂性，达到 $O(k \cdot n \cdot d + n \cdot d^2)$。然而，即使 $k=n$，可分离卷积的复杂度也等于自注意力层和逐点前馈层的组合，这是本文模型的方法。</p>
<p>作为附带的好处，自注意力可以产生更多可解释的模型，从我们的模型中检查注意力分布，并在附录中展示和讨论示例。 不仅单个注意力头清楚地学会执行不同的任务，而且多注意力头似乎表现出了与句子的句法和语义结构相关的行为。</p>
<h2 id="5，训练"><a href="#5，训练" class="headerlink" title="5，训练"></a>5，训练</h2><p>本节描述了我们模型的训练机制。</p>
<h3 id="5-1-训练数据和批量化"><a href="#5-1-训练数据和批量化" class="headerlink" title="5.1 训练数据和批量化"></a>5.1 训练数据和批量化</h3><p>我们在标准的 WMT 2014 英德数据集上进行了训练，该数据集包含大约 450 万个句子对。 句子使用字节对编码进行编码，它具有约 37000 个标记的共享source-target词汇表。 对于英语-法语，我们使用了明显更大的 WMT 2014 英语-法语数据集，该数据集由 3600 万个句子组成，并将标记拆分为 32000 个单词词汇表。 句子对按近似序列长度分批在一起，每个训练批次包含一组句子对，其中包含大约 25000 个source标记和 25000 个target标记。</p>
<h3 id="5-2-硬件和策略"><a href="#5-2-硬件和策略" class="headerlink" title="5.2 硬件和策略"></a>5.2 硬件和策略</h3><p>我们在一台配备 8 个 NVIDIA P100 GPU 的机器上训练我们的模型，使用整篇论文中描述的关于超参数的基础模型，每个训练步骤大约需要 0.4 秒。 我们对基本模型进行了总共 100,000 步或 12 小时的训练，对于大模型，（在表3的底行描述），步进时间为 1.0 秒。 大模型训练了 300,000 步（3.5 天）。</p>
<h3 id="5-3-优化器"><a href="#5-3-优化器" class="headerlink" title="5.3 优化器"></a>5.3 优化器</h3><p>我们使用了 Adam 优化器，其中 $\beta_1=0.9$、$\beta_2=0.98$ 和 $\epsilon=10^{-9}$。 我们根据以下公式在训练过程中更新学习率：</p>
<script type="math/tex; mode=display">
lrate=d^{-0.5}_{model}\cdot \min(step\_num^{-0.5},step\_num\cdot warmup\_steps^{-0.5})</script><p>这对应于第一个 $warmup_steps$ 训练步骤线性增加学习率，然后按步数的平方根按反比例减少学习率，本文使用了 $warmup_steps=4000$。</p>
<h3 id="5-4-正则化"><a href="#5-4-正则化" class="headerlink" title="5.4 正则化"></a>5.4 正则化</h3><p>我们在训练期间采用三种类型的正则化：</p>
<p><strong>Residual Dropout</strong><br>我们将 dropout 应用于每个子层的输出，然后再将其添加到子层输入并进行归一化。 此外，我们将 dropout 应用于编码器和解码器堆栈中的嵌入和位置编码的总和。 对于基本模型，我们使用 $P_{drop}=0.1$ 的比率。</p>
<p><strong>Label Smoothing</strong><br>在训练期间，我们采用了值为 $\epsilon_{ls}=0.1$ 的标签平滑。 这会伤害困惑度，因为模型会变得更加不确定，但会提高准确性和 BLEU 分数。</p>
<h2 id="6，结果"><a href="#6，结果" class="headerlink" title="6，结果"></a>6，结果</h2><h3 id="6-1-机器翻译"><a href="#6-1-机器翻译" class="headerlink" title="6.1 机器翻译"></a>6.1 机器翻译</h3><p>在 WMT 2014 英德翻译任务中，大Transformer模型中的 表格2中的Transformer (big)）比之前报告的最佳模型（包括集成模型）高出超过 $2.0$ 个BLEU ，建立了一个新的最先进的 BLEU 分数 $28.4$。该模型的配置列在表3的最后一行，在 8 块 P100 GPU 上训练花费了 3.5 天，甚至我们的基础模型也超过了之前发布的所有模型和集成，其训练成本仅为任何竞争模型的一小部分。</p>
<p>在 WMT 2014 英法翻译任务中，我们的大模型获得了 41.0 的 BLEU 分数，优于之前发布的所有单一模型，训练成本不到之前最好模型的 1/4。为英语到法语训练的 Transformer（大）模型使用 dropout 比例 $P_{drop}=0.1$，而不是 $0.3$。</p>
<p>对于基本模型，我们平均使用了最后 5 个训练checkpoints点获得的单一模型，这些检查点以每 10 分钟的间隔写入。对于大型模型，我们对最后 20 个checkpoints进行了平均。使用束搜索，束大小为 $4$，长度惩罚为 $\alpha=0.6$。这些超参数是在开发集上进行实验后选择的，在推理期间将最大输出长度设置为输入长度 + $50$，但尽可能提前终止。</p>
<p>表2总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较，通过乘以训练时间、使用的 GPU 数量和每个 GPU 的持续单精度浮点容量估计值来估计用于训练模型的浮点运算数。</p>
<h3 id="6-2-模型变体"><a href="#6-2-模型变体" class="headerlink" title="6.2 模型变体"></a>6.2 模型变体</h3><p>为了评估 Transformer 不同组件的重要性，我们以不同的方式改变基础模型，测量开发集 newstest2013 上英语到德语翻译的性能变化。使用了上一节中描述的束搜索，但没有检查点平均，在表3中展示了这些结果。</p>
<p>在表3的行(A)中，我们改变注意力头的数量以及注意力 $k$ 和 $v$ 的维度，保持计算量不变，如第3.2.2节所述。虽然单头注意力比最佳设置差 0.9 BLEU，但如果头太多，质量也会下降。</p>
<p>在表3的行(B)中，我们观察到减小注意力 $k$ 的维度大小 $d_k$ 会损害模型质量，这表明确定兼容性并不容易，比点积更复杂的兼容性函数可能是有益的。我们在 (C) 和 (D) 行中进一步观察到，正如预期的那样，模型越大越好，dropout 对避免过度拟合非常有帮助。在行 (E) 中，我们用学习的位置嵌入替换我们的正弦位置编码，并观察到与基本模型几乎相同的结果。</p>
<h3 id="6-3-英文选区解析"><a href="#6-3-英文选区解析" class="headerlink" title="6.3 英文选区解析"></a>6.3 英文选区解析</h3><p>为了评估 Transformer 是否可以推广到其他任务，我们对英语选区解析进行了实验。这项任务提出了具体的挑战：输出受到强大的结构约束，并且比输入长得多。此外，RNN 序列到序列模型无法在小数据范围内获得最先进的结果。</p>
<p>我们在 Penn Treebank 的华尔街日报 (WSJ) 部分训练了一个 $d_{model} = 1024$ 的 4 层Transformer，大约 40K 个训练句子，还在半监督环境中训练它，使用的数据是来自大约 1700 万个句子的高置信度和 BerkleyParser 语料库。我们为 WSJ only 设置使用了 16K tokens 的词汇表，为半监督设置使用了 32K tokens 的词汇表。</p>
<p>我们只进行了少量实验来选择 dropout，注意力和残差， 22 节开发集上的学习率和光束大小，所有其他参数与英语-德语基础翻译模型的参数保持不变。在推理过程中，我们将最大输出长度增加到输入长度 + $300$，对于仅 WSJ 和半监督设置，我们使用 $21$ 和 $\alpha=0.3$ 的光束大小。</p>
<p>我们在 Table4 中的结果表明，尽管缺乏针对特定任务的调整，我们的模型表现出奇的好，产生的结果比之前报告的所有模型都要好，循环神经网络语法除外。</p>
<p>与 RNN 序列到序列模型相比，Transformer 优于 BerkeleyParser，即使仅在 WSJ 40K 句子训练集上进行训练也是如此。</p>
<h2 id="7，结论"><a href="#7，结论" class="headerlink" title="7，结论"></a>7，结论</h2><p>在这项工作中，我们提出了 Transformer，这是第一个完全基于注意力的序列转换模型，用多头自注意力取代了编码器-解码器架构中最常用的循环层。</p>
<p>对于翻译任务，Transformer 的训练速度明显快于基于循环层或卷积层的架构。在 WMT 2014 英德和 WMT 2014 英法翻译任务中，我们都达到了新的水平。在前一个任务中，我们最好的模型甚至优于所有先前报告的集成模型，还通过英语选区分析实验表明我们模型的更广泛且适用性更好。</p>
<p>我们对基于注意力的模型的未来感到兴奋，并计划将它们应用到其他任务中，计划将 Transformer 扩展到涉及文本以外的输入和输出模式的问题，并研究局部的、受限的注意力机制，以有效处理图像、音频和视频等大型输入和输出，减少生成顺序是我们的另一个研究目标。</p>
<p>我们用于训练和评估模型的代码可在 <a href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a> 获得。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Fourier Transformer</title>
    <url>/2022/11/16/Transformer/Transformer%20with%20Fourier/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：Transformer with Fourier Integral Attentions</p>
<p><a href="https://arxiv.org/abs/2206.00206">essay link</a></p>
<p><strong>无参数核估计，Fourier积分原理等跟Transformer关联的文章，似懂非懂啊，老天爷！</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>多头注意力赋予了Transformer最近的成功，这些最先进的模型在序列建模及其他方面效果显著。这些注意力机制计算q和k之间的成对点积，使用未归一化的Gaussian核并假设q遵循混合Gaussian分布，但实践中并不能保证这个假设成立。</p>
<p>相对应的，本文首先将 Transformer 中的注意力解释为非参数核回归，然后提出了FourierFormer，一类将点积内核用新颖的广义Fourier积分内核取代的新Transformer。与点积核需要选择一个好的协方差矩阵来捕捉数据特征的依赖性不同，广义Fourier积分核可以自动捕捉这种依赖性，无需调试协方差矩阵。我们从理论上证明了本文提出的Fourier积分核可以有效地逼近任何k和q的分布。</p>
<p>与具有点积注意力的传统Transformer相比，FourierFormers 获得了更好的精度并减少了注意力头之间的冗余。我们经验性地证实了 FourierFormers 在包括语言建模和图像分类在内的各种实际应用中优于基线Transformer的优势。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>Transformers是强大的神经网络，在机器学习的许多领域都取得了巨大的成功并成为跨领域广泛应用的最先进模型，视频，点云，以及蛋白质序列。除了在监督学习任务上的出色表现外，Transformer 还可以有效地将学习到的知识从预训练任务转移到有限制或无监督的新任务。Transformers 的核心是点积自注意力，它主要是导致 Transformer 模型成功的原因。</p>
<p>这种点积自注意力通过估计给定标记相对于所有其他标记的相对重要性来学习输入序列中标记之间的自我对齐，然后它将每个标记转换为其他标记的特征表示的加权平均值，其中权重与每对标记之间的重要性分数成正比。 自注意力中的重要性分数使一个标记能够关注序列中的其他标记，从而捕获上下文表示。</p>
<h3 id="1-1-自注意力"><a href="#1-1-自注意力" class="headerlink" title="1.1 自注意力"></a>1.1 自注意力</h3><p>给定输入序列$\mathbf{X}:=[x_1,\cdots x_N]^T\in \mathbb{R}^{N\times D_x} $是$\mathbf{N}$维特征向量，自注意力计算$\mathbf{X}$给定下的$\mathbf{H}$输出序列。</p>
<p><strong>Step 1将输入序列映射到不同子空间</strong><br>输入序列是$\mathbf{X}$，会通过三个线性变换转换为查询矩阵$\mathbf{Q}$，关键矩阵$\mathbf{K}$和值矩阵$\mathbf{V}$，也就是：</p>
<script type="math/tex; mode=display">\mathbf{Q}=\mathbf{X}\mathbf{W}^T_Q;\quad \mathbf{K}=\mathbf{X}\mathbf{W}^T_K;\quad \mathbf{V}=\mathbf{X}\mathbf{W}^T_V</script><p>其中，$\mathbf{W}_Q , \mathbf{W}_K \in \mathbb{R}^{D\times D_x} $，有 $\mathbf{W}_V \in\mathbb{R}^{D_v\times D_x} $ 是权重矩阵。相应的$q,k,v$的矩阵表示如下：</p>
<script type="math/tex; mode=display">\mathbf{Q}:=[q_1,\cdots q_N]^T, \mathbf{K}:=[k_1,\cdots k_N]^T,  \mathbf{V}:=[v_1,\cdots v_N]^T</script><p><strong>Step 2 用加权平均计算输出</strong><br>输出$\mathbf{H}:=[h_1,\cdots h_N]^T $可以表示为：</p>
<script type="math/tex; mode=display">\mathbf{H}=softmax(\mathbf{Q}\mathbf{K}^{T}/\sqrt{\mathbf{D}})\mathbf{V := \mathbf{A}\mathbf{V}} \tag{1}</script><p>其中，softmax函数是作用在矩阵$(\mathbf{Q}\mathbf{K}^T)/\sqrt{D} $的每一行，对每一个查询向量$q_i,\; i=1,\cdots N $，方程(1)可以写成如下的向量形式来求每个输出向量$h_i$：</p>
<script type="math/tex; mode=display">h_{i}=\sum\limits_{j=1}^{N}softmax(q_{i}^{T}k_{j}/\sqrt{\mathbf{D}})v_{j} := \sum\limits_{j=1}^{N}a_{ij}v_{j} \tag{2}</script><p>矩阵$\mathbf{A} \in\mathbb{R}^{N\times N},\; a_{ij}\in\mathbf{A}\; i,j=1\cdots N$分别为注意力矩阵和主力已分值。</p>
<p>自注意力就是用方程(1)和方程(2)来计算的，也叫点乘注意力或者softmax注意力。在本文中，将使用这种注意力的Transformer称为具有点积注意力或点积Transformer的基线Transformer。 训练后的注意力矩阵 $\mathbf{A}$ 的结构决定了自注意力捕获每个标记的上下文表示能力。</p>
<p><strong>多头注意力</strong><br>每个输出序列 $\mathbf{H}$ 形成一个注意力头，多头注意力连接多个头来计算最终输出。令 $H$ 为头的数量，$\mathbf{W}^O \in \mathbb{R}^{HD_v \times HD_v}$ 为输出的投影矩阵。 多头注意力定义为：</p>
<script type="math/tex; mode=display">MultiHead(\{\mathbf{Q},\mathbf{K},\mathbf{V} \}^{H}_{i=1}) = Concat(\mathbf{H}_{1},\cdots \mathbf{H}_{H})\mathbf{W}^{O}</script><p>注意力机制的容量和学习不同句法和语义关系的能力决定了transformer的成功。然而，方程(1)和(2)意味着点积注意力会假设特征$q_i$和$k_j$是独立的。因此，点积注意力无法捕捉这些特征之间的相关性，从而限制了其表示能力并抑制了Transformer在实际任务中的性能，因为无法保证可以从复杂数据中学习到彼此独立的特征。捕获特征 $q_i$ 和 $k_j$ 之间相关性的一种解决方案是将协方差矩阵引入点积注意力的公式中，代价是计算复杂度显著增加，此外，选择好的协方差矩阵也很困难。</p>
<h3 id="1-2-贡献"><a href="#1-2-贡献" class="headerlink" title="1.2 贡献"></a>1.2 贡献</h3><p>在本文中，我们首先建立了自注意力和非参数核回归之间的对应关系。 在这种自注意力的新视角下，我们解释了点积自注意力的局限性，即它可能无法捕获查询向量与关键向量之间的相关性特征。 </p>
<p>然后，我们利用广义Fourier积分定理，它可以自动捕捉这些相关性，并为非参数回归问题推导出广义Fourier积分估计量。 使用这种新的密度估计器，我们提出了 FourierFormer，这是一种新型的Transformer，可以捕获自注意力中$q$和$k$的相关性。 总之，我们的贡献是三方面的：</p>
<ul>
<li>1, 我们从解决非参数核回归问题中推导出了自注意力的公式，从而为研究和进一步发展自注意力提供了非参数回归解释。</li>
<li>2, 我们为非参数回归问题开发了广义Fourier积分估计器，并为这些估计器提供了理论支持。</li>
<li>3, 我们提出了 FourierFormer，它的注意力使用广义Fourier积分估计器来更有效地捕获$q$和$k$的相关性。</li>
</ul>
<p>最后，我们凭经验证明，在包括 WikiText 语言建模和 ImageNet 图像分类在内的各种任务上，FourierFormer 比点积注意力的基线Transformer获得了明显更好的准确度。 我们还在实验中证明 FourierFormer 有助于减少注意力头之间的冗余。</p>
<p><strong>组织</strong><br>我们将本文的结构如下：在Section2中，我们提出了自我注意和非参数核回归之间的对应关系。 在 Section3中，我们讨论了广义Fourier积分估计器并定义了 FourierFormer。 我们在 Section4中验证并实证分析了 FourierFormer 的优势。 我们在 Section5中讨论相关工作。 论文以Section6的总结来结束。 附录中提供了技术证明和更多实验细节。</p>
<p><strong>记号</strong><br>对于任何 $\mathbf{N} \in \mathbb{N}$，我们表示 $[N] = \{1, 2, \ldots, N\}$。 对于任何 $D \geq 1$，我们用$\mathbb{L}_1(\mathbb{R}^D) $ 来表示 $ \mathbb{R}^D $ 上实值函数空间中的全体可积函数。 </p>
<p>对于任意两个序列 $\{a_N\}_{N \geq 1},\{ b_N \}_{N \geq 1} $， 我们用 $a_N = \mathcal{O}(b_N)$ 表示在所有 $N \geq 1$ 的情况下，都有 $a_{N} \leq C b_N$ ，其中 $C$ 是一些通用常数。</p>
<h2 id="2，自注意力的无参数回归解释"><a href="#2，自注意力的无参数回归解释" class="headerlink" title="2，自注意力的无参数回归解释"></a>2，自注意力的无参数回归解释</h2><p>在本节中，我们建立了自注意力和无参数核回归之间的联系。 特别是，我们将方程(2)中的自注意力推导为非参数核回归，其中向量 $k_j$ 和向量 $v_j$ 分别是训练输入和训练目标 , 而查询向量 $q_i$ 和输出向量 $h_i$ 形成了一组新的输入及其对应的待估计目标，且有 $i,j=1,\cdots,N$。 一般来说，我们将训练集 $\{ k_j, v_j\}$ for $j \in [N]$ 视为来自以下非参数回归模型：</p>
<script type="math/tex; mode=display">v_{j}=f(k_{j})+\varepsilon_{j} \tag{3}</script><p>其中 $\varepsilon_1,\cdots,\varepsilon_N$ 是独立的噪声，使得 $\mathbb{E}(\varepsilon_j) = 0$。 此外，我们考虑一个随机设计，其中$k$向量 $k_1,k_2,\cdots ,k_N$ 是来自 $p$ 为密度函数分布的采样。 通过混用符号，我们还将 $p$ 表示为联合密度，其中$k$和$v$向量 $(v_1, k_1), \cdots, (v_N, k_N)$ 也是来自 $p$ 为联合密度函数分布的采样。 在这里，$f$ 是一个真实但未知的函数，我们需要估计它。</p>
<p><strong>Nadaraya-Watson 验证</strong><br>我们估计函数 $f$ 的方法是基于 Nadaraya–Watson 的非参数核回归方法。 特别是，从非参数回归模型，对于所有 $ j \in [N]$，我们有 $\mathbb{E}[v_j|k_j] = f(k_j)$ 。 因此，在给定$k$向量的情况下，估计$v$向量的条件分布就足够了。 给定$k$向量的密度函数 $p$ 以及$k$和$v$向量的联合密度 $p$，对于任何从模型生成的向量对 $(v, k)$ 我们有：</p>
<script type="math/tex; mode=display">\mathbb{E}=\int_{\mathbb{R}^{D}}v\cdot p(v|k)dv=\int \frac{v\cdot p(v,k)}{p(k)}dv \tag{4}</script><p>条件期望的公式表明，只要我们能够估计联合密度函数$p(v, k)$和边际密度函数$p(v)$，我们就能够获得条件期望的估计，从而获得函数$f$。 这种方法被广泛称为 Nadaraya-Watson 的非参数核回归方法。</p>
<p><strong>核密度估计</strong><br>为了估计 $p(v, k)$ 和 $p(k)$，我们采用核密度估计方法。 特别是，通过使用带宽为 $\sigma$ 的各向同性Gaussian核，我们有以下 $p(v, k)$ 和 $p(k)$ 的估计量：</p>
<script type="math/tex; mode=display">\hat{p}_{\sigma}(v,k)=\frac{1}{N}\sum\limits_{j=1}^{N}\varphi_{\sigma}(v-v_{j})\varphi_{\sigma}(k-k_{j});\\ \hat{p}_{\sigma}(k)=\frac{1}{N}\sum\limits_{j=1}^{N}\varphi_{\sigma}(k-k_{j}) \tag{5}</script><p>其中 $\varphi_\sigma(.)$ 是对角协方差矩阵 $\sigma^2 I_D$ 的各向同性多元Gaussian密度函数。 给定核密度估计器，我们得到函数 $f$ 的以下估计：</p>
<script type="math/tex; mode=display">\hat{f}_{\sigma}(k)=\int_{\mathbb{R}^{D}}\frac{v\cdot \hat{p}_{\sigma}(v,k)}{\hat{p}_{\sigma}(k)}dv=\int_{\mathbb{R}^{D}}\frac{v\cdot \sum\limits_{j=1}^{N}\varphi_{\sigma}(v-v_{j})\varphi_{\sigma}(k-k_{j})}{\sum\limits_{j=1}^{N}\varphi_{\sigma}(k-k_{j})}dv \\ =\frac{\sum\limits_{j=1}^{N}\phi_{\sigma}(k-k_{j})\int v\cdot \varphi_{\sigma}(v-v_{j})dv}{\sum\limits_{j=1}^{N}\varphi_{\sigma}(k-k_{j})}=\frac{\sum\limits_{j=1}^{N}v_{j}\varphi_{\sigma}(k-k_{j})}{\sum\limits_{j=1}^{N}\varphi_{\sigma}(k-k_{j})} \tag{6}</script><p><strong>自注意力和无参数回归的关系</strong><br>通过将查询向量 $ q_i $ 代入方程(6) 中的函数  $\widehat{f}_\sigma $，我们得到</p>
<script type="math/tex; mode=display">\widehat{f}_{\sigma}(q_{i})=\frac{\sum\limits_{j}^{N}v_{j}exp(-||q_{i}-k_{j}||^{2}/2\sigma^{2})}{\sum\limits_{j}^{N}exp(-||q_{i}-k_{j}||^{2}/2\sigma^{2})} \\ =\frac{\sum\limits_{j}^{N}v_{j}exp[-(||q_{i}||^{2}+||k_{j}||^{2})/2\sigma^{2}]exp(q_{i}k_{j}^{T}/\sigma^{2})}{\sum\limits_{j}^{N}exp[-(||q_{i}||^{2}+||k_{j^{'}}||^{2})/2\sigma^{2}]exp(q_{i}k_{j}^{T}/\sigma^{2})}\tag{7}</script><p>如果我们进一步假设 $k_j$ 向量被归一化，通常在实践中这样做是为了稳定transformers的训练，方程(7)中$\hat{f}_\sigma (q_i)$的值为：</p>
<script type="math/tex; mode=display">\hat{f}_{\sigma}(q_{i})=\frac{\sum\limits_{j}^{N}v_{j}exp(q_{i}k_{j}^{T}/\sigma^{2})}{\sum\limits_{j}^{N}exp(q_{i}k_{j^{'}}^{T}/\sigma^{2})}=\sum\limits_{j}^{N}softmax(q_{i}^{T}k_{j}/\sigma^{2})v_{j} \tag{8}</script><p>当我们选择 $\sigma^2 = \sqrt{D}$，其中 $D$ 是 $q_i$ 和 $k_j$ 的维数，方程(8)和方程(2)中的自注意力等价，即$\widehat{f}_\sigma (q_i) = h_i$。 因此，我们已经证明自注意力使用各向同性Gaussian核执行非参数回归。</p>
<p><strong>注意1</strong><br>$k_j$ 被归一化的假设是为了恢复Transformer中成对点积的注意力。 一般来说，这个假设是不必要的。 事实上，方程(7)中的各向同性Gaussian核比成对点积注意力的方程(8)中的点积核更可取，因为前者是 Lipschitz 而后者不是。 Lipschitz约束有助于提高模型的鲁棒性并稳定模型训练。</p>
<p><strong>自注意力局限</strong><br>根据我们非参数回归的解释，自注意力源自使用各向同性Gaussian核进行核密度估计和非参数回归估计，这可能无法捕捉 $q_i$ 和 $k_j$中$D$维特征的复杂关联。 使用具有密集协方差矩阵的多元Gaussian核有助于捕捉这种相关性，然而，选择好的协方差矩阵具有挑战性且效率低下。 在下一节中，我们将讨论Fourier积分估计器及其作为计算自注意力的内核以克服这些限制。</p>
<h2 id="3，FourierForm：广义Fourier积分原理的Transformer"><a href="#3，FourierForm：广义Fourier积分原理的Transformer" class="headerlink" title="3，FourierForm：广义Fourier积分原理的Transformer"></a>3，FourierForm：广义Fourier积分原理的Transformer</h2><p>在下文中，我们介绍了能够捕捉$q$和$k$之间复杂特征关系的广义积分定理。 然后，我们将这些定理应用于密度估计和非参数回归问题，我们还建立了这些估计器的收敛速度。 鉴于这些密度估计器，我们引入了一个名为 FourierFormer 的新型Transformer家族，它将广义Fourier积分定理集成到标准Transformer的点积注意力步骤中。</p>
<h3 id="3-1-广义Fourier积分方法和应用"><a href="#3-1-广义Fourier积分方法和应用" class="headerlink" title="3.1 广义Fourier积分方法和应用"></a>3.1 广义Fourier积分方法和应用</h3><p>Fourier积分定理是数学中一个漂亮的结果，最近被用于非参数模式聚类、反卷积问题和生成建模，它是Fourier变换和Fourier逆变换的组合。 特别是，对于任何函数 $p \in \mathbb{L}_{1}(\mathbb{R}^{D})$，Fourier积分定理由下式给出：</p>
<script type="math/tex; mode=display">p(k)=\frac{1}{(2\pi)^{D}}\int_{\mathbb{R}^{D}}\int_{\mathbb{R}^{D}}cos(s^{T}(k-y))p(y)dyds \\ =\frac{1}{\pi^{D}}\lim_{\mathrm{R}\rightarrow\infty}\int_{\mathbb{R}^{D}}\prod_{j=1}^{D}\frac{sin(R(k_{j}-y_j))}{k_{j}-y_{j}}p(y)dy \tag{9}</script><p>其中，$k=(k_1,\cdots k_D), y=(y_1,\cdots y_D) $，上式表明：</p>
<script type="math/tex; mode=display">p_{\mathrm{R}}(k):=\frac{1}{\pi^{D}}\int_{\mathbb{R}^{D}}\prod_{j=1}^{D}\frac{sin(R(y_j-k_{j}))}{y_{j}-k_{j}}p(y)dy</script><p>$p_{\mathrm{R}}(k)$可用于估计函数$p$。</p>
<p><strong>用Gaussian核做Fourier积分的优点</strong><br>估计器 $p_R$ 有两个重要的好处：（i）即使 $p$ 是非常复杂和高维的函数，它也可以自动保留 $p$ 内的相关结构。与基于多元Gaussian核构建的标准核估计器形成鲜明对比，我们需要在多元Gaussian核中选择好的协方差矩阵来保证这样的估计器工作良好。我们注意到，由于标准的 softmax Transformer 是基于多元Gaussian核构建的，因此在点积Transformer中选择好的协方差矩阵的问题是不可避免的； (ii) 当 $R \to \infty$ 时，估计器 $p_{R}$ 中 sinc 核的乘积不会衰减到点质量，它与多元Gaussian核估计量截然不同，后者在协方差矩阵变为 0 时收敛到一个点质量，这表明 $p_R$ 是函数 $p$ 的非平凡估计量。最后，在密度估计和非参数回归问题中Fourier积分优于Gaussian核的这些好处的详细说明，也是我们刚刚证明这些好处与Transformer中的自注意力有关，可以在第 8 节中找到。</p>
<p><strong>广义Fourier积分估计</strong><br>借用Fourier积分估计器 $p_R$ 的上述优点，在本文中，我们想考虑该估计器的泛化，命名为广义Fourier积分估计器，由下式给出：</p>
<script type="math/tex; mode=display">p_{\mathrm{R}}^{\phi}(k)=\frac{\mathrm{R}^{D}}{A^{D}}\int_\mathrm{R^D}\prod^D_{j=1}\phi(\frac{sin(R(y_j-k_j))}{R(y_j-k_j)})p(y)dy \tag{10}</script><p>其中 $A : = \int_{\mathbb{R}} \phi \left(\frac{\sin(z)}{z}\right) dz$ 和 $\phi: \mathbb{R} \to \mathbb {R}$ 是给定的函数。 当 $\phi(k) = k$ 对所有 $k \in \mathbb{R}^D$ 时，广义Fourier积分估计量 $p_{R}^{\phi}$ 变为Fourier积分估计器 $p_R$。 在函数 $\phi$ 的适当条件下，估计器$p_R^{\phi}$收敛到真函数$p$，即，</p>
<script type="math/tex; mode=display">p(k)=\lim_{R\rightarrow \infty}p^{\phi}_R (k)=\lim_{R\rightarrow\infty}\frac{\mathrm{R}^{D}}{A^{D}}\int_\mathrm{R^D}\prod^D_{j=1}\phi(\frac{sin(R(y_j-k_j))}{R(y_j-k_j)})p(y)dy \tag{11}</script><p>我们将上述极限命名为广义Fourier积分定理。 此外，估计器 $p_R^{\phi}$ 也继承了Fourier积分估计器 $p_R$ 的类似优点。 因此，我们将使用广义Fourier积分定理作为构建密度估计器和非参数回归估计器的构建块，这对于开发 Section3.2 中的 FourierFormer 至关重要。</p>
<h4 id="3-1-1-用广义Fourier积分原理估计密度"><a href="#3-1-1-用广义Fourier积分原理估计密度" class="headerlink" title="3.1.1 用广义Fourier积分原理估计密度"></a>3.1.1 用广义Fourier积分原理估计密度</h4><p>我们首先将广义Fourier积分定理应用于密度估计问题。 为了简化表示，我们假设 $k_1, k_2, \ldots, k_N \in \mathbb{R}^D$ 是来自一个分布的样本，其密度函数为 $p$，其中 $D \geq 1$ 是维度。 受广义Fourier积分定理的启发，我们得到以下$p$的广义Fourier密度估计$p_{N,R}^{\phi}$如下：</p>
<script type="math/tex; mode=display">p^\phi_{N,R}(k):= \frac{R^D}{NA^D}\sum\limits^N_{j=1}\prod^{D}_{j=1}\phi\frac{sin(R(k_j-k_{ij}))}{R(k_j-j_{ij})} \tag{12}</script><p>其中 $A = \int_{\mathbb{R}} \phi \left(\frac{\sin(z)}{z}\right) dz$ 和 $k_i = (k_{i1}, \ ldots, k_{iD})$ 对于所有 $i \in [N]$。 为了量化广义Fourier密度估计 $p_{n,R}^{\phi}$ 和真实密度 $p$ 之间的误差，我们利用均方积分平方误差 MISE，给出如下表达式：</p>
<script type="math/tex; mode=display">MISE(p^\phi_{N,R},p):=\int_\mathrm{R^D}(p^\phi_{N,R}(k)-p(k))^2dk \tag{13}</script><p>我们从 $p_{n,R}^{\phi}$ 和 $p$ 之间的 MISE 的以下界限开始。</p>
<p><strong>定理1</strong><br>假设 $\int_{\mathbb{R}} \phi(\sin(z)/z)z^j dz = 0$ 对于所有 $j \in [m]$ 以及 $\int_{\mathbb{ R}} |\phi(\sin(z)/z)| |z|^{m + 1} dz &lt; \infty$ 对于一些 $m \in \mathbb{N}$。 然后，存在取决于 $d$ 和 $A$ 的通用常数 $C$ 和 $C^{‘}$，使得：</p>
<script type="math/tex; mode=display">MISE(p^\phi_{N,R},p)\leq\frac{C}{R^{m+1}}+\frac{C^{'}R^D}{N}</script><p>定理1证明见附录B.1，且以下论述是有序的。 首先，通过选择 $R$ 来平衡定理1中 MISE 边界的偏差和方差，我们得到最优 $R$ 为 $R = \mathcal{O}(N^{1 /(D + m + 1)})$，选择 $R$，$p_{N, R}^{\phi}$ 的 MISE 率为 $\mathcal{O}(N^{-(m+1)/(D + m + 1) })$。 其次，当 $l \geq 4$ 和 $z \in \mathbb{R}$ 的 $\phi(z) = z^{l}$ 时，定理1中的假设满足 $m = 1$。 在这种情况下，$p_{N,R}^{\phi}$ 的 MISE 率为 $\mathcal{O}(N^{-2/(D+2)})$。 但是，当 $\phi(z) = z^{l}$ 和 $l \in \{1,2,3\}$ 时，这些假设不满足，这是由于当前定理1证明技术的限制，是基于估计量 $p_{n,R}^{\phi}$ 的Taylor展开。</p>
<p>为了解决Taylor展开技术的局限性，我们利用Fourier分析中的 Plancherel 定理来建立当 $\phi(z) = z^{l}$ 和 $l \in \{1,2,3\}$时 $p_{N,R}^{\phi}$ 的 MISE 率。 这种设置的理论分析细节在附录A中。</p>
<h3 id="3-2-FourierFormer：Transformer带Fourier注意力"><a href="#3-2-FourierFormer：Transformer带Fourier注意力" class="headerlink" title="3.2 FourierFormer：Transformer带Fourier注意力"></a>3.2 FourierFormer：Transformer带Fourier注意力</h3><p>受广义Fourier积分定理中函数相关结构的保留以及密度估计器的理论保证的启发，在本节中，我们采用了Section2中自注意力的非参数回归解释并在 Section3.2.1 中提出广义Fourier非参数回归估计器，我们还建立了该估计器的收敛特性。 然后，基于广义Fourier非参数回归估计器，我们在 Section3.2.2 中开发了 Fourier 注意力及其对应的 FourierFormer。</p>
<h4 id="3-2-1-广义Fourier积分原理的非参数化回归"><a href="#3-2-1-广义Fourier积分原理的非参数化回归" class="headerlink" title="3.2.1 广义Fourier积分原理的非参数化回归"></a>3.2.1 广义Fourier积分原理的非参数化回归</h4><p>我们现在讨论广义Fourier积分定理在非参数回归设置中的应用，即我们假设 $(v_1, k_1), \ldots, (v_N, k_N)$ 是来自以下非参数回归模型的样本：</p>
<script type="math/tex; mode=display">v_j:=f(k_j)+\varepsilon_j</script><p>其中 $\varepsilon_1, \ldots, \varepsilon_N$ 是独立的噪声，使得 $\mathbb{E}(\varepsilon_j) = 0$ 和$k$向量 $k_1, k_2、\ldots、k_N$ 来自密度函数为 $p$ 的样本。 给定广义Fourier密度估计器，根据第2节中的参数，基于广义Fourier密度估计器的函数 $f$ 的 Nadaraya-Watson 估计器为：</p>
<script type="math/tex; mode=display">f_{N,R}(k):=\frac{\sum\limits^N_{i=1}v_i\prod^D_{j=1}\phi(\frac{sin(R(k_i-k_{ij}))}{R(k_i-k_{ij})})}{\sum\limits^N_{i=1}\prod^D_{j=1}\phi(\frac{sin(R(k_i-k_{ij}))}{R(k_i-k_{ij})})} \tag{14}</script><p>方程(14)中的广义Fourier非参数回归估计器$f_{N,R}$与方程(6)中估计器$\widehat{f}_{\sigma}$ 的主要区别是估计器 $f_{N,R}$ 利用广义Fourier密度估计器来估计给定$k$和$v$向量的条件分布，而不是如 $\widehat{f}_{\sigma}$ 中的各向同性Gaussian核密度估计器 。 </p>
<p>正如我们在 Section3 中强调的那样，广义Fourier密度估计器的一个重要好处是它可以捕获$k$和$v$向量特征的复杂依赖关系，而Gaussian核需要具有良好的协方差矩阵来做到这一点，这在实践中计算成本很高。</p>
<p>我们现在有以下结果，建立了 $f_{N,R}$ 的均方误差 (MSE)。</p>
<p><strong>定理2</strong><br>假设 $\int_{\mathbb{R}} \phi \left(\frac{\sin(z)}{z}\right) z^j dz = 0$ 对于所有 $1 \leq j \leq m $，且 $\int_{\mathbb{R}} \left|\phi \left(\frac{\sin(z)}{z}\right)\right| |z|^j dz &lt; \infty$ 对于任何 $m + 1 \leq j \leq 2m + 2$ 对于一些 $m \in \mathbb{N}$ 成立。 那么，对于任何 $k \in \mathbb{R}^D$，存在一般常数 $C_1, C_2, C_3, C_4$ 使得以下成立 ：</p>
<script type="math/tex; mode=display">\mathbb{E}[(f_{N,R}(k)-f(k))^2]\leq(\frac{C_1}{R^{2(m+1)}}+\frac{(f(k)+C_2)R^D}{N})/(p^2(kJ(R)))</script><p>其中 $J(R) = 1 - \frac{1}{p^2(k)} ( \frac{C_3}{R^{2(m+1)}} + \frac{ C_4 R^d \log (N R)}{N}) $。 这里，外部期望是关于$k$向量 $k_1,\ldots,k_N$ 和噪声 $\varepsilon_1,\ldots,\varepsilon_N$ 的。</p>
<p>定理2证明见附录B.3，且关于定理2的一些评论是有序的。 首先，通过选择 $R$ 来平衡非参数广义Fourier估计器 $f_{N,R}$ 的 MSE 范围内的偏差和方差，我们得到最优半径 $R$ 为 $R = \mathcal{O}(N^{\frac{1}{2(m + 1) + D}})$。 选择最优半径 $R$，$f_{N,R}$ 的比率为 $\mathcal{O}(N^{-\frac{2(m+1)}{D + 2(m +1)}})$。 其次，当 $\phi(z) = z^{l}$ 对 $l \geq 6$ 时，定理2的函数 $\phi$ 的假设满足 $m = 1$。 在这种情况下，$f_{N,R}$ 的比率变为 $\mathcal{O}(N^{-\frac{4}{D + 4}})$。 在附录A中，我们还提供了当 $\phi(z) = z^l$ 对于某些 $l \leq 5$ 时的 $f_{N,R}$ 的比率，其中包括原始Fourier积分定理。</p>
<h4 id="3-2-2-FourierFormer"><a href="#3-2-2-FourierFormer" class="headerlink" title="3.2.2 FourierFormer"></a>3.2.2 FourierFormer</h4><p>给定方程(14)中的广义Fourier非参数回归估计器 $f_{N,R}$，通过将$q$值 $q_1,\ldots,q_N$ 插入该函数 ，我们得到Fourier注意力的以下定义：</p>
<p><strong>定义1(Fourier注意力)</strong><br>Fourier注意力是一种多头注意力，它使用广义Fourier非参数回归估计器 $f_{N,R}$ 进行非参数回归。 Fourier注意力的输出 $\hat{h}_i$计算公式为</p>
<script type="math/tex; mode=display">\hat{h}_i:=f_{N,R}(q_i)=\frac{\sum\limits^N_{i=1}v_i\prod^D_{j=1}\phi(\frac{sin(R(q_{ij}-k_{ij}))}{R(q_{ij}-k_{ij})})}{\sum\limits^N_{i=1}\prod^D_{j=1}\phi(\frac{sin(R(q_{ij}-k_{ij}))}{R(q_{ij}-k_{ij})})} \quad \forall i\in [N]\tag{15}</script><p>给定定义1中的 Fourier注意力，我们接着给出 FourierFormer 的定义如下。</p>
<p><strong>定义2(FourierFormer)</strong><br>FourierFormer 是一种Transformer，它使用Fourier注意力来捕获输入序列中，标记之间的依赖关系，以及每个标记中特征之间的相关性。</p>
<p><strong>注意2 Fourier核的非负性</strong><br>通过第3.1.1节中的广义Fourier积分定理进行的密度估计不需要广义Fourier密度估计器是非负的。 然而，根据经验，我们观察到负密度估计器会导致训练FourierFormer的不稳定。 因此，在 FourierFormer 中，我们选择函数 $\phi$ 作为非负函数，以强制密度估计为非负。 特别是，我们选择 $\phi$ 为 $\phi(x) = x^{2m}$ 形式的幂函数，其中 $m$ 是一个正整数。 请注意，当 $m=2$ 和 $m=4$ 时，我们的广义Fourier积分估计器中的核是著名的 Fejer-de la Vallee Poussin 和 Jackson-de la Vallee Poussin 核。</p>
<h3 id="3-3-Fourier注意力的高效实现"><a href="#3-3-Fourier注意力的高效实现" class="headerlink" title="3.3 Fourier注意力的高效实现"></a>3.3 Fourier注意力的高效实现</h3><p>Fourier内核在 Pytorch 开发的 C++/CUDA 扩展中高效实现。 这个想法类似于函数 cdist ，它计算两个行向量集合中每对之间的 p 范数距离。 在我们的例子中，我们的目标是计算在定义1中代表Fourier注意力的核函数。 这个实现的核心是下面的Fourier度量函数$d_f$：</p>
<script type="math/tex; mode=display">d_f(q_i,k_j)=\prod^D_{d=1}\phi(\frac{sin(R(q_{id}-k_{jd}))}{(q_{id}-k_{jd})})</script><p>我们直接将 $d_f$ 实现为 torch.autograd.Function，其中我们提供了一种计算前向和后向函数的有效方法（$d_f$ 和 $d_f$ 的梯度）。 虽然前向函数的实现是直截了当的，但后向函数更加棘手，因为我们需要优化代码来一次计算与变量 $q$、$k$ 和 $R$ 的所有梯度有关的 $d_f$。 我们可以通过利用 GPU 架构和利用缩减技术来开发具有高度并行计算的后向函数。 计算时间与函数 cdist 相当； 因此，我们的 FourierFormer 实现在计算上是高效的。</p>
<h2 id="4，实验结果"><a href="#4，实验结果" class="headerlink" title="4，实验结果"></a>4，实验结果</h2><p>在本节中，我们在数值上证明了 FourierFormer 在两个大规模任务上优于基线点积Transformer的优势：WikiText-103 上的语言建模和 ImageNet 上的图像分类。 我们的目标是证明：(i) FourierFormer 在具有不同数据模式的各种实际任务中实现比基线 Transformer 更好的精度，并且 (ii) FourierFormer 与基线 Transformer 相比有助于减少头部冗余。</p>
<p>在本章节，我们将 FourierFormers 与相同配置的基线点积Transformer进行比较。 在所有实验中，我们将Fourier注意力中的常数 $R$ 设为可学习的标量，并设置选择函数 $\phi(x) = x^4 $。 我们所有的结果都是使用不同种子进行 5 次运行的平均值。 附录C中提供了有关模型和训练的更多详细信息。 我们还在附录D中提供了额外的实验结果。</p>
<h3 id="4-1-WikiText-103的语言建模"><a href="#4-1-WikiText-103的语言建模" class="headerlink" title="4.1 WikiText-103的语言建模"></a>4.1 WikiText-103的语言建模</h3><p>WikiText-103 是来自 Wikipedia 的文章集合，它们具有长期的上下文依赖关系。 训练集由大约 28K 数量的文章组成，其中包含 1.03M 的运行词；这对应于大约 3600 个单词的文本块。 验证集和测试集分别有 $218K$ 和 $246K$ 的运行词。 每篇文章都包含 60 篇文章和大约 268K 数量的单词。 我们的实验遵循标准设置并将训练数据拆分为$L$-word 独立的长片段。 为了评估，我们使用批量大小为 1，并使用大小为 $L$ 的滑动窗口处理文本序列。 最后一个位置用于计算除了第一段的困惑度（PPL），所有位置都参与评估。</p>
<p><strong>模型和基线</strong><br>我们的实现基于 schlag2021linear 的公共代码，我们在实验中使用他们的中小模型。 特别是对于小模型，key、value和query维度设置为128，训练和评估上下文长度设置为256。对于中模型，key、value和query维度设置为256，并且训练和评估上下文长度设置为 384。在两种配置中，head 的数量都是 8，前馈层维度是 2048，层数是 16。</p>
<p><strong>结果</strong><br>我们在 Table1 中报告了 FourierFormer 与带点积注意力的基线Transformer的验证和测试困惑度 (PPL)。 FourierFormers 在中小模型配置中都比基线获得了更好的 PPL。对于小模型配置，FourierFormer 相对于基线的改进是验证中的 1.29 PPL 和测试中的 1.44 PPL。对于中模型配置，这些改进是验证中的 1.39 PPL 和测试中的 1.59 PPL。这些结果表明，FourierFormer 相对于基线点积Transformer的优势随着模型的大小而增长。这符合我们的预期，因为更大的模型具有更大的$q$和$k$维度，例如本实验中配置中等的语言模型的$q$和$k$的维度为 256，而配置较小的语言模型为 128。由于 FourierFormer 的优势来自于 FourierFormer 可以捕获$q$和$k$之间的特征相关性，因此$q$和$k$维度越大，FourierFormer 的优势就越大。</p>
<h3 id="4-2-ImageNet上的图像分类任务"><a href="#4-2-ImageNet上的图像分类任务" class="headerlink" title="4.2 ImageNet上的图像分类任务"></a>4.2 ImageNet上的图像分类任务</h3><p><strong>数据集和指标</strong><br>ImageNet 数据集包含 1.28M 的训练图像和 50K 的验证图像。 对于这个基准，模型学习在 1000 个类别中预测输入图像的类别。 报告了前 1 和前 5 的分类精度。</p>
<p><strong>模型和基线</strong><br>我们使用 DeiT-tiny 模型，有 12 个transformer 层，每层 4 个注意力头，模型维度为 192。为了训练模型，我们遵循与基线相同的设置和配置。</p>
<p><strong>结果</strong><br>我们在 Table2 中总结了我们的结果。 与语言建模实验相同，对于这个图像分类任务，配备 FourierFormer 的 Deit 模型在 top-1 和 top-5 精度上都显着优于基线 Deit 点积Transformer。 该结果表明 FourierFormer 优于基线点积Transformer的优势适用于不同的数据模式。</p>
<h3 id="4-3-FourierFormer帮助减少多头冗余"><a href="#4-3-FourierFormer帮助减少多头冗余" class="headerlink" title="4.3 FourierFormer帮助减少多头冗余"></a>4.3 FourierFormer帮助减少多头冗余</h3><p>为了研究注意力头之间的多样性，给定 WikiText-103 语言建模任务训练的模型，我们计算每层头之间的平均 $\mathcal{L}_2$ 距离。 我们在 Table3 中显示了头部之间距离的层平均均值和方差。 Table3 中的结果表明，FourierFormer 在注意力头之间获得了比具有点积注意力的基线Transformer更大的 $\mathcal{L}_2$ 距离，因此有助于减少头冗余。<br>请注意，我们对两个模型都使用了 Section4.1 中指定的小模型配置。</p>
<h2 id="5，相关工作"><a href="#5，相关工作" class="headerlink" title="5，相关工作"></a>5，相关工作</h2><p><strong>Transformer中的注意力机制理解</strong><br>最近的工作试图从不同的角度了解Transformer的注意力。tsai2019transformer 认为注意力是在输入上用内核平滑，扩展这种内核方法，katharopoulos2020transformers,choromanski2021rethinking, wang2020linformer 线性化了点积注意力中的 softmax 核，并提出了一系列具有线性计算和内存复杂度的高效Transformer。cao2021choose 然后表明这些线性Transformer与 Petrov-Galerkin投影相当，表明点积注意力中的 softmax 归一化是足够的，但不是必需的。</p>
<p>其他作品通过常/偏微分方程提供了对Transformer注意力的理解，包括 lu2019understanding, sander2022sinkformers。此外，tang2021probabilistic等将Transformer中的注意力与Gaussian混合模型联系起来。一些工作还将注意力机制与图形模型中的图结构学习和消息传递联系起来，如wang2018non。我们的工作重点是推导自注意力和非参数核回归之间的联系，并探索更好的回归估计器，例如广义Fourier非参数回归估计器，以提高Transformer的性能。</p>
<p><strong>Transformer中的冗余</strong><br>dalvi2020analyzing等表明预训练Transformer中的神经元和注意力头是冗余的，并且可以在应用于下游任务时移除。 通过研究预训练网络中的上下文嵌入，已经证明从这些冗余模型中学习到的表示是高度各向异性的。 此外，sanh2019distilbert 等采用知识蒸馏和稀疏近似来提高Transformer的效率。 我们的 FourierFormer 是对这些方法的补充，可以与它们结合使用。</p>
<h2 id="6，结论"><a href="#6，结论" class="headerlink" title="6，结论"></a>6，结论</h2><p>在本文中，我们建立了非参数核回归与 Transformer 中的自注意力之间的对应关系。然后，我们开发了广义Fourier积分估计器并提出了 FourierFormer，这是一类新的Transformer，它使用广义Fourier积分估计器来构建它们的注意力，以有效地捕获$q$和$k$向量中的特征之间的相关性。我们从理论上证明了广义Fourier积分估计器的近似保证，并在精度和头部冗余减少方面通过点积注意验证了 FourierFormer 相对于基线Transformer的优势。将鲁棒内核合并到 FourierFormer 的非参数回归框架中以增强模型在数据扰动和对抗性攻击下的鲁棒性是很有趣的。 </p>
<p>FourierFormer 的一个限制是它仍然具有与具有点积注意力的基线Transformer相同的二次计算和内存复杂性。我们将实现线性计算和内存复杂性的 FourierFormer 线性版本的开发留作未来的工作。值得注意的是，FourierFormer 没有潜在的负面社会影响。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>Fourier</tag>
      </tags>
  </entry>
  <entry>
    <title>数学专业课学习资源</title>
    <url>/2022/11/15/math/learning_video_source/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>数学专业课的学习资源，大多是视频，后续会不断增加其他形式~</p>
<p><strong>多年前翘的课，早晚要还回去~</strong><br><span id="more"></span></p>
<h2 id="高等数学"><a href="#高等数学" class="headerlink" title="高等数学"></a>高等数学</h2><ul>
<li><a href="https://www.bilibili.com/video/av84881516/?vd_source=587c86d59d7db86347d0f0b0deaa5031">朱建民，国防科大</a></li>
<li><a href="https://www.bilibili.com/video/BV1FU4y1p7f3/?vd_source=587c86d59d7db86347d0f0b0deaa5031">樊顺厚，天津工业大学</a></li>
<li><a href="https://www.bilibili.com/video/BV1Eb411u7Fw/?spm_id_from=333.999.0.0&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">宋浩</a></li>
</ul>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1aW411Q7x1/?spm_id_from=333.999.0.0&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">宋浩</a></li>
<li><a href="https://www.bilibili.com/video/av58706273/?vd_source=587c86d59d7db86347d0f0b0deaa5031">麻省理工</a></li>
<li><a href="https://www.bilibili.com/video/BV1ys411472E/?spm_id_from=333.999.0.0">3Blue1Brown，线性代数的本质</a></li>
</ul>
<h2 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1UW411k7Jv/?spm_id_from=333.999.0.0">宋浩</a></li>
<li><a href="https://www.bilibili.com/video/BV1qW411N7FU/?spm_id_from=333.999.0.0">3Blue1Brown，微积分的本质</a></li>
</ul>
<h2 id="概率论与梳理统计"><a href="#概率论与梳理统计" class="headerlink" title="概率论与梳理统计"></a>概率论与梳理统计</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1ot411y7mU/?spm_id_from=333.999.0.0">宋浩</a></li>
</ul>
<h2 id="高等代数"><a href="#高等代数" class="headerlink" title="高等代数"></a>高等代数</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1cy4y1V79E/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">宋浩</a></li>
<li><a href="https://www.bilibili.com/video/BV1jR4y1M78W/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">北大丘维声</a></li>
<li><a href="https://www.bilibili.com/video/BV1mJ411r7ZB/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">复旦谢启鸿</a></li>
<li><a href="https://www.bilibili.com/video/BV16Z4y1U7oU/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">麻省理工MIT</a></li>
</ul>
<h2 id="数学分析"><a href="#数学分析" class="headerlink" title="数学分析"></a>数学分析</h2><ul>
<li><a href="https://www.bilibili.com/video/BV15v411g7VP/?vd_source=587c86d59d7db86347d0f0b0deaa5031">陈纪修</a></li>
</ul>
<h2 id="实变函数"><a href="#实变函数" class="headerlink" title="实变函数"></a>实变函数</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1o7411N7qx/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">陈闯老师，程其襄，仅实变部分</a></li>
</ul>
<h2 id="泛函分析"><a href="#泛函分析" class="headerlink" title="泛函分析"></a>泛函分析</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1zW411s7o8/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">孙炯老师</a></li>
</ul>
<h2 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1g7411b7r2/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">华中科技大学</a></li>
</ul>
<h2 id="数学物理方程"><a href="#数学物理方程" class="headerlink" title="数学物理方程"></a>数学物理方程</h2><p><a href="https://www.bilibili.com/video/BV117411m78E/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">马逍也</a></p>
<h2 id="抽象代数，微分几何，拓扑"><a href="#抽象代数，微分几何，拓扑" class="headerlink" title="抽象代数，微分几何，拓扑"></a>抽象代数，微分几何，拓扑</h2><ul>
<li><a href="https://space.bilibili.com/410564174?spm_id_from=333.337.0.0">无尽沙砾</a></li>
</ul>
<h2 id="常微分方程"><a href="#常微分方程" class="headerlink" title="常微分方程"></a>常微分方程</h2><ul>
<li><a href="https://www.bilibili.com/video/BV1Tr4y1w7Ef/?spm_id_from=333.337.search-card.all.click">宋浩</a></li>
<li><a href="https://www.bilibili.com/video/BV1bx411s7pb/?spm_id_from=333.337.search-card.all.click&amp;vd_source=587c86d59d7db86347d0f0b0deaa5031">川大数院</a></li>
<li><a href="https://www.bilibili.com/video/BV1qk4y1R7W8/?spm_id_from=333.337.search-card.all.click">童雯雯</a></li>
</ul>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>source</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读清单来了，下半部分</title>
    <url>/2022/11/15/other/readinglist_2023_half2/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>想给自己立flag了，能戒掉游戏不？能多读两本书吗？</p>
<p><strong>从2023就开始吧~</strong><br><span id="more"></span></p>
<h2 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h2><ul>
<li>叶圣陶《文心》</li>
<li>韦海生《教自己写作》</li>
<li>毕飞宇《小说课》</li>
<li>刘军强《写作是门手艺》</li>
<li>商务印书馆《新华字典》</li>
</ul>
<h2 id="散文诗歌类"><a href="#散文诗歌类" class="headerlink" title="散文诗歌类"></a>散文诗歌类</h2><ul>
<li>汪曾祺《人间草木》</li>
<li>史铁生《我与地坛》</li>
<li>余秋雨《文化苦旅》</li>
<li>余光中《听听那冷雨》</li>
<li>季羡林《无问东西》</li>
<li>林清玄《境明，千里皆明》</li>
<li>贾平凹《南北笔记》</li>
<li>梁实秋《梁实秋生活美学》</li>
<li>杨绛《将饮茶》</li>
<li>徐志摩《翡冷翠的一夜》</li>
<li>张枣《春秋来信》</li>
<li>林徽因《林徽因文集》</li>
<li>余秀华《月光落在左手上》</li>
</ul>
<h2 id="小说类"><a href="#小说类" class="headerlink" title="小说类"></a>小说类</h2><ul>
<li>钱钟书《围城》</li>
<li>余华《活着》</li>
<li>路遥《平凡的世界》</li>
<li>汪曾祺《受戒》</li>
<li>莫言《生死疲劳》</li>
<li>沈从文《边城》</li>
<li>萧红《呼兰河传》</li>
<li>宗璞《野葫芦引》</li>
<li>白先勇《孽子》</li>
<li>张爱玲《金锁记》</li>
<li>陈忠实《白鹿原》</li>
<li>老舍《我这一辈子》</li>
<li>巴金《寒夜》</li>
<li>刘震云《一句顶一万句》</li>
<li>陈春成《夜晚的潜水艇》</li>
</ul>
<h2 id="历史普本类"><a href="#历史普本类" class="headerlink" title="历史普本类"></a>历史普本类</h2><ul>
<li>傅乐成《中国通史》</li>
<li>马伯庸《长安的荔枝》</li>
<li>刘和平《大明王朝》</li>
<li>孙皓晖《大秦帝国》</li>
<li>黄仁宇《万历十五年》</li>
<li>唐浩明《曾国藩》</li>
<li>二月河《雍正皇帝》</li>
<li>顾城《南明史》</li>
</ul>
<h2 id="思想认知类"><a href="#思想认知类" class="headerlink" title="思想认知类"></a>思想认知类</h2><ul>
<li>王阳明《传习录》</li>
<li>鲁迅文集</li>
<li>王小波《沉默的大多数》</li>
<li>刘慈欣《三体》</li>
<li>韦启昌《人生的智慧》</li>
<li>蒋勋《孤独六讲》</li>
<li>梁文道《常识》</li>
<li>马克.奥勒留《沉思录》</li>
<li>瑞.达丽欧《原则》</li>
</ul>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>readinglist</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读清单来了，上半部分</title>
    <url>/2022/11/15/other/readinglist_2023_half1%20copy/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>想给自己立flag了，能戒掉游戏不？能多读两本书吗？</p>
<p><strong>从2023就开始吧~</strong><br><span id="more"></span></p>
<ul>
<li><p>《平凡的世界》 路遥 著<br>  高度浓缩了中国西北农村的历史变迁过程</p>
</li>
<li><p>《三体》 刘慈欣 著<br>  “中国科幻文学里程碑”式的作品</p>
</li>
<li><p>《万历十五年》 黄仁宇 著<br>  原来历史可以这样写，原来历史是如此的有趣、复杂、丰富</p>
</li>
<li><p>《如何阅读一本书》 [美]莫提默.J.艾德勒 查尔斯.范多伦 著<br>  一本指导人们如何阅读的名作</p>
</li>
<li><p>《围城》 钱钟书 著<br>  “围在城里的人想逃出来，城外的人想冲进去”</p>
</li>
<li><p>《红楼梦》 曹雪芹 著<br>  一部具有高度思想性和高度艺术性的伟大作品</p>
</li>
<li><p>《四世同堂》 老舍 著<br>  再现了那个时代普通人在国破家亡之际缓慢、痛苦而又艰难的觉醒历程</p>
</li>
<li><p>《从一到无穷大》 [美] 乔治.伽莫夫 著<br>  是当今世界最有影响的科普经典名著之一</p>
</li>
<li><p>《全球通史：从史前史到21世纪》 [美] 斯塔夫里阿诺斯 著<br>  从全球角度考察世界各地区人类文明的产生和发展</p>
</li>
<li><p>《资本论》 [德]马克思 著<br>  一部融哲学、政治经济学、科学社会主义为一体的马克思主义百科全书</p>
</li>
<li><p>《月亮和六便士》[英]毛姆 著<br>  “即使只靠一支画笔，沦陷于赤贫之中，我孤独而炽热的灵魂也无法和画画分开”</p>
</li>
<li><p>《学会提问》 [美]尼尔.布朗 斯图尔特.基利 著<br>  一本授人以渔的智慧之书，独立思考者的七点</p>
</li>
<li><p>《沈从文文集》沈从文 著<br>  他笔下的世界，给我们的心灵开辟了一方净土</p>
</li>
<li><p>《美的历程》 李泽厚 著<br>  中国美学的经典之作，如斯感性，如斯亲切</p>
</li>
<li><p>《我们仨》 杨绛 著<br>  注以简洁而沉重的语言，回忆一家三口那些快乐而艰难、爱与痛交织的日子。</p>
</li>
<li><p>《数学之美》 吴军 著<br>  把高深的数学原理讲得更加通俗易懂，让非专业读者也能领略数学的魅力。</p>
</li>
<li><p>《繁华》 金宇澄 著<br>  “建立了一座与南方有关与城市有关的人情世态的博物馆”</p>
</li>
<li><p>《一句顶一万句》 刘震云 著<br>  从小人物的命运变迁中去寻找破解孤独的钥匙，被誉为中国版《百年孤独》</p>
</li>
<li><p>《牵风记》 徐怀中 著<br>  将现实主义与浪漫主义相结合的方式描写战争，以特别的胆略探寻战火中的爱恋与人性</p>
</li>
<li><p>《主角》 陈彦 著<br>  一部动人心魄的命运之书，一个以中国古典审美方式讲述的“中国故事”</p>
</li>
<li><p>《爱的艺术》[美]艾里希.弗洛姆 著<br>  爱是一门艺术，想要掌握这门艺术的人，需要有这方面的知识并付出努力去学习</p>
</li>
<li><p>《应物兄》李洱 著<br>  借鉴经史子集的叙述方式，记叙了中国当代知识分子的言谈和举止</p>
</li>
<li><p>《乡土中国》费孝通 著<br>  学界共认的中国乡土社会传统文化和社会结构理论研究的代表作</p>
</li>
<li><p>《理想国》 [古希腊]柏拉图 著<br>  人类正义问题的开山之作，涉及柏拉图思想体系的各个方面</p>
</li>
<li><p>《战争与和平》[俄]列夫.托尔斯泰 著<br>  一部百科全书式的恢弘史诗，罗曼.罗兰称之为“伟大作家的伟大作品”</p>
</li>
<li><p>《解忧杂货店》 [日]东野圭吾 著<br>  不是推理小说，却更扣人心弦</p>
</li>
<li><p>《呼兰河传》 萧红 著<br>  一部充满童心、诗趣和灵感的“回忆式”长篇小说</p>
</li>
<li><p>《白鹿原》 陈忠实 著<br>  一部渭河平原的雄奇史诗，一幅中国农村的斑斓画卷</p>
</li>
<li><p>《长恨歌》 王安忆 著<br>  讲述一个女人四十年的情与爱，一座城市四十年的常与变</p>
</li>
<li><p>《霍乱时期的爱情》[哥伦比亚]加西亚.尔克斯 著<br>  讲述了一段跨越半个多世纪的爱情史诗，穷尽了所有爱情的可能性</p>
</li>
<li><p>《经济学原理》[美]曼昆 著<br>  一本享誉全球的经济学经典教科书。内容简明、语言生动、诙谐</p>
</li>
<li><p>《时间简史》[英]史蒂芬.霍金 著<br>  “我们生存在一个奇妙无比的宇宙中”</p>
</li>
<li><p>《百年孤独》[哥伦比亚]加西亚.马尔克 著<br>  魔幻现实主义文学的代表作，反映了拉丁美洲一个世纪以来风云变幻的历史</p>
</li>
<li><p>《人类简史：从动物到上帝》[以色列]尤瓦尔.赫拉利 著<br>  宏大的人类简史，也见微知著，让人类重新审视自己</p>
</li>
<li><p>《小王子》[法]圣一埃克苏佩里 著<br>  这本书是献给长成了大人的从前那个孩子</p>
</li>
<li><p>《思考，快与慢》[美]丹尼尔.卡尼曼 著<br>  指导我们如何作出更好的选择，以及如何应用不同技巧来避免那些常见的思维失误</p>
</li>
<li><p>《季羡林谈人生》 季羡林 著<br>  季羡林的思想像一本厚厚的百科全书，而他的品格却像一目见底的清水，大德大智隐于无形</p>
</li>
<li><p>《语言学的逻辑》 [美]塞缪尔.早川 艾伦.早川 著<br>  一次让人神往的语言学探秘之旅，一堂终身受用的语言学入门课</p>
</li>
<li><p>《西方美学史》 朱光潜 著<br>  西方美学三千年历史展现，奠定中国研究西方美学思想的基础</p>
</li>
<li><p>《心理学与生活》 [美]理查德.格里格 菲利普.津巴多 著<br>  “希望你能像一个心理学家那样去思考”</p>
</li>
</ul>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>readinglist</tag>
      </tags>
  </entry>
  <entry>
    <title>Several spaces in Real Variable Functions</title>
    <url>/2022/11/14/math/several%20spaces/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>实变函数中理不清的几个空间问题，不断更新中，一边学习，一边总结~</p>
<p><strong>实变函数与泛函分析中的几个空间，各种空间，还在学习中…</strong><br><span id="more"></span></p>
<h2 id="一，度量空间"><a href="#一，度量空间" class="headerlink" title="一，度量空间"></a>一，度量空间</h2><p>设 $\mathbf{X}$ 是集合，对 $\forall x,y\in{X}$ 都有实数 $d(x,y)$ 对应，而且满足：</p>
<ul>
<li>$d(x,y)\geqq 0,\quad d(x,y)=0 \; only \; if \; x=y$</li>
<li>$d(x,y)\leqq d(x,z)+d(y,z)\quad for \; all \; z$</li>
</ul>
<p>$d(x,y)$是距离，集合$\mathbf{X}$是<strong>度量空间</strong>，又叫<strong>距离空间</strong>。</p>
<h3 id="1，特例"><a href="#1，特例" class="headerlink" title="1，特例"></a>1，特例</h3><ul>
<li>Euclid空间<ul>
<li>实数集合 $\mathbf{R}^{n}$ 和 $ d(x,y)=(\sum\limits^n_{i=1}(\xi_i-\eta_i)^{2})^{\frac{1}{2}} $ 定义的距离下，$(\mathbf{R}^{n}, d(x,y)) $ 称为n维欧式空间，其中 $ d $ 称为欧几里得距离。</li>
<li>其他的距离定义，如1-范数，$\infty$-范数等。</li>
</ul>
</li>
</ul>
<h3 id="2，完备的度量空间"><a href="#2，完备的度量空间" class="headerlink" title="2，完备的度量空间"></a>2，完备的度量空间</h3><p>如果度量空间$(\mathbf{X},d)$中每个Cauchy点列收敛到该空间中的一点，则可称为完备的度量空间。</p>
<ul>
<li>$l^{\infty}$是完备度量空间</li>
</ul>
<h2 id="二，线性空间"><a href="#二，线性空间" class="headerlink" title="二，线性空间"></a>二，线性空间</h2><p>$\mathbf{X}$是非空集合，在其上定义元素的加法运算及其元素与实数（复数）的乘法运算，且满足一下两个条件：</p>
<ul>
<li>关于加法称为交换群，且满足性质：<ul>
<li>$x+y = y+x$</li>
<li>$(x+y)+z = x+(y+z)$</li>
<li>存在零元素且唯一，$\forall x\in\mathbf{X},\; x+0 = x$</li>
<li>存在负元素且唯一，$\forall x\in\mathbf{X},\; x+(-x)=0$</li>
</ul>
</li>
<li>定义$u=\alpha x$的数积运算，且满足性质:<ul>
<li>$1x=x$</li>
<li>$a(bx)=(ab)x\quad\forall a,b为实数或复数$</li>
<li>$(a+b)x=ax+bx,\quad a(x+y)=ax+ay$</li>
</ul>
</li>
</ul>
<p>则称$\mathbf{X}$按上述加法和数乘称为线性空间或向量空间，其中元素称为向量。</p>
<h3 id="1，特例-1"><a href="#1，特例-1" class="headerlink" title="1，特例"></a>1，特例</h3><ul>
<li>实线性空间$\mathbf{R}^{n}$</li>
<li>复线性空间</li>
</ul>
<h3 id="2，维度"><a href="#2，维度" class="headerlink" title="2，维度"></a>2，维度</h3><ul>
<li>无限维</li>
<li>有限维</li>
<li>零维</li>
</ul>
<h2 id="三，赋范线性空间"><a href="#三，赋范线性空间" class="headerlink" title="三，赋范线性空间"></a>三，赋范线性空间</h2><p>$\mathbf{X}$为实(复)的线性空间，$\forall x\in\mathbf{X}$都有实数(记为$||x||$)与之对应，并且满足：</p>
<ul>
<li>$||x||\geqq 0\quad ||x||=0\;only\; if\;x=0$</li>
<li>$||\alpha x||=|\alpha|||x||,\; \alpha为任意实(复)数$</li>
<li>$||x+y||\leqq ||x||+||y||$<br>则$||x||$为向量$x$的范数，$\mathbf{X}$为按范数$||x||$称为赋范线性空间。</li>
</ul>
<h3 id="1，举例"><a href="#1，举例" class="headerlink" title="1，举例"></a>1，举例</h3><h2 id="四，Banach空间-巴拿赫空间"><a href="#四，Banach空间-巴拿赫空间" class="headerlink" title="四，Banach空间(巴拿赫空间)"></a>四，Banach空间(巴拿赫空间)</h2><p>【完备的赋范线性空间】</p>
<h4 id="1，举例-1"><a href="#1，举例-1" class="headerlink" title="1，举例"></a>1，举例</h4><ul>
<li>欧式空间 $ \mathbf{R}^{n} $ 对任一元素 $ x=(\xi_1,\dots,\xi_n) $ 定义如下范数<script type="math/tex; mode=display">||x||=\sqrt{|\xi_{1}|^{2}+\dots+|\xi_{n}|^{2}}</script></li>
<li>$L^{p}[a,b]$对任一元素$f$定义如下范数<script type="math/tex; mode=display">||f||_{p}=(\int^{b}_{a}|f(t)|^{p}dt)^{\frac{1}{p}}</script></li>
</ul>
<h2 id="五，内积空间"><a href="#五，内积空间" class="headerlink" title="五，内积空间"></a>五，内积空间</h2><p>赋范线性空间只有长度(范数)，但没有角度，两个向量除了有长度概念外，还有夹角的概念。</p>
<script type="math/tex; mode=display">内积\rightarrow 内积空间\rightarrow 内积定义范数\rightarrow 内积中的正交</script><h4 id="1，定义内积"><a href="#1，定义内积" class="headerlink" title="1，定义内积"></a>1，定义内积</h4><p>在复欧式空间中，任意两个向量：</p>
<script type="math/tex; mode=display">a=(\xi_{1},\xi_{2}\dots\xi_{n})\quad b=(\eta_{1},\eta_{2},\dots\eta_{n})</script><p>定义$a$与$b$的内积为：</p>
<script type="math/tex; mode=display"><a,b>=\xi_{1}\bar\eta_{1}+\xi_{2}\bar\eta_{2}+\dots+\xi_{n}\bar\eta_{n}</script><ul>
<li>$\bar\eta_i$是$\eta_i$的复共轭。</li>
<li>若为实欧式空间，类似定义</li>
</ul>
<h4 id="2，定义内积空间"><a href="#2，定义内积空间" class="headerlink" title="2，定义内积空间"></a>2，定义内积空间</h4><p>设 $ \mathbf{X} $ 是复线性空间，对其中任意两个向量 $ x,y $，都有一个复数 $ &lt; x,y &gt; $ 与之对应，且满足如下条件：</p>
<ul>
<li>$ &lt; x,x &gt; \geq 0,\quad &lt; x,x &gt;=0\;only\;if\; x=0 $</li>
<li>$ &lt; \alpha x+\beta y, z &gt;=\alpha &lt; x,z &gt;+\beta &lt; y,z &gt;, \alpha 和\beta 为复数 $</li>
<li>$ &lt; x,y &gt;=\overline{&lt; y,x &gt;} $</li>
</ul>
<p>则称$&lt; x,y &gt;$为$x$与$y$的内积，空间 $ \mathbf{X} $ 为内积空间。</p>
<h4 id="3，用内积定义范数"><a href="#3，用内积定义范数" class="headerlink" title="3，用内积定义范数"></a>3，用内积定义范数</h4><p>若$\mathbf{X}$是内积空间，令<script type="math/tex">||x||=\sqrt{<x,x>}</script>，则$||x||$为$\mathbf{X}$上的范数。</p>
<h4 id="4，内积空间上的特殊（赋范线性空间没有）"><a href="#4，内积空间上的特殊（赋范线性空间没有）" class="headerlink" title="4，内积空间上的特殊（赋范线性空间没有）"></a>4，内积空间上的特殊（赋范线性空间没有）</h4><p>$\mathbf{X}$为内积空间，任意两个元素$a$和$b$正交等价于$&lt; a,b &gt;=0$</p>
<h4 id="5，重要的定义或公式"><a href="#5，重要的定义或公式" class="headerlink" title="5，重要的定义或公式"></a>5，重要的定义或公式</h4><ul>
<li>Schwarz不等式<script type="math/tex; mode=display">|<x,y>|\leq||x||\cdot||y||</script></li>
<li>投影定理</li>
<li>正交基和标准正交基</li>
</ul>
<h2 id="六，Hilbert空间"><a href="#六，Hilbert空间" class="headerlink" title="六，Hilbert空间"></a>六，Hilbert空间</h2><p>【完备的内积空间】<br>是赋范线性空间的特例</p>
<h2 id="理不清的关系"><a href="#理不清的关系" class="headerlink" title="理不清的关系"></a>理不清的关系</h2><ul>
<li>赋范线性空间是一种特殊的度量空间<br>  只要用范数来定义距离，$d(x,y)=||x-y||$</li>
</ul>
]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>real-variable-funcs</tag>
      </tags>
  </entry>
  <entry>
    <title>ODE Transformer</title>
    <url>/2022/11/14/Transformer/ODE%20transformer/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>文章：ODE Transformer-An Ordinary Differential Equation-Inspired Model for Neural Machine Translation</p>
<p><a href="https://arxiv.org/abs/2104.02308v1">essay link</a></p>
<p><strong>ODE 跟Transformer关联的文章，第一次读文章，先看懂！</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>前面有文献工作发现了，残量连接网络是ODE的Euler离散解法。本文探讨Transformer和ODE数值方法之间的关系，且发现了Transformer可以描述为ODE的高阶解法，同时也带领我们设计了一种新的架构（称为 ODE Transformer），类似于在 ODE 中受到很好启发的 Runge-Kutta 方法。</p>
<p>作为Transformer的自然拓展，ODE Transformer容易实现，参数也更加高效。我们对三个 WMT 任务的实验证明了该模型的通用性，以及在几个强大的基线上的性能大幅改进。本文在数据集WMT14的En-De和En-Fr上分别获得了30.76和44.11的BLEU的分值，这为 WMT’14 En-Fr 任务设置了新的sota。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>残差网络已被用作简化多层神经模型中信息流的标准方法，并取得了巨大成功。 给定输入 $y_{t}$，此类模型将深度 $t$ 处的层的输出定义为：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+F(y_{t},\theta_{t}) \tag{1}</script><p>其中，$F(\cdot,\cdot)$ 是网络层的函数，$\theta_{t}$是参数。</p>
<p>有意思的是，ML中最近的工作指出上式可以理解为ODE的Euler离散方法，类似下式：</p>
<script type="math/tex; mode=display">\frac{dy(t)}{dt}=F(y(t),\theta(t)) \tag{2}</script><p>其中，$y(t)$和$\theta(t)$ 是关于 $t$ 连续的。</p>
<p>用这样的方式，可以将残差网络理解为ODE模块，该发现也给理解残差网络从数值优化的角度提供了新思路。之后，可以尝试在多层网络上利用Euler方法，初始值就给定为$y(0)=y_0 $ 和 $\theta (0)=\theta_0 $。</p>
<p>方程 (2) 的解法前提，只有当 $\theta(t) $ 沿 $t$ 缓慢变化时，才有足够低的误差界限（称之为稳定解）。 但这种假设并不总是适用于最先进的自然语言处理 (NLP) 系统，其中模型是非线性和超参的。 例如，语言建模和机器翻译系统为不同的层学习完全不同的参数，尤其是当层接近模型输入时。 </p>
<p>此外，欧拉方法的截断误差不可忽略，因为它是真实解的一阶近似。 当更多的层被堆叠并且错误通过神经网络传播时，这些问题会使情况变得更糟。 这或许可以解释为什么最近的机器翻译 (MT) 系统无法从极深的模型中受益。</p>
<p>在本文中，我们继续研究 ODE 启发方法，基本思想是使用高阶更精确地对 ODE 进行数值求解，这会导致生成更大的 ODE 块，产生一系列中间近似值。 我们发现较大的 ODE 块足以等价于多个一阶解的 ODE 块，好处是显而易见的：使用较少的 ODE 块降低了在块切换中引入错误的风险，而高阶方法减少了每个 ODE 块中的近似误差。</p>
<p>我们的方法是参数有效的，因为$\theta(t) $在同一个 ODE 块中重复使用了，作为另一个“奖励”，可以通过学习块中不同中间近似的系数来改进模型。 我们在强 Transformer 系统中评估我们的方法，涵盖宽（和大）模型和深度模型。 它在 WMT14 En-De 和 En-Fr 测试集上获得了 30.76 和 44.11 的 BLEU 分数，该结果为 WMT14 En-Fr 任务设置了新的sota。</p>
<h2 id="2，Transformer和ODE"><a href="#2，Transformer和ODE" class="headerlink" title="2，Transformer和ODE"></a>2，Transformer和ODE</h2><p>本文从 Transformer 的描述开始，然后是它与 ODE 的关系。 我们选择 Transformer 进行讨论和实验，因为它是最近 MT(机器翻译) 评估中最先进的模型之一。</p>
<h3 id="2-1-Transformer"><a href="#2-1-Transformer" class="headerlink" title="2.1 Transformer"></a>2.1 Transformer</h3><p>Transformer 是编码器-解码器范式的一个例子，编码器是一堆相同的层，每层由一个自注意力块和一个前馈网络（FFN）块组成，它们都配备了残差连接和层归一化单元。 注意，术语“块”以许多不同的方式使用，在本文中指的是通过残差连接（有时称为残差块）增强的任何神经网络。</p>
<p>遵循 Pre-norm 架构 ，我们将块定义为</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+G(LN(y_{t}),\theta_{t}) \tag{3}</script><p>其中$LN(\cdot)$ 是层归一化函数，为了简单起见，我们去掉了$LN(\cdot)$的参数，$G(\cdot)$是 自注意力或前馈网络。 解码器具有类似的架构，在 self-attention 和 FFN 块之间有一个额外的编码器-解码器注意块。</p>
<h3 id="2-2-ODE"><a href="#2-2-ODE" class="headerlink" title="2.2 ODE"></a>2.2 ODE</h3><p>常微分方程是变量 $t$ 的函数 $y(t)$ 及其导数的方程，ODE 的一种简单形式是定义 $y(t)$ 的一阶导数的方程，如下所示:</p>
<script type="math/tex; mode=display">\frac{dy(t)}{dt}=f(y(t),t) \tag{4}</script><p>其中 $f(y(t),t)$ 定义了一个时间相关向量场，如果我们知道它在 $y$ 的所有点和所有时间点 $t$ 的值。 上述方程涵盖了广泛的问题，因为变量的变化由其当前值和时间变量$t$决定。</p>
<p>这个方程也适用于Pre-norm的Transformer块，为了记号简单，我们将$G(LN(y_t),\theta_t) $表示为一个新函数$F(y_t,\theta_t) $就有了：</p>
<script type="math/tex; mode=display">F(y_{t},\theta_{t})=G(LN(y_{t},\theta_{t})) \tag{5}</script><p>将$y_t $和$\theta_t $转换成时间的连续函数$y(t)$和$\theta(t) $，重写方程(3)为：</p>
<script type="math/tex; mode=display">y(t+\Delta t)=y(t)+\Delta t\cdot F(y(t),\theta(t)) \tag{6}</script><p>其中，$\Delta t$表示$t$的变化，也就是常说的步长，显然，在Transformer中有$\Delta t=1$，但可以使用一个约束来适应步长$\Delta t $，就得到了：</p>
<script type="math/tex; mode=display">\lim\limits_{\Delta t\rightarrow 0}\frac{y(t+\Delta t)-y(t)}{\Delta t}=F(y(t),\theta(t)) \tag{7}</script><p>实际上有$\lim_{\Delta t\rightarrow 0}\frac{y(t+\Delta t)-y(t)}{\Delta t}=\frac{dy(t)}{dt} $，方程(7)是方程(4)的特例，唯一的区别就是在方程(4)的右边引进了$\theta(t)$。</p>
<p>然后，我们说 Pre-norm Transformer 块描述了 ODE。 已经发现方程(3)与求解方程(7)中描述的 ODE 的欧拉方法具有相同的形式。 这建立了 Transformer 和 ODE 之间的关系，在给定 $F(\cdot,\cdot)$ 和学习参数 $\{\theta_t\}$ 的情况下，多块 Transformer 的前向传递是多次来运行欧拉方法。</p>
<h2 id="3-ODE-Transformer"><a href="#3-ODE-Transformer" class="headerlink" title="3 ODE Transformer"></a>3 ODE Transformer</h2><p>在 ODE 的数值方法中，我们希望以最少的计算步骤确保 ODE 的精确解。 但是Euler方法并不“精确”，因为它是一阶方法，自然会出现局部截断误差。 如果我们多次运行它，全局误差可能会更大{全局误差就是我们通常所说的误差，$y(t)$ 与真值之间的差异。 局部误差是单步引入的误差，假设$y(t-1)$为真解，$y(t)$与其的差值}。 Transformer 显然就是这种情况，尤其是当多层神经网络在求解 ODE 时，易出现更不稳定的风险。</p>
<h3 id="3-1-高阶ODE解法器"><a href="#3-1-高阶ODE解法器" class="headerlink" title="3.1 高阶ODE解法器"></a>3.1 高阶ODE解法器</h3><p>在这里，我们使用 Runge-Kutta 方法来获得 ODE 的高阶解，它们是具有不同阶精度的经典迭代方法。 正式地介绍$n$步求解的显式 Runge-Kutta 方法定义为：</p>
<script type="math/tex; mode=display">y_{t+1} =y_{t}+\sum\limits_{i=1}^{n}\gamma_{i}F_{i} \tag{8}</script><script type="math/tex; mode=display">F_{1} =hf(y_{t},t) \tag{9}</script><script type="math/tex; mode=display">F_{i} =hf(y_{t}+\sum\limits^{i-1}_{j=1}\beta_{ij}F_{j},t+\alpha_{i}h) \tag{10}</script><p>其中 $ h $ 是步长，在大多数情况下可能就是 1。 $\mathrm{F}_i$ 是步骤 $t+\alpha_i h$ 处解的中间近似。</p>
<p>$ \alpha $、$ \beta $ 和 $ \gamma $ 是可以由 $ y_{t+1} $ 的泰勒级数确定系数，方程(10)描述了在 $n $ 步 $\{t+\alpha_1 h,…,t+\alpha_n h \}$上的解近似序列 $\{F_1,…,F_n\}$，然后对这些近似值进行插值以形成最终解，如方程(8)。</p>
<p>Runge-Kutta 方法直接适用于 Transformer 模块的设计，我们所需要的只是将函数 $f$ 替换为函数 $F$， 优点是函数 $F$ 在块中被重用。 此外，模型参数$\theta_t$ 可以在块内共享。（虽然我们可以区分一个块中不同步骤的参数，但我们发现它没有帮助并且使模型难以学习。） 这样，可以省略方程(10)中的$t+\alpha_i h$，用下式来计算 $F_i$：</p>
<script type="math/tex; mode=display">F_{i}=F(y_{t}+\sum\limits^{i-1}_{j=1}\beta_{ij}F_{j},\theta_{t}) \tag{11}</script><p>这使得系统参数效率更高，正如我们的实验所示，高阶 Runge-Kutta 方法可以学习具有明显更小的模型和强大 NMT 系统。</p>
<p>Runge-Kutta 方法是通用的，例如欧拉方法就是它们的一阶特例。 对于二阶 Runge-Kutta (RK2) 块，我们有：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+\frac{1}{2}(F_{1}+F_{2}) \tag{12}</script><script type="math/tex; mode=display">F_{1}=F(y_{t},\theta_{t}) \tag{13}</script><script type="math/tex; mode=display">F_{2}=F(y_{t}+F_{1},\theta_{t}) \tag{14}</script><p>这也称为改进的欧拉方法，同样，我们可以将四阶 Runge-Kutta (RK4) 块定义为：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+\frac{1}{6}(F_{1}+2F_{2}+2F_{3}+F_{4}) \tag{15}</script><script type="math/tex; mode=display">F_{1}=F(y_{t},\theta_{t}) \tag{16}</script><script type="math/tex; mode=display">F_{2}=F(y_{t}+\frac{1}{2}F_{1},\theta_{t}) \tag{17}</script><script type="math/tex; mode=display">F_{3}=F(y_{t}+\frac{1}{2}F_{2},\theta_{t}) \tag{18}</script><script type="math/tex; mode=display">F_{4}=F(y_{t}+F_{3},\theta_{t}) \tag{19}</script><p>参见图2 以比较不同的 Runge-Kutta 块，需要注意的是，这里介绍的方法可以从表示细化的角度来解释。 它为函数提供了一种更新函数本身的方法，例如，Universal Transformer 使用相同的函数和相同的参数以块方式来细化输入序列的表示。 在这里，我们展示了内部块细化可以在良好的理论支持下建模。</p>
<h3 id="3-2-参数学习"><a href="#3-2-参数学习" class="headerlink" title="3.2 参数学习"></a>3.2 参数学习</h3><p>在我们的初步实验中，当模型很浅时，RK2 和 RK4 方法产生了有希望的 BLEU 改进，但发现对于更深层次的模型，这种改进并没有持续下去。 为了弄清楚为什么会发生这种情况，让我们从训练的角度回顾一下龙格-库塔方法，以 RK2 方法为例。 我们通过替换 $F_1$ 和 $F_2$重写方程(12)，如下：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+\frac{1}{2}F(y_{t}+F(y_{t},\theta_{t}),\theta_{t}) \tag{20}</script><p>令 $\mathcal{E}$ 为训练损失，$L$ 为模型的块数，$y_{L}$ 为模型输出，$\mathcal{E}$ 在 $y_t$ 处的梯度是</p>
<script type="math/tex; mode=display">\frac{\partial{\varepsilon}}{\partial{y_{t}}}=\frac{\partial\varepsilon}{\partial y_{L}}\cdot\frac{1}{2^{L-t}}\cdot\prod_{k=t}^{L-1}(1+g_{k}) \tag{21}</script><p>其中，</p>
<script type="math/tex; mode=display">g_{k}=(1+\frac{\partial F(y_{k},\theta_{k})}{\partial y_{k}})\cdot (1+\frac{\partial F(y_{k}+F(y_{k},\theta_{k}),\theta_{k})}{\partial y_{k}+F(y_{k},\theta_{k})}) \tag{22}</script><p>从等式(29)看，$\frac{\partial \mathcal{E}}{\partial y_{t}}$ 与因子 $\frac{1}{2^{L-t}}$成正比，这导致当 $L$ 较大时梯度消失的风险更高。</p>
<p>这个问题在某种程度上归因于$F_i$ 的小系数，即$\gamma_1 = \gamma_2 = \frac{1}{2}$。 一个自然的想法是根据经验设置 $\gamma_i = 1$ 以消除梯度计算中小于 1 的乘积因子，尽管这在理论上并不基于标准的龙格-库塔方法。 我们用新系数重写方程(20)，如下：</p>
<script type="math/tex; mode=display">y_{t+1}=y_{t}+F(y_{t},\theta_{t})+F(y_{t}+F(y_{t},\theta_{t}),\theta_{t}) \tag{23}</script><p>然后，可以求梯度为：</p>
<script type="math/tex; mode=display">\frac{\partial\varepsilon}{\partial y_{t}}=\frac{\partial\varepsilon}{\partial y_{L}}\cdot\prod^{L-1}_{k=t}g_{k} \tag{24}</script><p>这个模型很容易优化，因为 $\frac{\partial \mathcal{E}}{\partial_{y_L}}$ 可以传递给没有尺度的低级块。 请注意，这里的方法是参数共享的实例。 例如，在每个 ODE 块中，我们对所有中间步骤使用具有相同参数 $\theta_t$ 和相同函数 $F$。 设置 $\gamma_i = 1$，因为 $F_i$ 以相同的比例传递到下一步。 这里我们称之为隐式参数共享。</p>
<p>另一种缩放 $F_i$ 的方法是在训练数据上自动学习系数（初始值 $\gamma_i = 1$），它帮助系统学习在一个块中流动 $F_i$ 的方式。 我们的实验表明，自动系数学习对于获得更好的结果是必要的。</p>
<h2 id="4，实验"><a href="#4，实验" class="headerlink" title="4，实验"></a>4，实验</h2><h3 id="4-1-实验设置"><a href="#4-1-实验设置" class="headerlink" title="4.1 实验设置"></a>4.1 实验设置</h3><p>我们提出的方法在三个广泛使用的基准上进行了评估：WMT’14 英语-德语 (En-De)、WMT’14 英语-法语 (En-Fr) 和 WMT’16 英语-罗马尼亚语 (En-Ro) 翻译任务。</p>
<h4 id="数据集和评价"><a href="#数据集和评价" class="headerlink" title="数据集和评价"></a>数据集和评价</h4><p>对于 En-De 任务，训练数据由大约 4.5M的标记化句子对组成，所有句子都被分割成子词单元的序列，使用共享词汇表进行了 32K 的合并操作。 我们选择 newstest2013 作为验证数据，选择 newstest2014 作为测试数据。 </p>
<p>对于 En-Fr 任务，我们使用了 Fairseq 提供的数据集，即来自 WMT’14 的 36M 训练句子对，newstest2012+newstest2013 是验证数据，newstest2014 是测试数据。 </p>
<p>对于 En-Ro 任务，我们复制了文章mehta2020delight的设置，分别使用 600K/2K/2K 句子对进行训练、评估和推理。</p>
<p>我们根据 BLEU 来衡量性能，标记化的 BLEU 分数 和 sacrebleu 都是在 En-De 和 En-Fr 任务上报。 此外，我们报告了 En-Ro 任务的标记化 BLEU 分数，En-De 和 En-Fr 的beam尺寸和长度惩罚因子分别为 4 和 0.6 ，En-Ro 为 5 和 1.3 。</p>
<h4 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h4><p>正如 li-etal-2020-shallow 的工作所建议的，我们使用相对位置表示 (RPR) 来获得更强的基线 。 所有实验都在 8 块 GPU 上进行训练，每个 GPU 上有 4,096 tokens。 对于 En-De 和 En-Fr 任务，我们采用梯度累积策略，步长分别为 2 和 8 。 我们使用了 Adam 优化器，其超参数设置为 $(0.9, 0.997)$，学习率的最大点设置为 $0.002$，以实现快速收敛。 我们将 SAN 和 FFN 合并视为默认的 ODE 块。</p>
<p>更多细节参见我们提供的材料。</p>
<h3 id="4-2-结果"><a href="#4-2-结果" class="headerlink" title="4.2 结果"></a>4.2 结果</h3><h4 id="En-De-和-En-Fr-的结果"><a href="#En-De-和-En-Fr-的结果" class="headerlink" title="En-De 和 En-Fr 的结果"></a>En-De 和 En-Fr 的结果</h4><p>表1将 ODE Transformer 与几个最先进的系统进行了比较，RK2-block 和 RK4-block 在不同的模型容量下都大大优于基线。 例如，当深度为 6 时，RK2-block 使用基本配置获得了 0.97 的 BLEU 改进。 RK4 块在 RK2 块之上产生 +$0.17$ BLEU 点的收益， 这一观察从经验上验证了高阶 ODE 函数更有效的猜想。 当我们切换到深度模型时，RK2-block 可与 li-etal-2020-shallow 中报告的 48 层强系统相媲美，但参数显着减少，这表明我们的方法是参数有效的。</p>
<p>宽模型也可以受益于扩大层深度。 RK-2 ODE Transformer 在 En-De 和 En-Fr 任务上的 BLEU 分数分别为 30.76 和 44.11 ，显着超过标准 Big 模型 1.32 和 0.70 的 BLEU 分数。 这为这些任务设置了新的最先进技术，参数更少， 请注意，我们后续将报告更多关于 RK4 块的结果。</p>
<h4 id="Rn-Ro-结果"><a href="#Rn-Ro-结果" class="headerlink" title="Rn-Ro 结果"></a>Rn-Ro 结果</h4><p>表2展示了 En-Ro 任务中几个强大系统的模型参数、总训练步骤和 BLEU 分数。 同样，ODE Transformer 的性能优于这些基线。 如 mehta2020delight 中所述，他们对模型进行了高达 170 epoches 的训练，并通过 DeLight 模型获得了 34.70 的 BLEU 分数。 然而，这里的观察是完全不同的， 在 20 epoch 之后，验证的困惑度开始增加。 因此，我们的基线略逊于他们的基线，但与 lin2020towards 中报告的结果相匹配。 ODE Transformer 使用 DeLight 以更少的训练成本实现了更好的性能， 对于更大的模型（表2中的第 6 行），它获得了 35.28 的 BLEU 分数。</p>
<h4 id="参数有效性"><a href="#参数有效性" class="headerlink" title="参数有效性"></a>参数有效性</h4><p>表3总结了几个高效 Transformer 变体的结果，包括 Lite Transformer 、DeLight和 Evolved Transformer的轻型版本。 正如我们预期的那样，所提出的 ODE Transformer 有望用于较小的模型。 它在 BLEU 中与 DeLight 相当，但参数少了 9M。 在相同的模型容量下，它比 DeLight 高出 0.84 BLEU 点。 这些结果表明，所提出的方法与模型容量正交，它可能为在边缘设备上部署 NMT 系统提供新的选择。</p>
<h3 id="4-3-分析"><a href="#4-3-分析" class="headerlink" title="4.3 分析"></a>4.3 分析</h3><p>在这里，我们调查一些有趣的问题。 为简单起见，在下文中，我们将具有可学习系数的 RK2-block 称为 RK2-block-v2。</p>
<h4 id="BLEU-和编码深度"><a href="#BLEU-和编码深度" class="headerlink" title="BLEU 和编码深度"></a>BLEU 和编码深度</h4><p>图 3（左）描绘了几个 ODE Transformer 变体的 BLEU 分数和不同编码器深度下的基线。当深度 $\leq 24$ 时，所有 ODE Transformer 变体都明显优于基线。而 RK2-block-v2 几乎在所有深度上都达到了最佳性能，尤其是当模型变得更深时。直观地说，与 18 层基线系统相比，6 层 RK2 块能够提供相当的性能。同样，它表明所提出的方法是参数有效的。这里的另一个发现是 RK4 块在浅层模型上表现得很好，在表 1 中观察到了类似的现象。对于更深的模型，它不如 RK2-block，尽管高阶 ODE 求解器可以获得更低的误差。这是因为当模型很深时，原始系数可能会导致反向传播中的优化问题。此外，图 3（右）将 BLEU 绘制为当隐藏大小为 256 时模型大小的函数。我们的 RK2 方法使用更少的参数显着超过了基线。</p>
<h4 id="不同-F-cdot-cdot-上的参数学习"><a href="#不同-F-cdot-cdot-上的参数学习" class="headerlink" title="不同$F(\cdot,\cdot) $上的参数学习"></a>不同$F(\cdot,\cdot) $上的参数学习</h4><p>正如我们所说，$F(\cdot,\cdot)$ 函数可以是子层，例如 SAN、FFN 或两者兼有 (SAN+FFN)。 如图 4 所示，高阶 ODE 与 FFN 协同相比 SAN 协同的效果更好。 一个探索可能是FFN组件的参数比SAN组件多。将 FFN 和 SAN 合并为 ODE 块的模型表现出最佳性能。</p>
<h4 id="训练和验证难度"><a href="#训练和验证难度" class="headerlink" title="训练和验证难度"></a>训练和验证难度</h4><p>图 5 绘制了 RK 块和标准残差块的训练和验证困惑 (PPL) 曲线。 我们基于两种配置（基本模型和宽模型）比较行为。 直观地说，RK2 块在两种配置中都呈现较低的训练和验证 PPL。</p>
<h4 id="梯度归一化后的可视化"><a href="#梯度归一化后的可视化" class="headerlink" title="梯度归一化后的可视化"></a>梯度归一化后的可视化</h4><p>为了研究所提出的 ODE Transformer 的优越性，我们在训练期间收集了几个训练有素的系统的梯度范数。 图 6 绘制了 RK2-block、RK4-block 和标准残差块（基线）的梯度范数。 正如我们所见，Pre-Norm 残差块能够使训练稳定。 由于中间近似之间的隐式参数共享，RK2-block 和 RK4-block 都提供了更丰富的信号。 并且两条学习曲线同样看起来几乎相同，这与表 1 中的结果一致。</p>
<h4 id="不同ODE设计策略的比较"><a href="#不同ODE设计策略的比较" class="headerlink" title="不同ODE设计策略的比较"></a>不同ODE设计策略的比较</h4><p>然后，我们对几种ODE设计模式进行了综合分析。 正如 yiping2018beyond 中所述，计算机视觉中的几个模型，例如 LeapfrogNet、PolyNet 、Multi-step Net 也可以从 ODE 角度进行解释。 相关的 ODE 函数汇总在表 4 中。 在这里，我们使用相同的代码库重新实现这些方法以进行公平比较。 我们按照基本配置将编码器深度设置为 6 ，并对 En-De 任务进行了实验。</p>
<p>在时间 $t$，多步欧拉方法需要先前的状态，例如 $y_{t-1}$，生成当前近似值，而不是基于当前时间状态的迭代细化。 基本上，这些方法参数效率不高，并且性能不如我们的方法。 注意，DLCL也可以看成是多步欧拉法，在deep Transformer中更具竞争力。 但是在浅基线上只有很小的改进。</p>
<p>从理论上讲，Backward Euler 方法在数值分析中略好于 Forward Euler 方法，但改进微乎其微。 请注意，与上述方法相比，我们的 ODE Transformer 实现了一致的 BLEU 改进。 这里的原因是这种迭代细化使参数学习更加高效和有效。 所有模型都可以在我们的附件中找到。</p>
<h4 id="阶段误差量化"><a href="#阶段误差量化" class="headerlink" title="阶段误差量化"></a>阶段误差量化</h4><p>在这里，我们旨在量化截断误差。 但是，我们无法在 NMT 中获得每个块输出的“真实”解，因为我们主要在编码器端进行实验。相反，我们在语言建模任务上进行了实验，也就是单层模型输出与真值之间的损失 相当于没有错误传播的截断错误。</p>
<p>表 5 显示了 PTB 任务的 PPL。 所有 ODE Transformer 变体都显着减少了错误。 RK4-order 在两种设置上都达到了最低的 PPL。 此外，RK2 块甚至可以获得比 2 层残差块更低的 PPL。 这里的观察再次验证了我们的猜想。</p>
<h2 id="5，相关工作"><a href="#5，相关工作" class="headerlink" title="5，相关工作"></a>5，相关工作</h2><h3 id="深度Transformer模型"><a href="#深度Transformer模型" class="headerlink" title="深度Transformer模型"></a>深度Transformer模型</h3><p>最近，Deep Transformer 在机器翻译方面取得了巨大成功。 一种直接的方法是缩短从上层到下层的路径，从而缓解梯度消失或爆炸问题。 对于更深层次的模型，训练成本是不可忽略的。 为了加快训练速度，另一种方法是先训练一个浅层模型，然后逐渐增加模型深度。</p>
<p>除了模型架构改进之外，另一种简化优化的方法是利用精心设计的参数初始化策略，例如 depth-scale 、Lipschitz 约束 ，T-fixup  和 ADMIN 。</p>
<p>请注意，ODE Transformer 与上述方法是正交的，我们将在未来的工作中对这些方法进行测试。</p>
<h3 id="ODE"><a href="#ODE" class="headerlink" title="ODE"></a>ODE</h3><p>ResNet 和 ODE 之间的关系最早由 weinan2017proposal 提出。 这为社区带来了设计有效深度架构的全新视角。 一些有见地的架构zhang2017polynet,larsson2017fractalnet,yiping2018beyond,he2019ode 也可以从ODE的角度来解释。 但是，在自然语言处理中，从 ODE 角度设计模型的研究仍然很少见。 也许与我们最相关的工作是 lu2019understanding 的工作。 他们从多粒子动态系统的角度解释了 Transformer 架构，并将夹在 FFN 中的自注意力重新定位。 与他们的工作不同，我们认为堆叠的一阶 ODE 块可能会导致误差累积，从而阻碍模型性能。 我们通过引入高阶块来解决这个问题，并展示了显著的 BLEU 改进。</p>
<h2 id="6，结论"><a href="#6，结论" class="headerlink" title="6，结论"></a>6，结论</h2><p>在本文中，我们探讨了 Transformer 和 ODE 之间的关系，提出了一种新的架构（ODE Transformer）来帮助模型从高阶 ODE 解决方案中受益。 实验结果表明，在模型容量相同的情况下，ODE Transformer 可以显著优于基线。 它在 WMT’14 En-De 和 En-Fr 测试数据上获得了 30.76 和 44.11 的 BLEU 分数，这为 En-Fr 任务设置了新的sota。</p>
]]></content>
      <categories>
        <category>Transformer</category>
      </categories>
      <tags>
        <tag>ODE</tag>
      </tags>
  </entry>
  <entry>
    <title>区块链中的名词解释</title>
    <url>/2022/11/14/other/clock_chain_new_words/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>区块链中的各种链解释~</p>
<p><strong>区块链，去中心化的新技术~</strong><br><span id="more"></span></p>
<h2 id="1，公链"><a href="#1，公链" class="headerlink" title="1，公链"></a>1，公链</h2><p>比特币、以太坊是时下流行的公链。</p>
<p>公链全称是“公有链”，是指任何人都可读取的、任何人都能发送交易且交易能获得有效确认的、任何人都能参与其中共识过程的区块链。通常被认为是去中心化的。可以简单理解为公开的、公共可用的区块链。完全去中心化的公链通过共识机制和代币奖励机制来鼓励参与者{节点}竞争记账，共同维护链上数据的安全性。</p>
<p>公链具有开源、保护用户免受开发者的影响、访问门槛低、所有数据默认公开等特性。任何人也可以在公共链上发送交易，还可以随时参与网络上形成共识的过程，即决定哪个区块可以加入区块链并记录当前的网络状态。</p>
<h2 id="2，私链"><a href="#2，私链" class="headerlink" title="2，私链"></a>2，私链</h2><p>私有链是与公有链相对的一个概念，就是指不对外开放，仅在组织内部使用的系统，其创建及维护权限由一个组织拥有，非组织成员无法访问或仅拥有小规模访问权限的区块链网络。具有交易速度快、隐私保障性好、信息安全性高、交易成本很低甚至为零等特点。私有链在使用过程中，通常要求注册，即需要提交身份认证并具备一套权限管理系统。节点数量和节点状态通常是可控的，一般不需要通过竞争方式来筛选区块数据的打包者，可以采用更加节能环保的方式。</p>
<p>私有链的价值主要是提供安全、可追溯、不可篡改、自动执行的运算平台，同时防范来自内部和外部对数据安全的攻击。私有链的应用场景一般是企业内部，如企业的票据管理、账务审计、供应链管理。</p>
<h2 id="3，联盟链"><a href="#3，联盟链" class="headerlink" title="3，联盟链"></a>3，联盟链</h2><p>联盟链是由多个组织或者通过联盟形式组建的区块链，联盟参与者之间通过契约或其他形式建立信任和共识机制，构造的区块和链接功能仅限于联盟参与者，访问权限可以对外采取限制性开放。联盟链是一种需要注册许可的区块链，这种区块链也称为许可链，通常是使用在多个成员角色的环境下，比如银行之间的支付结算、企业之间的物流结算。</p>
<p>联盟链可以根据应用场景来决定对公众的开放程度。由于参与共识的节点比较少，联盟链一般不采用工作量证明的挖矿机制，而是多采用权益证明或PBFT（Practical ByzantineFault Tolarant）、RAFT等共识算法。联盟链对交易的确认时间、每秒交易数都与公共链有较大的区别，对安全和性能的要求也比公共链高。</p>
<h2 id="4，单链"><a href="#4，单链" class="headerlink" title="4，单链"></a>4，单链</h2><p>所谓单链，是指能够单独运行的区块链系统，这些区块链系统拥有完备的组件模块并自称一格体系。单链应用程序的运行需要独立的区块链系统的支持。</p>
<h2 id="5，侧链"><a href="#5，侧链" class="headerlink" title="5，侧链"></a>5，侧链</h2><p>侧链属于一种区块链的跨链技术。区块链系统与侧链系统本身都是一格独立的链系统，两者之间可以按照一定的协议进行数据互动，通过这种方式，侧链能起到对主链功能扩展的作用，如很多在主链中不方便实现的功能可以在侧链中实现，而侧链再通过与主链的数据交互增强自己的可靠性。本质上说，侧链并不会指某一区块链，是遵循侧链协议的全部区块链统称。侧链致力于完成双向锚定，让某类加密货币在主链和侧链两者之间互相“转移”。</p>
<h2 id="6，互联链"><a href="#6，互联链" class="headerlink" title="6，互联链"></a>6，互联链</h2><p>针对特定领域的应用可能会形成各自垂直领域的区块链，互联链就是一种通过跨链技术连接不同区块链的基础设施：包括数据结构和通信协议，其本身通常也是区块链。各种不同的区块链通过互联链互联互通并形成更大的区块链生态。与互联网一样，互联链的建立将形成区块链的全球网络。</p>
<p>与传统的软件不同的是，区块链系应用拥有独特的性质，比如数据不可篡改、完全性证明、自动网络共识、智能合约等，从最初的数字货币到未来可能的区块链可编程社会，这些不单单改变生活服务方式，还会促进社会治理的变革。如果说每一条链都是一条神经的话，一旦互联起来，就像是神经系统一样，将会给我们的社会发展带来更新层次的智能化。</p>
<h2 id="7，主链"><a href="#7，主链" class="headerlink" title="7，主链"></a>7，主链</h2><p>也就是部署在生产环境中的真正的区块链系统，软件在正式发布前会经过很多的内部测试版本，用于发现一些可能存在的bug，并且用来内部演示以便于查看结果，最后才会正式发布正式版。主链，也可以说是正式版客户端组成的区块链网络，只有主链才会真正被推广使用的，各项功能的设计都是相对完善的。另外有些时候，区块链系统会由于种种原因导致分叉，比如挖矿的时候临时产生的小分叉等等，因此最长的那条原始链称为主链。</p>
<h2 id="8，测试链"><a href="#8，测试链" class="headerlink" title="8，测试链"></a>8，测试链</h2><p>一是开发者为了方便大家学习使用而提供的测试用途的区块链网络，如比特币测试链、以太坊测试链等；<br>二是用户自行搭建的测试网络。</p>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>blockchain</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo上的next主题增量优化</title>
    <url>/2022/08/04/ubuntuOS/hexo-next-theme2/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>接着继续优化hexo上next的主题，现在基本就是，缺什么补上，看着什么补顺眼就美化，赞的啦~</p>
<p><strong>立志成为顺眼的博客~</strong><br><span id="more"></span></p>
<h2 id="1，代码折叠"><a href="#1，代码折叠" class="headerlink" title="1，代码折叠"></a>1，代码折叠</h2><h3 id="1-1-添加code-unfold-js"><a href="#1-1-添加code-unfold-js" class="headerlink" title="1.1 添加code-unfold.js"></a>1.1 添加code-unfold.js</h3><p>新增文件，路径为themes/next/source/js/code-unfold.js，代码为：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">var CODE_MAX_HEIGHT = 200;</span><br><span class="line">var containers = [];</span><br><span class="line"></span><br><span class="line">// 展开</span><br><span class="line">$(&#x27;body&#x27;).on(&#x27;click&#x27;, &#x27;.js_unfold_code_btn&#x27;, function () &#123;</span><br><span class="line">  $(this).closest(&#x27;.js_highlight_container&#x27;).addClass(&#x27;on&#x27;);</span><br><span class="line">&#125;);</span><br><span class="line">// 收起</span><br><span class="line">$(&#x27;body&#x27;).on(&#x27;click&#x27;, &#x27;.js_retract_code_btn&#x27;, function () &#123;</span><br><span class="line">  var $container = $(this).closest(&#x27;.js_highlight_container&#x27;).removeClass(&#x27;on&#x27;);</span><br><span class="line">  var winTop = $(window).scrollTop();</span><br><span class="line">  var offsetTop = $container.offset().top;</span><br><span class="line">  $(this).css(&#x27;top&#x27;, 0);</span><br><span class="line">  if (winTop &gt; offsetTop) &#123;</span><br><span class="line">    // 设置滚动条位置</span><br><span class="line">    $(&#x27;body, html&#x27;).animate(&#123;</span><br><span class="line">      scrollTop: $container.offset().top - CODE_MAX_HEIGHT</span><br><span class="line">    &#125;, 600);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line">// 滚动事件，触发动画效果</span><br><span class="line">$(window).on(&#x27;scroll&#x27;, function () &#123;</span><br><span class="line">  var scrollTop = $(window).scrollTop();</span><br><span class="line">  var temp = [];</span><br><span class="line">  for (let i = 0; i &lt; containers.length; i++) &#123;</span><br><span class="line">    var item = containers[i];</span><br><span class="line">    var &#123; $container, height, $hide, hasHorizontalScrollbar &#125; = item;</span><br><span class="line">    if ($container.closest(&#x27;body&#x27;).length === 0) &#123;</span><br><span class="line">      // 如果 $container 元素已经不在页面上, 则删除该元素</span><br><span class="line">      // 防止pjax页面跳转之后，元素未删除</span><br><span class="line">      continue;</span><br><span class="line">    &#125;</span><br><span class="line">    temp.push(item);</span><br><span class="line">    if (!$container.hasClass(&#x27;on&#x27;)) &#123;</span><br><span class="line">      continue;</span><br><span class="line">    &#125;</span><br><span class="line">    var offsetTop = $container.offset().top;</span><br><span class="line">    var hideBtnHeight = $hide.outerHeight();</span><br><span class="line">    // 减去按钮高度，减去底部滚动条高度</span><br><span class="line">    var maxTop = parseInt(height - (hasHorizontalScrollbar ? 17 : 0) - hideBtnHeight);</span><br><span class="line">    let top = parseInt(</span><br><span class="line">      Math.min(</span><br><span class="line">        Math.max(scrollTop - offsetTop, 0), // 如果小于 0 ，则取 0</span><br><span class="line">        maxTop,// 如果大于 height ，则取 height</span><br><span class="line">      )</span><br><span class="line">    );</span><br><span class="line">    // 根据 sin 曲线设置&quot;收起代码&quot;位置</span><br><span class="line">    var halfHeight = parseInt($(window).height() / 2 * Math.sin((top / maxTop) * 90 * (2 * Math.PI/360)));</span><br><span class="line">    $hide.css(&#x27;top&#x27;, Math.min(top + halfHeight, maxTop));</span><br><span class="line">  &#125;</span><br><span class="line">  containers = temp;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">// 添加隐藏容器</span><br><span class="line">function addCodeWrap($node) &#123;</span><br><span class="line">  var $container = $node.wrap(&#x27;&lt;div class=&quot;js_highlight_container highlight-container&quot;&gt;&lt;div class=&quot;highlight-wrap&quot;&gt;&lt;/div&gt;&lt;/div&gt;&#x27;).closest(&#x27;.js_highlight_container&#x27;);</span><br><span class="line"></span><br><span class="line">  // 底部 &quot;展开代码&quot; 与 侧边栏 &quot;收起代码&quot;</span><br><span class="line">  var $btn = $(`</span><br><span class="line">    &lt;div class=&quot;highlight-footer&quot;&gt;</span><br><span class="line">      &lt;a class=&quot;js_unfold_code_btn show-btn&quot; href=&quot;javascript:;&quot;&gt;展开代码&lt;i class=&quot;fa fa-angle-down&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    &lt;a class=&quot;js_retract_code_btn hide-btn&quot; href=&quot;javascript:;&quot;&gt;&lt;i class=&quot;fa fa-angle-up&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;收起代码&lt;/a&gt;</span><br><span class="line">  `);</span><br><span class="line"></span><br><span class="line">  $container.append($btn);</span><br><span class="line">  return $container;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">function codeUnfold () &#123;</span><br><span class="line">  $(&#x27;.highlight&#x27;).each(function () &#123;</span><br><span class="line">    // 防止重复渲染</span><br><span class="line">    if (this.__render__ === true) &#123;</span><br><span class="line">      return true;</span><br><span class="line">    &#125;</span><br><span class="line">    this.__render__ = true;</span><br><span class="line">    var $this = $(this);</span><br><span class="line">    var height = $(this).outerHeight();</span><br><span class="line">    if (height &gt; CODE_MAX_HEIGHT) &#123;</span><br><span class="line">      // 添加展开&amp;收起容器</span><br><span class="line">      var $container = addCodeWrap($this, height);</span><br><span class="line">      containers.push(&#123;</span><br><span class="line">        $container,</span><br><span class="line">        height,</span><br><span class="line">        $hide: $container.find(&#x27;.js_retract_code_btn&#x27;),</span><br><span class="line">        hasHorizontalScrollbar: this.scrollWidth &gt; this.offsetWidth,</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h3 id="1-2-添加jquery"><a href="#1-2-添加jquery" class="headerlink" title="1.2 添加jquery"></a>1.2 添加jquery</h3><p>主题配置文件<code>source/_data/next.yml</code>修改配置项<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fancybox: true</span><br></pre></td></tr></table></figure></p>
<h3 id="1-3-引用code-unfold-js"><a href="#1-3-引用code-unfold-js" class="headerlink" title="1.3 引用code-unfold.js"></a>1.3 引用code-unfold.js</h3><ul>
<li><p>修改文件<code>themes/next/layout/_scripts/index.swig</code>，在最后添加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 代码折叠</span><br><span class="line">&#123;&#123;- next_js(&#x27;code-unfold.js&#x27;) &#125;&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加引用<code>themes/next/source/js/next-boot.js</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NexT.boot.refresh = function () &#123;</span><br><span class="line">  // 添加一行代码</span><br><span class="line">  codeUnfold()</span><br><span class="line">  </span><br><span class="line">  // ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="1-4-创建highlight-styl"><a href="#1-4-创建highlight-styl" class="headerlink" title="1.4 创建highlight.styl"></a>1.4 创建highlight.styl</h3><p>添加<code>theme/next/source/css/_common/components/highlight.styl</code>文件<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 展开收起效果</span><br><span class="line">.highlight-container</span><br><span class="line">  position: relative</span><br><span class="line">  background-color: highlight-background</span><br><span class="line">  &amp;.on</span><br><span class="line">    .highlight-footer</span><br><span class="line">      display: none</span><br><span class="line">    .hide-btn</span><br><span class="line">      display: flex</span><br><span class="line">    .highlight-wrap</span><br><span class="line">      max-height: none</span><br><span class="line">  .highlight-wrap</span><br><span class="line">    overflow: hidden</span><br><span class="line">    max-height: 200px</span><br><span class="line">  .highlight-footer</span><br><span class="line">    position absolute</span><br><span class="line">    width: 100%</span><br><span class="line">    left: 0</span><br><span class="line">    bottom: 0</span><br><span class="line">    height: 60px</span><br><span class="line">    background-image: &#x27;linear-gradient(-180deg, rgba(255,255,255,0) 0%, %s 65%)&#x27; % highlight-background;</span><br><span class="line">    text-align: center</span><br><span class="line">  .show-btn</span><br><span class="line">    font-size: 12px</span><br><span class="line">    color: #fff</span><br><span class="line">    position: absolute</span><br><span class="line">    left: 50%</span><br><span class="line">    transform: translateX(-50%)</span><br><span class="line">    bottom: 0</span><br><span class="line">    line-height: 2em</span><br><span class="line">    text-decoration: none</span><br><span class="line">    padding: 0 0.8em</span><br><span class="line">    text-align: center</span><br><span class="line">    border-radius: 4px 4px 0</span><br><span class="line">    &amp;:hover</span><br><span class="line">      text-decoration: none</span><br><span class="line">  .hide-btn</span><br><span class="line">    color: #fff</span><br><span class="line">    font-size: 12px</span><br><span class="line">    width: 22px</span><br><span class="line">    position: absolute</span><br><span class="line">    left: -21px</span><br><span class="line">    top: 0</span><br><span class="line">    line-height: 1em</span><br><span class="line">    text-decoration: none</span><br><span class="line">    text-align: center</span><br><span class="line">    display: none</span><br><span class="line">    flex-direction: column</span><br><span class="line">    background-color: highlight-background</span><br><span class="line">    border-radius: 4px 0 0 4px</span><br><span class="line">    padding: 0.1em 0 0.6em</span><br><span class="line">    transition: top ease 0.35s</span><br><span class="line">  .fa-angle-up,</span><br><span class="line">  .fa-angle-down</span><br><span class="line">    font-style: normal</span><br><span class="line">    color: #fff</span><br><span class="line">  .fa-angle-up:before</span><br><span class="line">    content:&quot;\f106&quot;</span><br><span class="line">  .fa-angle-down:before</span><br><span class="line">    content:&quot;\f107&quot;</span><br><span class="line">    margin-left: 0.5em</span><br><span class="line">  .js_unfold_code_btn, .js_retract_code_btn</span><br><span class="line">    background: rgba(0,0,0,0.5)</span><br><span class="line">    border-bottom: none !important</span><br><span class="line">    &amp;:hover</span><br><span class="line">      border-bottom-color: none !important</span><br></pre></td></tr></table></figure></p>
<h3 id="1-5-引用样式"><a href="#1-5-引用样式" class="headerlink" title="1.5 引用样式"></a>1.5 引用样式</h3><p>修改文件<code>themes/next/source/css/_common/components/components.styl</code>，在最后添加一行<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@import &#x27;post&#x27;;</span><br><span class="line">@import &#x27;pages&#x27;;</span><br><span class="line">@import &#x27;third-party&#x27;;</span><br><span class="line">// 添加这一行</span><br><span class="line">@import &#x27;highlight&#x27;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>ssh环境构建和常用命令</title>
    <url>/2022/08/04/ubuntuOS/ssh-server-on-ubuntu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ubuntu上的ssh服务，包含客户端和服务端的应用环境。在docker中使用ssh服务，若没有自动启动，docker关闭重启后服务会挂掉，再次连接就会报<code>Connection refused</code>问题，怎么解决呢？</p>
<p>本文中的命令主要基于ubuntu的系统，大多为ubuntu20.04。</p>
<p><strong>ssh也是常用的工具呀~</strong><br><span id="more"></span></p>
<h2 id="客户端openssh-client"><a href="#客户端openssh-client" class="headerlink" title="客户端openssh-client"></a>客户端openssh-client</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-client     //安装客户端</span><br></pre></td></tr></table></figure>
<h2 id="服务端openssh-server"><a href="#服务端openssh-server" class="headerlink" title="服务端openssh-server"></a>服务端openssh-server</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dpkg -l | grep ssh  //查看ubuntu是否安装openssh-server服务</span><br><span class="line">sudo apt-get install openssh-server</span><br><span class="line">ps -e | grep ssh    //确认服务启动</span><br><span class="line"></span><br><span class="line">sudo /etc/init.d/ssh start  //启动服务命令1</span><br><span class="line">sudo service ssh start      //启动服务命令2</span><br></pre></td></tr></table></figure>
<p>其中，ssh-server配置文件位于/etc/ssh/sshd_config</p>
<h2 id="docker使用-从头构建镜像"><a href="#docker使用-从头构建镜像" class="headerlink" title="docker使用-从头构建镜像"></a>docker使用-从头构建镜像</h2><p>可以在Dockfile中添加关于ssh自动启动的部分，加上如下代码即可：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RUN mkdir /var/run/sshd</span><br><span class="line">EXPOSE 22</span><br><span class="line">CMD [&quot;/user/sbin/ssh&quot;,&quot;-D&quot;]</span><br></pre></td></tr></table></figure><br>释放22端口给ssh服务，同时启动ssh服务到后台运行。</p>
<h2 id="docker使用-已经在运行的系统或docker"><a href="#docker使用-已经在运行的系统或docker" class="headerlink" title="docker使用-已经在运行的系统或docker"></a>docker使用-已经在运行的系统或docker</h2><p>进入docker内部，手动重启ssh服务<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it xinwen bash</span><br><span class="line">sudo /etc/init.d/ssh start  //启动服务命令1</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>Python替你七夕表白</title>
    <url>/2022/08/04/other/blessing/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>金风玉露一相逢，便胜却人间无数~</p>
<p><strong>七夕节快乐~</strong><br><span id="more"></span></p>
<h2 id="精彩截图"><a href="#精彩截图" class="headerlink" title="精彩截图"></a>精彩截图</h2><p><img src="first.png" alt="blessing-first"><br><img src="next.png" alt="blessing-next"><br><img src="end.png" alt="blessing-end"><br>实际是个动画，挺惊喜哦~</p>
<h2 id="表白代码"><a href="#表白代码" class="headerlink" title="表白代码"></a>表白代码</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#1.导入</span><br><span class="line">import random</span><br><span class="line">from math import *</span><br><span class="line">import turtle</span><br><span class="line">import time</span><br><span class="line">from turtle import mainloop, hideturtle</span><br><span class="line"></span><br><span class="line">#2.生成斐波契那数列</span><br><span class="line">def FRt(n):</span><br><span class="line">    if n&lt;=0:</span><br><span class="line">        return 0</span><br><span class="line">    elif n==1:</span><br><span class="line">        return 1</span><br><span class="line">    else:</span><br><span class="line">        return FRt(n-1)+FRt(n-2)</span><br><span class="line">def FR(n):</span><br><span class="line">    result_list=[]</span><br><span class="line">    for i in range(1,n+3):</span><br><span class="line">        result_list.append(FRt(i))</span><br><span class="line">    return result_list</span><br><span class="line"></span><br><span class="line">#3.定义生成叶子方法</span><br><span class="line">def leaf(x,y,node):</span><br><span class="line">    til=turtle.heading()</span><br><span class="line">    i=random.random()</span><br><span class="line">    an=random.randint(10,180)</span><br><span class="line">    ye=random.randint(6,9)</span><br><span class="line">    ye=ye/10</span><br><span class="line">    turtle.color(ye,ye*0.9,0)</span><br><span class="line">    turtle.fillcolor(ye+0.1,ye+0.05,0)</span><br><span class="line">    turtle.pensize(1)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.setheading(an+90)</span><br><span class="line">    turtle.forward(8*i)</span><br><span class="line">    px=turtle.xcor()</span><br><span class="line">    py=turtle.ycor()</span><br><span class="line">    turtle.begin_fill()</span><br><span class="line">    turtle.circle(7.5*i,120) #画一段120度弧线</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(px,py) #回到圆点位置</span><br><span class="line">    turtle.setheading(an+90)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.circle(-7.5*i,120)</span><br><span class="line">    turtle.setheading(an+100)</span><br><span class="line">    turtle.circle(10.5*i,150)</span><br><span class="line">    turtle.end_fill()</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x,y)</span><br><span class="line">    turtle.setheading(til)</span><br><span class="line">    turtle.pensize(node/2+1)</span><br><span class="line"></span><br><span class="line">#定义生成树</span><br><span class="line">def draw(node,length,level,yu,button):</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    tu=cos(radians(turtle.heading()+5))/8+0.25</span><br><span class="line">    turtle.pencolor(tu*1.6,tu*1.2,tu*1.4)</span><br><span class="line">    turtle.pensize(node/1.2)</span><br><span class="line">    x=random.randint(0,10)  #生成随机数决定要画树枝还是飘落的叶子</span><br><span class="line"></span><br><span class="line">    if level==top and x&gt;6:</span><br><span class="line">        turtle.forward(length)</span><br><span class="line">        yu[level]=yu[level]-1</span><br><span class="line">        c=random.randint(2,10)</span><br><span class="line">        for i in range(1,c):</span><br><span class="line">            leaf(turtle.xcor(),turtle.ycor(),node)</span><br><span class="line">            if random.random()&gt;0.3:</span><br><span class="line">                turtle.penup()</span><br><span class="line">                #飘落</span><br><span class="line">                t1=turtle.heading()</span><br><span class="line">                an1=-40+random.random()*40</span><br><span class="line">                turtle.setheading(an1)</span><br><span class="line">                dis=int(800*random.random()*0.5+400*random.random()*0.3+200*random.random()*0.2)</span><br><span class="line">                turtle.forward(dis)</span><br><span class="line">                turtle.setheading(t1)</span><br><span class="line">                turtle.right(90)</span><br><span class="line">                #画叶子</span><br><span class="line">                leaf(turtle.xcor(),turtle.ycor(),node)</span><br><span class="line">                turtle.left(90)</span><br><span class="line">                t2=turtle.heading()</span><br><span class="line">                turtle.setheading(an1)</span><br><span class="line">                turtle.backward(dis)</span><br><span class="line">                turtle.setheading(t2)</span><br><span class="line">    elif level==top and x&lt;7:</span><br><span class="line">        turtle.penup()</span><br><span class="line">        turtle.forward(length)</span><br><span class="line">    elif level&gt;3 and x&gt;6:</span><br><span class="line">        turtle.pendown()</span><br><span class="line">        turtle.forward(length)</span><br><span class="line">        c=random.randint(4,6)</span><br><span class="line">        for i in range(3,c):</span><br><span class="line">            leaf(turtle.xcor(),turtle.ycor(),node)</span><br><span class="line">        leaf(turtle.xcor(),turtle.ycor(),node)</span><br><span class="line">        button=1</span><br><span class="line">    else:</span><br><span class="line">        turtle.forward(length)</span><br><span class="line">        yu[level]=yu[level]-1</span><br><span class="line">    if node&gt;0 and button==0:</span><br><span class="line">        right=random.random()*5+17</span><br><span class="line">        left=random.random()*20+19</span><br><span class="line">        child_length=length*(random.random()*0.25+0.7)</span><br><span class="line">        r=random.randint(0,1)</span><br><span class="line">        if r==1:</span><br><span class="line">            turtle.right(right)</span><br><span class="line">            level=level+1</span><br><span class="line">        else:</span><br><span class="line">            turtle.left(right)</span><br><span class="line">            level=level+1</span><br><span class="line">        draw(node-1,child_length,level,yu,button)</span><br><span class="line">        yu[level]=yu[level]+1</span><br><span class="line">        if yu[level]&gt;1:</span><br><span class="line">            if r==1:</span><br><span class="line">                turtle.left(right+left)</span><br><span class="line">                draw(node-1,child_length,level,yu,button)</span><br><span class="line">                turtle.right(left)</span><br><span class="line">                yu[level]=yu[level]-1</span><br><span class="line">            else:</span><br><span class="line">                turtle.right(right + left)</span><br><span class="line">                draw(node - 1, child_length, level, yu, button)</span><br><span class="line">                turtle.left(left)</span><br><span class="line">                yu[level] = yu[level] - 1</span><br><span class="line">        else:</span><br><span class="line">            if r==1:</span><br><span class="line">                turtle.left(right+left)</span><br><span class="line">                turtle.right(left)</span><br><span class="line">            else:</span><br><span class="line">                turtle.right(right+left)</span><br><span class="line">                turtle.left(left)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.backward(length)</span><br><span class="line"></span><br><span class="line">def clear_all():</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(0, 0)</span><br><span class="line">    turtle.color(&#x27;white&#x27;)</span><br><span class="line">    turtle.pensize(800)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.fd(300)</span><br><span class="line">    turtle.bk(600)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 重定位海龟的位置</span><br><span class="line">def go_to(x, y, state):</span><br><span class="line">    turtle.pendown() if state else turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_heart(size):</span><br><span class="line">    turtle.color(&#x27;red&#x27;, &#x27;pink&#x27;)</span><br><span class="line">    turtle.pensize(2)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.setheading(150)</span><br><span class="line">    turtle.begin_fill()</span><br><span class="line">    turtle.fd(size)</span><br><span class="line">    turtle.circle(size * -3.745, 45)</span><br><span class="line">    turtle.circle(size * -1.431, 165)</span><br><span class="line">    turtle.left(120)</span><br><span class="line">    turtle.circle(size * -1.431, 165)</span><br><span class="line">    turtle.circle(size * -3.745, 45)</span><br><span class="line">    turtle.fd(size)</span><br><span class="line">    turtle.end_fill()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 画出发射爱心的小人</span><br><span class="line">def draw_people(x, y):</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.pensize(2)</span><br><span class="line">    turtle.color(&#x27;black&#x27;)</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.circle(60, 360)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.setheading(90)</span><br><span class="line">    turtle.fd(75)</span><br><span class="line">    turtle.setheading(180)</span><br><span class="line">    turtle.fd(20)</span><br><span class="line">    turtle.pensize(4)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.circle(2, 360)</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.fd(40)</span><br><span class="line">    turtle.pensize(4)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.circle(-2, 360)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line">    turtle.setheading(-90)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.fd(20)</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.fd(35)</span><br><span class="line">    turtle.setheading(60)</span><br><span class="line">    turtle.fd(10)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line">    turtle.setheading(-90)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.fd(40)</span><br><span class="line">    turtle.setheading(0)</span><br><span class="line">    turtle.fd(35)</span><br><span class="line">    turtle.setheading(-60)</span><br><span class="line">    turtle.fd(10)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(x, y)</span><br><span class="line">    turtle.setheading(-90)</span><br><span class="line">    turtle.pendown()</span><br><span class="line">    turtle.fd(60)</span><br><span class="line">    turtle.setheading(-135)</span><br><span class="line">    turtle.fd(60)</span><br><span class="line">    turtle.bk(60)</span><br><span class="line">    turtle.setheading(-45)</span><br><span class="line">    turtle.fd(30)</span><br><span class="line">    turtle.setheading(-135)</span><br><span class="line">    turtle.fd(35)</span><br><span class="line">    turtle.penup()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 绘制文字</span><br><span class="line">def draw_text(text, t_color, font_size):</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-350, 0)</span><br><span class="line">    turtle.color(t_color)</span><br><span class="line">    turtle.write(text, font=(&#x27;宋体&#x27;, font_size, &#x27;normal&#x27;))</span><br><span class="line">    time.sleep(2)</span><br><span class="line">    clear_all()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 爱心发射</span><br><span class="line">def draw_():</span><br><span class="line">    turtle.speed(0)</span><br><span class="line">    draw_people(-250, 20)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-150, -30)</span><br><span class="line">    draw_heart(14)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-200, -200)</span><br><span class="line">    turtle.color(&#x27;pink&#x27;)</span><br><span class="line">    turtle.write(&#x27;Biu~&#x27;, font=(&#x27;宋体&#x27;, 60, &#x27;normal&#x27;))</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-40, -60)</span><br><span class="line">    draw_heart(25)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(-28, -200)</span><br><span class="line">    turtle.color(&#x27;pink&#x27;)</span><br><span class="line">    turtle.write(&#x27;Biu~&#x27;, font=(&#x27;宋体&#x27;, 60, &#x27;normal&#x27;))</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(240, -100)</span><br><span class="line">    draw_heart(45)</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.goto(150, -200)</span><br><span class="line">    turtle.color(&#x27;pink&#x27;)</span><br><span class="line">    turtle.write(&#x27;七夕快乐~&#x27;, font=(&#x27;宋体&#x27;, 60, &#x27;normal&#x27;))</span><br><span class="line">    turtle.hideturtle()</span><br><span class="line">    time.sleep(3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    # 隐藏海龟</span><br><span class="line">    hideturtle()</span><br><span class="line">    turtle.setup(width=0.75,height=0.75)</span><br><span class="line"></span><br><span class="line">    draw_text(&quot;Are You Readly？&quot;, &quot;black&quot;, 60)</span><br><span class="line">    draw_text(&quot;接下来&quot;, &quot;skyblue&quot;, 60)</span><br><span class="line">    draw_text(&quot;大飞哥，七夕快乐！&quot;, &quot;pink&quot;, 80)</span><br><span class="line">    draw_()</span><br><span class="line">    # 使用mainloop防止窗口卡死</span><br><span class="line">    mainloop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#主函数</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    turtle.setup(width=0.75,height=0.75)             #屏幕占比</span><br><span class="line">    turtle.hideturtle()                            #隐藏</span><br><span class="line">    turtle.speed(0)                              #速度0-9</span><br><span class="line">    turtle.penup()</span><br><span class="line">    turtle.left(90)</span><br><span class="line">    turtle.backward(300)</span><br><span class="line">    top=9</span><br><span class="line">    yu=FR(top)</span><br><span class="line">    yu.remove(yu[0])</span><br><span class="line">    button=0</span><br><span class="line">    draw(9,120,0,yu,button)</span><br><span class="line">    turtle.write(&quot;    相爱相知!\n&quot;,font=(&quot;微软雅黑&quot;,14,&quot;normal&quot;))</span><br><span class="line">    turtle.write(&quot;  ~七夕快乐！&quot;,font=(&quot;微软雅黑&quot;,16,&quot;normal&quot;))</span><br><span class="line">    time.sleep(3)</span><br><span class="line">    turtle.clear()</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>Python运行就好啦，拿走不谢~~</p>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>blessing</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode连接远程机器上的docker</title>
    <url>/2022/08/02/ubuntuOS/vscode-docker-otherPC/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>vscode连接远程机器上的docker，应用太多了，此处是用win的PC连接远程ubuntuPC上运行的docker，docker是基于ubuntu20.04的博客部署系统~</p>
<p><strong>又一个利器~</strong><br><span id="more"></span></p>
<h2 id="1，前提"><a href="#1，前提" class="headerlink" title="1，前提"></a>1，前提</h2><p>宿主机连接本机上运行的docker，并用vscode连接，详见<a href="https://sophia-hxw.github.io/2022/08/01/ubuntuOS/vscode-docker-onePC/">宿主机上vscode连接本机docker</a></p>
<h2 id="2，docker上的ssh配置"><a href="#2，docker上的ssh配置" class="headerlink" title="2，docker上的ssh配置"></a>2，docker上的ssh配置</h2><p>在docker中修改sshd服务的配置文件，即<code>/etc/ssh/sshd_config</code>，将端口号的配置项去掉注释即可。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Port 22</span><br></pre></td></tr></table></figure><br>注意：docker使用的ssh默认的端口号22，映射到主机上的端口号是8022（详情见<a href="https://sophia-hxw.github.io/2022/08/01/ubuntuOS/vscode-docker-onePC/">宿主机上vscode连接本机docker</a>）</p>
<h2 id="3，远程主机-非docker-开放端口"><a href="#3，远程主机-非docker-开放端口" class="headerlink" title="3，远程主机(非docker)开放端口"></a>3，远程主机(非docker)开放端口</h2><p>此处主机上开放的端口是8022，也就是<code>docker run</code>时用来映射docker内22的端口，就不赘述了~<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo iptables -I INPUT -p tcp --dport 8022 -j ACCEPT</span><br></pre></td></tr></table></figure></p>
<h2 id="4，测试连接"><a href="#4，测试连接" class="headerlink" title="4，测试连接"></a>4，测试连接</h2><ul>
<li>配置免密访问<br>密码登录请自行测试，为了方便，本文采用了密钥对来免密访问。</li>
</ul>
<p>复制本机（我的路径是C:\Users\xinwen.ssh\id_rsa.pub）的公钥到docker的可信赖机器中（/root/.ssh/authorized_keys）。</p>
<ul>
<li>ssh连接测试<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 本机连接远程主机</span><br><span class="line">ssh xinwen@192.168.1.18</span><br><span class="line"></span><br><span class="line"># 本机连接远程主机上的docker</span><br><span class="line">ssh root@192.168.1.18 -p 8022</span><br></pre></td></tr></table></figure>
其中，192.168.1.18是远程主机的ip，xinwen是远程主机上的用户，8022是远程主机上开放给docker的端口，root是docker中部署了hexo博客系统的用户。</li>
</ul>
<h2 id="5，本机vscode上的插件"><a href="#5，本机vscode上的插件" class="headerlink" title="5，本机vscode上的插件"></a>5，本机vscode上的插件</h2><p>Remote-SSH</p>
<p>登录命令<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh root@192.168.1.18 -p 8022</span><br></pre></td></tr></table></figure></p>
<h2 id="6，总结"><a href="#6，总结" class="headerlink" title="6，总结"></a>6，总结</h2><p>docker(端口22)-&gt;远程主机(端口8022)-&gt;本机</p>
<p>本文中对应的系统：<br>ubuntu20.04-&gt;ubuntu20.04-&gt;windows</p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode连接本机docker</title>
    <url>/2022/08/01/ubuntuOS/vscode-docker-onePC/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>vscode连接docker方便开发，此处主要用在hexo的博客编写，很方便~</p>
<p><strong>安心码文了~</strong><br><span id="more"></span></p>
<h2 id="1，docker启动"><a href="#1，docker启动" class="headerlink" title="1，docker启动"></a>1，docker启动</h2><h3 id="1-1-blog镜像的配置"><a href="#1-1-blog镜像的配置" class="headerlink" title="1.1 blog镜像的配置"></a>1.1 blog镜像的配置</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM ubuntu:20.04</span><br><span class="line"></span><br><span class="line">WORKDIR /home</span><br><span class="line">LABEL maintainer=&quot;XINWEN&quot;</span><br><span class="line">RUN mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">RUN echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list </span><br><span class="line"></span><br><span class="line">ENV TZ=Asia/Shanghai</span><br><span class="line">RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get upgrade -y</span><br><span class="line"></span><br><span class="line">RUN DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends apt-utils \</span><br><span class="line">    &amp;&amp; apt-get -y install dialog</span><br><span class="line"></span><br><span class="line">RUN apt-get -y install dialog clang-format nfs-common \</span><br><span class="line">    &amp;&amp; apt-get -y install openssh-server \</span><br><span class="line">    &amp;&amp; apt-get -y install vim python3-pip git curl</span><br><span class="line"></span><br><span class="line">RUN pip3 install -i http://mirrors.aliyun.com/pypi/simple -U pip --trusted-host mirrors.aliyun.com \</span><br><span class="line">    &amp;&amp; pip3 config set global.index-url http://mirrors.aliyun.com/pypi/simple/ \</span><br><span class="line">    &amp;&amp; pip3 config set install.trusted-host mirrors.aliyun.com</span><br><span class="line"></span><br><span class="line">RUN apt-get install -y nodejs npm</span><br><span class="line"></span><br><span class="line">RUN node -v \</span><br><span class="line">    &amp;&amp; npm install n -g \</span><br><span class="line">    &amp;&amp; n stable \</span><br><span class="line">    &amp;&amp; hash -r \</span><br><span class="line">    &amp;&amp; cp /usr/local/bin/node /usr/bin/node \</span><br><span class="line">    &amp;&amp; node -v</span><br><span class="line"></span><br><span class="line">RUN mkdir /var/run/sshd</span><br><span class="line">EXPOSE 22</span><br><span class="line">CMD [&quot;/user/sbin/ssh&quot;,&quot;-D&quot;]</span><br><span class="line"></span><br><span class="line">ENTRYPOINT /bin/bash</span><br></pre></td></tr></table></figure>
<p>构建命令，在Dockerfile的同目录运行：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -f Dockerfile -t blog .</span><br></pre></td></tr></table></figure></p>
<h3 id="1-2-创建容器"><a href="#1-2-创建容器" class="headerlink" title="1.2 创建容器"></a>1.2 创建容器</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it --name=xinwen -p 4000:4000 -p 8022:22 -v /home/xinwen/xinwenblog:/xinwen blog</span><br></pre></td></tr></table></figure>
<h2 id="2，宿主机和docker中关于ssh的配置"><a href="#2，宿主机和docker中关于ssh的配置" class="headerlink" title="2，宿主机和docker中关于ssh的配置"></a>2，宿主机和docker中关于ssh的配置</h2><h3 id="2-1-docker允许远程登录"><a href="#2-1-docker允许远程登录" class="headerlink" title="2.1 docker允许远程登录"></a>2.1 docker允许远程登录</h3><p>在docker中修改sshd服务的配置文件，在<code>/etc/ssh/sshd_config</code>位置，将下面的配置项去掉注释即可。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PermitRootLogin yes</span><br><span class="line">PubkeyAuthentication yes    </span><br><span class="line">PasswordAuthentication yes</span><br></pre></td></tr></table></figure><br>保存后重启sshd服务，也就是：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ssh restart</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-宿主机查看docker的ip"><a href="#2-2-宿主机查看docker的ip" class="headerlink" title="2.2 宿主机查看docker的ip"></a>2.2 宿主机查看docker的ip</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker inspect xinwen</span><br></pre></td></tr></table></figure>
<p>在宿主机上运行，xinwen是docker别名。</p>
<h3 id="2-3-登录宿主机进行连接验证"><a href="#2-3-登录宿主机进行连接验证" class="headerlink" title="2.3 登录宿主机进行连接验证"></a>2.3 登录宿主机进行连接验证</h3><p>密码登录或者ssh免密登录方式都可以，二者择一即可。</p>
<ul>
<li><p>密码登录<br>修改docker的root用户密码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">passwd</span><br></pre></td></tr></table></figure>
</li>
<li><p>ssh免密登录<br>宿主机上生成自己的秘钥对，邮箱也相应更改为自己的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;hxinwen1218@sina.com&quot;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>复制宿主机的公钥到docker中<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -i /home/xinwen/.ssh/id_rsa.pub root@172.17.0.2</span><br></pre></td></tr></table></figure><br>root是docker的用户，<code>172.17.0.2</code>是docker的ip，<code>/home/xinwen/.ssh/id_rsa.pub</code>是宿主机的公钥。</p>
<ul>
<li>验证登录<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh root@172.17.0.2</span><br></pre></td></tr></table></figure>
输出下面信息就成功了<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Welcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.15.0-41-generic x86_64)</span><br><span class="line"></span><br><span class="line"> * Documentation:  https://help.ubuntu.com</span><br><span class="line"> * Management:     https://landscape.canonical.com</span><br><span class="line"> * Support:        https://ubuntu.com/advantage</span><br><span class="line"></span><br><span class="line">This system has been minimized by removing packages and content that are</span><br><span class="line">not required on a system that users do not log into.</span><br><span class="line"></span><br><span class="line">To restore this content, you can run the &#x27;unminimize&#x27; command.</span><br><span class="line">Last login: Mon Aug  1 17:55:51 2022 from 172.17.0.1</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3，VSCODE插件"><a href="#3，VSCODE插件" class="headerlink" title="3，VSCODE插件"></a>3，VSCODE插件</h2><p>主要是两个插件<code>Docker</code>和<code>Remote-Containers</code></p>
<p>点击左下角”&gt;&lt;“标志，在顶部的输入下拉框选择<code>Attach to Running Container...</code>就能打开docker的home目录。</p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo上的next主题优化项</title>
    <url>/2022/08/01/ubuntuOS/hexo-next-theme/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>hexo上next的主题优化项，选择了一些个人的偏爱项，基本就是当前博客的展示效果了~</p>
<p><strong>安心码文了~</strong><br><span id="more"></span></p>
<h2 id="1，下载主题文件"><a href="#1，下载主题文件" class="headerlink" title="1，下载主题文件"></a>1，下载主题文件</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd hexo #本文blog的主目录的名称为hexo</span><br><span class="line"></span><br><span class="line">git clone https://github.com/theme-next/hexo-theme-next themes/next</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2，主题优化"><a href="#2，主题优化" class="headerlink" title="2，主题优化"></a>2，主题优化</h2><h3 id="2-1-显示本地图片"><a href="#2-1-显示本地图片" class="headerlink" title="2.1 显示本地图片"></a>2.1 显示本地图片</h3><ul>
<li><p>安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-marked</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改站点文件<code>_config.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 文章资源文件夹</span><br><span class="line">post_asset_folder: true</span><br><span class="line"># 以下内容需要添加</span><br><span class="line">marked:</span><br><span class="line">  prependRoot: true</span><br><span class="line">  postAsset: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>引用图片的方式<br>在文章的同级目录放置与文章同名的文件夹来存放需要在本文中展示的图片</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用这种方式引用图片</span><br><span class="line">![](image.jpg)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-2-图片放大预览功能"><a href="#2-2-图片放大预览功能" class="headerlink" title="2.2 图片放大预览功能"></a>2.2 图片放大预览功能</h3><ul>
<li>修改<code>next.yml</code>文件<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fancybox: true #添加图片放大预览功能</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-3-添加关于、标签、分类页面"><a href="#2-3-添加关于、标签、分类页面" class="headerlink" title="2.3 添加关于、标签、分类页面"></a>2.3 添加关于、标签、分类页面</h3><ul>
<li><p>修改主题配置文件<code>next.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 去除下面的注释</span><br><span class="line">menu:</span><br><span class="line">  home: / || fa fa-home</span><br><span class="line">  about: /about/ || fa fa-user</span><br><span class="line">  tags: /tags/ || fa fa-tags</span><br><span class="line">  categories: /categories/ || fa fa-th</span><br><span class="line">  archives: /archives/ || fa fa-archive</span><br></pre></td></tr></table></figure>
</li>
<li><p>新建文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new page &quot;about&quot;</span><br><span class="line">hexo new page &quot;tags&quot;</span><br><span class="line">hexo new page &quot;categories&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改文件和配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source\about\index.md</span><br><span class="line">source\tags\index.md</span><br><span class="line">source\categories\index.md</span><br></pre></td></tr></table></figure>
<p>相应文件内修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 关于</span><br><span class="line">type: &quot;about&quot;</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">title: 标签</span><br><span class="line">type: &quot;tags&quot;</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">title: 分类</span><br><span class="line">type: &quot;categories&quot;</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-4-添加搜索功能"><a href="#2-4-添加搜索功能" class="headerlink" title="2.4 添加搜索功能"></a>2.4 添加搜索功能</h3><ul>
<li><p>安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure>
</li>
<li><p>站点文件<code>_config.yml</code>添加搜索</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-5-回到顶部按钮"><a href="#2-5-回到顶部按钮" class="headerlink" title="2.5 回到顶部按钮"></a>2.5 回到顶部按钮</h3><p>修改配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">back2top:</span><br><span class="line">  enable: true</span><br><span class="line">  # 将返回按钮设置在侧边栏</span><br><span class="line">  sidebar: false</span><br><span class="line">  # 按钮上显示进度百分比</span><br><span class="line">  scrollpercent: true</span><br></pre></td></tr></table></figure></p>
<h3 id="2-6-字数统计和预计阅读时间"><a href="#2-6-字数统计和预计阅读时间" class="headerlink" title="2.6 字数统计和预计阅读时间"></a>2.6 字数统计和预计阅读时间</h3><ul>
<li><p>安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-symbols-count-time --save</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改站点配置文件<code>_config.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  symbols: true  # 文章字数</span><br><span class="line">  time: true  # 阅读时长</span><br><span class="line">  total_symbols: true  # 所有文章总字数</span><br><span class="line">  total_time: true  # 所有文章阅读中时长</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改主题配置文件<code>next.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: true # 是否换行显示 字数统计 及 阅读时长</span><br><span class="line">  item_text_post: true  # 文章 字数统计 阅读时长 使用图标 还是 文本表示</span><br><span class="line">  item_text_total: true # 博客底部统计 字数统计 阅读时长 使用图标 还是 文本表示</span><br><span class="line">  awl: 4</span><br><span class="line">  wpm: 275</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-7-代码块复制和代码高亮"><a href="#2-7-代码块复制和代码高亮" class="headerlink" title="2.7 代码块复制和代码高亮"></a>2.7 代码块复制和代码高亮</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">codeblock:</span><br><span class="line">  # 代码高亮</span><br><span class="line">  highlight_theme: night bright</span><br><span class="line">  # 复制</span><br><span class="line">  copy_button:</span><br><span class="line">    enable: true</span><br><span class="line">    # 显示文本复制结果</span><br><span class="line">    show_result: true</span><br><span class="line">    # 可以选择的样式: default | flat | mac</span><br><span class="line">    style: mac</span><br></pre></td></tr></table></figure></p>
<h3 id="2-8-文章内链接文本样式"><a href="#2-8-文章内链接文本样式" class="headerlink" title="2.8 文章内链接文本样式"></a>2.8 文章内链接文本样式</h3><ul>
<li>在<code>hexo/source/_data</code>中新增样式文件<code>styles.styl</code><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 修改链接样式</span><br><span class="line">.post-body p a&#123;</span><br><span class="line">  color: #0593d3;</span><br><span class="line">  border-bottom: none;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    color: #ff106c;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">a, span.exturl &#123;</span><br><span class="line">  border-bottom: none;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    color: #ff106c;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>修改主题配置文件<code>next.yml</code>，去掉<code>styles.styl</code>的注释<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">style: source/_data/styles.styl</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-9-文章底部增加版权信息"><a href="#2-9-文章底部增加版权信息" class="headerlink" title="2.9 文章底部增加版权信息"></a>2.9 文章底部增加版权信息</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">creative_commons:</span><br><span class="line">  license: by-nc-sa</span><br><span class="line">  sidebar: false # 不显示在侧边栏</span><br><span class="line">  post: true</span><br><span class="line">  language:</span><br></pre></td></tr></table></figure></p>
<h3 id="2-10-文章底部tag图标"><a href="#2-10-文章底部tag图标" class="headerlink" title="2.10 文章底部tag图标"></a>2.10 文章底部tag图标</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tag_icon: true</span><br></pre></td></tr></table></figure></p>
<h3 id="2-11-侧边栏文章目录设置"><a href="#2-11-侧边栏文章目录设置" class="headerlink" title="2.11 侧边栏文章目录设置"></a>2.11 侧边栏文章目录设置</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  enable: true</span><br><span class="line">  # 自动编号</span><br><span class="line">  number: false</span><br><span class="line">  # 超出宽度跨行</span><br><span class="line">  wrap: true</span><br><span class="line">  # 展开所有</span><br><span class="line">  expand_all: true</span><br><span class="line">  # 最大标题深度</span><br><span class="line">  max_depth: 6</span><br></pre></td></tr></table></figure></p>
<h3 id="2-12-侧边社交链接"><a href="#2-12-侧边社交链接" class="headerlink" title="2.12 侧边社交链接"></a>2.12 侧边社交链接</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">social:</span><br><span class="line">  GitHub: https://github.com/sophia-hxw || fab fa-github</span><br><span class="line">  CSDN: https://blog.csdn.net/sophia_xw || crosshairs</span><br><span class="line">  E-Mail: mailto:xinwen618@gmail.com || fa fa-envelope</span><br></pre></td></tr></table></figure></p>
<h3 id="2-13-文章置顶"><a href="#2-13-文章置顶" class="headerlink" title="2.13 文章置顶"></a>2.13 文章置顶</h3><ul>
<li><p>安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-index-pin-top --save</span><br></pre></td></tr></table></figure>
<p>其中，hexo-generator-index是hexo默认的文章排序插件，hexo-generator-index-pin-top是替换掉默认的排序插件，且有置顶功能。</p>
</li>
<li><p>需要置顶的文章中添加置顶项</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: hexo上文章怎么置顶？</span><br><span class="line">date: 2022-06-22 23:28:09</span><br><span class="line">tags: </span><br><span class="line">- hexo</span><br><span class="line">categories:</span><br><span class="line">- ubuntuOS</span><br><span class="line">top: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加文章置顶标志<br>打开<code>/blog/themes/next/layout/_macro</code>目录下的<code>post.swig</code>文件，在<code>&lt;div class=&quot;post-meta&quot;&gt;</code>标签下添加下面内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% if post.top %&#125;</span><br><span class="line">  &lt;i class=&quot;fa fa-thumb-tack&quot;&gt;&lt;/i&gt;</span><br><span class="line">  &lt;font color=7D26CD&gt;置顶&lt;/font&gt;</span><br><span class="line">  &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-14-专业数学公式引擎"><a href="#2-14-专业数学公式引擎" class="headerlink" title="2.14 专业数学公式引擎"></a>2.14 专业数学公式引擎</h3><p>Hexo默认使用”hexo-renderer-marked”引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线<code>_</code>代表斜体，会被渲染引擎处理为<code>&lt;em&gt;</code>标签。</p>
<p>类似的语义冲突的符号还包括<code>&#39;*&#39;, &#39;&#123;&#39;, &#39;&#125;&#39;, &#39;\&#39;</code>等。</p>
<ul>
<li>安装依赖<br>更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<ul>
<li>修改新渲染引擎的歧义<br>然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为hexo-renderer-kramed引擎也有语义冲突的问题。</li>
</ul>
<p>接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//  escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">  escape: /^\\([`*\[\]()#$+\-.!_&gt;])/</span><br></pre></td></tr></table></figure>
<p>这一步是在原基础上取消了对\,{,}的转义(escape)。<br>同时把第20行的em变量也要做相应的修改。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//  em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">  em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><br></pre></td></tr></table></figure>
<p>再重新启动hexo，若没有解决问题就需要打开主题中mathjax的支持了</p>
<ul>
<li><p>修改主题配置文件<code>next.yml</code><br>进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># MathJax Support</span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: true</span><br></pre></td></tr></table></figure>
</li>
<li><p>文章使用渲染工具<br>在文章的Front-matter里打开mathjax开关，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: index.html</span><br><span class="line">date: 2016-12-28 21:01:30</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">--</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-15-主题布局改为圆角"><a href="#2-15-主题布局改为圆角" class="headerlink" title="2.15 主题布局改为圆角"></a>2.15 主题布局改为圆角</h3><ul>
<li><p>在 <code>hexo/source/_data</code> 目录下新建 <code>variables.styl</code> 文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 圆角设置</span><br><span class="line">$border-radius-inner     = 20px 20px 20px 20px;</span><br><span class="line">$border-radius           = 20px;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改主题配置文件<code>next.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 去掉注释</span><br><span class="line">variable: source/_data/variables.styl</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-16-设置博客背景图片"><a href="#2-16-设置博客背景图片" class="headerlink" title="2.16 设置博客背景图片"></a>2.16 设置博客背景图片</h3><ul>
<li><p>背景图片位置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo/source/images </span><br></pre></td></tr></table></figure>
</li>
<li><p>修改<code>hexo/source/_data/styles.styl</code>样式代码<br>// 设置背景图片<br>body {<br>  background:url(/images/background.jpg);<br>  background-repeat: no-repeat;<br>  background-attachment:fixed; //不重复<br>  background-size: cover;      //填充<br>  background-position:50% 50%;<br>}</p>
</li>
</ul>
<h3 id="2-17-设置博客文章透明度"><a href="#2-17-设置博客文章透明度" class="headerlink" title="2.17 设置博客文章透明度"></a>2.17 设置博客文章透明度</h3><p>修改<code>hexo/source/_data/styles.styl</code>样式代码<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//博客内容透明化</span><br><span class="line">//文章内容的透明度设置</span><br><span class="line">.content-wrap &#123;</span><br><span class="line">  opacity: 0.9;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//侧边框的透明度设置</span><br><span class="line">.sidebar &#123;</span><br><span class="line">  opacity: 0.9;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//菜单栏的透明度设置</span><br><span class="line">.header-inner &#123;</span><br><span class="line">  background: rgba(255,255,255,0.9);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//搜索框（local-search）的透明度设置</span><br><span class="line">.popup &#123;</span><br><span class="line">  opacity: 0.9;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="2-18-顶部阅读进度条"><a href="#2-18-顶部阅读进度条" class="headerlink" title="2.18 顶部阅读进度条"></a>2.18 顶部阅读进度条</h3><p>修改主题配置文件<code>next.yml</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">reading_progress:</span><br><span class="line">  enable: true</span><br><span class="line">  # 显示在顶部</span><br><span class="line">  position: top</span><br><span class="line">  color: &quot;#06d633&quot;</span><br><span class="line">  height: 3px</span><br></pre></td></tr></table></figure></p>
<h3 id="2-19-文章阴影"><a href="#2-19-文章阴影" class="headerlink" title="2.19 文章阴影"></a>2.19 文章阴影</h3><p>在<code>hexo/source/_data/styles.styl</code>中添加样式代码<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 文章阴影</span><br><span class="line">.post &#123;</span><br><span class="line">   margin-top: 50px;</span><br><span class="line">   margin-bottom: 50px;</span><br><span class="line">   padding: 25px;</span><br><span class="line">   -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5);</span><br><span class="line">   -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="2-20-修改分类页面样式"><a href="#2-20-修改分类页面样式" class="headerlink" title="2.20 修改分类页面样式"></a>2.20 修改分类页面样式</h3><p>添加<code>hexo/source/_data/styles.styl</code>样式代码<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 分类&amp;&amp;标签 页面样式</span><br><span class="line">// 分类&amp;&amp;标签 页面样式</span><br><span class="line">.post-block.page &#123;</span><br><span class="line">    margin-top: 40px;</span><br><span class="line">&#125;</span><br><span class="line">// 分类页面page</span><br><span class="line">.category-all-page &#123;</span><br><span class="line">    box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.4);</span><br><span class="line">    background-color: #797D7F;</span><br><span class="line">    padding: 20px 30px 60px 30px;</span><br><span class="line">    border-radius: 25px 25px 25px 25px;</span><br><span class="line">&#125;</span><br><span class="line">.category-all-title &#123;</span><br><span class="line">    font-family: Impact;</span><br><span class="line">    font-size: 24px;</span><br><span class="line">    color: aqua;</span><br><span class="line">&#125;</span><br><span class="line">.category-list &#123;</span><br><span class="line">    overflow: auto;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li &#123;</span><br><span class="line">    height: 100%;</span><br><span class="line">    float: left;</span><br><span class="line">    border-right: 3px solid #222;</span><br><span class="line">    padding: 0 20px;</span><br><span class="line">&#125;</span><br><span class="line">.category-all ul li &#123;</span><br><span class="line">    list-style: none!important;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li:last-child &#123;</span><br><span class="line">    border-right: none;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li a &#123;</span><br><span class="line">    font-size: 16px;</span><br><span class="line">    text-decoration: none;</span><br><span class="line">    color: aqua;</span><br><span class="line">    font-family: Helvetica, Verdana, sans-serif;</span><br><span class="line">    text-transform: uppercase;</span><br><span class="line">    -webkit-transition: all 0.5s ease;</span><br><span class="line">    -moz-transition: all 0.5s ease;</span><br><span class="line">    -o-transition: all 0.5s ease;</span><br><span class="line">    -ms-transition: all 0.5s ease;</span><br><span class="line">    transition: all 0.5s ease;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li a:hover &#123;</span><br><span class="line">    color: black;</span><br><span class="line">&#125;</span><br><span class="line">.category-list li.active a &#123;</span><br><span class="line">    font-weight: bold;</span><br><span class="line">    color: black;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="3，总结"><a href="#3，总结" class="headerlink" title="3，总结"></a>3，总结</h2><p>以上是本博客中已经部署的样式，可以按需选用~</p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>PIFuHD model</title>
    <url>/2022/07/28/3D_reconstruction/PIFuHD/</url>
    <content><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>title: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization</p>
<p><a href="https://arxiv.org/abs/2004.00452">essay link</a></p>
<p><strong>PIFuHD是PIFu原作者的最新工作，发表于CVPR2020(oral)，论文主要是基于单张RGB实现3D人体重建，效果非常惊艳~</strong><br><span id="more"></span></p>
]]></content>
      <categories>
        <category>3DReconstruction</category>
      </categories>
      <tags>
        <tag>PIFuHD</tag>
      </tags>
  </entry>
  <entry>
    <title>XR, VR和AR的概念区别</title>
    <url>/2022/07/26/other/xr-vr-ar/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>元宇宙与人机交互中的几个基本概念，比如XR，VR，AR等，你能正确理解么？</p>
<p><strong>元宇宙，新世界啦~</strong><br><span id="more"></span></p>
<h2 id="XR"><a href="#XR" class="headerlink" title="XR"></a>XR</h2><p>Extended Reality，拓展现实。</p>
<p>指通过计算机将现实与虚拟相结合，打造一个人机交互的虚拟环境，是VR，AR和MR等技术的统称。</p>
<h2 id="VR"><a href="#VR" class="headerlink" title="VR"></a>VR</h2><p>Virtual Reality，虚拟现实。</p>
<p>模拟一个虚拟世界，利用计算机生成一种模拟环境，使用户利用设备沉浸到该环境中，让人有种身临其境的感觉，强调用户与虚拟世界的实时交互，带来封闭式、沉浸式的虚拟世界体验，这个虚拟世界不是我们直接就能看到的，而是利用设备（如VR眼镜）才能看到，所以称为“虚拟现实”。</p>
<h2 id="AR"><a href="#AR" class="headerlink" title="AR"></a>AR</h2><p>Augmented Reality，增强现实。</p>
<p>一种将现实世界信息和虚拟世界信息“无缝”集成的新技术，它把原本在现实世界的一定时间范围内很难体验到的实体信息（视觉、听觉、味觉、触觉、嗅觉等信息）通过计算机等科学技术，模拟仿真后再叠加到现实世界，被人类感官所感知，从而达到了超越现实的感官体验，现实环境和虚拟的物体实时地叠加到了同一个画面或空间同时存在，从而实现对现实世界的“增强”，所以称之为“增强现实”。</p>
<h2 id="MR"><a href="#MR" class="headerlink" title="MR"></a>MR</h2><p>Mixed Reality，混合现实。</p>
<p>混合现实技术是虚拟现实技术的进一步发展，该技术通过在现实场景呈现虚拟场景信息，在现实世界、虚拟世界和用户之间搭起一个交互反馈的信息回路，以增强用户体验的真实感。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单地说，VR的画面都是假的，AR的画面是一半真的，一半假的。</p>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>XR</tag>
      </tags>
  </entry>
  <entry>
    <title>BlockChain Example</title>
    <url>/2022/07/24/leshine/exam1/</url>
    <content><![CDATA[<h1 id="Turtleneck-dress"><a href="#Turtleneck-dress" class="headerlink" title="Turtleneck dress"></a>Turtleneck dress</h1><span id="more"></span>
<p><img src="show.jpg" alt="img"></p>
<h2 id="Designer"><a href="#Designer" class="headerlink" title="Designer"></a>Designer</h2><p><a href="https://leshinee.com/collections/jade-abbot">Jade Abbot</a></p>
<h2 id="Patterning"><a href="#Patterning" class="headerlink" title="Patterning"></a>Patterning</h2><p>Zhao Chuan<br>2022.05.20</p>
<h2 id="Sample-Garment"><a href="#Sample-Garment" class="headerlink" title="Sample Garment"></a>Sample Garment</h2><p>Li Jun<br>2022.05.23</p>
<h2 id="Major-Fabric-Origin"><a href="#Major-Fabric-Origin" class="headerlink" title="Major Fabric Origin"></a>Major Fabric Origin</h2><p>Hangzhou, China</p>
<h2 id="Manufacturer"><a href="#Manufacturer" class="headerlink" title="Manufacturer"></a>Manufacturer</h2><p>Guangzhou Chaoyang Garment<br>2022.06.01</p>
<h2 id="Quality-Control"><a href="#Quality-Control" class="headerlink" title="Quality Control"></a>Quality Control</h2><p>Shanghai Quality Inspection Center<br>2022.06.03</p>
<h2 id="Warehouse"><a href="#Warehouse" class="headerlink" title="Warehouse"></a>Warehouse</h2><p>London center. UK<br>2022.06.05</p>
]]></content>
      <categories>
        <category>leshine</category>
      </categories>
      <tags>
        <tag>blockchain</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之独立成分分析（note11）</title>
    <url>/2022/07/22/cs229/note11/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note11的主要内容：。。。</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>liquid学习资源</title>
    <url>/2022/07/20/frontend/liquid-intro/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Liquid 是一门灵活、安全的模版语言，被用于许多不同环境中。Liquid 被创建之初是用在 Shopify 商店系统中的，后来也被广泛用于 Jekyll 网站中。随着时间的推移，Shopify 和 Jekyll 分别为 Liquid 添加了针对各自用途的对象（object）、标记（tag）和过滤器（filter）。目前最流行的 Liquid 版本包括 Liquid、Shopify Liquid 和 Jekyll Liquid。</p>
<p><a href="https://liquid.bootcss.com/basics/introduction/">websit</a></p>
<p><strong>学习新内容了，要加油哦~</strong><br><span id="more"></span></p>
<h2 id="1，简介"><a href="#1，简介" class="headerlink" title="1，简介"></a>1，简介</h2><p>对象告诉Liquid 在页面的哪个位置展示内容。对象和变量名由双花括号标识：$\{\{ … \}\}$。</p>
<p>标记创造了模板的逻辑和控制流。他们由单括号加百分号标识：$\{\% … \%\}$。<br>标记并不产生任何可见的文本输出。这意味着你可以用他们为变量赋值、创建条件和循环逻辑，并且不在页面上显示出任何 Liquid 逻辑代码。</p>
<p>标记的作用：</p>
<ul>
<li>控制流</li>
<li>迭代</li>
<li>变量赋值</li>
</ul>
<p>过滤器改变Liquid对象的输出。他们被用在输出上，通过一个 $|$ 符号分隔。</p>
<h2 id="2，操作符"><a href="#2，操作符" class="headerlink" title="2，操作符"></a>2，操作符</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">==</span><br><span class="line">!=</span><br><span class="line">&lt;</span><br><span class="line">&lt;=</span><br><span class="line">&gt;</span><br><span class="line">&gt;=</span><br><span class="line">or</span><br><span class="line">and</span><br></pre></td></tr></table></figure>
<p><strong>contains</strong>操作符作用：</p>
<ul>
<li>检查在一个字符串中是否存在某个子串。</li>
<li>用于检查一个字符串数组中是否存在某个字符串。</li>
</ul>
<p>Note: 只能用于搜索字符串。你不能将其用于从一个对象数组中检查是否存在某个对象。</p>
<h2 id="3，真值和假值"><a href="#3，真值和假值" class="headerlink" title="3，真值和假值"></a>3，真值和假值</h2>]]></content>
      <categories>
        <category>frontend</category>
      </categories>
      <tags>
        <tag>liquid</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之感知器和大边界分类器（note6）</title>
    <url>/2022/07/12/cs229/note6/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note6的主要内容：感知器和大边界分类器</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="感知器和大边界分类器"><a href="#感知器和大边界分类器" class="headerlink" title="感知器和大边界分类器"></a>感知器和大边界分类器</h2><p>本章是讲义中关于学习理论的最后一部分，我们来介绍另一种机器学习模式。在之前的内容中，我们考虑的都是批量学习的情况，即给了我们训练样本集用于学习，然后用学习得到的假设 $h $来评估和判别测试数据。在本章，我们要讲一种新的机器学习模式：在线学习，这种情况下，我们的学习算法要在进行学习的同时给出预测。</p>
<p>学习算法会获得一个样本序列，其中内容为有次序的学习样本，$(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), …(x^{(m)},y^{(m)}) $。最开始获得的就是 $x^{(1)} $，然后需要预测 $y^{(1)} $。在完成了这个预测之后，再把 $y^{(1)} $ 的真实值告诉给算法（然后算法就利用这个信息来进行某种学习了）。接下来给算法提供 $x^{(2)} $，再让算法对 $y^{(2)} $ 进行预测，然后再把 $y^{(2)} $ 的真实值告诉给算法，这样算法就又能学习到一些信息了。这样的过程一直持续到最末尾的样本 $(x^{(m)},y^{(m)}) $。在这种在线学习的背景下，我们关心的是算法在此过程中出错的总次数。因此，这适合需要一边学系一边给出预测的应用情景。</p>
<p>接下来，我们将对感知器学习算法的在线学习误差给出一个约束。为了让后续的推导更容易，我们就用正负号来表征分类标签，即设 $y \in \{−1, 1\} $。<br>回忆一下感知器算法（在第二章中有讲到），其参数 $\theta \in \mathbb{R}^{n+1} $，该算法据下面的方程来给出预测： </p>
<script type="math/tex; mode=display">h_{\theta}(x) = g(\theta^{T}x)</script><p>其中：</p>
<script type="math/tex; mode=display">g(z) = \left \{\begin{array}{ll} 1 & if \quad z\geq 0 \\ -1 & if \quad z<0 \end{array} \right.</script><p>然后，给定一个训练样本 $(x, y) $，感知器学习规则就按照如下所示来进行更新。如果 $h_{\theta}(x) = y $，那么不改变参数。若二者相等关系不成立，则进行更新。</p>
<script type="math/tex; mode=display">\theta := \theta +yx</script><p>当感知器算法作为在线学习算法运行的时候，每次对样本给出错误判断的时候，则更新参数，下面的定理给出了这种情况下的在线学习误差的约束边界。要注意，下面的错误次数的约束边界与整个序列中样本的个数 $m $不具有特定的依赖关系，和输入特征的维度 $n $也无关。</p>
<p><strong>定理</strong> (Block, 1962, and Novikoff, 1962)。设有一个样本序列：$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots (x^{(m)}, y^{(m)}) $。假设对于所有的 $i $，都有 $||x^{(i)}|| ≤ \mathcal{D} $，更进一步存在一个单位长度向量 $u, \quad ||u||_{2} = 1 $ 对序列中的所有样本都满足 $y^{(i)}\cdot (u^{T} x^{(i)}) \geq \gamma $（例如，$u^{T} x^{(i)} \geq \gamma\quad if\quad y^{(i)} = 1 $, 而 $u^{T}x^{(i)} \leq −\gamma $，若 $y^{(i)} = −1 $，则 $u $就以一个宽度至少为 $\gamma $的边界分开了样本数据。）而此感知器算法针对这个序列给出错误预测的综述的上限为 $(\mathcal{D}/\gamma)^{2} $。</p>
<p><strong>证明</strong>感知器算法每次只针对出错的样本进行权重更新，设 $\theta^{(k)} $ 为犯了第 $k $个错误的时候的权重。则 $\theta^{(1)} =\vec{0}  $（因为初始权重为零），若第 $k $个错误发生在样本 $(x^{(i)}, y^{(i)}) $，则$g((x^{(i)})^{T}\theta^{(k)}) \neq y^{(i)} $，也就意味着：</p>
<script type="math/tex; mode=display">(x^{(i)})^{T}\theta^{(k)}y^{(i)} \leq 0</script><p>另外根据感知器算法的定义，我们知道 $\theta^{(k+1)} = \theta^{(k)} + y^{(i)}x^{(i)} $<br>然后就得到： </p>
<script type="math/tex; mode=display">\begin{array}{ll} (\theta^{(k+1)})^{T}u & = (\theta^{(k)})^{T}u+y^{(i)}(x^{(i)})^{T}u \\ & \geq (\theta^{(k)})^{T}u+\gamma \end{array}</script><p>利用一个简单的归纳法得到：</p>
<script type="math/tex; mode=display">(\theta^{(k+1)})^{T}u \geq k\gamma</script><p>同时根据感知器算法的定义能得到： </p>
<script type="math/tex; mode=display">\begin{array}{ll} ||\theta^{(k+1)}||^{2} & = ||\theta^{(k)}+y^{(i)}x^{(i)} ||^{2} \\ & = ||\theta^{(k)}||^{2}+||x^{(i)}||^{2}+2y^{(i)}(x^{(i)})^{T}\theta^{(i)}\\ & \leq ||\theta^{(k)}||^{2}+||x^{(i)}||^{2}\\ & \leq ||\theta^{(k)}||^{2}+\mathcal{D}^{2} \end{array}</script><p>所以有：</p>
<script type="math/tex; mode=display">||\theta^{(k+1)}||^{2}\leq k\mathcal{D}^{2}</script><p>把上面的两个式子结合起来：</p>
<script type="math/tex; mode=display">\begin{array}{ll}\sqrt{k}\mathcal{D} & \geq ||\theta^{(k+1)}|| \\ & \geq (\theta^{(k+1)})^{T}u \\ &\geq k\gamma  \end{array}</script><p>上面第二个不等式是基于 $u $是一个单位向量（$z^{T}u = ||z||\cdot||u|| cos\phi \leq ||z|| \cdot ||u|| $，其中的$\phi $ 是向量 $z $和向量 $u $的夹角）。结果则表明 $k \leq (\mathcal{D}/\gamma)^{2} $。因此，如果感知器犯了一个第 $k $个错误，则 $k ≤ (\mathcal{D}/\gamma )^{2} $。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之kmeans聚类算法（note7）</title>
    <url>/2022/07/11/cs229/note7/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note7的主要内容：kmeans算法</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="kmeans聚类算法"><a href="#kmeans聚类算法" class="headerlink" title="kmeans聚类算法"></a>kmeans聚类算法</h2><p>在聚类的问题中，我们得到了一组训练样本集 $\{x^{(1)}, \cdots, x^{(m)}\} $，然后想要把这些样本划分成若干个相关的“类群”。其中的 $x^{(i)}\in\mathbb{R}^{n}$，而并未给出分类标签 $y^{(i)} $。所以这就是一个无监督学习的问题了。</p>
<p>K 均值聚类算法如下所示：</p>
<ul>
<li>1, 随机初始化聚类中心 $μ_{1}, μ_{2},\cdots, μ_{k}\in\mathbb{R}^{n} $。 </li>
<li>2, 重复下列过程直到收敛:<br>  { <ul>
<li>对每个 $i $，设 $c^{(i)} := arg\mathop{min}\limits_{j}||x^{(i)}-u_{j} ||^{2} $</li>
<li>对每个 $j $，设 $u_{j} := \frac{\sum_{i=1}^{m}1\{c^{(i)} = j\}x^{(i)}}{\sum^{m}_{i=1}1\{c^{(i)} = j \}} $<br>} </li>
</ul>
</li>
</ul>
<p>在上面的算法中，$k $是我们这个算法的一个参数，也就是我们要分出来的群组个数；而聚类重心 $μ_{j} $表示的是我们对各个聚类的中心位置的当前猜测。在上面算法的第一步当中，需要初始化聚类中心，可以这样实现：随机选择 $k $个训练样本，然后设置聚类中心等于这 $k $ 个样本各自的值。（当然也还有其他的初始化方法。） </p>
<p>算法的第二步当中，循环体内重复执行两个步骤：   </p>
<ul>
<li>（i）将每个训练样本 $x^{(i)} $“分配”给距离最近的聚类重心 $μ_{j} $；</li>
<li>（ii）把每个聚类重心 $μ_{j} $ 移动到所分配的样本点的均值位置。下面的 图1 就展示了运行 k均值聚类算法的过程。</li>
</ul>
<p><img src="kmeans.png" alt="kmeans-algorithms"></p>
<p>图1：$k $均值聚类算法。图中的圆形点表示的是训练样本，交叉符号表示的是聚类重心。</p>
<ul>
<li>(a) 原始训练样本数据集。 </li>
<li>(b) 随机初始化的聚类重心（这里的初始化方法就跟我们上面说的不一样，并没有从训练样本中选择两个点）。</li>
<li>(c-f) 运行 $k $均值聚类算法中的两步迭代的示意图。在每一次迭代中，我们把每个训练样本分配给距其最近的聚类重心（用同样颜色标识出），然后把聚类重心移动到所分配的样本的均值位置。（用颜色区分效果最好了。）</li>
</ul>
<p>$k $均值聚类算法能保证收敛性么？可以的，至少在一定意义上能这么说。尤其是我们可以定义一个下面这样的函数作为失真函数：</p>
<script type="math/tex; mode=display">J(c,u) = \sum\limits_{i=1}^{m}||x^{(i)}-u_{c^{(i)}} ||^{2}</script><p>这样就可以用 $J $来衡量每个样本 $x^{(i)} $和对应的聚类重心 $μ_{c^{(i)}} $之间距离的平方和，明显能看出 $k $均值聚类算法正好就是对 $J $的坐标下降过程。尤其是内部的循环体中，$k $均值聚类算法重复对 $J $进行最小化，当 $u $固定的时候用 $c $来最小化 $J $，当 $c $固定的时候则用 $u $ 最小化 $J $。这样就保证了 $J $是单调降低的，它的值也就必然收敛。（通常这也表明了 $c $和 $u $也收敛。在理论上来讲，$k $均值可能会在几种不同的聚类之间摆动，也就是说某些组不同值的 $c $和 $u $对应有完全相同的 $J $值，不过在实践中这种情况几乎不会遇到。）</p>
<p>失真函数 $J $，是一个非凸函数，所以对 $J $进行坐标下降并不一定能够收敛到全局最小值，也就是说，$k $均值聚类算法可能只是局部最优的。通常除了这个问题之外，$k $均值聚类效果都不错，能给出很好的聚类。如果你担心陷入到某些比较差的局部最小值，通常可以多次运行 $k $均值距离（使用不同的随机值进行来对聚类重心 $μ_{j} $ 进行初始化）。然后从所有的不同聚类方案中，选择能提供最小失真 $J(c,μ) $ 的。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之学习理论（note4）</title>
    <url>/2022/07/08/cs229/note4/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note4的主要内容：偏差和方差权衡，假设集合有限/无限的学习理论</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="Part-VI-学习理论"><a href="#Part-VI-学习理论" class="headerlink" title="Part VI 学习理论"></a>Part VI 学习理论</h2><h3 id="1，偏差和方差的权衡"><a href="#1，偏差和方差的权衡" class="headerlink" title="1，偏差和方差的权衡"></a>1，偏差和方差的权衡</h3><p>在讲线性回归的时候，讨论过这样的问题：拟合数据的时候，选择线性的$y = \theta_{0} +\theta_{1}x $“简单”模型，还是选择多项式的$y=\theta_{0}+\theta_{1}x+···\theta_{5}x^{5} $这种“复杂”模型。如下图所示：</p>
<p><img src="bias-variance.png" alt="bias-variance"></p>
<p>最右侧图所示，用一个五次多项式来进行拟合，得到的并不是一个好模型。这个五次多项式对于训练集中的每一个 $x $都给出了非常好的预测的 $y $ 值，我们也不能指望这个模型能够对训练集之外的点给出靠谱的预测。换句话说，用这种高次多项式来对训练集进行学习，得到的模型根本不能扩展运用到其他数据上。一个模型的泛化误差正是那些不属于训练集的样本预测偏差。</p>
<p>最左边的线性拟合和最右边的高次多项式拟合都有非常大的泛化误差，这两个模型各自出的问题是很不一样的。如果 $y $ 和 $x $ 之间的关系不是线性的，那么即便我们有一个非常大规模的训练集，然后用来进行线性拟合，得到的线性模型都还是不能够准确捕捉到数据的分布。我们粗略地将一个模型的偏差（bias）定义为预测的泛化误差，即便我们要去拟合的对象是一个非常大的甚至是无限的训练数据集。这样的话，对于上面三幅图中所展示的那个情况来看，最左边的那个线性模型就具有特别大的偏差，可能是对数据欠拟合（也就是说，没有捕捉到数据所体现的结构特征）。</p>
<p>除了这个偏差之外，还有另外一个构成泛化误差的因素，也就是模型拟合过程的方差（variance）。在最右边的图中，使用了五次多项式进行了拟合，这样有很大的风险，很可能我们基于数据拟合出来的模型可能碰巧只适合于眼下这个小规模的有限的训练集，而并不能反映 $x $ 和 $y $ 之间更广泛的关系。例如，在实际中，可能我们选择的训练集中的房屋碰巧就是一些比平均价格要稍微贵一些的房屋，也可能有另外的一些比平均值要低一点的房屋，等等。通过对训练集拟合得到的这个“不太靠谱的”的模式，我们得到的可能也就是一个有很大泛化误差的模型。这样的话，我们就说这个模型的方差很大。</p>
<p>通常情况下，咱们需要在偏差（bias）和方差（variance）之间进行权衡妥协。如果我们的模型过于“简单”，而且参数非常少，那这样就可能会有很大的偏差（bias），而方差（variance）可能就很小；如果我们的模型过于“复杂”，有非常多的参数，那就可能反过来又特别大的方差（variance），而偏差（bias）就会小一些。在上面三种不同拟合的样例中，用二次函数来进行拟合得到的效果，明显是胜过一次线性拟合，也强于五次多项式拟合。</p>
<h3 id="2，预备知识"><a href="#2，预备知识" class="headerlink" title="2，预备知识"></a>2，预备知识</h3><p>这部分，开始进入到机器学习的理论，本章内容非常有趣，而且有启发性，还能帮助我们培养直觉，能得到在不同背景下如何最佳应用学习算法的经验规则。此外，我们还会探究一些问题：首先，上文我们刚刚谈论到的偏差和方差，能不能总结一下？这个问题还会引出关于模型选择的方法，这些方法可以在对一个训练集进行拟合的时候来帮助确定要用的多项式应该是几阶的。其次，在机器学习的过程中，我们真正关注的也就是泛化误差，不过绝大部分的学习算法都是将训练集和模型结合的。那么针对训练集的表现好坏程度，为何就能告诉我们泛化误差的信息呢？例如，我们能将训练集的误差和泛化误差联系起来么？第三个，也是最后一点，是否存在某些条件，又能否在这些条件下证明某些学习算法能够良好工作？</p>
<p>我们先来给出两个很简单又很有用的引理。</p>
<p>引理1  (联合约束，The union bound)。设 $A_{1}, A_{2}, …, A_{k}$  是 $K$个不同事件（但不一定互相独立），则有： 在概率论中，联合约束通常被当做是公理，实际上也很直观： $k $ 个事件同时发生的概率最多是 $k $ 个不同的事件每个都发生的概率的总和。</p>
<script type="math/tex; mode=display">P(A_{1}\bigcup\cdots\bigcup A_{k})\leq P(A_{1})+...+P(A_{k})</script><p>引理2  (Hoeffding 不等式) 。设 $Z1, … , Zm$ 是$m $ 个独立且服从伯努利分布的随机变量。例如：$P(Z_{i}=1)=\phi $ 而 $P(Z_{i}=0)=1−\phi $. 设$\hat{\phi}=\frac{1}{m}\sum_{i=1}^{m}Z_{i}$是这些随机变量的平均值，然后设任意的 $\gamma &gt; 0$ 为某一固定值，则有： </p>
<script type="math/tex; mode=display">P(|\phi - \hat{\phi}|>\gamma)\leq 2exp(-2\gamma^{2}m)</script><p>这个引理（也称为切尔诺夫约束）表明，如果我们我们从一个伯努利分布的随机变量中选取平均值 $\hat\phi$来作为对 $\phi$ 的估计值，那么只要 $m $ 足够大，我们偏移真实值很远的概率就比较小。另外一种表述方式是：如果你有一个硬币，抛起来落下人头朝上的概率是 $\phi$，如果你抛了 $m $次，然后计算人头朝上的比例，若 $m $ 非常大，那么这个比例的值，就是一个对 $\phi$ 的一个概率很好的估计。</p>
<p>基于上面这两个引理，我们就可以去证明在机器学习理论中一些很深刻和重要的结论了。</p>
<p>为了简化表述，我们先关注一下二分法分类，其中的标签简化为 $y \in \{0, 1\} $。然后我们即将讲到的所有内容也都会推广到其它问题中，例如回归问题以及多类别的分类问题等等。</p>
<p>假设我们有一个给定的训练集 $S = \{(x^{(i)},y^{(i)});i = 1,…,m\} $，其样本规模为 $m $，集合中的训练样本 $(x^{(i)},y^{(i)}) $ 是可以符合某概率分布 $\mathcal{D} $ 的独立且同分布的随机变量。设一个假设为 $h $，我们则用如下的方法定义训练误差（也成为学习理论中的经验风险或者经验误差 ）：</p>
<script type="math/tex; mode=display">\hat{\epsilon}(h) = \frac{1}{m}\sum\limits_{i=1}^{m}1\{h(x^{(i)})\neq y^{(i)} \}</script><p>这个值只是假设模型 $h $ 分类错误样本占据训练样本总数的分数。如果我们要特定指针对某个训练样本集合 $S$ 的经验误差 $\hat{\epsilon}(h) $，可以写作 $\hat{\epsilon}_{S}(h) $。然后我们就可以定义泛化误差（generalization error）为：</p>
<script type="math/tex; mode=display">\epsilon (h) = P_{(x,y)\sim \mathcal{D}}(h(x)\neq y)</script><p>经验误差 $\hat{\epsilon}(h) $ 的这个定义实际上也就相当于，基于分布 $D $ 给出的一个新的样本 $(x, y) $ ，假设模型 $h $ 对该样本分类错误的概率。 </p>
<p>要注意，这里我们有一个预先假设，也就是训练集的数据与要用来检验假设用的数据都服从同一个分布 $D $（这一假设存在于对泛化误差的定义中），这个假设通常也被认为是 PAC 假设之一。</p>
<p>考虑线性分类的情况，假设 $h_{\theta}(x) = 1\{θ^{T}x \geq 0\} $，拟合参数 $\theta$ 的合理方法是什么呢？一个思路就是可以使训练误差最小化，然后选择取最小值时候的$\theta$：</p>
<script type="math/tex; mode=display">\hat{\theta} = arg\mathop{min}\limits_{\theta}\hat{\epsilon}(h_{\theta})</script><p>我们把上面这个过程称之为经验风险最小化（empirical risk minimization，缩写为 ERM），而这种情况下通过学习算法得到的假设结果就是 $\hat{h} = h_{\hat{\theta}}$ 。我们把 ERM 看做为最“基础”的学习算法，在这一系列的讲义中我们主要关注的就是这种算法。（其他的例如逻辑回归等等算法也可以看作是对 ERM 的某种近似。）</p>
<p>在咱们关于机器学习理论的研究中，有一种做法很有用，就是把具体的参数化抽象出去，也把是否使用线性分选器之类的问题也抽象出去。我们把学学习算法所使用的假设类$\mathcal{H}$定义为所有分类器的集合。对于线性分类问题来说，$\mathcal{H} = \{ h_{\theta}: h_{\theta}(x) = 1{θ^{T}x \geq 0}, \theta \in \mathcal{R}^{n+1}\} $，是一个对 $X $进行分类的所有分类器的集合，其中所有分类边界为线性。更广泛来说，假设我们研究神经网络，那么可以设 $H $ 为能表示某些神经网络结构的所有分类器的集合。</p>
<p>现在就可以把经验风险最小化（ERM）看作是对函数类 $H $ 的最小化，其中由学习算法来选择假设（hypothesis）：</p>
<script type="math/tex; mode=display">\hat{h} = arg\mathop{min}_{h\in \mathcal{H}}\hat{\epsilon}(h)</script><h3 id="3，集合-mathcal-H-有限"><a href="#3，集合-mathcal-H-有限" class="headerlink" title="3，集合$\mathcal{H}$有限"></a>3，集合$\mathcal{H}$有限</h3><p>首先来考虑假设类有限的学习问题，其中假设类 $\mathcal{H} = {h_{1}, …, h_{k}} $，由 $k $ 个不同假设组成。因此，$\mathcal{H} $ 实际上就是由 $k $ 个从输入特征 $\mathcal{X} $ 映射到集合 $\{0, 1\} $ 的函数组成的集合，而经验风险最小化（ERM）就是从这样的 $k $ 个函数中选择训练误差最小的作为$\hat{h} $。</p>
<p>我们希望能够确保 $\hat{h} $ 的泛化误差，需要两个步骤：首先要表明$\hat{\epsilon}(h) $ 是对所有 $h $ 的 $\epsilon(h) $ 的一个可靠估计。其次就需要表明这个 $\hat{\epsilon}(h) $ 位于 $\hat{h}$ 泛化误差的上界。</p>
<p>任选一个固定的 $h_{i}\in\mathcal{H} $，假如有一个伯努利随机变量$\mathcal{Z} $，其分布下面会定义。 然后我们从 $\mathcal{D} $ 中取样 $(x, y) $，并设 $\mathcal{Z} = 1\{ h_{i}(x) \neq y\} $。也就是说，我们会选择一个样本，然后令 $\mathcal{Z} $ 指示 $h_{i} $ 是否对该样本进行了错误分类。类似地，我们还定义了一个 $\mathcal{Z}_{j} = 1\{ h_{i}(x^{(j)}) \neq y^{(j)}\} $。由于我们的训练样本都是从 $\mathcal{D} $ 中取来的独立随机变量，所以在此基础上构建的 $\mathcal{Z} $ 和 $\mathcal{Z}_{j} $ 也都服从相同的分布。</p>
<p>这样就能找到针对随机选取的训练样本进行错误分类的概率$\epsilon(h) $ — 正好就是 $\mathcal{Z} $ （以及 $\mathcal{Z}_{j} $ ）的期望值。然后，就可以把训练误差写成下面这种形式：</p>
<script type="math/tex; mode=display">\hat{\epsilon}(h_{i}) = \frac{1}{m}\sum\limits^{m}_{j=1}\mathcal{Z}_{j}</script><p>因此，$\hat{\epsilon}(h_{i}) $就正好是 $m $ 个随机变量 $Z_{j} $ 的平均值，而这个 $Z_{j} $ 是服从伯努利分布的独立随机变量，其均值就是$\epsilon (h_{i}) $。接下来，就可以使用 Hoeffding 不等式，得到下面的式子：</p>
<script type="math/tex; mode=display">P(|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|>\gamma)\leq 2exp(-2\gamma^{2}m)</script><p>这就表明，对于我们给定的某个固定的 $h_{i} $，假如训练样本的规模 $m $ 规模很大的时候，训练误差很接近泛化误差的概率是很高的。然而我们不仅仅满足于针对某一个特定的 $h_{i} $ 能保证 $\epsilon(h_{i}) $ 接近 $\hat{\epsilon}(h_{i}) $ 且接近的概率很高，还要证明同时针对所有的 $h\in\mathcal{H} $ 这个结论都成立。为了证明这个结论，我们设 $A_{i} $ 来表示事件 $|\epsilon(h_{i}) − \hat{\epsilon}(h_{i})| &gt; \gamma $，已经证明了，对于任意的固定的 $A_{i} $，都有 $P(A_{i}) \leq 2exp(−2\gamma^{2}m) $ 成立。接下来，使用联合约束（union bound），就可以得出下面的关系：</p>
<script type="math/tex; mode=display">\begin{array}{ll} P(\exists h\in\mathcal{H}.|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|>\gamma ) & = \quad P(A_{1}\cup\cdots\cup A_{k}) \\ & \leq \quad\sum\limits^{k}_{i=1}P(A_{i}) \\ & \leq\quad\sum\limits^{k}_{i=1}2exp(-2\gamma^{2}m)\\ & = \quad 2kexp(-2\gamma^{2}m) \end{array}</script><p>如果等式两边都用 1 来减去原始值，则不等关系改变为： </p>
<script type="math/tex; mode=display">\begin{array}{ll} P(\neg\exists h\in\mathcal{H}.|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|>\gamma) & = \quad P(\forall h\in\mathcal{H}.|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|\leq\gamma) \\ &\geq\quad 1-2kexp(-2\gamma^{2}m) \end{array}</script><p>如上所示，至少有 $1-2kexp(-2\gamma^{2}m)$ 的概率，能确保对于所有的 $h\in\mathcal{H} $，$\epsilon(h) $在 $\hat{\epsilon}(h) $附近的 $\gamma$ 范围内。这种结果就叫做一致收敛结果，因为这是一个针对所有的 $h \in\mathcal{H} $ 都同时成立的约束（与之相反的是只针对某一个 $h $ 才成立的情况）。</p>
<p>在上面的讨论中，我们涉及到的是针对某些 $m $ 和 $\gamma $ 的特定值，给定一个概率约束：对于某些 $h \in\mathcal{H} $ , 都有$|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|&gt;\gamma $。这里我们感兴趣的变量有三个：$m $, $\gamma $, 以及误差的概率；我们可以将其中的任意一个用另外两个来进行约束。</p>
<p>例如，我们可以提出下面这样的一个问题：给定一个 $\gamma$ 以及某个 $\delta &gt; 0 $，那么如果要保证训练误差处于泛化误差附近 $\gamma$ 的范围内的概率最小为 $1 – \delta$，那么 $m $ 应该要多大呢？可以设 $\delta = 2kexp(-2\gamma^{2}m) $ 然后解出来 $m $（自己给自己证明一下这样是对的吧！），然后我们就发现，如果有：</p>
<script type="math/tex; mode=display">m\geq \frac{1}{2\gamma^{2}}log\frac{2k}{\delta}</script><p>并且概率最小为 $1 – \delta $，就能保证对于所有的 $h\in\mathcal{H} $ 都有$|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|\leq\gamma) $。（反过来，这也表明，对于某些 $h\in\mathcal{H} $， $|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|&gt;\gamma) $的概率最大为$\delta$。）这种联合约束也说明了需要多少数量的训练样本才能对结果有所保证。是某些特定的方法或者算法所需要训练集的规模 $m $ 来实现一定程度的性能，这样的训练集规模 $m $ 也叫做此类算法的样本复杂度。</p>
<p>上面这个约束的关键特性在于要保证结果，所需的训练样本数量只有 $k $ 的对数级别，$k $ 即假设集合 $\mathcal{H} $ 中的假设个数。这个特性稍后会很重要。</p>
<p>同理，我们也可以将 $m $ 和 $\delta $ 设置为固定值，然后通过上面的等式对 $\gamma $ 进行求解，然后表明对于所有的 $h\in\mathcal{H}$ ，都有概率为 $1 – \delta $（这里还是要你自己去证明了，不过你相信这个是对的就好了。）。</p>
<p>现在，我们假设这个联合收敛成立，也就是说，对于所有的 $h\in\mathcal{H}$，都有 $|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|\leq\gamma $。我们的学习算法选择了 $\hat{h} = arg\mathop{min}_{h\in\mathcal{H}}\hat{\epsilon}(h) $，关于这种算法的泛化，我们能给出什么相关的证明呢？</p>
<p>将 $h^{\ast } = arg\mathop{min}_{h\in\mathcal{H}}\epsilon(h) $定义为 $\mathcal{H} $ 中最佳可能假设。这里要注意此处的 $h^{\ast } $ 是我们使用假设集合 $\mathcal{H} $ 所能找出的最佳假设，所以很自然地，我们就能理解可以用这个 $h^{\ast } $ 来进行性能对比了。则有：</p>
<script type="math/tex; mode=display">\begin{array}{ll} \epsilon(\hat{h}) & \leq\quad \hat{\epsilon}(\hat{h})+\gamma \\ & \leq\quad \hat{\epsilon}(h^{\ast })+\gamma \\ & \leq\quad \epsilon(h^{\ast })+2\gamma  \end{array}</script><p>上面的第一行用到了定理$|\epsilon(h_{i})-\hat{\epsilon}(h_{i})|\leq\gamma $（可以通过上面的联合收敛假设来推出）。第二行用到的定理是 $\hat{h}$ 是选来用于得到最小 $\hat{\epsilon}(h) $，然后因此对于所有的 $h $ 都有 $\hat{\epsilon}(\hat{h})\leq\hat{\epsilon}(h) $，也就自然能推出$\hat{\epsilon}(\hat{h})\leq\hat{\epsilon}(h^{\ast }) $。第三行再次用到了上面的联合收敛假设，此假设表明 $\hat{\epsilon}(h^{\ast }) \leq \epsilon(h^{\ast })+\gamma $。所以，我们就能得出下面这样的结论：如果联合收敛成立，那么 $\hat{h}$ 的泛化误差最多也就与 $\mathcal{H} $ 中的最佳可能假设相差 $2\gamma $。</p>
<p>好了，咱们接下来就把上面这一大堆整理成一条定理（theorem）。</p>
<p><strong>定理</strong> 设 $|\mathcal{H}| = k $，$\mathcal{H} $中的元素个数为$k $，然后设 $m $ 和 $\delta $ 为任意的固定值。然后概率至少为 $1 − \delta $，则有：</p>
<script type="math/tex; mode=display">\epsilon(\hat{h})\leq (\mathop{min}\limits_{h\in\mathcal{H}}\epsilon(h))+2\sqrt{\frac{1}{2m}log\frac{2k}{\delta}}</script><p>上面这个可以通过令 $\gamma $ 等于平方根的形式，然后利用我们之前得到的概率至少为 $1 – \delta $ 的情况下联合收敛成立，接下来利用联合收敛能表明 $\epsilon(h)$最多比 $\epsilon(h^{∗ }) = \mathop{min}\limits_{h\in\mathcal{H}}\epsilon(h) $多  $2\gamma $（这个前面我们已经证明过了）。</p>
<p>这也对我们之前提到过的在模型选择的过程中在偏差/方差之间的权衡给出了定量方式。例如，加入我们有某个假设类 $\mathcal{H} $，然后考虑切换成某个更大规模的假设类 $\mathcal{H}\subseteq\mathcal{H}’ $。如果我们切换到了 $\mathcal{H}’ $ ，那么第一次的 $min_{h}\epsilon(h)$只可能降低（因为我们这次在一个更大规模的函数集合里面来选取最小值了）。因此，使用一个更大规模的假设类来进行学习，我们的学习算法的“偏差”只会降低。然而，如果 $k $ 值增大了，那么第二项的那个二倍平方根项也会增大。这一项的增大就会导致我们使用一个更大规模的假设的时候，“方差”就会增大。</p>
<p>通过保持 $\gamma $ 和 $\delta $ 为固定值，然后像上面一样求解 $m $，我们还能够得到下面的样本复杂度约束：</p>
<p><strong>推论（Corollary）</strong>：设 $ |\mathcal{H}| = k $，然后令 $\delta $,$\gamma $ 为任意的固定值。对于满足概率最少为 $1 − \delta $ 的 $\epsilon(\hat{h})\leq min_{h\in\mathcal{H}}\epsilon(h) + 2\gamma $，下面等式关系成立： </p>
<script type="math/tex; mode=display">\begin{array}{ll} m & \geq \frac{1}{2\gamma^{2}}log\frac{2k}{\delta} \\ & = O(\frac{1}{\gamma^{2}}log\frac{k}{\delta}) \end{array}</script><h3 id="4，集合-mathcal-H-无限"><a href="#4，集合-mathcal-H-无限" class="headerlink" title="4，集合$\mathcal{H}$无限"></a>4，集合$\mathcal{H}$无限</h3><p>我们已经对有限个假设类的情况证明了一些有用的定理。然而有很多的假设类都包含有无限个函数，其中包括用实数参数化的类（比如线性分类问题）。那针对这种无限个假设的情况，我们能证明出类似的结论么？ </p>
<p>我们先从一些看似不太“准确”论证的内容开始。当然也有更好的更通用的论证，但先从这种不太“准确”的内容出发，将有助于锻炼我们在此领域内的直觉。</p>
<p>若我们有一个假设集合 $\mathcal{H} $，使用 $d $ 个实数来进行参数化。由于我们使用计算机表述实数，而 IEEE 的双精度浮点数（ C 语言里面的 double 类型）使用了 64 bit 来表示一个浮点数，这就意味着如果我们在学习算法中使用双精度浮点数，那我们的算法就由 $64d $ 个 bit 来进行参数化。这样我们的这个假设类实际上包含的不同假设的个数最多为 $k = 264d $ 。结合上一节的最后一段那个推论（Corollary），我们就能发现，要保证 $\epsilon(\hat{h})\leq\epsilon(h^{\ast })+2\gamma $，同时还要保证概率至少为 $1 − \delta $ ，则需要训练样本规模 $m $ 满足 $m\geq O(\frac{1}{\gamma^{2}}log\frac{2^{64d}}{\delta}) = O(\frac{d}{\gamma^{2}}log\frac{1}{\delta}) = O_{\gamma,\delta}(d) $。（这里的 $\gamma,\delta$ 下标表示最后一个大$O$ 可能是一个依赖于$\gamma $和$\delta $的隐藏常数。）因此，所需的训练样本规模在模型参数中最多也就是线性的。</p>
<p>由于我们要依赖 64bit 浮点数，所以上面的论证还不能完全令人满意，但这个结论大致上是正确的。如果我们试图使训练误差最小化，那么为了使用具有 $d $ 个参数的假设类的学习效果“较好”，通常就需要 $d $ 的线性数量来确定训练样本规模。</p>
<p>（这里要注意的是，对于使用经验风险最小化的学习算法，上面这些结论已经被证明适用。因此，样本复杂度对 $d $ 的线性依赖性通常适用于大多数分类识别学习算法，但训练误差或者训练误差近似值的最小化，就未必适用于分类识别了。对很多的非 ERM 学习算法提供可靠的理论论证，仍然是目前很活跃的一个研究领域。）</p>
<p>前面的论证还有另外一部分让人不太满意，就是依赖于对 $\mathcal{H}$ 的参数化。根据直觉来看，这个参数化似乎应该不会有太大影响：我们已经把线性分类器写成了 $h_{\theta}(x) = 1\{\theta_{0} + \theta_{1}x_{1} + ···\theta_{n}x_{n} \geq 0\} $的形式，其中有 $n+1 $ 个参数 $\theta_{0},…,\theta_{n} $。但也可以写成 $h_{u,v}(x) = 1\{(u^{2}_{0} − v_{0}^{2}) + (u^{2}_{1} − v_{1}^{2})x_{1} + ··· (u^{2}_{n} − v_{n}^{2})x_{n} \geq 0\} $的形式，这样就有 $2n+2 $ 个参数 $u_{i}, v_{i} $了。然而这两种形式都定义了同样的一个 $\mathcal{H}：$ 一个 $n$ 维的线性分类器集合。</p>
<p>要推导出更让人满意的论证结果，我们需要再额外定义一些概念。</p>
<p>给定一个点的集合 $\mathcal{S} = \{x^{(i)}, …, x^{(d)}\} $（与训练样本集合无关），其中 $x^{(i)} \in \mathcal{X} $，如果 $\mathcal{H} $ 能够对 集合 $\mathcal{S} $ 实现任意的标签化，则称 $\mathcal{H} $  打散（shatter）了 $\mathcal{S} $。例如，对于任意的标签集合 $\{y^{(1)}, …, y^{(d)}\} $，都有类 $\mathcal{H} $ 中的某个函数 $h $ 满足 $h(x^{(i)}) = y^{(i)} $，其中 $i = 1, \cdots d $。</p>
<p>给定一个假设类 $\mathcal{H} $，我们定义其 VC维度，写作$VC(\mathcal{H}) $，这个值也就是能被 $\mathcal{H} $ 打散的最大的集合规模。（如果 $\mathcal{H} $ 能打散任意大的集合，那么 $ VC(H) = \infty $。）</p>
<p>例如，若一个集合由下图所示的三个点组成：<br><img src="exam-set.png" alt="exam-set"></p>
<p>那么二维线性分类器 $(h(x) = 1\{\theta_{0} +\theta_{1}x_{1} + \theta_{2}x_{2} \geq 0\}) $的集合 $\mathcal{H} $ 能否将上图所示的这个集合打散呢？答案是能。具体来看则如下图所示，以下八种分类情况中的任意一个，我们都能找到一种用能够实现 “零训练误差” 的线性分类器：<br><img src="pts-classify.png" alt="pts-classify"></p>
<p>此外，这也有可能表明，这个假设类 $\mathcal{H} $ 不能打散4 个点构成的集合。因此，$\mathcal{H} $ 可以打散的最大集合规模为 3，也就是说 $VC(\mathcal{H})= 3 $。</p>
<p>这里要注意，$\mathcal{H} $ 的 VC 维度为3，即便有某些 3 个点的集合不能被 $\mathcal{H} $ 打散。例如如果三个点都在一条直线上（如下图左侧的图所示），那就没办法能够用线性分类器来对这三个点的类别进行划分了（如下图右侧所示）。<br><img src="pts-not-classify.png" alt="pts-not-classify"></p>
<p>换个方式来说，在 VC 维 的定义之下，要保证 $VC(\mathcal{H}) $至少为 $\mathcal{D} $，只需要证明至少有一个规模为 $d $ 的集合能够被 $\mathcal{H} $ 打散就可以了。</p>
<p>这样就能够给出下面的定理了，该定理来自 Vapnik。</p>
<p><strong>定理</strong>：给定 $\mathcal{H} $，设 $d = VC(\mathcal{H}) $。然后对于所有的 $h\in\mathcal{H} $，都有至少为 $1−\delta $的概率使下面的关系成立：</p>
<script type="math/tex; mode=display">|\epsilon(h)-\hat{\epsilon}(h)|\leq O(\sqrt{\frac{d}{m}log\frac{m}{d}+\frac{1}{m}log\frac{1}{\delta}})</script><p>此外，有至少为 $1−\delta $ 的概率：</p>
<script type="math/tex; mode=display">\epsilon(\hat{h})\leq\epsilon(h^{\ast })+O(\sqrt{\frac{d}{m}log\frac{m}{d}+\frac{1}{m}log\frac{1}{\delta}})</script><p>换句话说，如果一个假设类有有限的 VC 维，那么只要训练样本规模 $m $ 增大，就能够保证联合收敛成立，和之前一样，这就能够让我们以 $\epsilon(h^{∗ }) $的形式来给 $\epsilon(h) $ 建立一个约束，此外还有下面的推论：</p>
<p><strong>推论</strong>（Corollary）：对于所有的 $h \in\mathcal{H} $成立的 $|\epsilon(h)-\hat{\epsilon}(h)|\leq\gamma $ （因此也有 $\epsilon(\hat{h})\leq\epsilon(h^{\ast })+2\gamma $），则有至少为 $1 – \delta $的概率，满足 $m = O_{\gamma,\delta}(d) $。</p>
<p>换个方式来说，要保证使用 $\mathcal{H}$ 训练的算法的学习效果“良好”，那么训练集样本规模 $m $ 需要与 $\mathcal{H} $ 的 VC 维度线性相关。这也表明，对于“绝大多数”假设类来说，（假设是“合理”参数化的）VC 维度也大概会和参数的个数线性相关。把这些综合到一起，我们就能得出这样的一个结论：<strong>对于一个试图将训练误差最小化的学习算法来说：训练样本个数通常与假设类 $\mathcal{H} $ 的参数个数线性相关</strong>。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之主成分分析（note10）</title>
    <url>/2022/07/06/cs229/note10/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note10的主要内容：PCA</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="PART-XI-主成分分析"><a href="#PART-XI-主成分分析" class="headerlink" title="PART XI 主成分分析"></a>PART XI 主成分分析</h2><p>之前介绍了因子分析，用$k $ 维子空间对 $x\in \mathcal{R}^{n} $ 进行近似建模，有 $k ≪ n $。具体来说，已知点 $x{(i)} $ ，在 $k $ 维仿射空间$\{\Lambda z + \mu; z \in \mathcal{R}^{k}\} $ 中生成某个 $z^{(i)} $ ，然后增加 $\Psi $-协方差噪音。因子分析是基于概率模型，然后参数估计使用了迭代期望最大化算法。</p>
<p>在本章讲义中，学习一种新的方法，主成分分析（Principal Components Analysis，缩写为 PCA），这个方法也是用来对数据近似所处的子空间进行判别。然而，主成分分析算法会更加直接，只需要进行一种特征向量计算，并且不需要再去使用期望最大化（EM）算法。</p>
<p>假如我们有一个数据集 ${x^{(i)}; i = 1, . . ., m} $，其中包括了 $m $ 种不同汽车的属性，例如最大速度，转弯半径等等。设其中每个 $i $ 都有 $x^{(i)}\in \mathcal{R}^{n} $，(且$n ≪ m $)。但对于两个不同的属性，例如 $x_{i} $ 和 $x_{j} $，对应着以英里每小时（mph）为单位的最高速度和以公里每小时（kph）为单位的最高速度，这两个属性应该是线性相关的，只在对 mph 和 kph 进行四舍五入时候会有引入一些微小差异。所以，这个数据实际上应该是近似处于一个 $n-1 $ 维的子空间中。我们如何自动检测和删除掉这一冗余（redundancy）呢？</p>
<p>举一个不那么麻烦的例子，设想有一个数据集，其中包含的是对一个无线电遥控直升机（radio-controlled helicopters）飞行员协会进行调查得到的数据，其中的 $x_{1}^{(i)} $ 指代的是飞行员 $i $ 的飞行技能，而 $x_{2}^{(i)} $ 指代的是飞行员对飞行的喜爱程度。无线电遥控直升机是很难操作的，只有那些非常投入，并且特别热爱飞行的学生，才能成为好的飞行员。所以，上面这两个属性 $x_{1} $ 和 $x_{2} $ 之间的相关性是非常强的。如下图，可以认为在数据中沿着对角线方向表征了一个人对飞行投入程度的内在“源动力（karma）”，只有少量的噪音脱离这个对角线方向。如下图所示，我们怎么来自动去计算出 $u_{1} $ 的方向呢？</p>
<p><img src="pilot-exam1.png" alt="pilot-skill"></p>
<p>接下来介绍主成分分析算法，但在运行 PCA 之前，我们首先要进行一些预处理（pre-process），正则化（normalize）数据的均值（mean）和方差（variance），如下所示：</p>
<ul>
<li>1，设$\mu = \frac{1}{m}\sum_{i=1}^{m}x^{(i)} $</li>
<li>2，$x^{(i)} = x^{(i)} − \mu $</li>
<li>3，设 $\sigma^{2}_{j} = \frac{1}{m}\sum_{i}(x^{(i)}_{j})^{2} $</li>
<li>4，$x^{(i)}_{j} = x^{(i)}_{j}/\sigma_{j} $ </li>
</ul>
<p>第（1-2）步把数据的平均值清零，然后可以省略掉所有有零均值的数据。第（3-4）步将每个坐标缩放，使之具有单位方差，这确保了不同属性的数据都在同样的“尺度”上来进行处理。例如，如果 $x_{1} $ 是汽车的最大速度，然后 $x_{2} $ 是汽车的座位数量，这样这个重新正则化（renormalization）就把不同的属性进行了缩放，然后这些不同属性就更具有对比性。如果对不同的属性有先验知识，就可以省略第（3-4）步。例如，如果每个数据点表示灰度图像中的每个数据点，而每个 $x_{j}^{(i)} $ 就从 $\{0, 1, . . . , 255\} $ 中取值，对应的也就是在图像 $i $ 中像素 $j $ 位置的灰度值。</p>
<p>正则化之后，对数据近似所处的方向，也就是“主要变化轴”$u $，该如何去计算呢？一种方法是找出一个单位向量$u $，使得数据投影在 $u $ 的方向上的时候，投影的数据的方差最大。</p>
<p>直观来看，在这个方向上，数据开始有一定规模的方差/信息量。我们要选择的是这样一个方向的单位向量 $u $，数据能近似投放到与单位向量 $u $ 一致的方向/子空间，并且尽可能多地保留上面的方差。</p>
<p>设下面的数据集，我们已经进行了正则化步骤：</p>
<p><img src="pts-exam1.png" alt="pts-exam1"></p>
<p>现在，加入我们选择的单位向量 $u $ ，下图中的圆点表示的就是原始数据在这条线上面的投影。</p>
<p><img src="pts-exam2.png" alt="pts-exam2"></p>
<p>可以看到，上面投影得到的数据依然较大的方差，而这些点距离零点也都比较远。如下图所示，选择另外一个方向的单位向量：</p>
<p><img src="pts-exam3.png" alt="pts-exam3"></p>
<p>上面这幅图的投影中的方差就明显小了很多，而且投影得到的点位置也距离原点更近很多。</p>
<p>我们希望能自动地选择出来如上面两幅图中第一幅那样的方向的单位向量 $u $。要对这个过程进行公式化（formalize），给定一个向量 $u $ 和一个点 $x $，$x $ 投影到 $u $ 上的投影长度就可以用 $x^{T}u $ 来得到。也就是说，如果 $x^{(i)} $ 是我们数据集中的一个点（上面几个图中画叉的 $x $ 点中的一个），那么这个点在 $u $ 上的投影（对应的是图中的圆点）就是从原点到  $x^{T}u $ 的距离。因此，要最大化投影的方差，就要找到一个能够将下面式子最大化的单位长度向量 $u $：</p>
<script type="math/tex; mode=display">\begin{array}{ll}\frac{1}{m}\sum\limits_{i=1}^{m}((x^{(i)})^{T}u)^{2} & = \quad \frac{1}{m}\sum\limits_{i=1}^{m}u^{T}x^{(i)}(x^{(i)})^{T}u\\
& = \quad u^{T}(\frac{1}{m}\sum\limits^{m}_{i=1}x^{(i)}(x^{(i)})^{T})u\end{array}</script><p>容易发现，要让上面的式子最大化，$||u||_{2} = 1 $ 给出了$\sum = \frac{1}{m}\sum^{m}_{i=1}x^{(i)}(x^{(i)})^{T}$的主特征向量，而这也正好就是数据的经验协方差矩阵（假设零均值）。</p>
<p>总结一下，如果我们要找一个 1 维子空间来近似数据，就要选择 $\sum $ 的主特征向量作为单位向量 $u $。更广义地理解，就是如果要将数据投影到一个 $k $ 维子空间$(k &lt; n) $，就应当选择 $\sum $ 的 $k $ 个特征向量来作为单位向量 $u_{1}, . . ., u_{k} $。这里的 $u_{i} $ 就成了数据的一组新的正交基。</p>
<p>然后，要使用这组正交基来表示 $x^{(i)} $，只需要计算对应的向量：</p>
<script type="math/tex; mode=display">y^{(i)} = \begin{bmatrix} u^{T}_{1}x^{(i)} \\ u^{T}_{2}x^{(i)} \\ \vdots \\u^{T}_{k}x^{(i)} \end{bmatrix}</script><p>$x^{(i)} \in \mathcal{R}^{n}$，向量 $y^{(i)} $就是对 $x^{(i)} $ 的近似表示。因此，主成分分析算法也被称为是一种维度降低算法，其中的单位向量 $u_{1},…,u_{k} $ 也就叫做数据集的前 $k $ 个主成分。</p>
<p><strong>Remark</strong>虽然仅当 $k = 1 $的情况下，可使用特征向量的特性，很明显，在所有可能的正交基中，选择的那一组就能使得$\sum_{i}||y^{(i)}||^{2}_{2}$取最大值。因此，我们对基向量的选择应当是尽可能保留原始数据的方差信息。</p>
<p>主成分分析算法也可以有另外一种推导方式：将数据投影到 $k $ 维子空间中，选择一组基向量，使得投影引起的近似误差最小。</p>
<p>主成分分析算法有很多应用；接下来给出若干样例。首先是压缩，用更低维度的 $y^{(i)} $ 来表示 $x^{(i)} $ ，这个用途很明显了。如果我们把高维度的数据降维到 $k = 2$ 或者 $3 $，那么就可以将 $y^{(i)} $ 进行可视化了。例如，如果我们把汽车数据降维到 2 维，那么就可以把压缩后的数据投影（例如这时候投影中的一二点可能就代表了骑车的类型），来看看哪些车彼此相似，以及这些车可以聚集成那些组。</p>
<p>另一个常用应用就是使用 $x^{(i)} $ 作为输入特征，进行监督学习算法之前降低数据维度的预处理步骤。除了有利于缓解计算性能压力之外，降低数据维度还可以降低假设类的复杂度，然后避免过拟合（例如，低维度的输入特征控件上的线性分类器会有更小的 VC 维度）。</p>
<p>最后，正如在遥控直升机飞行员样例，可以把 PCA 作为一种降噪算法。在那个例子中，算法从对遥控飞行技巧和热爱程度的带噪的衡量中估计了直观的“遥控飞行原动力”。同时，还能把这种思路用于人脸图像，得到的就是特征脸算法，其中每个点 $x^{(i)} \in \mathcal{R}^{100×100} $ 都是一个 $10000 $ 维的向量，每个坐标对应的是一个 $100x100 $ 的人脸图像中的一个像素灰度值。使用主特征分析算法，我们就可以用更低维度的 $y^{(i)} $ 来表示每个图像 $x^{(i)} $。在这个过程中，我们希望主成分能够保存有用的信息和面孔之间的系统变化，能捕获到一个人看上去的模样，而不是由于细微的光线变化、轻微的拍摄状况差别等而引起的图像中的“噪音”。然后通过降低维度计算 $||y^{(i)} − y^{(j)}||_{2} $ 来测量人脸 $i $ 和 $j $ 之间的距离，这样就能得到面部匹配和检索算法。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229之正则化与模型选择（note5）</title>
    <url>/2022/07/04/cs229/note5/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note5的主要内容：模型选择，特征选择，贝叶斯估计</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="Part-VII-正则化和模型选择"><a href="#Part-VII-正则化和模型选择" class="headerlink" title="Part VII 正则化和模型选择"></a>Part VII 正则化和模型选择</h2><p>设想给一个机器学习的问题选择模型，例如，若选择多项式回归模型$h_{\theta}(x) = g(\theta_{0}+\theta_{1}x+\theta_{2}x^{2}…+\theta_{k}x^{k})$，然后$k$的取值。那怎么才能自动选择一个可在偏差（bias）和方差（variance）之间平衡的模型呢？或者换一个说法，怎么自动选出来一个带宽参数（bandwidth parameter） $\tau $ 用于局部加权回归（locally weighted regression），或者怎么自动选出一个参数 C 用于拉格朗日正则化的支持向量机算法（l1-regularized SVM）？</p>
<p>为了具体一些，假设个数有限的模型集合$M = {M_{1},…,M_{d}} $。例如，在上面提到的例子，$M_{i} $ 就是 $i $次多项式回归模型。换个说法就是，如果我们要从支持向量机算法（SVM）、神经网络算法（neural network）、逻辑回归算法（logistic regression）当中三选一，那么这里的 M 就应该都包含这些模型。</p>
<h3 id="1，交叉验证"><a href="#1，交叉验证" class="headerlink" title="1，交叉验证"></a>1，交叉验证</h3><p>假设有一个训练集$S $，了解了经验风险最小化（empirical risk minimization，缩写为 ERM），就像算法初始化一样，接下来通过ERM来进行模型选择：</p>
<ul>
<li>1，对训练集 $S $ 中的每一个模型$M_{i} $ 进行训练，得到相应的假设$h_{i} $。</li>
<li>2，从这些假设中选取训练误差最小的假设（hypotheses）。</li>
</ul>
<p><strong>这个算法是行不通的。</strong></p>
<p>若考虑多项式模型，要考虑多项式的阶（最高次项的次数），阶越高，对训练集 S 的拟合程度就越好，训练误差自然也就更小。然而，这个方法选出来的总是那种波动非常强（high-variance）的高次多项式模型（high-degree polynomial model），这种情况之前讨论过，通常都是很差的选择。</p>
<p>下面这个算法就更好一些，叫做保留交叉验证（hold-out cross validation），也叫做简单交叉验证（simple cross validation），步骤如下：</p>
<ul>
<li>1，随机拆分训练集 $S $ 成 $S_{train} $ (如可选 70% 的比例) 和 $S_{cv} $ (训练集中剩余的 30%用于验证)。这里的 $S_{cv} $ 就叫做保留交叉验证集。</li>
<li>2，只对集合 $S_{train} $ 中的每一个模型 Mi 进行训练，然后得到假设$h_{i} $。</li>
<li>3，筛选并输出验证集上有最小误差的 $\hat{\epsilon}S_{cv}(h_{i}) $假设$h_{i} $ 。</li>
</ul>
<p>这样通过在部分未进行训练的样本集 $S_{cv}$ 上进行测试，我们对每个假设 hi 的真实泛化误差（generalization error）就能得到相对更好的估计，然后就能选择出来一个最小估计泛化误差的假设了。通常可以选择 $\frac{1}{4} ~ \frac{1}{3} $ 的数据样本用来作为交叉验证集，30% 是一个很典型的选择。</p>
<p>还有另外一种备选方法，就是在第三步的时候，选择与最小估计经验误差 $\hat{\epsilon}S_{cv}(h_{i}) $ 对应的模型 $M_{i} $ ，然后在完整数据集 S 上用 $M_{i} $ 来再次训练。（这个思路通常都不错，但有一种情景例外，就是学习算法对初始条件和数据的扰动非常敏感的情况。在这样的方法中，适用于$S_{train} $ 的模型未必就能够同样适用于 $S_{cv} $，这样就最好还是放弃再训练的步骤。）</p>
<p>使用保留交叉验证集的一个弊端就是“浪费”了30% 左右的训练样本数据集，甚至即便对整个训练集重新训练模型，也无非是尝试在一个 0.7m 规模的训练样本集上试图寻找一个好的模型来解决一个机器学习问题，而并不是使用了全部的 m 个训练样本，因为我们每次都是在仅 0.7 m 规模样本上进行训练得到模型。当然了，如果数据非常充足，或者是很廉价的话，也可以用这种方法，而如果训练样本数据本身就很稀缺的话，那就最好用其他方法了。</p>
<p>下面就是一种这样的方法，名字叫k-折交叉验证(k-fold cross validation)，这样验证集数据规模都更小：</p>
<ul>
<li>1，随机将训练集 $S $ 切分成 $k $ 个不相交的子集，其中每一个子集的规模为 $\frac{m}{k} $ 个训练样本，把这些子集称为 $S_{1},…,S_{k} $。</li>
<li>2，对每个模型 $M_{i} $，我们都按照下面的步骤来进行评估：对$j = 1, …, k $，在 $ S_{1}···\bigcup S_{j−1} \bigcup S_{j+1}\bigcup ···S_{k} $上（也就是除了 $S_{j} $ 之外的其他所有数据）对模型 $M_{i} $ 进行训练，然后得到假设 $h_{ij} $ 。接下来针对 $S_{j} $ 使用假设 $h_{ij} $ 进行测试，得到经验误差 $\hat{\epsilon}S_{j}(h_{ij}) $，对其取平均值（也就是对所有的 $j $ 都计算然后取平均值），计算得到的值就当做是模型 $M_{i} $ 的估计泛化误差。</li>
<li>3，选择具有最小估计泛化误差的模型 $M_{i} $，然后在整个训练样本集 $S $ 上重新训练该模型，这样得到的假设就可以输出作为最终结果了。</li>
</ul>
<p>通常这里折叠的次数 $k $ 一般是 10，即 $k = 10 $。这样每次进行保留用于验证的数据块就只有 $\frac{1}{k} $ ，这就比之前的 30% 要小多了，当然这样一来这个过程也要比简单的保留交叉验证方法消耗更多算力成本，因为现在需要对每个模型都进行 k 次训练。</p>
<p>虽然通常选择都是设置 $k = 10 $，不过如果一些问题中数据量确实很匮乏，那有时候也可以走一点极端，设 $k = m $，这样是为了每次能够尽可能多地利用数据，尽可能少排除数据。这种情况下，我们需要在训练样本集 $S $ 中除了某一个样本外，在其他所有样本上进行训练，然后在保留出来的单独样本上进行检验。然后把计算出来的 $m = k $个误差放到一起求平均值，这样就得到了对一个模型的泛化误差的估计。这个方法有专门的名字；由于每次都保留了一个训练样本，所以这个方法就叫做弃一法交叉验证（leave-one-out cross validation）。</p>
<p>最后总结一下，咱们讲了不同版本的交叉验证，在上文中是用来作为选择模型的方法，实际上也可以更单纯地用来对一个具体的模型或者算法进行评估。例如，如果你已经实现了某种学习算法，然后想要估计一下算法的性能表现（或者是你创造了一种新的学习算法，然后希望在技术论文中报告你的算法在不同测试集上的表现），交叉验证都是个很好的解决方法。</p>
<h3 id="2，特征选择"><a href="#2，特征选择" class="headerlink" title="2，特征选择"></a>2，特征选择</h3><p>模型选择的重要阶段就是特征选择，设想你面对一个监督学习问题，其中特征值的数量 $n $ 特别大（甚至可能比训练样本集规模还大，即$n &gt;&gt; m $），然而可能只有小部分的特征是与学习任务“相关”的。即便是针对 $n $ 个输入特征值使用一个简单的线性分类器，假设类的 VC 维也依然能达到 O(n)，就很有过拟合的潜在风险，除非训练样本集也足够巨大。</p>
<p>在这样的背景下，就可以用特征选择算法来降低特征值的数目。假设有 $n $ 个特征，那么就有 $2^{n} $ 种可能的特征子集（因为 n 个特征中的任意一个都可以被某个特征子集包含或者排除），因此特征选择就可以看做是对 $2^{n} $ 个可能的模型进行选择。对于特别大的 $n $，要是彻底枚举和对比全部 $2^{n} $ 种模型，成本就太高了，所以通常的做法都是使用启发式搜索过程（heuristic search procedure）来找到一个好的特征子集。下面的搜索过程叫做向前搜索（forward search）：</p>
<ul>
<li>1，初始化集合  $ \mathcal{F} = \emptyset $. </li>
<li>2，循环下面的过程<br>  {<br>  (a) 对于 $ i =1, …, n $ ，如果 $ i\notin \mathcal{F} $, 则令 $\mathcal{F}_{i} = \mathcal{F} \bigcup \{i\} $，然后使用某种交叉验证来评估特征  $\mathcal{F}_{i} $。（也就是说，仅仅使用  $\mathcal{F}_{i} $ 当中的特征来训练你的学习算法，然后估计一下泛化误差。）<br>  (b) 将  $\mathcal{F} $ 设为步骤 (a) 中的最佳特征子集。<br>  } </li>
<li>3，整个搜索过程中筛选出来了最佳特征子集，将其输出。</li>
</ul>
<p>上述算法最外层的循环体可以在 $\mathcal{F} = \{1, … , n\} $  为全部特征时终止，或者也可以在 $|\mathcal{F}| $ 超过某个预设阈值时终止（当算法要用到的特征数量达到了最大值）。</p>
<p>这个算法本质是模型特征选择包装器的一个实例，算法本身就是将学习算法进行“打包”的过程，然后重复调用这个学习算法来评估算法对不同特征子集的效果。除了向前搜索，还有其他的搜索过程，例如逆向搜索，从 $\mathcal{F} = \{1, …, n\} $ ，即全部特征开始，然后重复，每次删减一个特征，直到 $\mathcal{F} $ 为空集，即 $ \mathcal{F} = \emptyset $ 时终止。</p>
<p>这种包装器特征选择算法通常效果不错，不过对算力开销也很大，尤其是要对学习算法进行多次调用。实际上，完整的向前搜索（是 $\mathcal{F} $ 从空集开始，到最终达到整个样本集规模，即 $\mathcal{F} = \{1, …, n\} $ 终止），要对学习算法调用约 $O(n^{2}) $ 次。</p>
<p><strong>过滤器特征选择算法</strong>（Filter feature selection methods）给出的特征子集选择方法更具有启发性，而且在算力上的开销成本也更低。思路是，计算一个简单的分值 $S(i)$，用来衡量每个特征 $x_{i} $  对分类标签 $y $ 的信息量。然后，只需找到最大信息量分值 $S(i) $ 的一组，选择使用其中的 $k $ 个特征。</p>
<p>怎么去定义用于衡量信息量的分值 $S(i) $ 呢？一种思路是使用 $x_{i} $ 和 $y $ 之间相关系数的值（或其绝对值），这可以在训练样本数据中算出。这样我们选出的就是与分类标签$y $关系最密切的特征值。实践中，通常（尤其当特征 $x_{i} $ 为离散值时）选择 $x_{i} $ 和 $y $ 的互信息（mutual information）来作为 $S(i) $，缩写为 $MI(x_{i}, y) $。</p>
<script type="math/tex; mode=display">MI(x_{i},y) = \sum_{x_{i}\in\{0,1\}}\sum_{y\in\{0,1\}}p(x_{i},y)log\frac{p(x_{i},y)}{p(x_{i})p(y)}</script><p>上式中，假设了 $x_{i} $ 和 $y $ 都已经二值化，更广泛的情况下总和将会超过变量的范围。$p(x_{i},y)$， $p(x_{i}) $和 $p(y) $ 的概率都可以根据它们在训练集上的经验分布而推测得到。</p>
<p>要对这个信息量$x_{i}$的作用有一个更直观的印象，也可以将互信息表达成 KL 散度（Kullback-Leibler divergence，也称 KL 距离，常用来衡量两个概率分布的距离）：</p>
<script type="math/tex; mode=display">MI(x_{i},y) = KL(p(x_{i},y)||p(x_{i})p(y))</script><p>在下一节当中，会更多描述 KL ，这里比较通俗地说，这个概念对 $p(x_{i},y) $ 和 $p(xi)p(y)$ 的概率分布差异大小给出一个衡量。如果 $x_{i} $ 和 $y $ 是两个独立的随机变量，那么必然有 $p(x_{i}, y) = p(x_{i})p(y) $，而两个分布之间的 KL 散度就应该是 $0 $。这也符合下面这种很自然的认识：如果 $x_{i} $ 和 $y $ 相互独立，那么 $x_{i} $ 很明显对 $y $ 是“完全无信息量”的，因此对应的信息量分值 $S(i) $ 就应该很小。与之相反地，如果 $x_{i} $ 对 $y $ “有很大的信息量”，那么这两者的互信息$MI(x_{i},y) $ 就应该很大。</p>
<p>最后一个细节：现在已经根据信息量分值 $S(i) $ 的高低来对特征组合进行了排序，那么要如何选择特征个数 $k$ 呢？一个标准办法就是使用交叉验证从可能的不同 $k $ 值中进行筛选。例如，在对文本分类使用朴素贝叶斯方法），这个问题中的词汇规模$n $ 通常都会特别大，使用交叉验证的方法来选择特征子集，一般都能提高分类器精度。</p>
<h3 id="3，贝叶斯估计和正则化"><a href="#3，贝叶斯估计和正则化" class="headerlink" title="3，贝叶斯估计和正则化"></a>3，贝叶斯估计和正则化</h3><p>在本章，我们要讲一下我们“军火库”中的另外一种工具，用于我们对抗过拟合。</p>
<p>在本章的开头部分，我们谈到了使用最大似然来进行参数拟合，然后根据下面的式子来选择参数：</p>
<script type="math/tex; mode=display">\theta_{ML} = arg\mathop{max}\limits_{\theta}\prod\limits_{i=1}^{m}p(y^{(i)}|x^{(i)}; \theta)</script><p>在随后的讨论中，我们将$\theta$视为一个未知参数，概率统计中将$\theta $视为未知常值，并不是随机的，但恰好是未知的，我们的工作就是提出统计处理流程（如最大似然）来尝试估计这个参数。</p>
<p>另外一种解决参数估计的方法是贝叶斯估计，$\theta$ 当做是未知的随机变量，先给定一个在 $\theta $ 上的先验分布$p(\theta) $，这个分布表示关于参数的“预先判断”。给定一个训练集合 $S = \{(x^{(i)},y^{(i)})\}^{m}_{i=1} $，当对一个新的 $x $ 进行预测的时候，可以计算在参数上的后验分布（posterior distribution）：</p>
<script type="math/tex; mode=display">\begin{array}{ll}p(\theta|S) & = \quad \frac{p(S|\theta)p(\theta)}{p(S)}\\
& = \quad \frac{(\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta))p(\theta)}{\int_{\theta}(\prod_{i=1}^{m}p(y^{(i)}|x^{(i)}, \theta)p(\theta))d\theta}\end{array}</script><p>上式中，$p(y^{(i)}|x^{(i)},\theta) $ 是机器学习问题中的模型。例如，如果是贝叶斯逻辑回归，可能就会选择 $p(y^{(i)}|x^{(i)}, \theta) = h_{\theta}(x^{(i)})^{y(i)}(1−h_{\theta}(x^{(i)}))(1−y^{(i)}) $，其中 $h_{\theta}(x^{(i)}) = 1/(1 + exp(−\theta^{T}x^{(i)}))^{3} $。</p>
<p>若有一个新的测试样本 $x $，需要进行预测，可以使用$\theta$ 上的后验分布来计算分类标签上的后验分布：</p>
<script type="math/tex; mode=display">p(y|x, \theta) = \int_{\theta}p(y|x,\theta)p(\theta|S)d\theta</script><p>上面等式中，$p(\theta|S) $ 前面介绍过，如果目标是要根据给定的 $x $ 来预测对应的 $y $ 值，那就可以输出：</p>
<script type="math/tex; mode=display">E[y|x,S] = \int_{y}yp(y|x,S)dy</script><p>简单概述这个过程，可认为是一种“完全贝叶斯”预测，其中预测是通过计算相对于 $\theta$ 上的后验概率 $p(\theta|S) $ 的平均值而得出的。但是，这个后验分布的计算通常是比较困难的，需要对 $\theta$ 进行积分，而 $\theta$ 通常是高维度的，这通常是不能以闭合形式来实现的。</p>
<p>因此在实际应用中，我们都是用一个与 $\theta$ 后验分布近似的分布来替代。常用的一个近似是把对 $\theta$ 的后验分布替换为一个单点估计。对 $\theta$的最大后验估计（MAP，maximum a posteriori estimate）为：</p>
<script type="math/tex; mode=display">\theta_{MAP} = arg\mathop{max}\limits_{\theta}\prod\limits_{i=1}^{m}p(y^{(i)}|x^{(i)},\theta)p(\theta)</script><p>注意到了么，这个式子基本和对 $\theta$ 的最大似然估计（ML,maximum likelihood estimate）形式一样，除了末尾多了一个先验概率分布 $p(\theta)$。</p>
<p>实际应用里面，对先验概率分布 $p(\theta) $ 的常见选择是假设 $\theta \sim N(0 ,\tau^{2}I) $。使用这样的一个先验概率分布，拟合出来的参数 $θ_{MAP} $ 将比最大似然估计得到的参数范数更小。在实践中，贝叶斯最大后验估计（Bayesian MAP estimate）对比参数的最大似然估计（ML estimate of the parameters），前者就更易于避免过拟合。例如，贝叶斯逻辑回归就是一种非常有效率的文本分类算法，即便在文本分类中参数规模 $n $ 通常是远远大于样本规模 $m $ 的，即 $n ≫ m $。</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>cs229的lecture note1</title>
    <url>/2022/06/22/cs229/note1/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>cs229讲义 斯坦福大学的CS229课程是学习机器学习的必备之课，之前是由吴恩达主讲的课程，后来由于不明原因课程被斯坦福大学下架。</p>
<p>note1的主要内容：监督学习概览，线性回归机器概率解释，逻辑回归…..</p>
<p><strong>重新理解，加油~</strong><br><span id="more"></span></p>
<h2 id="PART-II-分类和逻辑回归"><a href="#PART-II-分类和逻辑回归" class="headerlink" title="PART II 分类和逻辑回归"></a>PART II 分类和逻辑回归</h2><p>接下来讨论分类的问题，其实本质和回归问题很像，只是待预测的$y$值值域为个数有限的离散值的集合。</p>
<p>首先来看二分类问题，$y$只有两个取值，0 或者 1（此处讨论的问题也可以拓展到多类的情况）。</p>
<p>例如，假如要建立一个垃圾邮件筛选器，那么就可以用 $x^{i}$ 表示一个邮件中的若干特征，邮件是垃圾邮件时$y=1$，称为正类别（positive class），否则 $y=0$，被称为负类别，有的情况下也分别表示成 $“+”$ 和 $“-”$ 。</p>
<p>对于给定的一个 $x^{i}$，对应的$y^{i}$ 也称为训练样本的标签（label）。</p>
<h3 id="5-逻辑回归"><a href="#5-逻辑回归" class="headerlink" title="5, 逻辑回归"></a>5, 逻辑回归</h3><p>忽略$y$取值于离散集合后，也可以按照前面线性回归的算法来由$x$预测$y$，但是这样构建的例子性能和表现都会比较差。而且，直观来看，$y\in\{0,1\}$，当$\quad h_{\theta}(x)&gt;1 \quad or\quad h_{\theta}(x)&lt;0\quad$都没有意义。</p>
<p>所以，换一个假设函数$h_{\theta}(x)$</p>
<script type="math/tex; mode=display">h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}</script><p>其中，$g(z) = \frac{1}{1+e^{-z}}$称为逻辑函数或者sigmoid函数，下图是$g(z)$的函数图像<br><img src="sigmoid-func.png" alt="逻辑函数"></p>
<ul>
<li>$g(z)\rightarrow 1 \quad if \quad z\rightarrow\infty$，同时，$g(z)\rightarrow0 \quad if \quad z\rightarrow -\infty$；</li>
<li>$g(z)\in(0,1)\quad and \quad h(x)\in(0,1)$；</li>
<li>$g(z)$在$(0,1)$上光滑递增；</li>
<li>$g(z)$还有些其他的性质会在后面讲到。</li>
</ul>
<p>前面，约定$x_{0}=1$后，重写$h_{\theta}(x)=\theta^{T}x=\theta_{0}+\sum^{n}_{j=1}\theta_{j}x_{j}$。此处我们选定$g$来作为估计函数，先讨论sigmoid导数的某些好用的特性：</p>
<script type="math/tex; mode=display">g^{'}(z) = g(z)(1-g(z))</script><p>自行补充推导过程。</p>
<p>那么，给定逻辑回归模型后，怎么去拟合一个合适的$\theta$呢？</p>
<p>之前已经证明过，在一系列假设的前提下，最小二乘法回归可以通过最大似然估计来推出。接下来就给分类模型做一系列的统计学假设，然后用最大似然法来拟合参数吧。</p>
<p>首先假设</p>
<script type="math/tex; mode=display">p(y|x;\theta) = (h_{\theta}(x))^{y}(1-h_{\theta}(x))^{1-y}</script><p>假设$m$个训练样本是独立生成，那么可以将带参数的似然函数写做：</p>
<script type="math/tex; mode=display">\begin{array}{ll} L(\theta) & = \quad p(\vec{y}|X;\theta)\\ & = \quad \prod^{m}_{i=1}p(y^{i}|x^{i};\theta)\\ & = \quad \prod^{m}_{i=1}(h_{\theta}(x^{i}))^{y^{i}}(1-h_{\theta}(x^{i}))^{1-y^{i}}\end{array}</script><p>类似常规处理方法，取对数之后更方便计算</p>
<script type="math/tex; mode=display">\begin{array}{ll}  l(\theta) & = \quad log L(\theta) \\ & = \quad \sum^{m}_{i=1}y^{i}logh(x^{i})+(1-y^{i})log(1-h(x^{i})) \end{array}</script><p>怎么最大化似然函数呢？与线性回归中用到的求导方法类似，咱们这次就是用梯度上升法（gradient ascent）。仍然用向量的形式来对参数进行更新，也就是：</p>
<script type="math/tex; mode=display">\theta := \theta + \alpha\nabla_{\theta}l(\theta)</script><p>因为是求极大值，所以注意下式子中是+号而不是-号，用一个样本$(x,y)$来推导随机梯度法的导数公式：</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial\theta_{j}}l(\theta) = (y-h_{\theta}(x))x_{j}</script><p><strong>请自行推导哦</strong></p>
<p>所以，随机梯度上升的更新公式为：</p>
<script type="math/tex; mode=display">\theta_{j} := \theta_{j}+\alpha(y^{i}-h_{\theta}(x^{i}))x_{j}^{i}</script><p>跟之前的 LMS 更新规则相对比，形式上挺相似的，不过这并不是同一个算法，因为这里的$h_{\theta}(x^{i})$现在定义成了一个$\theta^{T}x^{i}$的非线性函数。尽管如此，面对不同的学习问题使用了不同的算法，却得到了看上去一样的更新规则，这个还是有点让人吃惊。这是一个巧合么，还是背后有更深层次的原因呢？在我们学到了 GLM 广义线性模型时就会得到答案了。</p>
<h3 id="6，题外话：感知学习算法"><a href="#6，题外话：感知学习算法" class="headerlink" title="6，题外话：感知学习算法"></a>6，题外话：感知学习算法</h3><p>现在岔开一下话题，简要地聊另一个算法，这个算法的历史很有趣，并且之后在讲学习理论时还要讲到它。</p>
<p>设想一下，对逻辑回归方法修改一下，强制它输出的值只能是0或1。要实现这个目的，很自然就应该把函数 $g$ 的定义修改一下，改成一个阈值函数（threshold function）</p>
<script type="math/tex; mode=display">g(z) = \left\{\begin{array}{ll} 1 & if\quad z\geq0\\ 0 & if\quad z<0 \end{array}\right.</script><p>然后，令$h_{\theta}(x)=g(\theta^{T}x)$，使用上面定义的阈值函数$g$，然后更新规则即为：</p>
<script type="math/tex; mode=display">\theta_{j} := \theta_{j}+\alpha(y^{i}-h_{\theta}(x^{i}))x^{i}_{j}</script><p>这就是感知学习算法。</p>
<p>在 1960 年代，“感知器（perceptron）”被认为是对大脑中单个神经元工作方法的一个粗略建模，这个简单的算法也是我们后续在本课程中讲学习理论时的起点。</p>
<p>但一定要注意，虽然这个感知学习算法可能看上去跟之前讲的其他算法挺相似，但实际上这是一个和逻辑回归、最小二乘线性回归等算法在本质上完全不同的算法。</p>
<p>尤其重要的是，很难对感知器的预测赋予有意义的概率解释，也很难作为一种最大似然估计算法来推出感知器学习算法。</p>
<h3 id="7，最大化-l-theta-的另一种算法"><a href="#7，最大化-l-theta-的另一种算法" class="headerlink" title="7，最大化$l(\theta)$的另一种算法"></a>7，最大化$l(\theta)$的另一种算法</h3><p>回到$g(z)$为sigmoid函数时的逻辑回归算法，重新讨论下最大化$l(\theta)$的算法。</p>
<p>首先来考虑牛顿法来求方程零点，如有函数$f: R-&gt;R$，求参数$\theta$使得$f(\theta)=0$，此处的$\theta\in R$ 是实数。</p>
<p>牛顿法的更新规则是：</p>
<script type="math/tex; mode=display">\theta := \theta-\frac{f(\theta)}{f'(\theta)}</script><p>这个方法可以通过一个很自然的解释，把它理解成用一个线性函数来对函数 $f$ 进行逼近，这条直线是 $f$ 的切线，而猜测值是 $\theta$，求解的方法就是找到线性方程等于零的点，把这一个零点作为 $\theta$ 值设置为下一次猜测，然后依次类推。</p>
<p><img src="newtons method.png" alt="newtons_method"></p>
<p>在最左边的图里面，可以看到函数 $f$ 和 $y=0$ 的图像，想要找一个 $\theta$ 来让 $f(\theta)=0$，这时候发现这个 $\theta$ 值大概在 1.3 左右。加入咱们猜测的初始值设定为 $\theta =4.5$，牛顿法就是在 $\theta =4.5$ 这个位置画一条切线（中间的图）。这样就给出了下一个 $\theta$  猜测值的位置，也就是这个切线的零点，大概是2.8。最右面的图中的是再运行一次这个迭代产生的结果，这时候 θ 大概是1.8。就这样几次迭代之后，很快就能接近 $\theta =1.3$。</p>
<p>牛顿方法给出了求解$f(\theta)=0$，怎么用它来最大化损失函数$l$呢？$l$取最大值的点应该是导数$l’(\theta)$的第一个零点。所以，令$f(\theta)=l’(\theta)$，可以用同样的算法来最大化$l$，能得到如下更新规则：</p>
<script type="math/tex; mode=display">\theta := \theta-\frac{f'(\theta)}{f''(\theta)}</script><p>思考下：如果是最小化函数而不是最大化函数呢？应该是怎样的更新规则？</p>
<p>近期介绍的逻辑回归中，$\theta$是向量，所以需要将牛顿方法一般化。多维空间中的牛顿方法（也叫Newton-Raphson方法）更新规则为：</p>
<script type="math/tex; mode=display">\theta := \theta - H^{-1}\nabla_{\theta}l(\theta)</script><p>此处的$\nabla_{\theta}l(\theta)$是$l(\theta)$关于$\theta_{i}$的导数，$H$是$n$维的矩阵（加上偏置项是$n+1$维矩阵），叫Hessian矩阵，表达式：</p>
<script type="math/tex; mode=display">H_{ij}=\frac{\partial^{2}l(\theta)}{\partial\theta_{i}\partial\theta_{j}}</script><p>牛顿方法比（批量）梯度下降法更快收敛，更少的迭代次数就能获得极小。但是，一次牛顿迭代比梯度下降的计算量大很多，需要求Hessian矩阵及其逆矩阵，如果$n$不大的情况下，牛顿法明显有更快的优势。当用牛顿方法来解决逻辑回归的似然函数的最大化问题时，通常也把求解过程叫做Fisher scoring。</p>
<h2 id="PART-III-广义线性模型（GLM）"><a href="#PART-III-广义线性模型（GLM）" class="headerlink" title="PART III 广义线性模型（GLM）"></a>PART III 广义线性模型（GLM）</h2><p>目前位置，我们讨论了一个回归($y|x;\theta\sim\mathcal{N}(u,\sigma^{2})$)和一个分类($y|x;\theta\sim Bernoulli(\phi)$)案例，$u$和$ \phi $是定义在$x$和$ \theta $上的函数。在本节，我们会发现这两种方法都是一个更广泛使用的模型的特例，这种更广泛使用的模型就叫做广义线性模型。我们还会讲一下广义线性模型中的其他模型是如何推出的，以及如何应用到其他的分类和回归问题上。</p>
<h3 id="指数簇"><a href="#指数簇" class="headerlink" title="指数簇"></a>指数簇</h3><p>在学习 GLMs 之前，我们要先定义一下指数组分布。如果一个分布能用下面的方式来写出来，我们就说这类分布属于指数族：</p>
<script type="math/tex; mode=display">p(y;\eta) = b(y)exp(\eta^{T}T(y)-a(\eta))</script><p>上面的式子中，$\eta$叫做此分布的自然参数（也叫典范参数），$T(y)$ 叫做充分统计量，我们目前用的这些分布中通常$T(y) = y$；而 $a(\eta)$ 是一个对数分割函数，$e^{-a(\eta)}$这个量本质上扮演了归一化常数的角色，也就是确保 $p(y;\eta)$的总和等于1</p>
]]></content>
      <categories>
        <category>cs229</category>
      </categories>
      <tags>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>shopify-cli在ubuntu20.04上搭建使用环境</title>
    <url>/2022/06/17/ubuntuOS/shopify-cli/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在本地构建shopify中主题和app的开发环境，在ubuntu和docker环境中构建的主要步骤~</p>
<p>主题涉及到liquid、css和html，前端可能相对更熟悉。</p>
<p><strong>先尝试主题的构建，再逐步到app开发~</strong><br><span id="more"></span></p>
<h2 id="1，基础docker"><a href="#1，基础docker" class="headerlink" title="1，基础docker"></a>1，基础docker</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it --name=shopify -p 9292:9292 -p 3456:3456 -v C:\Coding\shopifyStoreTheme:/home ubuntu20.04</span><br></pre></td></tr></table></figure>
<p>9292是本地预览的端口，3456是在关联到远端shopify店铺时的信息交互端口</p>
<h2 id="2，shopify-cli环境配置"><a href="#2，shopify-cli环境配置" class="headerlink" title="2，shopify-cli环境配置"></a>2，shopify-cli环境配置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br><span class="line"></span><br><span class="line"># 安装ruby</span><br><span class="line">apt-get install -y ruby-full</span><br><span class="line"></span><br><span class="line"># 验证，ruby --version和gem --version可查看版本号</span><br><span class="line"></span><br><span class="line"># 更改ruby的源到国内</span><br><span class="line">gem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/</span><br><span class="line"></span><br><span class="line"># 更新缓存</span><br><span class="line">gem sources -u</span><br><span class="line"></span><br><span class="line"># shopify-cli是ruby的依赖管理器gem的一个依赖包，类似nodejs对于npm一样。</span><br><span class="line">gem install shopify-cli</span><br><span class="line"></span><br><span class="line"># 需要这个组件</span><br><span class="line">apt-get install -y git</span><br></pre></td></tr></table></figure>
<h2 id="3，shopify-cli命令"><a href="#3，shopify-cli命令" class="headerlink" title="3，shopify cli命令"></a>3，shopify cli命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用shopify help &lt;command&gt; 显示有关特定命令的详细信息。</span><br><span class="line"> </span><br><span class="line"># 本地预览</span><br><span class="line">shopify theme serve --host=0.0.0.0 --port=9292</span><br></pre></td></tr></table></figure>
<p>Note：默认是120.0.0.1:9292的访问地址，但是因为在docker中，所以host改成了0.0.0.0，端口不变</p>
<h2 id="4，参考文献"><a href="#4，参考文献" class="headerlink" title="4，参考文献"></a>4，参考文献</h2><p><a href="https://shopify.dev/themes">帮助文档</a></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>shopify-cli</tag>
      </tags>
  </entry>
  <entry>
    <title>jetson性能对比</title>
    <url>/2022/06/09/cv_engineering/jetson-performance/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>yolo, mask rcnn和其他图像分类算法在常用jetson硬件上的表现，仅供参考哦~</p>
<p><strong>工程上选模型可做参考~</strong><br><span id="more"></span></p>
<h2 id="1，yolo系列"><a href="#1，yolo系列" class="headerlink" title="1，yolo系列"></a>1，yolo系列</h2><div class="table-container">
<table>
<thead>
<tr>
<th>network</th>
<th>device</th>
<th>activation</th>
<th>precision</th>
<th>batch</th>
<th>DLA</th>
<th>framework</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>yolov3</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>24ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>18ms</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>30ms</td>
</tr>
<tr>
<td>-</td>
<td>TX2</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>99ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>90ms(22.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>58ms(14.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>TX2</td>
<td>leaky</td>
<td>fp16</td>
<td>32</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>2930ms(91.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>32</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>440ms(13.75ms each)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>leaky</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>104ms(26ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>20ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>12.5ms</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>leaky</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>20ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>66ms(16.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>36ms(9ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>32</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>256ms(8ms each)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>leaky</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>64ms(16ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>52ms(13ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>10ms</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>relu</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>17ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>30ms(7.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT7.1.0</td>
<td>58ms(14.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>54ms(13.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>1050ti</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>45ms(11.25ms each)</td>
</tr>
<tr>
<td>yolov3-tiny</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>5ms</td>
</tr>
<tr>
<td>yolo-resnet</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>14ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>44ms(11ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>12ms</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>leaky</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>39ms(10ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>fp16</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>30ms(7.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>fp16</td>
<td>4</td>
<td>yes</td>
<td>TRT5.1.6</td>
<td>68ms(17ms each)</td>
</tr>
<tr>
<td>-</td>
<td>Xavier</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>22ms(5.5ms each)</td>
</tr>
<tr>
<td>-</td>
<td>1050ti</td>
<td>relu</td>
<td>int8</td>
<td>4</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>24ms(6ms each)</td>
</tr>
</tbody>
</table>
</div>
<h2 id="2，mask-rcnn模型"><a href="#2，mask-rcnn模型" class="headerlink" title="2，mask rcnn模型"></a>2，mask rcnn模型</h2><div class="table-container">
<table>
<thead>
<tr>
<th>device</th>
<th>input shape</th>
<th>precision</th>
<th>batch</th>
<th>framework</th>
<th>pure enqueue time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1050ti</td>
<td>1024x1024</td>
<td>fp32</td>
<td>1</td>
<td>TRT7.0</td>
<td>364ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>-</td>
<td>-</td>
<td>140ms</td>
</tr>
<tr>
<td>Xavier</td>
<td>-</td>
<td>fp16</td>
<td>-</td>
<td>TRT7.1</td>
<td>136ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>-</td>
<td>-</td>
<td>103ms</td>
</tr>
<tr>
<td>NX</td>
<td>-</td>
<td>fp32</td>
<td>-</td>
<td>-</td>
<td>871ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>fp16</td>
<td>-</td>
<td>-</td>
<td>239ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>-</td>
<td>-</td>
<td>165ms</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3，常用图像分类模型"><a href="#3，常用图像分类模型" class="headerlink" title="3，常用图像分类模型"></a>3，常用图像分类模型</h2><div class="table-container">
<table>
<thead>
<tr>
<th>network</th>
<th>device</th>
<th>precision</th>
<th>batch</th>
<th>DLA</th>
<th>framework</th>
<th>pure enqueue time</th>
</tr>
</thead>
<tbody>
<tr>
<td>google net</td>
<td>apex</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>1.5ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>3.5ms(avg 0.9ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>5.5ms(avg 0.7ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>17.5ms(avg 0.55ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>64ms(avg 0.5ms)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>half</td>
<td>1</td>
<td>-</td>
<td>TRT7.1</td>
<td>3ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>2ms</td>
</tr>
<tr>
<td>-</td>
<td>tx2</td>
<td>half</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>5.2ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>118ms(avg 3.7ms)</td>
</tr>
<tr>
<td>-</td>
<td>1050ti</td>
<td>float32</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>3.5ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>11ms(avg 2.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>16ms(avg 2ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>61ms(avg 1.9ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>236ms(avg 1.84ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>1.5ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>4.5ms(avg 1.1ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>6ms(avg 0.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>24ms(avg 0.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>90ms(avg 0.7ms)</td>
</tr>
<tr>
<td>resnet50</td>
<td>apex</td>
<td>int8</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>2.2ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>4.3ms(avg 1.1ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>7.5ms(avg 0.9ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>25ms(avg 0.8ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>94.5ms(avg 0.74ms)</td>
</tr>
<tr>
<td>-</td>
<td>NX</td>
<td>half</td>
<td>1</td>
<td>-</td>
<td>TRT7.1</td>
<td>6ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>103ms(avg 3.2ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>3.8ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>64ms(avg 2ms)</td>
</tr>
<tr>
<td>-</td>
<td>tx2</td>
<td>half</td>
<td>1</td>
<td>-</td>
<td>TRT5.1.6</td>
<td>13ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>320ms(avg 10ms)</td>
</tr>
<tr>
<td>-</td>
<td>1050ti</td>
<td>float32</td>
<td>1</td>
<td>no</td>
<td>TRT5.1.6</td>
<td>8ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>23ms(avg 5.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>38ms(avg 4.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>133ms(avg 4ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>510ms(avg 4ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>int8</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>3ms</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4</td>
<td>-</td>
<td>-</td>
<td>8ms(avg 2ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>-</td>
<td>-</td>
<td>14ms(avg 1.75ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>32</td>
<td>-</td>
<td>-</td>
<td>44ms(avg 1.4ms)</td>
</tr>
<tr>
<td>-</td>
<td>-</td>
<td>-</td>
<td>128</td>
<td>-</td>
<td>-</td>
<td>167ms(avg 1.3ms)</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>jetson</tag>
      </tags>
  </entry>
  <entry>
    <title>人脸识别的算法演变</title>
    <url>/2022/06/08/facerecog/face-recognition-algorithms/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>人脸识别的算法演变之路，从传统方法到CNN，再到各种loss和其他的优化方向等等~</p>
<p><strong>抓紧时间看论文啊，会不断更新的~</strong><br><span id="more"></span></p>
<h2 id="1，一些传统思路"><a href="#1，一些传统思路" class="headerlink" title="1，一些传统思路"></a>1，一些传统思路</h2><h3 id="Eigenface"><a href="#Eigenface" class="headerlink" title="Eigenface"></a>Eigenface</h3><h3 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h3><h3 id="LBP-Joint-Bayes"><a href="#LBP-Joint-Bayes" class="headerlink" title="LBP+Joint Bayes"></a>LBP+Joint Bayes</h3><h2 id="2，CNN基线"><a href="#2，CNN基线" class="headerlink" title="2，CNN基线"></a>2，CNN基线</h2><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><h2 id="3，CNN和各种Loss"><a href="#3，CNN和各种Loss" class="headerlink" title="3，CNN和各种Loss"></a>3，CNN和各种Loss</h2><h3 id="Pairwise"><a href="#Pairwise" class="headerlink" title="Pairwise"></a>Pairwise</h3><h3 id="Triplet"><a href="#Triplet" class="headerlink" title="Triplet"></a>Triplet</h3><h3 id="DeepFace"><a href="#DeepFace" class="headerlink" title="DeepFace"></a>DeepFace</h3><h3 id="DeepID1"><a href="#DeepID1" class="headerlink" title="DeepID1"></a>DeepID1</h3><h3 id="DeepID2"><a href="#DeepID2" class="headerlink" title="DeepID2"></a>DeepID2</h3><h3 id="DeepID2-1"><a href="#DeepID2-1" class="headerlink" title="DeepID2+"></a>DeepID2+</h3><h3 id="DeepID3"><a href="#DeepID3" class="headerlink" title="DeepID3"></a>DeepID3</h3><h3 id="FaceNet"><a href="#FaceNet" class="headerlink" title="FaceNet"></a>FaceNet</h3><h3 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h3><h2 id="归一化等"><a href="#归一化等" class="headerlink" title="归一化等"></a>归一化等</h2><h3 id="Lsoftmax"><a href="#Lsoftmax" class="headerlink" title="Lsoftmax"></a>Lsoftmax</h3><h3 id="Asoftmax"><a href="#Asoftmax" class="headerlink" title="Asoftmax"></a>Asoftmax</h3><h3 id="NormFace-Coco-Loss"><a href="#NormFace-Coco-Loss" class="headerlink" title="NormFace/Coco Loss"></a>NormFace/Coco Loss</h3><h3 id="Feature-Incay"><a href="#Feature-Incay" class="headerlink" title="Feature Incay"></a>Feature Incay</h3><h3 id="AMSoftmax-CosFace"><a href="#AMSoftmax-CosFace" class="headerlink" title="AMSoftmax/CosFace"></a>AMSoftmax/CosFace</h3><h3 id="Arcface-InsightFace"><a href="#Arcface-InsightFace" class="headerlink" title="Arcface/InsightFace"></a>Arcface/InsightFace</h3>]]></content>
      <categories>
        <category>facerecog</category>
      </categories>
      <tags>
        <tag>face recognition</tag>
      </tags>
  </entry>
  <entry>
    <title>UI设计资源</title>
    <url>/2022/06/02/frontend/UI-source/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>设计师资源，日常APP或者网页设计中用得上的，基本免费，当然也有消费的项目~</p>
<p><strong>物美价廉~</strong><br><span id="more"></span></p>
<h2 id="1-最大的免费图标库-Flaticon"><a href="#1-最大的免费图标库-Flaticon" class="headerlink" title="1, 最大的免费图标库-Flaticon"></a>1, 最大的免费图标库-Flaticon</h2><p><a href="https://www.flaticon.com/">official link</a></p>
<p>英文关键字搜索，常用图标格式，矢量图或像素图</p>
<h2 id="2-阿里巴巴矢量图标库-iconfont"><a href="#2-阿里巴巴矢量图标库-iconfont" class="headerlink" title="2, 阿里巴巴矢量图标库-iconfont"></a>2, 阿里巴巴矢量图标库-iconfont</h2><p><a href="https://www.iconfont.cn/">official link</a></p>
<p><strong>哈哈，我的最爱~</strong><br>中英文，拼音啥的都可以搜索</p>
<p>多种图标格式，矢量和像素图</p>
<h2 id="3-Easyicon"><a href="#3-Easyicon" class="headerlink" title="3, Easyicon"></a>3, Easyicon</h2><p><a href="https://www.easyicon.net/">official link</a></p>
<p>多种图标格式，可转换第三方图标为微软或mac图标</p>
<h2 id="4-扁平化APP图标库-iconsDB"><a href="#4-扁平化APP图标库-iconsDB" class="headerlink" title="4, 扁平化APP图标库-iconsDB"></a>4, 扁平化APP图标库-iconsDB</h2><p><a href="https://www.iconsdb.com/">official link</a></p>
<p>大部分支持个人和商业用途</p>
<h2 id="5-爱看图标网-IconPng"><a href="#5-爱看图标网-IconPng" class="headerlink" title="5, 爱看图标网-IconPng"></a>5, 爱看图标网-IconPng</h2><p><a href="http://www.iconpng.com/">official link</a></p>
<h2 id="6-图标设计综合网站-IconDeposit"><a href="#6-图标设计综合网站-IconDeposit" class="headerlink" title="6, 图标设计综合网站-IconDeposit"></a>6, 图标设计综合网站-IconDeposit</h2><p><a href="https://www.icondeposit.com/">official link</a></p>
<h2 id="7-社区爱-icons8"><a href="#7-社区爱-icons8" class="headerlink" title="7, 社区爱-icons8"></a>7, 社区爱-icons8</h2><p><a href="https://icons8.cn/">official link</a></p>
<h2 id="8-iconfinder"><a href="#8-iconfinder" class="headerlink" title="8, iconfinder"></a>8, iconfinder</h2><p><a href="https://www.iconfinder.com/">official link</a></p>
<h2 id="9-axure商城"><a href="#9-axure商城" class="headerlink" title="9, axure商城"></a>9, axure商城</h2><p><a href="https://www.axureshop.com/">official link</a></p>
<p>很多原型，各个业务领域，高保真、低保真都有</p>
]]></content>
      <categories>
        <category>frontend</category>
      </categories>
      <tags>
        <tag>UI source</tag>
      </tags>
  </entry>
  <entry>
    <title>useful pip source in China</title>
    <url>/2022/05/20/python/pip-source-in-China/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>python安装模块常用的是pip，但是source在国外，如果没有好用的梯子，那就换成国内的镜像源吧，本文基本内容如下：</p>
<ul>
<li>国内的常用pip源；</li>
<li>两种pip应用国内源的方式；</li>
<li>清理pip的缓存；</li>
</ul>
<span id="more"></span>
<h2 id="1，国内源"><a href="#1，国内源" class="headerlink" title="1，国内源"></a>1，国内源</h2><ul>
<li>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></li>
<li>阿里云：<a href="http://mirrors.aliyun.com/pypi/simple">http://mirrors.aliyun.com/pypi/simple</a></li>
<li>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple">https://pypi.mirrors.ustc.edu.cn/simple</a></li>
<li>华中理工大学：<a href="http://pypi.hustunique.com">http://pypi.hustunique.com</a></li>
<li>山东理工大学：<a href="http://pypi.sdutlinux.org">http://pypi.sdutlinux.org</a></li>
<li>豆瓣：<a href="http://pypi.douban.com/simple">http://pypi.douban.com/simple</a></li>
</ul>
<h2 id="2，临时使用"><a href="#2，临时使用" class="headerlink" title="2，临时使用"></a>2，临时使用</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install -i http://mirrors.aliyun.com/pypi/simple/</span><br><span class="line">or</span><br><span class="line">pip3 install -i http://mirrors.aliyun.com/pypi/simple/</span><br></pre></td></tr></table></figure>
<h2 id="3，永久使用"><a href="#3，永久使用" class="headerlink" title="3，永久使用"></a>3，永久使用</h2><ul>
<li>linux<br>修改或增加文件<code>~/.pip/pip.conf</code>, 内容为：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host=mirrors.aliyun.com</span><br></pre></td></tr></table></figure></li>
<li>windows<br>文件路径：<code>C:\Users\WQP\pip\pip.ini</code></li>
</ul>
<h2 id="4，清理pip缓存"><a href="#4，清理pip缓存" class="headerlink" title="4，清理pip缓存"></a>4，清理pip缓存</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># linux</span><br><span class="line">~/.cache/pip</span><br><span class="line"># win</span><br><span class="line">%LocalAppData%\pip\Cache</span><br><span class="line"># OS X</span><br><span class="line">~/Library/Caches/pip</span><br></pre></td></tr></table></figure>
<h2 id="5，pip包下载"><a href="#5，pip包下载" class="headerlink" title="5，pip包下载"></a>5，pip包下载</h2><p><a href="https://pypi.org/">pypi</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pip source</tag>
      </tags>
  </entry>
  <entry>
    <title>vmware中的ubuntu突然没有网络</title>
    <url>/2022/05/20/ubuntuOS/vmware-ubuntu1804-wlan-error/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在vmware虚拟机中使用ubuntu系统时，有次突然出现没有了网络，几番查找后，找到了解决办法，但是至今不晓得问题的成因，尴尬了~~尽管如此，先把问题解决了再说吧^_^</p>
<p><strong>come on, girl~~</strong><br><span id="more"></span></p>
<h2 id="1，首先查看Ubuntu的网络设置"><a href="#1，首先查看Ubuntu的网络设置" class="headerlink" title="1，首先查看Ubuntu的网络设置"></a>1，首先查看Ubuntu的网络设置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /var/lib/NetworkManager</span><br><span class="line">sudo cat NetworkManager.state</span><br></pre></td></tr></table></figure>
<p>如果NetworkingEnabled=false，那么进入下一步，否则查找其他原因</p>
<h2 id="2，解决方案"><a href="#2，解决方案" class="headerlink" title="2，解决方案"></a>2，解决方案</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 关闭网络</span><br><span class="line">sudo service network-manager stop</span><br><span class="line"># 用vi或者gedit修改NetworkManager.state</span><br><span class="line">NetworkingEnabled=true</span><br><span class="line"># 重新开启网络</span><br><span class="line">sudo service network-manager start</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>ubuntu wlan error</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu中解决PL2303的驱动问题</title>
    <url>/2022/05/20/hardware/ubuntu-PL2303/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个问题出现在，win10的PC+VMWare的ubuntu+hi3559a环境搭建时ubuntu系统出现的驱动问题，若有类似情况，亦可用下文方案解决。</p>
<span id="more"></span>
<h2 id="1，详细步骤"><a href="#1，详细步骤" class="headerlink" title="1，详细步骤"></a>1，详细步骤</h2><ul>
<li><p>文件复制</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /lib/modules/4.2.0-27-generic/kernel/drivers/usb/serial/pl2303.ko /usr/src/linux-headers-4.2.0-27-generic/drivers/usb/serial</span><br></pre></td></tr></table></figure>
<p>内核不同，可能路径不同</p>
</li>
<li><p>安装命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ modprobe usbserial</span><br><span class="line">$ modprobe pl2303</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证<br>输入<code>lsmod | grep usbserial</code>可以看到<code>usbserial</code>信息说明安装成功；<br>输入<code>dmesg | tail</code>可以看到<code>usb pl2303</code>等信息亦说明安装成功；</p>
</li>
</ul>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>ubuntu PL2303</tag>
      </tags>
  </entry>
  <entry>
    <title>边缘提取方法</title>
    <url>/2022/05/19/cv_engineering/edge-extract/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Chen Mingming老师组的三个边缘提取的模型，还有部分数据集，效果不错~</p>
<p><strong>用边缘特征做过一些图像应用哦</strong><br><span id="more"></span></p>
<h2 id="1，边缘提取hed"><a href="#1，边缘提取hed" class="headerlink" title="1，边缘提取hed"></a>1，边缘提取hed</h2><p><a href="https://github.com/s9xie/hed">github link</a></p>
<ul>
<li>作者<br>南开大学程明明</li>
<li>对应文章   <ul>
<li>author = {“Xie, Saining and Tu, Zhuowen”},</li>
<li>Title = {Holistically-Nested Edge Detection},</li>
<li>Booktitle = “Proceedings of IEEE International Conference on Computer Vision”,</li>
<li>Year = {2015},</li>
</ul>
</li>
<li>结果<br>BSDS benchmark数据集：<br>ODS=.790 and OIS=.808<br>BSD500 数据集：<br>ODS F-score of .790<br>NYU Depth 数据集：<br>ODS F-score of .746<br>速度： (0.4s per image)</li>
</ul>
<h2 id="2，边缘提取rcf"><a href="#2，边缘提取rcf" class="headerlink" title="2，边缘提取rcf"></a>2，边缘提取rcf</h2><p><a href="https://github.com/meteorshowers/RCF-pytorch">pytorch版本：github</a><br><a href="https://github.com/yun-liu/rcf">caffe版本：github</a></p>
<ul>
<li>作者<br>南开大学程明明</li>
<li>文章<br>title={Richer Convolutional Features for Edge Detection},<br>author={Liu, Yun and Cheng, Ming-Ming and Hu, Xiaowei and Bian, Jia-Wang and Zhang, Le and Bai, Xiang and Tang, Jinhui},<br>journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},<br>year={2019}</li>
<li>特性<br>Using VGG16 network；<br>BSDS500 benchmark 数据集：<br>ODS F-measure of 0.811， speed (8 FPS)<br>fast version of RCF 快速版本：<br>ODS F-measure of 0.806 with 30 FPS</li>
</ul>
<h2 id="3，边缘提取dfi"><a href="#3，边缘提取dfi" class="headerlink" title="3，边缘提取dfi"></a>3，边缘提取dfi</h2><p><a href="https://github.com/backseason/DFI">github link</a></p>
<ul>
<li>作者<br>南开大学程明明</li>
<li>文章<br>title={Dynamic Feature Integration for Simultaneous Detection of Salient Object, Edge and Skeleton},<br>author={Jiang-Jiang Liu and Qibin Hou and Ming-Ming Cheng},<br>journal={IEEE Transactions on Image Processing},<br>year={2020}</li>
</ul>
]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>edge extract</tag>
      </tags>
  </entry>
  <entry>
    <title>hi3559a开发记录</title>
    <url>/2022/05/19/hardware/hi3559a-coding/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>hisi视觉模型开发，主要是检测任务，三个类别，本文包含项目开发的基本流程，包括模型训练，模型转化以及hisi官方提供的关于yolov3的源码及其调用分析。</p>
<span id="more"></span>
<h2 id="1，模型训练"><a href="#1，模型训练" class="headerlink" title="1，模型训练"></a>1，模型训练</h2><h3 id="1-1-运行环境"><a href="#1-1-运行环境" class="headerlink" title="1.1 运行环境"></a>1.1 运行环境</h3><p>在caffe的基础docker中操作的，方便后续其他视觉框架转到caffe的模型，其实训练框架用的是darknet。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull bvlc/caffe</span><br></pre></td></tr></table></figure></p>
<h3 id="1-2-使用darknet进行训练"><a href="#1-2-使用darknet进行训练" class="headerlink" title="1.2 使用darknet进行训练"></a>1.2 使用darknet进行训练</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd darknet</span><br></pre></td></tr></table></figure>
<ul>
<li>编译darknet<br>  <a href="https://www.cnblogs.com/gocodinginmyway/p/13747221.html">darket环境搭建参考</a></li>
<li>根据kmeans算法生成对应数据集和对应尺寸的anchors，并修改cfg<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./darknet detector calc_anchors data/visdrone.data -num_of_clusters 6 -width 416 -height 416</span><br></pre></td></tr></table></figure></li>
<li>训练yolo-fastest<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./darknet detector train ./data/visdrone.data ./cfg/yolo-fastest.cfg yolo-fastest.conv.109</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="1-3-其他训练命令"><a href="#1-3-其他训练命令" class="headerlink" title="1.3 其他训练命令"></a>1.3 其他训练命令</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./darknet detector train ./data/visdrone.data ./cfg/yolo-fastest-xl.cfg yolo-fastest-xl.conv.109</span><br><span class="line">./darknet detector train ./data/visdrone.data ./cfg/yolov4-visdrone.cfg yolov4.conv.137</span><br><span class="line">./darknet detector train ./data/visdrone.data ./cfg/MobileNetV2-yolov3-lite.cfg MobileNetV2--Lite.conv.57</span><br></pre></td></tr></table></figure>
<h2 id="2，模型转化"><a href="#2，模型转化" class="headerlink" title="2，模型转化"></a>2，模型转化</h2><h3 id="2-1-转化为caffe模型"><a href="#2-1-转化为caffe模型" class="headerlink" title="2.1 转化为caffe模型"></a>2.1 转化为caffe模型</h3><p><a href="https://github.com/ChenYingpeng/darknet2caffe">yolov4转caffe_link</a><br><a href="https://github.com/dog-qiuqiu/MobileNet-Yolo/tree/master/darknet2caffe">yolofastest转caffe_link</a><br><a href="./darknet2caffe-hi3559a.markdown">自训练的yolov4模型darknet转caffe</a></p>
<h3 id="2-2-仿真模型转化"><a href="#2-2-仿真模型转化" class="headerlink" title="2.2 仿真模型转化"></a>2.2 仿真模型转化</h3><ul>
<li><p>仿真模型转化参数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[prototxt_file] ./mark_prototxt/yolov4-visdrone-2class_mark_nnie_20210219113115.prototxt</span><br><span class="line">[caffemodel_file] D:\BaiduNetdiskDownload\yolov4-visdrone-2class-shortcut16\yolov4-visdrone-2class.caffemodel</span><br><span class="line">[batch_num] 1</span><br><span class="line">[net_type] 0</span><br><span class="line">[sparse_rate] 0</span><br><span class="line">[compile_mode] 0</span><br><span class="line">[is_simulation] 1</span><br><span class="line">[log_level] 2</span><br><span class="line">[instruction_name] ./../data/detection/yolov4/inst/yolov4_func</span><br><span class="line">[RGB_order] BGR</span><br><span class="line">[data_scale] 0.0039062</span><br><span class="line">[internal_stride] 16</span><br><span class="line">[image_list] ./../data/detection/yolov4/image_ref_list.txt</span><br><span class="line">[image_type] 1</span><br><span class="line">[mean_file] null</span><br><span class="line">[norm_type] 3</span><br></pre></td></tr></table></figure>
</li>
<li><p>仿真模型转化输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Start [RuyiStudio Wk NNIE Mapper] [D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_func.cfg] sample_simulator (2021-02-19 11:32:18)</span><br><span class="line">Mapper Version 1.1.3.0_B010 (NNIE_1.1) 1905091707159355</span><br><span class="line"></span><br><span class="line">begin net parsing....</span><br><span class="line"></span><br><span class="line">end net parsing</span><br><span class="line"></span><br><span class="line">begin prev optimizing....</span><br><span class="line"></span><br><span class="line">end prev optimizing....</span><br><span class="line"></span><br><span class="line">begin net quantalizing(GPU)....</span><br><span class="line"></span><br><span class="line">end quantalizing</span><br><span class="line"></span><br><span class="line">begin POST optimizing....</span><br><span class="line"></span><br><span class="line">end POST optimizing</span><br><span class="line"></span><br><span class="line">begin NNIE[0] mem allocation....</span><br><span class="line"></span><br><span class="line">.end NNIE[0] memory allocating</span><br><span class="line"></span><br><span class="line">begin NNIE[0] instruction generating....</span><br><span class="line"></span><br><span class="line">..............end NNIE[0] instruction generating</span><br><span class="line"></span><br><span class="line">begin lbs binary code generating....</span><br><span class="line"></span><br><span class="line">end lbs binary code generating</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">===============D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_func.cfg Successfully!===============</span><br><span class="line"></span><br><span class="line">End [RuyiStudio Wk NNIE Mapper] [D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_func.cfg] sample_simulator (2021-02-19 11:34:58)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2-3-板子模型转化"><a href="#2-3-板子模型转化" class="headerlink" title="2.3 板子模型转化"></a>2.3 板子模型转化</h3><ul>
<li><p>板子模型转化参数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[prototxt_file] ./mark_prototxt/yolov4-visdrone-2class_mark_nnie_20210219114447.prototxt</span><br><span class="line">[caffemodel_file] D:\BaiduNetdiskDownload\yolov4-visdrone-2class-shortcut16\yolov4-visdrone-2class.caffemodel</span><br><span class="line">[batch_num] 1</span><br><span class="line">[net_type] 0</span><br><span class="line">[sparse_rate] 0</span><br><span class="line">[compile_mode] 0</span><br><span class="line">[is_simulation] 0</span><br><span class="line">[log_level] 2</span><br><span class="line">[instruction_name] ./../data/detection/yolov4/inst/yolov4__inst</span><br><span class="line">[RGB_order] BGR</span><br><span class="line">[data_scale] 0.0039062</span><br><span class="line">[internal_stride] 16</span><br><span class="line">[image_list] ./../data/detection/yolov4/image_ref_list.txt</span><br><span class="line">[image_type] 1</span><br><span class="line">[mean_file] null</span><br><span class="line">[norm_type] 3</span><br></pre></td></tr></table></figure>
</li>
<li><p>板子转化输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Start [RuyiStudio Wk NNIE Mapper] [D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_inst.cfg] sample_simulator (2021-02-19 11:45:53)</span><br><span class="line">Mapper Version 1.1.3.0_B010 (NNIE_1.1) 1905091707159355</span><br><span class="line"></span><br><span class="line">begin net parsing....</span><br><span class="line"></span><br><span class="line">end net parsing</span><br><span class="line"></span><br><span class="line">begin prev optimizing....</span><br><span class="line"></span><br><span class="line">end prev optimizing....</span><br><span class="line"></span><br><span class="line">begin net quantalizing(GPU)....</span><br><span class="line"></span><br><span class="line">end quantalizing</span><br><span class="line"></span><br><span class="line">begin optimizing....</span><br><span class="line"></span><br><span class="line">end optimizing</span><br><span class="line"></span><br><span class="line">begin NNIE[0] mem allocation....</span><br><span class="line"></span><br><span class="line">end NNIE[0] memory allocating</span><br><span class="line"></span><br><span class="line">begin NNIE[0] instruction generating....</span><br><span class="line"></span><br><span class="line">...............end NNIE[0] instruction generating</span><br><span class="line"></span><br><span class="line">begin parameter compressing....</span><br><span class="line"></span><br><span class="line">end parameter compressing</span><br><span class="line"></span><br><span class="line">begin compress index generating....</span><br><span class="line"></span><br><span class="line">end compress index generating</span><br><span class="line"></span><br><span class="line">begin binary code generating....</span><br><span class="line"></span><br><span class="line">.end binary code generating</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">===============D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_inst.cfg Successfully!===============</span><br><span class="line"></span><br><span class="line">End [RuyiStudio Wk NNIE Mapper] [D:\hi3559a\SVP_PC\HiSVP_PC_V1.1.3.0\software\data\detection\yolov4\yolov4_inst.cfg] sample_simulator (2021-02-19 11:48:43)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="3，hisi源码调用分析【yolov3为例】"><a href="#3，hisi源码调用分析【yolov3为例】" class="headerlink" title="3，hisi源码调用分析【yolov3为例】"></a>3，hisi源码调用分析【yolov3为例】</h2><p>函数入口源文件，<code>sample_nnie_main.c</code>   </p>
<p>选择<code>Yolov3</code>，会进入<code>sammple_nnie.c</code> 中的<code>SAMPLE_SVP_NNIE_Yolov3</code>，其中可以配置加载模型与图片文件的顺序；<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HI_CHAR *pcSrcFile = &quot;./data/nnie_image/rgb_planar/dog_bike_car_416x416.bgr&quot;;</span><br><span class="line">HI_CHAR *pcModelName = &quot;./data/nnie_model/detection/inst_yolov3_cycle.wk&quot;;</span><br></pre></td></tr></table></figure></p>
<p>而后分别调用：</p>
<ul>
<li>SAMPLE_COMM_SVP_CheckSysInit();</li>
<li>SAMPLE_COMM_SVP_NNIE_LoadModel();</li>
<li>SAMPLE_SVP_NNIE_Yolov3_ParamInit();</li>
<li>SAMPLE_SVP_NNIE_FillSrcData();</li>
<li>SAMPLE_SVP_NNIE_Forward();</li>
<li>SAMPLE_SVP_NNIE_Yolov3_GetResult;</li>
<li>SAMPLE_SVP_NNIE_Detection_PrintResult()<br>依次是系统初始化、NNie加载模型、参数初始化、读取数据、前向推导、获取结果、结果打印。<br>至此，yolov3例程的函数调用基本完成</li>
</ul>
<h2 id="4，关于yolov4的说明"><a href="#4，关于yolov4的说明" class="headerlink" title="4，关于yolov4的说明"></a>4，关于yolov4的说明</h2><p>yolov4的原版模型中，在转仿真或者板子模型时，不支持mish激活函数，有两种解决思路：</p>
<ul>
<li><p>用leakyReLU替换mish【本次交付模型方案】<br>相对于mish作为激活函数，leakyReLU在准确性上略有降低，但是在效率上有优势，因此更加适用于yolov4在嵌入式中的运行；   </p>
</li>
<li><p>数学公式替换<br>caffe中定义好了6种常用的激活函数：ReLu、Sigmod、Tanh、Absval、Power、BNll<br>mish表达式：Mish = x*tanh(ln(1+e^x))<br>BNLL表达式：f(x) = log(1+exp(x))<br>Tanh表达式<br>caffe中eltwise层有PROD类型操作可计算基于元素的乘法   </p>
</li>
</ul>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>hi3559a cv model</tag>
      </tags>
  </entry>
  <entry>
    <title>rcnn，yolo和ssd系列模型</title>
    <url>/2022/05/19/detection/fasterrcnn-yolo-ssd/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>简述目标检测算法早期三个比较出名的系列~</p>
<p><strong>太灌水了，质量要提高啊~</strong><br><span id="more"></span></p>
<h2 id="1，rcnn系列"><a href="#1，rcnn系列" class="headerlink" title="1，rcnn系列"></a>1，rcnn系列</h2><p>针对之前RCNN系列selective search的方法导致算法没有实时性，所以faster rcnn提出RPN网络来取代之前的方法，可以理解为fasterrcnn=fast rcnn+rpn网络，且rpn网络和fast rcnn的分类，回归网络共用特征提取层，这样使得引入RPN网络不会增加太多计算量。整体流程为先使用RPN网络找出可能存在object的区域，再将这些区域送入fast rcnn中进一步定位和分类。所以faster rcnn是典型的Two stage算法。因为faster rcnn中包含了两次定位，所以其精度一般高于YOLO和SSD算法，所以速度一般慢于YOLO和SSD。</p>
<h2 id="2，yolo系列"><a href="#2，yolo系列" class="headerlink" title="2，yolo系列"></a>2，yolo系列</h2><p>YOLO算法的特点是将检测问题转换成回归问题，即YOLO直接通过回归一次既产生坐标，又产生每种类别的概率。YOLO中将每张图分成7*7的网格，每个网格默认可能属于2个object，即在一张图片上提取98个region proposal，相比于faster rcnn使用Anchor机制提取20k个anchor再从中提取最终的300个region proposal，所以faster rcnn的精度比YOLO要高，但是由于需要处理更多region proposal，所以faster rcnn的速度要比YOLO慢。</p>
<h2 id="3，ssd系列"><a href="#3，ssd系列" class="headerlink" title="3，ssd系列"></a>3，ssd系列</h2><p>SSD相比于faster rcnn使用了多层网络特征，而不仅仅使用最后一层feature map。SSD还借鉴了YOLO算法中将检测任务转换为回归任务的思想，且SSD也借鉴了faster rcnn中的anchor机制，只是SSD的anchor不是每个位置的精调，而是类似于YOLO那样在feature map上分割出网格，在网格上产生anchor。但是SSD和YOLO不需要selective search步骤，所以SSD和YOLO同属于One-Stage算法。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>rcnn</tag>
        <tag>yolo</tag>
        <tag>ssd</tag>
      </tags>
  </entry>
  <entry>
    <title>deepstream sample test4</title>
    <url>/2022/05/19/deepstream/deepstream_sample_test4/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>准备系统学习下deepstream的相关内容，先跑下示例，是基于docker的。</p>
<p><strong>继续加油，还有很多内容哦~</strong><br><span id="more"></span></p>
<h2 id="1，deepstream的test4实例"><a href="#1，deepstream的test4实例" class="headerlink" title="1，deepstream的test4实例"></a>1，deepstream的test4实例</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-docker run -it -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY -v/home/hello/code:/home/code --name=kafkads nvcr.io/nvidia/deepstream:5.1-21.02-samples</span><br><span class="line"></span><br><span class="line">apt install python3-gi python3-gst-1.0 -y</span><br><span class="line">apt install python3-distutils</span><br><span class="line">cd /opt/nvidia/deepstream/deepstream-5.1/lib</span><br><span class="line">python3 setup.py install</span><br><span class="line"></span><br><span class="line">apt-get install libglib2.0 libglib2.0-dev</span><br><span class="line">apt-get install libjansson4  libjansson-dev</span><br><span class="line">apt-get install librdkafka1=0.11.3-1build1</span><br><span class="line"></span><br><span class="line">apt-get install libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev</span><br><span class="line">apt-get install libgstrtspserver-1.0-dev libx11-dev</span><br><span class="line"></span><br><span class="line">apt-get install nvidia-cuda-toolkit</span><br><span class="line"></span><br><span class="line">deepstream-test4-app -i /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.h264 -p /opt/nvidia/deepstream/deepstream-5.1/lib/libnvds_kafka_proto.so --conn-str=&quot;192.168.8.116;9092;test&quot; -c cfg_kafka.txt -s 0 --no-display</span><br><span class="line"></span><br><span class="line">python3 deepstream_test_4.py -i /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.h264 -p /opt/nvidia/deepstream/deepstream-5.1/lib/libnvds_kafka_proto.so --conn-str=&quot;192.168.8.116;9092;test&quot; -c cfg_kafka.txt -s &quot;0&quot; --no-display</span><br></pre></td></tr></table></figure>
<h2 id="2，报错"><a href="#2，报错" class="headerlink" title="2，报错"></a>2，报错</h2><ul>
<li><p>python3.6m的动态库    </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ImportError: libpython3.6m.so.1.0: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/weixin_43952432/article/details/100077912">解决方案参考</a></p>
</li>
<li><p>找不到cuda的runtime头文件   </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fatal error: cuda_runtime.h: No such file or directory</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/seaun163/article/details/98962185">解决方案参考</a></p>
</li>
</ul>
<h2 id="3，参考文献"><a href="#3，参考文献" class="headerlink" title="3，参考文献"></a>3，参考文献</h2><p><a href="https://forums.developer.nvidia.com/t/cant-install-deepstream-5-1-on-rtx2070-dgpu/174842/7">nvidia论坛</a></p>
]]></content>
      <categories>
        <category>deepstream</category>
      </categories>
      <tags>
        <tag>deepstream sample</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorrt parser的构建dockerfile</title>
    <url>/2022/05/19/cv_engineering/trt_parser.Dokcerfile/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>trtparser的自构建镜像，是基于官方的trtparser，自己添加了一些常用软件，特点如下：</p>
<ul>
<li>ubuntu换源；</li>
<li>构建pth-&gt;onnx-&gt;trt转模型需要的环境；</li>
<li>python3的常用包；</li>
</ul>
<p><strong>有dockerfile还真的方便~</strong></p>
<span id="more"></span>
<h2 id="dockerfile代码"><a href="#dockerfile代码" class="headerlink" title="dockerfile代码"></a>dockerfile代码</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM nvcr.io/nvidia/tensorrt:20.02-py3</span><br><span class="line"></span><br><span class="line">LABEL maintainer=&quot;XINWEN&quot;</span><br><span class="line">RUN mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">RUN echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; apt-get update \</span><br><span class="line">    &amp;&amp; apt-get -y install clang-format nfs-common \</span><br><span class="line">    &amp;&amp; apt-get -y install openssh-client </span><br><span class="line"></span><br><span class="line">RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py &amp;&amp; python3 get-pip.py --force-reinstall</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ADD ./soft/* /softs/</span><br><span class="line">WORKDIR /softs</span><br><span class="line">RUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip \</span><br><span class="line">    &amp;&amp; pip3 install torch-1.3.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install Pillow-6.0.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install tensorboard-1.15.0-py3-none-any.whl \</span><br><span class="line">    &amp;&amp; pip3 install onnx-1.6.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install opencv_python-4.2.0.32-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install torchvision-0.4.1-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install onnxruntime-1.1.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; cd onnx-simplifier-0.2.2 &amp;&amp; python3 setup.py install \</span><br><span class="line"># RUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip \</span><br><span class="line">#     &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple torch==1.3.0 torchvision==0.4.1 \</span><br><span class="line">#     &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple onnx==1.6.0 onnxruntime==1.1.0 onnx-simplifier==0.2.2 \</span><br><span class="line">#     &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv_python==4.2.0 Pillow==6.0.0 \</span><br><span class="line">#     &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple tensorflow_gpu==1.15.0 \</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pytest pyhamcrest pytest-cov black==19.3b0 isort==4.3.21 flake8 &#x27;pillow&lt;7.0.0&#x27; cython dvc</span><br><span class="line"></span><br><span class="line">WORKDIR /workspace</span><br><span class="line">RUN rm -rf /softs</span><br><span class="line"></span><br><span class="line"># RUN apt-get -y install isort clang-format nfs-common</span><br><span class="line">ENTRYPOINT /bin/bash</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>tensorrt parser</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorrt client的构建dockerfile</title>
    <url>/2022/05/19/cv_engineering/trt_client.Dockerfile/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>trtserver的docker镜像，官方的基础镜像，自己加了一些常用包，然后写了dockerfile来构建，基本内容如下：</p>
<ul>
<li>基础镜像是：tensorrtserver:20.02-py3-clientsdk</li>
<li>包含ubuntu换源；</li>
<li>pip3安装常用包，使用的是清华源；</li>
<li>提前下载了部分whl包到同dockerfile目录的</li>
</ul>
<p><strong>dockerfile加持后真的方便呀~</strong><br><span id="more"></span></p>
<h2 id="dockerfile源码"><a href="#dockerfile源码" class="headerlink" title="dockerfile源码"></a>dockerfile源码</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM nvcr.io/nvidia/tensorrtserver:20.02-py3-clientsdk</span><br><span class="line"></span><br><span class="line">LABEL maintainer=&quot;XINWEN&quot;</span><br><span class="line">RUN mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">RUN echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; echo &quot;deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse&quot; &gt;&gt; /etc/apt/sources.list \</span><br><span class="line">    &amp;&amp; apt-get update \</span><br><span class="line">    &amp;&amp; apt-get -y install clang-format nfs-common</span><br><span class="line"></span><br><span class="line">RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py &amp;&amp; python3 get-pip.py --force-reinstall</span><br><span class="line"></span><br><span class="line">ADD ./soft/* /softs/</span><br><span class="line">WORKDIR /softs</span><br><span class="line">RUN pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip \</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple bcolz \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple onnx==1.5.0 \</span><br><span class="line">    &amp;&amp; pip3 install torch-1.3.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install Pillow-6.0.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install opencv_python-4.2.0.32-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl \</span><br><span class="line">    &amp;&amp; pip3 install torchvision-0.4.1-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    # &amp;&amp; pip3 install onnxruntime-1.1.0-cp36-cp36m-manylinux1_x86_64.whl \</span><br><span class="line">    # &amp;&amp; cd onnx-simplifier-0.2.2 &amp;&amp; python3 setup.py install \</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pytest pyhamcrest pytest-cov black==19.3b0 isort==4.3.21 flake8 &#x27;pillow&lt;7.0.0&#x27; cython dvc \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple scipy \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple sklearn \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple matplotlib \</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple numpy \</span><br><span class="line">    # &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pymongo</span><br><span class="line">    &amp;&amp; pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pymysql</span><br><span class="line"></span><br><span class="line">WORKDIR /workspace</span><br><span class="line">RUN rm -rf /softs</span><br><span class="line"></span><br><span class="line">ENTRYPOINT /bin/bash</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>tensorrt client</tag>
      </tags>
  </entry>
  <entry>
    <title>makefile example--socket &amp; mysql</title>
    <url>/2022/05/19/cpp/makefile%20exam1-SOCKET%20mysql/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>用makefile的方式来编译socket服务端代码，包含常用makefile语法的使用~</p>
<p><strong>好久没这么用cpp了，耍起来啊~</strong><br><span id="more"></span></p>
<h2 id="1，socket服务端编译-用到mysql数据库"><a href="#1，socket服务端编译-用到mysql数据库" class="headerlink" title="1，socket服务端编译(用到mysql数据库)"></a>1，socket服务端编译(用到mysql数据库)</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#用变量定义文件目录</span><br><span class="line">INCL=-I/usr/local/mysql/include -I$(HOME)/incl</span><br><span class="line">#注意动态库或者静态库的写法</span><br><span class="line">LIB=-L/usr/local/mysql/lib -lmysqlclient -lmysqld -lmysqlservices -L$(HOME)/lib -lbanktest</span><br><span class="line">BINDIR=$(HOME)/bin</span><br><span class="line"></span><br><span class="line">.SUFFIXES: .cpp .c</span><br><span class="line"></span><br><span class="line">#后缀为cpp的文件怎么编译成.o</span><br><span class="line">.cpp.o:</span><br><span class="line">	g++ $&#123;INCL&#125; -c $&lt;</span><br><span class="line"></span><br><span class="line">#后缀为c的文件怎么编译成.o</span><br><span class="line">.c.o:</span><br><span class="line">	gcc $(INCL) -c $&lt;</span><br><span class="line"></span><br><span class="line">all: clean server</span><br><span class="line"></span><br><span class="line">server:server.o</span><br><span class="line">	@echo &quot;============开始编译============&quot;</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	@mv $@ $(BINDIR)</span><br><span class="line">	@echo &quot;============编译结束============&quot;</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">	@rm -f *.o</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>makefile mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>makefile example--des &amp; md5 &amp; base64</title>
    <url>/2022/05/19/cpp/makefile%20exam2-des%20md5%20base64/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>makefile编译代码，有其他需要链接的库，包含mysql，base64等等三方服务，详情看代码吧~</p>
<p><strong>写这个大概在五年之前了哦~</strong><br><span id="more"></span></p>
<h2 id="编译des-md5-base64密码服务"><a href="#编译des-md5-base64密码服务" class="headerlink" title="编译des md5 base64密码服务"></a>编译des md5 base64密码服务</h2><ul>
<li>all 规则示例   <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">INCL=-I/usr/local/mysql/include -I$(HOME)/incl</span><br><span class="line">LIB=-L/usr/local/mysql/lib -lmysqlclient -lmysqld -lmysqlservices</span><br><span class="line">BINDIR=$(HOME)/bin</span><br><span class="line">LIBDIR=$(HOME)/lib</span><br><span class="line"></span><br><span class="line">.SUFFIXES: .cpp .c</span><br><span class="line"></span><br><span class="line">.cpp.o:</span><br><span class="line">	g++ $&#123;INCL&#125; -c $&lt;</span><br><span class="line">.c.o:</span><br><span class="line">	gcc $(INCL) -c $&lt;</span><br><span class="line"></span><br><span class="line">#[NOTE]</span><br><span class="line">all: clean des md5 base64</span><br><span class="line"></span><br><span class="line">des:des.o main_des.o</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">md5:md5.o main_md5.o</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">base64test:base64.o main_base64.o</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">rsa:rsa.o main_rsa.o</span><br><span class="line">	gcc -o $@ $? $(LIB)</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">libjiami.a:des.o md5.o base64.o</span><br><span class="line">	ar -r $@ $?</span><br><span class="line">	mv $@ $(LIBDIR)</span><br><span class="line"></span><br><span class="line">libdestest:main_des.o</span><br><span class="line">	gcc -o $@ $? $(LIB) -L$(HOME)/lib -ljiami</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">libtest.so:des.c md5.c base64.c</span><br><span class="line">	gcc -o $@ -fPIC -shared $?</span><br><span class="line">	mv $@ $(LIBDIR)</span><br><span class="line"></span><br><span class="line">libmd5test:main_md5.o</span><br><span class="line">	gcc -o $@ $? $(LIB) -L$(HOME)/lib -ltest</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">libbanktest.a:banktest.o banksql.o</span><br><span class="line">	ar -r $@ $?</span><br><span class="line">	mv $@ $(LIBDIR)</span><br><span class="line"></span><br><span class="line">banktest:banktest.o banksql.o</span><br><span class="line">	gcc -o $@ $? $(LIB) -L$(HOME)/lib -ltest</span><br><span class="line">	mv $@ $(BINDIR)</span><br><span class="line"></span><br><span class="line">clean:</span><br><span class="line">	rm -f *.o</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>cpp</category>
      </categories>
      <tags>
        <tag>makefile md5 base64</tag>
      </tags>
  </entry>
  <entry>
    <title>mongodb的构建docker</title>
    <url>/2022/05/19/backend/mongodb-docker/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>早期的博客，搬运过来的，貌似仍然能用，可以很方便的构建mongodb数据库~</p>
<p><strong>温故而知新，加油~</strong><br><span id="more"></span></p>
<h2 id="1，Docker搭建Mongodb"><a href="#1，Docker搭建Mongodb" class="headerlink" title="1，Docker搭建Mongodb"></a>1，Docker搭建Mongodb</h2><h3 id="1-1-基本搭建步骤"><a href="#1-1-基本搭建步骤" class="headerlink" title="1.1 基本搭建步骤"></a>1.1 基本搭建步骤</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 获取docker镜像</span><br><span class="line">docker pull mongo</span><br><span class="line"># 创建mongodb容器</span><br><span class="line">docker run --name  my-mongo  -p 27017:27017  -d mongo --auth</span><br><span class="line"># 如果加需要验证就加--auth，不需要验证，就去掉。默认mongodb是不使用用户认证</span><br><span class="line"># 进入容器设置用户</span><br><span class="line">docker exec -it 容器id /bin/bash</span><br><span class="line"># 用户设置命令</span><br><span class="line">mongo</span><br><span class="line">use admin</span><br><span class="line">db.createUser(&#123;user:&quot;root&quot;,pwd:&quot;root&quot;,roles:[&#123;role:&#x27;root&#x27;,db:&#x27;admin&#x27;&#125;]&#125;)   //创建用户,此用户创建成功,则后续操作都需要用户认证</span><br><span class="line">exit</span><br><span class="line"># 或者直接进入admin用户</span><br><span class="line">docker exec -it ly-mongo mongo admin</span><br><span class="line">db.createUser(&#123;user:&quot;root&quot;,pwd:&quot;root&quot;,roles:[&#123;role:&#x27;root&#x27;,db:&#x27;admin&#x27;&#125;]&#125;)   //创建用户,此用户创建成功,则后续操作都需要用户认证</span><br><span class="line">exit</span><br></pre></td></tr></table></figure>
<h3 id="1-2-测试"><a href="#1-2-测试" class="headerlink" title="1.2 测试"></a>1.2 测试</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mongo  宿主机ip/admin  -utest -p</span><br></pre></td></tr></table></figure>
<p>查看是否连接成功</p>
<h2 id="2，维护mongoDB"><a href="#2，维护mongoDB" class="headerlink" title="2，维护mongoDB"></a>2，维护mongoDB</h2><h3 id="2-1-指定MongoDB配置文件"><a href="#2-1-指定MongoDB配置文件" class="headerlink" title="2.1 指定MongoDB配置文件"></a>2.1 指定MongoDB配置文件</h3><p>当我们需要修改配置文件时，我们只需要在宿主机上创建一个mongodb.conf文件，并将该文件所在的文件夹映射到容器的/data/configdb文件夹中，同时，在容器的启动命令中添加—configsvr参数即可。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name some-mongo -d mongo --configsvr</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-数据持久化"><a href="#2-2-数据持久化" class="headerlink" title="2.2 数据持久化"></a>2.2 数据持久化</h3><p>在使用MongoDB的容器时，数据持久化有很多种方式，下面我们将描述一种推荐的方式:<br>在宿主机上创建一个数据存储目录，并将其映射到容器中的目录中。<br>这将数据库文件放在主机系统中的已知位置，并便于主机系统上的工具和应用程序访问文件。<br>缺点是用户需要确保目录存在，例如，主机系统上的目录权限和其他安全机制配置正确。<br>使用方法如下：<br>在宿主机中创建一个目录，例如/my/own/datadir。<br>如下命令启动容器：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name some-mongo -v /my/own/datadir:/data/db -d mongo:tag</span><br></pre></td></tr></table></figure></p>
<h3 id="2-3-数据库授权"><a href="#2-3-数据库授权" class="headerlink" title="2.3 数据库授权"></a>2.3 数据库授权</h3><p>默认情况下，Mongo数据库没有添加认证约束，也就是说任何人只要知道数据库服务的地址和端口，就可以正常访问数据库并对数据库进行增删改查。<br>为了增强数据库的安全性，我们需要对数据库添加授权认证。<br>添加方式如下：<br>在启动数据库容器命令中添加—auth参数。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name some-mongo -d mongo --auth</span><br></pre></td></tr></table></figure></p>
<p>使用exec命令进入命令行，并添加用户名和密码。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it some-mongo mongo admin</span><br><span class="line">db.createUser(&#123; user: &#x27;jsmith&#x27;, pwd: &#x27;some-initial-password&#x27;, roles: [ &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ] &#125;);</span><br></pre></td></tr></table></figure></p>
<h3 id="2-4-数据库备份"><a href="#2-4-数据库备份" class="headerlink" title="2.4 数据库备份"></a>2.4 数据库备份</h3><p>通常情况下，我们需要对数据库进行备份。<br>首先，我们需要将本地磁盘的某个文件夹映射到容器中的备份文件夹中：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name mongo -v /mnt/mongo/backup:/data/backup -d mongo</span><br></pre></td></tr></table></figure></p>
<p>数据库备份的方式如下：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec mongo sh -c &#x27;exec var=`date +%Y%m%d%H%M` &amp;amp;&amp;amp; mongodump -h localhost --port 27017 -u test -p test1 -d dbname -o /data/backup/$var_test1.dat&#x27;</span><br></pre></td></tr></table></figure></p>
<h2 id="3，推荐用法"><a href="#3，推荐用法" class="headerlink" title="3，推荐用法"></a>3，推荐用法</h2><p>执行如下命令拉取Mongo镜像：<br>docker pull mongo<br>创建Mongo专用的文件夹：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /mnt</span><br><span class="line">mkdir mongodb</span><br><span class="line">cd ./mongodb</span><br><span class="line">mkdir data</span><br><span class="line">mkdir backup</span><br></pre></td></tr></table></figure><br>执行如下命令启动MongoDB：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run --name mongo -p 27017:27017 -v /mnt/mongodb/data:/data/db -v /mnt/mongodb/backup:/data/backup -d mongo --auth</span><br></pre></td></tr></table></figure><br>接下来，我们需要进入容器的命令行去创建用户名和密码：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it mongo mongo admin</span><br><span class="line">db.createUser(&#123; user: &#x27;jsmith&#x27;, pwd: &#x27;password&#x27;, roles: [ &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ] &#125;);</span><br><span class="line">use test;</span><br><span class="line">db.createUser(&#123;user:&quot;testuser&quot;,pwd:&quot;testpass&quot;,roles:[&quot;readWrite&quot;]&#125;);</span><br><span class="line">db.auth(&quot;testuser&quot;,&quot;testpass&quot;)</span><br></pre></td></tr></table></figure><br>在运行一段时间以后，我们可以执行如下命令进行数据库备份：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec mongo sh -c &#x27;exec var=`date +%Y%m%d%H%M` &amp;amp;&amp;amp; mongodump -h localhost --port 27017 -u jsmith -p password -d dbname -o /data/backup/$var_test1.dat&#x27;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>backend</category>
      </categories>
      <tags>
        <tag>mongodb docker</tag>
      </tags>
  </entry>
  <entry>
    <title>几个电视台的直播流</title>
    <url>/2022/05/19/cv_engineering/several-livestream/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>常用电视台的直播流，rtsp流，无意中检索到的，就保存了~</p>
<p><strong>网速好，可以直接看任意台哦</strong><br><span id="more"></span></p>
<h2 id="常用电视台的直播流"><a href="#常用电视台的直播流" class="headerlink" title="常用电视台的直播流"></a>常用电视台的直播流</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># cap = cv2.VideoCapture(&quot;rtmp://58.200.131.2:1935/livetv/hunantv&quot;) #湖南台</span><br><span class="line"># cap = cv2.VideoCapture(&quot;rtmp://58.200.131.2:1935/livetv/dftv&quot;) #东方台</span><br><span class="line"># cap = cv2.VideoCapture(&quot;rtmp://58.200.131.2:1935/livetv/gdtv&quot;) #广东台</span><br><span class="line"># cap = cv2.VideoCapture(&quot;rtmp://58.200.131.2:1935/livetv/gxtv&quot;) #广西台</span><br><span class="line"></span><br><span class="line">CCTV1高清：http://ivi.bupt.edu.cn/hls/cctv1hd.m3u8</span><br><span class="line">CCTV3高清：http://ivi.bupt.edu.cn/hls/cctv3hd.m3u8</span><br><span class="line">CCTV5高清：http://ivi.bupt.edu.cn/hls/cctv5hd.m3u8</span><br><span class="line">CCTV5+高清：http://ivi.bupt.edu.cn/hls/cctv5phd.m3u8</span><br><span class="line">CCTV6高清：http://ivi.bupt.edu.cn/hls/cctv6hd.m3u8</span><br><span class="line"></span><br><span class="line">CCTV-1综合:rtmp://58.200.131.2:1935/livetv/cctv1</span><br><span class="line">CCTV-2财经:rtmp://58.200.131.2:1935/livetv/cctv2</span><br><span class="line">CCTV-3综艺:rtmp://58.200.131.2:1935/livetv/cctv3</span><br><span class="line">CCTV-4中文国际:rtmp://58.200.131.2:1935/livetv/cctv4</span><br><span class="line">CCTV-5体育:rtmp://58.200.131.2:1935/livetv/cctv5</span><br><span class="line">CCTV-6电影:rtmp://58.200.131.2:1935/livetv/cctv6</span><br><span class="line">CCTV-7军事农业:rtmp://58.200.131.2:1935/livetv/cctv7</span><br><span class="line">CCTV-8电视剧:rtmp://58.200.131.2:1935/livetv/cctv8</span><br><span class="line">CCTV-9记录:rtmp://58.200.131.2:1935/livetv/cctv9</span><br><span class="line">CCTV-10科教:rtmp://58.200.131.2:1935/livetv/cctv10</span><br><span class="line">CCTV-11戏曲:rtmp://58.200.131.2:1935/livetv/cctv11</span><br><span class="line">CCTV-12社会与法:rtmp://58.200.131.2:1935/livetv/cctv12</span><br><span class="line">CCTV-13新闻:rtmp://58.200.131.2:1935/livetv/cctv13</span><br><span class="line">CCTV-14少儿:rtmp://58.200.131.2:1935/livetv/cctv14</span><br><span class="line">CCTV-15音乐:rtmp://58.200.131.2:1935/livetv/cctv15</span><br><span class="line">安徽卫视:rtmp://58.200.131.2:1935/livetv/ahtv</span><br><span class="line">兵团卫视:rtmp://58.200.131.2:1935/livetv/bttv</span><br><span class="line">重庆卫视:rtmp://58.200.131.2:1935/livetv/cqtv</span><br><span class="line">东方卫视:rtmp://58.200.131.2:1935/livetv/dftv</span><br><span class="line">东南卫视:rtmp://58.200.131.2:1935/livetv/dntv</span><br><span class="line">广东卫视:rtmp://58.200.131.2:1935/livetv/gdtv</span><br><span class="line">广西卫视:rtmp://58.200.131.2:1935/livetv/gxtv</span><br><span class="line">甘肃卫视:rtmp://58.200.131.2:1935/livetv/gstv</span><br><span class="line">贵州卫视:rtmp://58.200.131.2:1935/livetv/gztv</span><br><span class="line">湖北卫视:rtmp://58.200.131.2:1935/livetv/hbtv</span><br><span class="line">湖南卫视:rtmp://58.200.131.2:1935/livetv/hunantv</span><br><span class="line">河北卫视:rtmp://58.200.131.2:1935/livetv/hebtv</span><br><span class="line">河南卫视:rtmp://58.200.131.2:1935/livetv/hntv</span><br><span class="line">黑龙江卫视:rtmp://58.200.131.2:1935/livetv/hljtv</span><br><span class="line">江苏卫视:rtmp://58.200.131.2:1935/livetv/jstv</span><br><span class="line">江西卫视:rtmp://58.200.131.2:1935/livetv/jxtv</span><br><span class="line">吉林卫视:rtmp://58.200.131.2:1935/livetv/jltv</span><br><span class="line">辽宁卫视:rtmp://58.200.131.2:1935/livetv/lntv</span><br><span class="line">内蒙古卫视:rtmp://58.200.131.2:1935/livetv/nmtv</span><br><span class="line">宁夏卫视:rtmp://58.200.131.2:1935/livetv/nxtv</span><br><span class="line">青海卫视:rtmp://58.200.131.2:1935/livetv/qhtv</span><br><span class="line">四川卫视:rtmp://58.200.131.2:1935/livetv/sctv</span><br><span class="line">山东卫视:rtmp://58.200.131.2:1935/livetv/sdtv</span><br><span class="line">山西卫视:rtmp://58.200.131.2:1935/livetv/sxrtv</span><br><span class="line">陕西卫视:rtmp://58.200.131.2:1935/livetv/sxtv</span><br><span class="line">山东教育:rtmp://58.200.131.2:1935/livetv/sdetv</span><br><span class="line">中国教育-1:rtmp://58.200.131.2:1935/livetv/cetv1</span><br><span class="line">中国教育-3:rtmp://58.200.131.2:1935/livetv/cetv3</span><br><span class="line">中国教育-4:rtmp://58.200.131.2:1935/livetv/cetv4</span><br><span class="line">CCTV-第一剧场:rtmp://58.200.131.2:1935/livetv/dyjctv</span><br><span class="line">CCTV-国防军事:rtmp://58.200.131.2:1935/livetv/gfjstv</span><br><span class="line">CCTV-怀旧剧场:rtmp://58.200.131.2:1935/livetv/hjjctv</span><br><span class="line">CCTV-风云剧场:rtmp://58.200.131.2:1935/livetv/fyjctv</span><br><span class="line">CCTV-风云足球:rtmp://58.200.131.2:1935/livetv/fyzqtv</span><br><span class="line">CCTV-风云音乐:rtmp://58.200.131.2:1935/livetv/fyyytv</span><br><span class="line">CCTV-世界地理:rtmp://58.200.131.2:1935/livetv/sjdltv</span><br><span class="line">CCTV-1HD:rtmp://58.200.131.2:1935/livetv/cctv1hd</span><br><span class="line">CCTV-2HD:rtmp://58.200.131.2:1935/livetv/cctv2hd</span><br><span class="line">CCTV-3HD:rtmp://58.200.131.2:1935/livetv/cctv3hd</span><br><span class="line">CCTV-4HD:rtmp://58.200.131.2:1935/livetv/cctv4hd</span><br><span class="line">CCTV-5HD:rtmp://58.200.131.2:1935/livetv/cctv5hd</span><br><span class="line">CCTV5+HD:rtmp://58.200.131.2:1935/livetv/cctv5phd</span><br><span class="line">CCTV-6HD:rtmp://58.200.131.2:1935/livetv/cctv6hd</span><br><span class="line">CCTV-7HD:rtmp://58.200.131.2:1935/livetv/cctv7hd</span><br><span class="line">CCTV-8HD:rtmp://58.200.131.2:1935/livetv/cctv8hd</span><br><span class="line">CCTV-9HD:rtmp://58.200.131.2:1935/livetv/cctv9hd</span><br><span class="line">CCTV-10HD:rtmp://58.200.131.2:1935/livetv/cctv10hd</span><br><span class="line">CCTV-12HD:rtmp://58.200.131.2:1935/livetv/cctv12hd</span><br><span class="line">CCTV-14HD:rtmp://58.200.131.2:1935/livetv/cctv14hd</span><br><span class="line">CGTN-新闻:rtmp://58.200.131.2:1935/livetv/cctv16</span><br><span class="line">CETV-1:rtmp://58.200.131.2:1935/livetv/cetv1</span><br><span class="line">CETV-3:rtmp://58.200.131.2:1935/livetv/cetv3</span><br><span class="line">CETV-4:rtmp://58.200.131.2:1935/livetv/cetv4</span><br><span class="line">北京卫视高清:rtmp://58.200.131.2:1935/livetv/btv1hd</span><br><span class="line">北京影视高清:rtmp://58.200.131.2:1935/livetv/btv4hd</span><br><span class="line">北京体育高清:rtmp://58.200.131.2:1935/livetv/btv6hd</span><br><span class="line">北京新闻高清:rtmp://58.200.131.2:1935/livetv/btv9hd</span><br><span class="line">北京纪实高清:rtmp://58.200.131.2:1935/livetv/btv11hd</span><br><span class="line">北京卫视:rtmp://58.200.131.2:1935/livetv/btv1</span><br><span class="line">北京文艺:rtmp://58.200.131.2:1935/livetv/btv2</span><br><span class="line">北京科教:rtmp://58.200.131.2:1935/livetv/btv3</span><br><span class="line">北京影视:rtmp://58.200.131.2:1935/livetv/btv4</span><br><span class="line">北京财经:rtmp://58.200.131.2:1935/livetv/btv5</span><br><span class="line">北京体育:rtmp://58.200.131.2:1935/livetv/btv6</span><br><span class="line">北京生活:rtmp://58.200.131.2:1935/livetv/btv7</span><br><span class="line">北京青年:rtmp://58.200.131.2:1935/livetv/btv8</span><br><span class="line">北京新闻:rtmp://58.200.131.2:1935/livetv/btv9</span><br><span class="line">北京卡酷:rtmp://58.200.131.2:1935/livetv/btv10</span><br><span class="line">北京文艺高清:rtmp://58.200.131.2:1935/livetv/btv2hd</span><br><span class="line">安徽卫视高清:rtmp://58.200.131.2:1935/livetv/ahhd</span><br><span class="line">重庆卫视高清:rtmp://58.200.131.2:1935/livetv/cqhd</span><br><span class="line">东方卫视高清:rtmp://58.200.131.2:1935/livetv/dfhd</span><br><span class="line">天津卫视高清:rtmp://58.200.131.2:1935/livetv/tjhd</span><br><span class="line">东南卫视高清:rtmp://58.200.131.2:1935/livetv/dnhd</span><br><span class="line">江西卫视高清:rtmp://58.200.131.2:1935/livetv/jxhd</span><br><span class="line">河北卫视高清:rtmp://58.200.131.2:1935/livetv/hebhd</span><br><span class="line">湖南卫视高清:rtmp://58.200.131.2:1935/livetv/hunanhd</span><br><span class="line">湖北卫视高清:rtmp://58.200.131.2:1935/livetv/hbhd</span><br><span class="line">辽宁卫视高清:rtmp://58.200.131.2:1935/livetv/lnhd</span><br><span class="line">四川卫视高清:rtmp://58.200.131.2:1935/livetv/schd</span><br><span class="line">江苏卫视高清:rtmp://58.200.131.2:1935/livetv/jshd</span><br><span class="line">浙江卫视高清:rtmp://58.200.131.2:1935/livetv/zjhd</span><br><span class="line">山东卫视高清:rtmp://58.200.131.2:1935/livetv/sdhd</span><br><span class="line">广东卫视高清:rtmp://58.200.131.2:1935/livetv/gdhd</span><br><span class="line">深圳卫视高清:rtmp://58.200.131.2:1935/livetv/szhd</span><br><span class="line">黑龙江卫视高清:rtmp://58.200.131.2:1935/livetv/hljhd</span><br><span class="line">CHC高清电影:rtmp://58.200.131.2:1935/livetv/chchd</span><br><span class="line">上海纪实高清:rtmp://58.200.131.2:1935/livetv/docuchina</span><br><span class="line">金鹰纪实高清:rtmp://58.200.131.2:1935/livetv/gedocu</span><br><span class="line">全纪实高清:rtmp://58.200.131.2:1935/livetv/documentaryhd</span><br><span class="line">凤凰卫视中文台:rtmp://58.200.131.2:1935/livetv/fhzw</span><br><span class="line">凤凰卫视资讯台:rtmp://58.200.131.2:1935/livetv/fhzx</span><br><span class="line">凤凰卫视电影台:rtmp://58.200.131.2:1935/livetv/fhdy</span><br><span class="line">星空卫视:rtmp://58.200.131.2:1935/livetv/startv</span><br><span class="line">Star Sports:rtmp://58.200.131.2:1935/livetv/starsports</span><br><span class="line">Channel[V]:rtmp://58.200.131.2:1935/livetv/channelv</span><br><span class="line">探索频道:rtmp://58.200.131.2:1935/livetv/discovery</span><br><span class="line">国家地理频道:rtmp://58.200.131.2:1935/livetv/natlgeo</span><br><span class="line">CHC家庭影院:rtmp://58.200.131.2:1935/livetv/chctv</span><br><span class="line">CHC动作电影:rtmp://58.200.131.2:1935/livetv/chcatv</span><br><span class="line">美国电视频道:rtmp://media3.scctv.net/live/scctv_800</span><br><span class="line">香港财经:rtmp://202.69.69.180:443/webcast/bshdlive-pc</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>cv engineering</category>
      </categories>
      <tags>
        <tag>livestream</tag>
      </tags>
  </entry>
  <entry>
    <title>hi3559a and nfs</title>
    <url>/2022/05/19/hardware/hi3559a-nfs/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在进行板子调试的时候，用串口传输文件会比较繁琐，尤其是文件小而多时，所以此处采用了nfs服务，将PC机上的一个文件夹挂载到板子上共享存储服务。</p>
<p><strong>可以使劲、快速的板砖啦…啦啦啦啦…</strong><br><span id="more"></span></p>
<h2 id="1，PC上的安装和配置"><a href="#1，PC上的安装和配置" class="headerlink" title="1，PC上的安装和配置"></a>1，PC上的安装和配置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 安装</span><br><span class="line">sudo apt-get install nfs-kernel-server</span><br><span class="line"># 共享文件夹</span><br><span class="line">sudo mkdir /home/demo/hi3559a_share</span><br><span class="line"># 配置文件，打开```/etc/exports```文件，在最后加上：</span><br><span class="line">/home/demo/hi3559a_share *(rw,sync,no_subtree_check)</span><br></pre></td></tr></table></figure>
<p><strong>NOTE:参数说明找百度哦</strong><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 重启服务，上面的配置完成之后，重启服务：</span><br><span class="line">sudo /etc/init.d/rpcbind restart//重启 rpcbind</span><br><span class="line">sudo /etc/init.d/nfs-kernel-server restart //重启 NFS</span><br></pre></td></tr></table></figure></p>
<h2 id="2，开发板上的安装和配置"><a href="#2，开发板上的安装和配置" class="headerlink" title="2，开发板上的安装和配置"></a>2，开发板上的安装和配置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 安装客户端</span><br><span class="line">sudo apt-get install nfs-common</span><br><span class="line"># 挂载文件夹</span><br><span class="line">mkdirs -p /home/hihope/hi3559av100</span><br><span class="line">chmod 777 /home/hihope/hi3559av100</span><br></pre></td></tr></table></figure>
<h2 id="3，常用命令"><a href="#3，常用命令" class="headerlink" title="3，常用命令"></a>3，常用命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 挂载命令</span><br><span class="line">sudo mount -t nfs -o nolock,nfsvers=3,vers=3 192.168.86.105:/home/demo/hi3559a_share /home/hihope/hi3559av100</span><br><span class="line"># 卸载命令</span><br><span class="line">umount /home/hihope/hi3559av100</span><br><span class="line">df	//查看挂载信息</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>hi3559a nfs</tag>
      </tags>
  </entry>
  <entry>
    <title>hi3559a，windows和vmware&amp;ubuntu的开发环境配置</title>
    <url>/2022/05/19/hardware/hi3559a-windows-vmware-software/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="桥接模式"><a href="#桥接模式" class="headerlink" title="桥接模式"></a>桥接模式</h3><p>原理就找大佬们理解，也就不多说了。<br>我的理解是，PC和板子用网线连接，PC和虚拟机是逻辑连接，需要保证pc, ubuntu（vmware中的）和板子在同一个网段中。</p>
<h3 id="windows系统需要确认"><a href="#windows系统需要确认" class="headerlink" title="windows系统需要确认"></a>windows系统需要确认</h3><ul>
<li>关闭win10防火墙</li>
<li>确认网口和网线正常<br>  怎么确认就各显神通了啊~~~</li>
</ul>
<p><strong>然后就是干货了~</strong></p>
<span id="more"></span>
<h2 id="1，参数配置"><a href="#1，参数配置" class="headerlink" title="1，参数配置"></a>1，参数配置</h2><h3 id="1-1-板子配置"><a href="#1-1-板子配置" class="headerlink" title="1.1 板子配置"></a>1.1 板子配置</h3><ul>
<li>连接板子<br> 【Setup-&gt;General setup】     <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">端口：COM3</span><br><span class="line">语言：UTF-8</span><br><span class="line">界面语言：Simplified Chinese.lng</span><br></pre></td></tr></table></figure>
 【设置-&gt;串口】  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">端口: COM3</span><br><span class="line">比特率：115200</span><br><span class="line">数据位：8 bit</span><br><span class="line">校验位：none</span><br><span class="line">停止位：1 bit</span><br><span class="line">流量控制：none</span><br></pre></td></tr></table></figure></li>
<li><p>验证板子<br>板子usb或杜邦线转usb连接到台式机，上电或者重启可看到tera term端的输出</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Freeing unused kernel memory: 320K (ffffffc0047b0000 - ffffffc004800000)</span><br><span class="line"></span><br><span class="line">            _ _ _ _ _ _ _ _ _ _ _ _</span><br><span class="line">            \  _  _   _  _ _ ___</span><br><span class="line">            / /__/ \ |_/</span><br><span class="line">           / __   /  -  _ ___</span><br><span class="line">          / /  / /  / /</span><br><span class="line">  _ _ _ _/ /  /  \_/  \_ ______</span><br><span class="line">___________\___\__________________</span><br><span class="line"></span><br><span class="line">[RCS]: /etc/init.d/S00devs</span><br><span class="line">[RCS]: /etc/init.d/S01udev</span><br></pre></td></tr></table></figure>
</li>
<li><p>板子参数设置</p>
<ul>
<li>设置命令<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">setenv ITEMNAME ITEMVALUE   //ITEMNAME是设置项名称，ITEMVALUE是设置项值</span><br><span class="line">saveenv     //保存设置</span><br></pre></td></tr></table></figure></li>
<li>uboot中设置<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">setenv ipaddr 192.168.86.114</span><br><span class="line">setenv netmask 255.255.255.0</span><br><span class="line">setenv gatewayip 192.168.86.1</span><br><span class="line">setenv serverip 192.168.86.105</span><br></pre></td></tr></table></figure></li>
<li>板子中的系统文件（我的是/etc/init.d/S80…）<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ipaddr = 192.168.86.114</span><br><span class="line">netmask = 255.255.255.0</span><br><span class="line">gateway = 192.168.86.1</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h3 id="1-2-vmware"><a href="#1-2-vmware" class="headerlink" title="1.2 vmware"></a>1.2 vmware</h3><ul>
<li>soft install</li>
<li>OS install   <ul>
<li>ubuntu1804</li>
<li>network—-“www”<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IPv4 Method: Automatic(DHCP)</span><br></pre></td></tr></table></figure></li>
<li>network—-“Profile 1”<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IPv4 Method: Manual</span><br><span class="line">Addresses Address: 192.168.86.105</span><br><span class="line">Addresses Netmask: 255.255.255.0</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>增加桥接模式<br>  【编辑-&gt;虚拟网络编辑器-&gt;添加网络】</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">表格中</span><br><span class="line">名称：VMnet0</span><br><span class="line">类型：桥接模式</span><br><span class="line">外部连接：Intel(R) Ethernet ...（你自己的台式机网卡）</span><br><span class="line">“VMnet信息”中</span><br><span class="line">选择第一个，“桥接模式(将虚拟机直接连接到外部网络)”</span><br><span class="line">已桥接至(G):Intel(R) Ethernet Connection (2) I219-V</span><br></pre></td></tr></table></figure>
<p>  【虚拟机-&gt;设置-&gt;硬件-&gt;网络适配器-&gt;网络连接】</p>
  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">自定义(U):特定虚拟网络，下拉项选择上一步新增加的VMnet0</span><br></pre></td></tr></table></figure></li>
<li>桥接模式和联网模式的切换   <ul>
<li>关闭虚拟机的ubuntu系统   </li>
<li>虚拟机-&gt;设置-&gt;硬件-&gt;网络适配器-&gt;网络连接-&gt;NAT模式(N)：用于共享主机的IP地址</li>
</ul>
</li>
</ul>
<h3 id="1-3-PC设置"><a href="#1-3-PC设置" class="headerlink" title="1.3 PC设置"></a>1.3 PC设置</h3><p>网络和Internet-&gt;网络连接-&gt;以太网-&gt;属性(右键)-&gt;Internet协议版本4(TCP/IPv4)<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">勾选,&quot;使用下面的IP地址(S)&quot;</span><br><span class="line">IP地址: 192.168.86.189</span><br><span class="line">子网掩码k: 255.255.255.0</span><br></pre></td></tr></table></figure><br>确认即可</p>
<h2 id="2，互通验证"><a href="#2，互通验证" class="headerlink" title="2，互通验证"></a>2，互通验证</h2><ul>
<li><p>PC和vmware中ubuntu相互ping通</p>
</li>
<li><p>PC和板子相互ping通</p>
</li>
<li><p>vmware中ubuntu和板子相互ping通</p>
</li>
</ul>
<h2 id="3，关联资源"><a href="#3，关联资源" class="headerlink" title="3，关联资源"></a>3，关联资源</h2><p>参考连接：<a href="https://www.cnblogs.com/czjk/p/11699198.html">https://www.cnblogs.com/czjk/p/11699198.html</a></p>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>Hi3559a win10 vmware</tag>
      </tags>
  </entry>
  <entry>
    <title>anaconda安装，换源及构建虚拟环境</title>
    <url>/2022/05/17/python/anaconda-basic/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>anaconda的安装，换源和虚拟环境构建，对于python语言来说，anaconda是个不错的环境管理方案，可以隔离不同的python应用和包需求。</p>
<p><strong>工欲善其事必先利其器~</strong><br><span id="more"></span></p>
<h2 id="anaconda安装并换源："><a href="#anaconda安装并换源：" class="headerlink" title="anaconda安装并换源："></a>anaconda安装并换源：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bash Anaconda3-5.2.0-Linux-x86_64.sh</span><br><span class="line">vim ~/.bashrc</span><br><span class="line">export PATH=/home/XXX/anaconda3/bin:$PATH（XXX为自己的用户名）（在文件末尾处添加该语句）</span><br><span class="line">source ~/.bashrc</span><br><span class="line"></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line">之后vim ~/.condarc，把defaults删掉</span><br></pre></td></tr></table></figure>
<h2 id="虚拟环境"><a href="#虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n exp38 python==3.8</span><br><span class="line">conda activate exp38</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>anaconda basic</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn的前向传播过程</title>
    <url>/2022/05/16/detection/forward-of-faster-rcnn/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>faster rcnn的前传过程和训练步骤</p>
<p><strong>经典的二阶段算法啊~</strong></p>
<span id="more"></span>
<h2 id="1，faster-rcnn前向过程"><a href="#1，faster-rcnn前向过程" class="headerlink" title="1，faster rcnn前向过程"></a>1，faster rcnn前向过程</h2><p>输入一张待检测图片-&gt;vgg16网络conv layers提取整张图片的特征，输出feature map分别输入到RPN和Fast RCNN网络开头-&gt;RPN网络得出region proposal，将这些候选框信息送入到Fast RCNN网络开头-&gt;利用候选框在之前送到的feature map提取特征，并通过ROI Pooling层得到规定大小的feature map-&gt;将这些feature map送入Fast RCNN网络中进行分类和回归坐标，最终得到需检测物体的坐标。</p>
<h2 id="2，faster-rcnn训练步骤"><a href="#2，faster-rcnn训练步骤" class="headerlink" title="2，faster rcnn训练步骤"></a>2，faster rcnn训练步骤</h2><ul>
<li>第一步，训练RPN，该网络用ImageNet预训练的模型初始化，并端到端微调，用于生成region proposal；</li>
<li>第二步，训练Fast R-CNN，由imageNet model初始化，利用第一步的RPN生成的region proposals作为输入数据，训练Fast R-CNN一个单独的检测网络，这时候两个网络还没有共享卷积层；</li>
<li>第三步，调优RPN，用第二步的fast-rcnn model初始化RPN再次进行训练，但固定共享的卷积层，并且只微调RPN独有的层，现在两个网络共享卷积层了；</li>
<li>第四步，调优Fast R-CNN,由第三步的RPN model初始化fast-RCNN网络，输入数据为第三步生成的proposals。保持共享的卷积层固定，微调Fast R-CNN的fc层。这样，两个网络共享相同的卷积层，构成一个统一的网络。</li>
</ul>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>faster rcnn forward</tag>
      </tags>
  </entry>
  <entry>
    <title>faster rcnn中的类别不均衡问题</title>
    <url>/2022/05/16/detection/class-imbalance-in-faster-rcnn/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>faster rcnn是17年前后He Kaiming应用比较广泛的大作之一，是典型的二阶段方法，其中有哪些比较好的处理类别不均衡的思想呢？可以从论文中窥探一二~</p>
<p><strong>多看论文啊~</strong><br><span id="more"></span></p>
<h2 id="1，论文中的解决方法"><a href="#1，论文中的解决方法" class="headerlink" title="1，论文中的解决方法"></a>1，论文中的解决方法</h2><ul>
<li><p>根据前景score的高低过滤得到可能是前景的exam，约1k~2k个，这样可过滤掉大部分简单负样本；</p>
</li>
<li><p>根据IoU的大小来调整正负样本比，比如1:3，这样可防止负样本过多；</p>
</li>
</ul>
<h2 id="2，Faster-RCNN怎么筛选正负anchor"><a href="#2，Faster-RCNN怎么筛选正负anchor" class="headerlink" title="2，Faster RCNN怎么筛选正负anchor"></a>2，Faster RCNN怎么筛选正负anchor</h2><ul>
<li>给两种锚点分配一个正标签<ul>
<li><ol>
<li>具有与实际边界框的重叠最高交并比（IoU）的锚点；</li>
</ol>
</li>
<li><ol>
<li>具有与实际边界框的重叠超过0.7 IoU的锚点。IoU比率低于0.3，</li>
</ol>
</li>
</ul>
</li>
<li>给非正面的锚点分配一个负标签。</li>
</ul>
<h2 id="3，衍生到通用类别不均衡问题"><a href="#3，衍生到通用类别不均衡问题" class="headerlink" title="3，衍生到通用类别不均衡问题"></a>3，衍生到通用类别不均衡问题</h2><p>限制正负样本比例为1:1，如果正样本不足，就用负样本补充，这种方法后面研究工作用的不多。通常针对类别不平衡问题可以从调整样本数或修改loss weight两方面去解决，常用的方法有OHEM、OHNM、class balanced loss和Focal loss。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>faster rcnn</tag>
        <tag>class imbalance</tag>
      </tags>
  </entry>
  <entry>
    <title>rpn的原理和作用</title>
    <url>/2022/05/16/detection/principle-and-effects-of-rpn/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>rpn原理和作用</p>
<span id="more"></span>
<h2 id="1，作用"><a href="#1，作用" class="headerlink" title="1，作用"></a>1，作用</h2><p>RPN专门用来提取候选框，一方面RPN耗时少，另一方面RPN可以很容易结合到Fast RCNN中，成为一个整体。</p>
<h2 id="2，实现细节"><a href="#2，实现细节" class="headerlink" title="2，实现细节"></a>2，实现细节</h2><p>一个特征图（Faster RCNN的公共Feature Map）经过sliding window处理，得到256维特征，对每个特征向量做两次全连接操作，一个得到2个分数，一个得到4个坐标{然后通过两次全连接得到结果2k个分数和4k个坐标[k指的是由锚点产生的K个框(K anchor boxes)]}</p>
<h2 id="3，anchor-box是怎么选取的？"><a href="#3，anchor-box是怎么选取的？" class="headerlink" title="3，anchor box是怎么选取的？"></a>3，anchor box是怎么选取的？</h2><p>滑窗的中心在原像素空间的映射点称为anchor，以此anchor为中心，生成k(paper中default k=9, 3 scales and 3 aspect ratios/不同尺寸和不同长宽比)个proposals。三个面积尺寸$(128^{2}，256^{2}，512^{2})$，然后在每个面积尺寸下，取三种不同的长宽比例（1:1,1:2,2:1）</p>
<h2 id="4，为什么提出anchor-box？"><a href="#4，为什么提出anchor-box？" class="headerlink" title="4，为什么提出anchor box？"></a>4，为什么提出anchor box？</h2><p>主要有两个原因：一个窗口只能检测一个目标、无法解决多尺度问题。<br>目前anchor box尺寸的选择主要有三种方式：人为经验选取、k-means聚类、作为超参数进行学习<br>为什么使用不同尺寸和不同长宽比？ 为了得到更大的交并比(IOU)。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>rpn</tag>
      </tags>
  </entry>
  <entry>
    <title>roi align</title>
    <url>/2022/05/16/detection/roi-align/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>roi align产生的背景，原理和应用场景</p>
<p><strong>主要对比roi pooling</strong><br><span id="more"></span></p>
<h2 id="1，背景"><a href="#1，背景" class="headerlink" title="1，背景"></a>1，背景</h2><p>在mask rcnn中提出，是为了解决roi pooling的两次量化带来的位置精度损失问题。</p>
<h2 id="2，思路"><a href="#2，思路" class="headerlink" title="2，思路"></a>2，思路</h2><p>取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作。值得注意的是，在具体的算法操作上，ROI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，大致算法流程为：</p>
<ul>
<li>遍历每一个候选区域，保持浮点数边界不做量化。</li>
<li>将候选区域分割成k x k个单元，每个单元的边界也不做量化。</li>
<li>在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。</li>
</ul>
<h2 id="3，区别"><a href="#3，区别" class="headerlink" title="3，区别"></a>3，区别</h2><p>ROI Align舍去了近似像素取整数的量化方法，改用双线性插值的方法确定特征图坐标对应于原图中的像素位置.ROI Align很好地解决了ROI Pooling操作中两次量化造成的区域不匹配(mis-alignment)的问题。</p>
<h2 id="4，应用场景"><a href="#4，应用场景" class="headerlink" title="4，应用场景"></a>4，应用场景</h2><p>对于检测图片中大目标物体时，两种方案的差别不大，而如果是图片中有较多小目标物体需要检测，则优先选择RoiAlign，更精准些。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>roi align</tag>
      </tags>
  </entry>
  <entry>
    <title>nms的原理和作用</title>
    <url>/2022/05/16/detection/-priciple-and-effects-of-nms/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>nms的原理，应用和发展方向</p>
<p><strong>尤其在anchor based中用得多~</strong><br><span id="more"></span></p>
<h2 id="1，作用和原理"><a href="#1，作用和原理" class="headerlink" title="1，作用和原理"></a>1，作用和原理</h2><ul>
<li><p>作用<br>本质是搜索局部极大值，抑制非极大值元素。</p>
</li>
<li><p>原理<br>NMS为非极大值抑制，用来抑制检测时冗余的框。</p>
</li>
</ul>
<h2 id="2，算法流程为"><a href="#2，算法流程为" class="headerlink" title="2，算法流程为"></a>2，算法流程为</h2><ul>
<li>1.对所有预测框的置信度降序排序</li>
<li>2.选出置信度最高的预测框，确认其为正确预测，并计算他与其他预测框的IOU </li>
<li>3.根据2中计算的IOU去除重叠度高的，IOU&gt;threshold阈值就删除 </li>
<li>4.剩下的预测框返回第1步，直到没有剩下的为止</li>
</ul>
<p><strong>注意：</strong><br>NMS一次处理一个类别，如果有N个类别，NMS就需要执行N次。</p>
<h2 id="3，待改进问题？"><a href="#3，待改进问题？" class="headerlink" title="3，待改进问题？"></a>3，待改进问题？</h2><p>假设两个目标靠的很近，则会识别成一个bbox，会有什么问题，怎么解决？</p>
<p>当两个目标靠的非常近时，置信度低的会被置信度高的框抑制掉，从而两个目标靠的非常近时会被识别成一个bbox。为了解决这个问题，可以使用softNMS。</p>
<p>softNMS基本思想：用稍低一点的分数来代替原有的分数，而不是直接置零）</p>
<h2 id="4，发展新动向？"><a href="#4，发展新动向？" class="headerlink" title="4，发展新动向？"></a>4，发展新动向？</h2>]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>nms</tag>
      </tags>
  </entry>
  <entry>
    <title>roi pooling</title>
    <url>/2022/05/16/detection/roi-pooling/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>roi pooling的基本原理和应用</p>
<p><strong>加油，夯实基础~</strong><br><span id="more"></span></p>
<h2 id="1，基本原理"><a href="#1，基本原理" class="headerlink" title="1，基本原理"></a>1，基本原理</h2><p>RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定$(w * h)$的矩形框，最大的好处就在于极大地提高了处理速度。</p>
<p><img src="roi_pooling.gif" alt="obj_detec"> </p>
<h2 id="2，实现"><a href="#2，实现" class="headerlink" title="2，实现"></a>2，实现</h2><ul>
<li>1，根据输入image，将ROI映射到feature map对应位置</li>
<li>2，将映射后的区域划分为相同大小的sections（数量与输出的维度相同）；</li>
<li>3，对每个sections进行max pooling操作；</li>
</ul>
<p>怎么做的映射?<br>映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”。</p>
<h2 id="3，优点"><a href="#3，优点" class="headerlink" title="3，优点"></a>3，优点</h2><ul>
<li>1.允许我们对CNN中的feature map进行reuse</li>
<li>2.可以显著加速training和testing速度；</li>
<li>3.允许end-to-end的形式训练目标检测系统。</li>
</ul>
<h2 id="4，缺点"><a href="#4，缺点" class="headerlink" title="4，缺点"></a>4，缺点</h2><p>由于 RoIPooling 采用的是最近邻插值（即INTER_NEAREST） ，在resize时，对于缩放后坐标不能刚好为整数的情况，采用了粗暴的舍去小数，相当于选取离目标点最近的点，损失一定的空间精度。</p>
<p>在这个过程中会有两次量化操作。对于一个region proposal，首先从原图经过全卷积网络到特征图，得到的候选框位置可能存在浮点数，进行取整操作从而出现第一次量化；其次，在ROI Pooling求取每个小网格的位置时也同样存在浮点数取整的情况。这两次量化的结果都使得候选框的位置会出现偏差，在论文里，作者把它总结为“不匹配问题（misalignment）。</p>
<p>经过上述两次量化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>roi pooling</tag>
      </tags>
  </entry>
  <entry>
    <title>yolo4中的数据增强</title>
    <url>/2022/05/16/detection/-data-augmentation-in-yolo4/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>常用数据增强方式和yolov4中使用的数据增强方式及其有效性。</p>
<p><strong>加油啊，还没总结完~</strong><br><span id="more"></span></p>
<h2 id="1，常用数据增强方式"><a href="#1，常用数据增强方式" class="headerlink" title="1，常用数据增强方式"></a>1，常用数据增强方式</h2><h3 id="1-1-畸变"><a href="#1-1-畸变" class="headerlink" title="1.1 畸变"></a>1.1 畸变</h3><ul>
<li>光照畸变</li>
<li>几何畸变</li>
</ul>
<h3 id="1-2-图像遮挡"><a href="#1-2-图像遮挡" class="headerlink" title="1.2 图像遮挡"></a>1.2 图像遮挡</h3><h4 id="1-2-1-随机擦除"><a href="#1-2-1-随机擦除" class="headerlink" title="1.2.1 随机擦除"></a>1.2.1 随机擦除</h4><ul>
<li>做法<br>选定图像一块区域，用随机像素或者平均像素来填充；</li>
<li>功能<br>防止模型记忆训练数据和过拟合；</li>
</ul>
<h4 id="1-2-2-cutout"><a href="#1-2-2-cutout" class="headerlink" title="1.2.2 cutout"></a>1.2.2 cutout</h4><ul>
<li>做法<br>训练中掩盖一个正方形区域，只对CNN第一层遮挡，填充使用的仍然是常数像素；</li>
<li>功能<br>防止过拟合；</li>
</ul>
<h4 id="1-2-3-Hide-and-Seek"><a href="#1-2-3-Hide-and-Seek" class="headerlink" title="1.2.3 Hide and Seek"></a>1.2.3 Hide and Seek</h4><ul>
<li>做法<br>将图像分割成s*s的patch，每个patch以一定概率隐藏</li>
<li>功能<br>让模型了解物体是什么样子，而不只是学习单个部分是什么样子</li>
</ul>
<h4 id="1-2-4-Grid-Mask"><a href="#1-2-4-Grid-Mask" class="headerlink" title="1.2.4 Grid Mask"></a>1.2.4 Grid Mask</h4><ul>
<li>做法<br>mask的网格，将图像隐藏其中</li>
<li>功能<br>让模型学习组成物体的做成部分</li>
</ul>
<h4 id="1-2-5-MixUp"><a href="#1-2-5-MixUp" class="headerlink" title="1.2.5 MixUp"></a>1.2.5 MixUp</h4><ul>
<li>做法<br>图像对及其标签的凸叠加</li>
<li>功能</li>
</ul>
<h2 id="2，yolov4数据增强"><a href="#2，yolov4数据增强" class="headerlink" title="2，yolov4数据增强"></a>2，yolov4数据增强</h2><h3 id="2-1-CutMix"><a href="#2-1-CutMix" class="headerlink" title="2.1 CutMix"></a>2.1 CutMix</h3><ul>
<li><p>做法</p>
</li>
<li><p>功能</p>
</li>
<li><p>问题思考？<br>  对比mixup和cutout，为什么cutmix会好？</p>
</li>
</ul>
<h3 id="2-2-Mosaic"><a href="#2-2-Mosaic" class="headerlink" title="2.2 Mosaic"></a>2.2 Mosaic</h3><ul>
<li><p>做法<br>把4张图片，通过随机缩放、随机裁减、随机排布的方式进行拼接</p>
</li>
<li><p>优点<br>学习到比正常尺寸小的物体；<br>可以“省”训练数据或迭代次数；<br>丰富了检测物体的背景和小目标，并且在计算Batch Normalization的时候一次会计算四张图片的数据，使得mini-batch大小不需要很大，一个GPU就可以达到比较好的效果；<br>丰富数据集：随机使用4张图片，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好；<br>减少GPU：直接计算4张图片的数据，使得Mini-batch大小并不需要很大，一个GPU就可以达到比较好的效果；</p>
</li>
<li><p>缺点<br>  如果我们的数据集本身就有很多的小目标，那么Mosaic数据增强会导致本来较小的目标变得更小，导致模型的泛化能力变差</p>
</li>
<li><p>问题思考</p>
</li>
</ul>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>data augmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>一阶段和两阶段目标检测网络</title>
    <url>/2022/05/16/detection/one-stage-vs-two-stage/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>一阶段和二阶段目标检测算法</p>
<p><strong>总结不到位，请重来~</strong><br><span id="more"></span></p>
<h2 id="1，one-stage"><a href="#1，one-stage" class="headerlink" title="1，one stage"></a>1，one stage</h2><p>One-Stage检测算法，没有selective search产生region proposal的阶段，直接产生物体的类别概率和位置坐标，经过单次检测即可直接获得最终的检测结果。相比Two-Stage有更快的速度，但准确度低。代表网络有YOLO v1/v2/v3/9000,SSD,Retina-Net. （two-stage算法中的roi pooling会对目标做resize, 小目标的特征被放大，其特征轮廓也更为清晰，因此检测也更为准确）</p>
<h2 id="2，two-stage"><a href="#2，two-stage" class="headerlink" title="2，two stage"></a>2，two stage</h2><p>先由算法生成一系列作为样本的候选框，再通过卷积神经网络进行样本分类。</p>
<p>对于Two-stage的目标检测网络，主要通过一个卷积神经网络来完成目标检测过程，其提取的是CNN卷积特征，在训练网络时，其主要训练两个部分，第一步是训练RPN网络，第二步是训练目标区域检测的网络。网络的准确度高、速度相对One-stage慢。</p>
<p>Two-Stage检测算法将检测问题划分成两个阶段，首先是获取region proposal进行位置精修和分类阶段。相比于One-Stage,精度高，漏检率也低，但是速度较慢，代表网络有Fast rcnn，Faster rcnn，mask rcnn等。</p>
<h2 id="3，异同"><a href="#3，异同" class="headerlink" title="3，异同"></a>3，异同</h2><p>Two-Stage先对前景背景做了筛选，再进行回归，回归效果比较好，准度高但是相比较慢，One-Stage是直接对特征上的点进行直接回归，优点是速度快，因为用了多层特征图出框可能小目标效果比较好一点，缺点是因为正负样本失衡导致效果较差，要结合难例挖掘。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>one stage</tag>
        <tag>two stage</tag>
      </tags>
  </entry>
  <entry>
    <title>基于框和无框方法的目标检测</title>
    <url>/2022/05/16/detection/-anchor-based-vs-anchor-free/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>anchor based和anchor free两种方法对比，基本内容包含：</p>
<ul>
<li>各自定义</li>
<li>优缺点</li>
<li>应用场景</li>
<li>常用方法</li>
</ul>
<p><strong>一点一点加油~</strong><br><span id="more"></span></p>
<h2 id="1，定义"><a href="#1，定义" class="headerlink" title="1，定义"></a>1，定义</h2><h2 id="2，优缺点"><a href="#2，优缺点" class="headerlink" title="2，优缺点"></a>2，优缺点</h2><h3 id="2-1-anchor-base存在的问题"><a href="#2-1-anchor-base存在的问题" class="headerlink" title="2.1 anchor-base存在的问题"></a>2.1 anchor-base存在的问题</h3><ul>
<li>在训练时，size ratio和anchor number等超参很敏感，需仔细调参；</li>
<li>与锚点框相关超参 (scale、aspect ratio、IoU Threshold) 会较大影响最终预测效果；</li>
<li>预置的锚点大小、比例在检测差异较大物体时不够灵活，尤其是小目标物体，同时也限制了模型的泛化能力；</li>
<li>需要数量较多的anchor，但是大量的锚点会导致运算复杂度增大，产生的参数较多；</li>
<li>容易导致训练时negative与positive的比例失衡；</li>
</ul>
<h3 id="2-2-Anchor-free算法的优点"><a href="#2-2-Anchor-free算法的优点" class="headerlink" title="2.2 Anchor-free算法的优点"></a>2.2 Anchor-free算法的优点</h3><p>• 使用类似分割的思想来解决目标检测问题；<br>• 不需要调优与anchor相关的超参数；<br>• 避免大量计算GT boxes和anchor boxes 之间的IoU，使得训练过程占用内存更低。</p>
<h2 id="3，主要应用场景"><a href="#3，主要应用场景" class="headerlink" title="3，主要应用场景"></a>3，主要应用场景</h2><h2 id="4，常用代表性方法"><a href="#4，常用代表性方法" class="headerlink" title="4，常用代表性方法"></a>4，常用代表性方法</h2><h3 id="4-1-anchor-based"><a href="#4-1-anchor-based" class="headerlink" title="4.1 anchor based"></a>4.1 anchor based</h3><p>基于anchor-based的技术包括一个阶段和两个阶段的检测。  </p>
<p>一阶段的检测技术包括：</p>
<ul>
<li>SSD</li>
<li>DSSD</li>
<li>RetinaNet</li>
<li>RefineDet</li>
<li>YOLOV3</li>
<li>…</li>
</ul>
<p>二阶段技术包括：</p>
<ul>
<li>Faster-RCNN</li>
<li>R-FCN</li>
<li>FPN</li>
<li>Cascade R-CNN</li>
<li>SNIP</li>
<li>…</li>
</ul>
<p>一般的，两个阶段的目标检测会比一个阶段的精度要高，但一个阶段的算法速度会更快。</p>
<h3 id="4-2-anchor-free"><a href="#4-2-anchor-free" class="headerlink" title="4.2 anchor free"></a>4.2 anchor free</h3><p>anchor-free的技术包括基于Keypoint与Segmentation两类。</p>
<p>基于Keypoint技术包括</p>
<ul>
<li>CornerNet</li>
<li>CenterNet</li>
<li>CornerNet-Lite</li>
<li>…</li>
</ul>
<p>基于Segmentation的技术包括</p>
<ul>
<li>FSAF</li>
<li>FCOS</li>
<li>FoveaBox</li>
<li>…</li>
</ul>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>anchor based</tag>
        <tag>anchor free</tag>
      </tags>
  </entry>
  <entry>
    <title>小目标检测</title>
    <url>/2022/05/16/detection/tiny-obj-det/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>早期经典方法为何对小目标检测效果不好？小目标检测有哪几种方案？</p>
<p><strong>一步一步来~</strong><br><span id="more"></span></p>
<h2 id="1，faster-rcnn-yolo-ssd等对小目标为啥不work？"><a href="#1，faster-rcnn-yolo-ssd等对小目标为啥不work？" class="headerlink" title="1，faster rcnn, yolo, ssd等对小目标为啥不work？"></a>1，faster rcnn, yolo, ssd等对小目标为啥不work？</h2><p>SSD，YOLO等单阶段多尺度算法，小目标检测需要较高的分辨率，SSD对于高分辨的低层特征没有再利用，而这些层对于检测小目标很重要。按SSD的设计思想，其实SSD对小目标应该有比较好的效果，但是需要重新精细设计SSD中的default box，比如重新设计min_sizes参数，扩大小default box的数量来cover住小目标。但是随着default box数量的增加，网络速度也会降低。YOLO网络可以理解为是强行把图片分割成7*7个网格，每个网格预测2个目标，相当于只有98个anchor，所以不管是小目标，还是大目标，YOLO的表现都不是很理想，但是由于只需处理少量的anchor，所以YOLO的速度上有很大优势。</p>
<p>Faster rcnn系列对小目标检测效果不好的原因是faster rcnn只用卷积网络的最后一层，但是卷积网络的最后一层往往feature map太小，导致之后的检测和回归无法满足要求。甚至一些小目标在最后的卷积层上直接没有特征点了。所以导致faster rcnn对小目标检测表现较差。</p>
<p><strong>难点</strong><br>分辨率低，图像模糊，携带的信息少。</p>
<h2 id="2，可选解决方案"><a href="#2，可选解决方案" class="headerlink" title="2，可选解决方案"></a>2，可选解决方案</h2><h3 id="2-1-借鉴FPN的思想"><a href="#2-1-借鉴FPN的思想" class="headerlink" title="2.1 借鉴FPN的思想"></a>2.1 借鉴FPN的思想</h3><p>在FPN之前目标检测的大多数方法都是和分类一样，使用顶层的特征来进行处理。虽然这种方法只是用到了高层的语义信息，但是位置信息却没有得到，尤其在检测目标的过程中，位置信息是特别重要的，而位置信息又是主要在网络的低层。因此FPN采用了多尺度特征融合的方式，采用不同特征层特征融合之后的结果来做预测。</p>
<h3 id="2-2-要让输入的分布尽可能地接近模型预训练的分布"><a href="#2-2-要让输入的分布尽可能地接近模型预训练的分布" class="headerlink" title="2.2 要让输入的分布尽可能地接近模型预训练的分布"></a>2.2 要让输入的分布尽可能地接近模型预训练的分布</h3><p>先用ImageNet做预训练，之后使用原图上采样得到的图像来做微调，使用微调的模型来预测原图经过上采样的图像。该方法提升效果比较显著。</p>
<h3 id="2-3-多尺度"><a href="#2-3-多尺度" class="headerlink" title="2.3 多尺度"></a>2.3 多尺度</h3><p>采用多尺度输入训练方式来训练网络；</p>
<h3 id="2-4-借鉴Cascade-R-CNN的设计思路"><a href="#2-4-借鉴Cascade-R-CNN的设计思路" class="headerlink" title="2.4 借鉴Cascade R-CNN的设计思路"></a>2.4 借鉴Cascade R-CNN的设计思路</h3><p>优化目标检测中Two-Stage方法中的IOU阈值。检测中的IOU阈值对于样本的选取是至关重要的，如果IOU阈值过高，会导致正样本质量很高，但是数量会很少，会出现样本比例不平衡的影响；如果IOU阈值较低，样本数量就会增加，但是样本的质量也会下降。如何选取好的IOU，对于检测结果来说很重要。⑤采用分割代替检测方法，先分割，后回归bbox来检测微小目标。</p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>tiny objects det</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测网络小结</title>
    <url>/2022/05/16/detection/object-detection-models/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目标检测的经典网络，会不断更新哦~</p>
<p><strong>催就更~</strong><br><span id="more"></span></p>
<h2 id="目标检测经典网络"><a href="#目标检测经典网络" class="headerlink" title="目标检测经典网络"></a>目标检测经典网络</h2><p><img src="det_models.png" alt="obj_detec">   </p>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>objext detection</tag>
      </tags>
  </entry>
  <entry>
    <title>传统目标检测小结</title>
    <url>/2022/05/16/detection/traditional-obj-det/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>传统目标检测的发展历程和优缺点</p>
<p><strong>有时间整理下HOG+SVM的原理吧~</strong><br><span id="more"></span></p>
<h2 id="1，主线"><a href="#1，主线" class="headerlink" title="1，主线"></a>1，主线</h2><p>区域选择-&gt;特征提取-&gt;分类器</p>
<h2 id="2，算法基本流程"><a href="#2，算法基本流程" class="headerlink" title="2，算法基本流程"></a>2，算法基本流程</h2><ul>
<li><ol>
<li>使用不同尺度的滑动窗口选定图像的某一区域为候选区域；</li>
</ol>
</li>
<li><ol>
<li>从对应的候选区域提取如Harr HOG LBP LTP等一类或者多类特征；</li>
</ol>
</li>
<li><ol>
<li>使用Adaboost SVM 等分类算法对对应的候选区域进行分类，判断是否属于待检测的目标。</li>
</ol>
</li>
</ul>
<h2 id="3，缺点"><a href="#3，缺点" class="headerlink" title="3，缺点"></a>3，缺点</h2><ul>
<li>1）基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余</li>
<li>2）手工设计的特征对于多样性的变化没有很好的鲁棒性</li>
</ul>
]]></content>
      <categories>
        <category>detection</category>
      </categories>
      <tags>
        <tag>object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Nvidia GTX3090配置ubuntu20.04环境</title>
    <url>/2022/05/14/hardware/GTX3090-server-env/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>主机环境：DELL T7920图形工作站，2<em>银牌4214R，24核 2.4G 64G内存，2</em>1TB M.2固态，RTX3090，ubuntu2004系统，主要用于深度学习视觉方向的开发环境搭建。</p>
<span id="more"></span>
<h2 id="1，系统安装"><a href="#1，系统安装" class="headerlink" title="1，系统安装"></a>1，系统安装</h2><p><a href="https://github.com/sophia-hxw/uglyBlog/blob/main/ubuntu/UEFI%E5%92%8CLegacy.md">ubuntu2004安装</a></p>
<h2 id="2，显卡驱动"><a href="#2，显卡驱动" class="headerlink" title="2，显卡驱动"></a>2，显卡驱动</h2><h3 id="2-1-文件下载"><a href="#2-1-文件下载" class="headerlink" title="2.1 文件下载"></a>2.1 文件下载</h3><p><a href="https://www.nvidia.cn/Download/index.aspx">下载链接</a></p>
<h3 id="2-2-禁用nouveau"><a href="#2-2-禁用nouveau" class="headerlink" title="2.2 禁用nouveau"></a>2.2 禁用nouveau</h3><p>报错信息：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ERROR: The Nouveau kernel driver is currently in use by your system. This  driver is incompatible with the NVIDIA driver, and must be disabled before proceeding.</span><br><span class="line">Please consult the NVIDIA driver README and your Linux distribution&#x27;s documentation</span><br><span class="line">for details on how to correctly  disable the Nouveau kernel driver.</span><br></pre></td></tr></table></figure><br>如果出现上述报错，则需要禁用nouveau：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 首先打开或新建文件  </span><br><span class="line">sudo vim /etc/modprobe.d/blacklist-nouveau.conf</span><br><span class="line"># 在文件中添加如下内容</span><br><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line"># 执行命令使其生效并重启</span><br><span class="line">sudo update-initramfs -u</span><br><span class="line">sudo reboot</span><br><span class="line"># 查看禁用情况</span><br><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure></p>
<h3 id="2-3-驱动安装"><a href="#2-3-驱动安装" class="headerlink" title="2.3 驱动安装"></a>2.3 驱动安装</h3><p>显卡驱动：NVIDIA-Linux-x86_64-455.23.04.run<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo bash NVIDIA-Linux-...</span><br></pre></td></tr></table></figure><br>安装完成后用<code>nvidia-smi</code>可以查看显卡信息</p>
<h2 id="3，基本环境"><a href="#3，基本环境" class="headerlink" title="3，基本环境"></a>3，基本环境</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">py37或py38</span><br><span class="line">cuda11.0</span><br><span class="line">cudnn8.0.4</span><br><span class="line">tf2.5（tf-nightly）或 tf1.15.4</span><br><span class="line">pytorch1.7</span><br><span class="line">keras2.3</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>GTX3090</tag>
      </tags>
  </entry>
  <entry>
    <title>static IP of ubuntu20.04</title>
    <url>/2022/05/14/ubuntuOS/staticIP-of-ubuntu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自己搭建了一个简单的服务系统，就有了固定服务器ip的需求，方法不多说了，见下文~</p>
<p><strong>so easy~</strong><br><span id="more"></span></p>
<h2 id="1，详细步骤"><a href="#1，详细步骤" class="headerlink" title="1，详细步骤"></a>1，详细步骤</h2><p>系统版本：20.04<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip addr</span><br><span class="line"># 打开文件</span><br><span class="line">sudo vi /etc/netplan/01-netplan-manager-all.yaml</span><br><span class="line"># 添加内容：</span><br><span class="line">network:</span><br><span class="line">    ethernets:</span><br><span class="line">        ens33:#网卡名称，可能不一样</span><br><span class="line">            dhcp4: false</span><br><span class="line">            addresses: [192.168.1.18/24]</span><br><span class="line">            optional: true</span><br><span class="line">            gateway4: 192.168.1.1</span><br><span class="line">            nameservers:</span><br><span class="line">                addresses: [192.168.1.1,114.114.114.114]</span><br><span class="line">    version: 2</span><br><span class="line"># 使配置生效</span><br><span class="line">sudo netplan apply</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>static IP</tag>
      </tags>
  </entry>
  <entry>
    <title>backup and recovery of ubuntu OS</title>
    <url>/2022/05/14/ubuntuOS/ubuntu-backup-and-recovery/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ubuntu物理机系统，以前崩溃了就一直重装，然后各种软件挺头疼，想起在windows中屡试不爽的备份和换源，想在ubuntu系统中实践下，印象中，测试过两次，效果不太好，有待继续考证~~</p>
<p><strong>路漫漫其修远兮，壮士加油~~</strong><br><span id="more"></span></p>
<h2 id="1，系统备份"><a href="#1，系统备份" class="headerlink" title="1，系统备份"></a>1，系统备份</h2><h3 id="1-1-先清理"><a href="#1-1-先清理" class="headerlink" title="1.1 先清理"></a>1.1 先清理</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 清理旧版本的软件缓存</span><br><span class="line">sudo apt-get autoclean</span><br><span class="line"># 清理所有软件缓存</span><br><span class="line">sudo apt-get clean</span><br><span class="line"># 删除系统不再使用的孤立软件</span><br><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure>
<h3 id="1-2-优盘备份"><a href="#1-2-优盘备份" class="headerlink" title="1.2 优盘备份"></a>1.2 优盘备份</h3><ul>
<li>插入优盘，<code>df -h</code>查看优盘的位置，我的位置是<code>/media/tongtong/</code></li>
<li>切换到系统用户<code>sudo su</code></li>
<li>备份命令<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -cvpzf /media/tongtong/KINGSTON/ubuntu_backup@`date +%Y-%m+%d`.tar.gz --exclude=/proc --exclude=/media --exclude=/tmp --exclude=/home --exclude=/lost+found --exclude=/mnt --exclude=/run / </span><br></pre></td></tr></table></figure>
命令参数：<br>-c： 新建一个备份文档<br>-v： 显示详细信息<br>-p： 保存权限，并应用到所有文件<br>-z： 用gzip压缩备份文档，减小空间<br>-f： 指定备份文件的路径<br>–exclude： 排除指定目录，不进行备份   </li>
</ul>
<p><strong>请注意，如果没有把/home或者/boot目录单独分一个区，一定不要加–exclude=/home或–exclude=/boot参数！！！</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/proc：一个虚拟文件系统，系统运行的每一个进程都会自动在这个目录下面创建一个进程目录。既然是系统自动创建，也就没必要备份的必要了。</span><br><span class="line">/tmp：一个临时文件夹，系统的一些临时文件会放在这里。</span><br><span class="line">/lost+found：系统发生错误时（比如非法关机），可以在这里找回一些丢失文件。</span><br><span class="line">/media：多媒体挂载点，像u盘、移动硬盘、windons分区等都会自动挂载到这个目录下。</span><br><span class="line">/mnt：临时挂载点，你可以自己挂载一些文件系统到这里。</span><br><span class="line">/run：系统从启动以来产生的一些信息文件。</span><br><span class="line">/home：用户家目录，存放用户个人文件和应用程序。</span><br><span class="line">/boot：和系统启动相关的文件，像grub相关文件都放在这里，这个目录很重要！</span><br></pre></td></tr></table></figure>
<ul>
<li>备份<code>/root</code>目录<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -cvpzf /media/tongtong/KINGSTON/ubuntu_root_backup@`data +%Y-%m+%d`.tar.gz /root &gt;/dev/null</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>最后还要提一下就是，有可能备份到最后系统会提示”tar: 由于前次错误,将以上次的错误状态退出”，这个警告可以忽略，没什么影响的。</strong></p>
<h2 id="2，系统还原"><a href="#2，系统还原" class="headerlink" title="2，系统还原"></a>2，系统还原</h2><p><a href="https://blog.csdn.net/qq_35523593/article/details/78545530">reference_link</a></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>ubuntu backup</tag>
        <tag>ubuntu recovery</tag>
      </tags>
  </entry>
  <entry>
    <title>docker server on ubuntu20.04</title>
    <url>/2022/05/14/ubuntuOS/docker-server-of-ubuntu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ubuntu中docker server的搭建，也是常用的工程化工具了~</p>
<p><strong>go on~</strong><br><span id="more"></span></p>
<h2 id="1，安装步骤"><a href="#1，安装步骤" class="headerlink" title="1，安装步骤"></a>1，安装步骤</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 添加https源</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-common</span><br><span class="line"></span><br><span class="line">## 导入源仓库的 GPG key</span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line"></span><br><span class="line">## Docker APT源添加到系统</span><br><span class="line">sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;</span><br><span class="line"></span><br><span class="line">## 列出当前的docker版本</span><br><span class="line">sudo apt update</span><br><span class="line">apt list -a docker-ce</span><br><span class="line"></span><br><span class="line">## 安装特定版本docker</span><br><span class="line">sudo apt install docker-ce=&lt;VERSION&gt; docker-ce-cli=&lt;VERSION&gt; containerd.io</span><br><span class="line"></span><br><span class="line">## 阻止Docker自动更新</span><br><span class="line">sudo apt-mark hold docker-ce</span><br></pre></td></tr></table></figure>
<h2 id="2，报错解决"><a href="#2，报错解决" class="headerlink" title="2，报错解决"></a>2，报错解决</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post &quot;http://....&quot;: dial unix /var/run/docker.sock: connect: permission denied.</span><br></pre></td></tr></table></figure>
<p>用“给非root用于添加docker权限”来解决<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo groupadd docker     #添加docker用户组</span><br><span class="line">sudo gpasswd -a $USER docker     #将登陆用户加入到docker用户组中</span><br><span class="line">newgrp docker     #更新用户组</span><br><span class="line">docker ps    #测试docker命令是否可以使用sudo正常使用</span><br></pre></td></tr></table></figure></p>
<h2 id="3，docker镜像查询"><a href="#3，docker镜像查询" class="headerlink" title="3，docker镜像查询"></a>3，docker镜像查询</h2><p><a href="https://hub.docker.com/">official docker hub</a></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>macOS中brew安装</title>
    <url>/2022/05/14/macOS/brew-error-fix/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>macOS中常用brew来安装其他的软件，若brew命令找不到，那就需要自行安装了，详细步骤见下文哦~</p>
<p><strong>MacOS还是不太熟练，加油~</strong><br><span id="more"></span></p>
<h2 id="1，macOS-brew-install"><a href="#1，macOS-brew-install" class="headerlink" title="1，macOS brew install"></a>1，macOS brew install</h2><p>mac环境下，如何解决brew command not found错误，在终端下，执行以下命令，即可安装brew：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">% /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</span><br><span class="line"></span><br><span class="line">% echo &#x27;eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;&#x27; &gt;&gt; /Users/xinwen/.zprofile</span><br><span class="line"></span><br><span class="line">% eval &quot;$(/opt/homebrew/bin/brew shellenv)&quot;</span><br></pre></td></tr></table></figure><br>在终端环境下，brew —version 查看brew的版本，也可以验证brew是否安装成功</p>
]]></content>
      <categories>
        <category>macOS</category>
      </categories>
      <tags>
        <tag>M1 Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker on Windows10</title>
    <url>/2022/05/10/windows10/win10-WSL2-docker/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>想在win10中使用docker desktop，所以采用了常用的WSL2组件的方式，亲测可用，用docker来配置一系列的开发环境还是比较省心的，若有定制的环境，还可以用dockerfile来构建，避免重复造轮子~</p>
<p><strong>速度杠杠的了~</strong><br><span id="more"></span></p>
<h2 id="1，windows环境"><a href="#1，windows环境" class="headerlink" title="1，windows环境"></a>1，windows环境</h2><p>win10内部版本高于19041，可在电脑属性中查看；<br>找到”启动或者关闭Windows功能”中打开“虚拟机平台”；</p>
<p>下载并安装<a href="https://docs.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package">Linux内核</a></p>
<p>在“启动或者关闭Windows功能”中确认打开“适用于 Linux 的 Windows 子系统”；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 重启系统并设置WSL 2 设置为默认版本</span><br><span class="line">wsl --set-default-version 2</span><br><span class="line"># 查看是不是WSL2</span><br><span class="line">wsl -l -v</span><br></pre></td></tr></table></figure>
<h2 id="2，安装Docker-Desktop-for-windows"><a href="#2，安装Docker-Desktop-for-windows" class="headerlink" title="2，安装Docker Desktop for windows"></a>2，安装Docker Desktop for windows</h2><p>下载<a href="https://www.docker.com/products/docker-desktop/">docker桌面版</a>并安装；</p>
<p>启动Docker Desktop for Windows，点击“设置”按钮，启用基于WSL2的引擎复选框（Use the WSL 2 based engine）；</p>
<h2 id="3，理论啥的"><a href="#3，理论啥的" class="headerlink" title="3，理论啥的"></a>3，理论啥的</h2><p>Docker Desktop for windows方式，其实质是利用docker的C/S架构，将windows模式下的docker对应docker.sock，docker客户端二进制和docker的数据目录挂载到WSL2里面的linux机器，在此linux机器下执行docker命令(docker命令为docker客户端)，实质为客户端通过 挂载的/var/run/docker.sock文件与windows里面的dockerd服务端进程通信。</p>
]]></content>
      <categories>
        <category>windows10</category>
      </categories>
      <tags>
        <tag>Win10 docker</tag>
      </tags>
  </entry>
  <entry>
    <title>M1的MacOS系统恢复</title>
    <url>/2022/05/10/macOS/m1-MacOS-Recovery/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>macOS系统恢复出厂设置，2020款的MacPro，M1芯片的~</p>
<p><strong>M1的兼容性还有待各位大神填坑了~</strong><br><span id="more"></span></p>
<h2 id="1，Apple-Silicon-M1-Mac如何恢复出厂设置"><a href="#1，Apple-Silicon-M1-Mac如何恢复出厂设置" class="headerlink" title="1，Apple Silicon M1 Mac如何恢复出厂设置"></a>1，Apple Silicon M1 Mac如何恢复出厂设置</h2><ul>
<li>关闭计算机，然后按住电源按钮。</li>
<li>首次出现Apple徽标时，您会在其下方看到文本，让您知道继续按住它可以访问启动选项。</li>
<li>持续按住按钮约5秒钟，直到文本切换为“正在加载启动选项”。</li>
<li>接下来，单击选项&gt;继续。</li>
<li>选择具有管理员特权的用户，并在询问时输入帐户密码。</li>
</ul>
<h2 id="2，新的恢复工具"><a href="#2，新的恢复工具" class="headerlink" title="2，新的恢复工具"></a>2，新的恢复工具</h2><p>登录用户帐户后，您会看到部分恢复选项列表。</p>
<h3 id="2-1-从Time-Machine还原"><a href="#2-1-从Time-Machine还原" class="headerlink" title="2.1 从Time Machine还原"></a>2.1 从Time Machine还原</h3><p>如果要从以前的Time Machine备份还原Mac，请使用此选项。如果您丢失了许多文件，更改了设置或安装了导致Mac出现严重问题的应用程序，这将很有帮助。</p>
<h3 id="2-2-重新安装MacOS，具体步骤见后文"><a href="#2-2-重新安装MacOS，具体步骤见后文" class="headerlink" title="2.2 重新安装MacOS，具体步骤见后文"></a>2.2 重新安装MacOS，具体步骤见后文</h3><p>如果MacOS出现问题，可以尝试使用此选项重新安装最新版本的MacOS，而不删除任何文件或丢失任何数据。</p>
<h3 id="2-3-Safari"><a href="#2-3-Safari" class="headerlink" title="2.3 Safari"></a>2.3 Safari</h3><p>您可以使用Apple的浏览器搜索并解决如何修复Mac。</p>
<h3 id="2-4-磁盘实用程序"><a href="#2-4-磁盘实用程序" class="headerlink" title="2.4 磁盘实用程序"></a>2.4 磁盘实用程序</h3><p>用于修复，排除硬盘驱动器或对其进行故障排除的工具。</p>
<h2 id="3，擦除硬盘驱动器"><a href="#3，擦除硬盘驱动器" class="headerlink" title="3，擦除硬盘驱动器"></a>3，擦除硬盘驱动器</h2><p>要从硬盘驱动器中完全删除所有信息并重新安装MacOS，请打开“磁盘工具”，然后选择标有Macintosh HD的内部磁盘。单击“擦除”，然后按照提示进行操作。保留卷名称和格式，但作为参考，名称应为“ Macintosh HD”，格式应为AFPS。单击擦除。</p>
<h2 id="4，重新安装MacOS"><a href="#4，重新安装MacOS" class="headerlink" title="4，重新安装MacOS"></a>4，重新安装MacOS</h2><p>登录用户候，从选项列表中选择“重新安装MacOS ”。系统会要求您选择要安装的位置，该位置应为Macintosh HD（如果决定更改，则为硬盘驱动器的任何名称）。<br>然后，您的Mac将下载最新版本的MacOS，进行安装。</p>
]]></content>
      <categories>
        <category>macOS</category>
      </categories>
      <tags>
        <tag>M1 Mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Legacy and UEFI mode on computers</title>
    <url>/2022/05/10/hardware/UEFI-and-Legacy-mode-of-PC/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>模式问题在安装系统时遇到比较多，老点的机器大多是MBR模式，以前用的光盘，优盘和恢复系统基本问题都不大。现在新的机器越来越多UEFI的模式了，所以装机时会有些需要注意的点，当然还有二者的混合模式，总而言之，这里只是简单的区别，想彻底了解的朋友还是自行查看更多资料哦~</p>
<span id="more"></span>
<h2 id="1，传统Legacy（MBR）模式"><a href="#1，传统Legacy（MBR）模式" class="headerlink" title="1，传统Legacy（MBR）模式"></a>1，传统Legacy（MBR）模式</h2><ul>
<li>如果机型相对比较旧，要么是Legacy启动模式，要么是Legacy+UEFI混合模式。</li>
<li>这种模式启动流程复杂，耗时长。</li>
<li>如果你是在Legacy模式下安装的系统，也只能在legacy模式下进系统。</li>
<li>Legacy兼容性很好，32位和64位都不在话下</li>
<li>最多只能支持4个主分区，而且只能控制2TB的分区，有一定限制</li>
</ul>
<h2 id="2，UEFI（GPT）模式"><a href="#2，UEFI（GPT）模式" class="headerlink" title="2，UEFI（GPT）模式"></a>2，UEFI（GPT）模式</h2><ul>
<li>Unified Extensible Firmware Interface   </li>
<li>如今大部分新机型的电脑基本都是采用UEFI启动模式。   </li>
<li>这是一种详细描述全新类型接口的标准。这种接口用于操作系统自动从预启动的操作环境，加载到一种操作系统上，从而使开机程序化繁为简，节省时间。   </li>
<li>在UEFI模式下安装的系统，只能用UEFI模式引导</li>
<li>UEFI只支持64位系统且磁盘分区必须为gpt模式</li>
<li>最多能够支持128个分区，最高能够支持18EB的容量</li>
<li>UEFI模式的安全引导功能，能够很好地防止病毒在引导开机时自行加载</li>
</ul>
<h2 id="3，Legacy-UEFI的混合模式"><a href="#3，Legacy-UEFI的混合模式" class="headerlink" title="3，Legacy+UEFI的混合模式"></a>3，Legacy+UEFI的混合模式</h2><h3 id="3-1-安装时报错"><a href="#3-1-安装时报错" class="headerlink" title="3.1 安装时报错"></a>3.1 安装时报错</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">No EFI system partition was found, the system will likely not be able to boot successfully...</span><br></pre></td></tr></table></figure>
<ul>
<li>解决办法<br>在手动分区时新建一个/boot/efi分区，然后将其格式设为efi，剩下的分区就照常设置就好</li>
</ul>
<h3 id="3-2-重装后系统启动报错"><a href="#3-2-重装后系统启动报错" class="headerlink" title="3.2 重装后系统启动报错"></a>3.2 重装后系统启动报错</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">No Boot Device Found. Press any key to reboot the machine.</span><br></pre></td></tr></table></figure>
<ul>
<li><p>问题来源<br>操作系统与当前BIOS模式不匹配。在UEFI模式下安装的系统，只能用UEFI模式引导；同理，如果你是在Legacy模式下安装的系统，也只能在legacy模式下进系统。如果更改过BIOS模式，可能会出现这种情况。</p>
</li>
<li><p>解决办法<br>正常情况下还是可以进入BIOS的，进去后更改Boot mode即可。<br>特殊情况：Default Boot Device Missing or Boot Failed且Boot Manager下无选择项。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>legacy and uefi</tag>
        <tag>PC bios</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo的主要搭建过程</title>
    <url>/2022/05/10/ubuntuOS/hexo-main/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个基本就是简单的搭建示例，对前端小白很友好，也够简单整洁，是喜欢的风格没跑了~</p>
<p><strong>安心码文了~</strong><br><span id="more"></span></p>
<h2 id="1，安装hexo"><a href="#1，安装hexo" class="headerlink" title="1，安装hexo"></a>1，安装hexo</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使用 npm 一键安装 Hexo 博客程序</span><br><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo init      # 初始化</span><br><span class="line">npm install    # 安装组件</span><br><span class="line">hexo g   # 生成页面</span><br><span class="line">hexo s   # 启动预览</span><br></pre></td></tr></table></figure>
<p>访问 <a href="http://localhost:4000，出现">http://localhost:4000，出现</a> Hexo 默认页面，本地博客安装成功！</p>
<h2 id="2，连接github"><a href="#2，连接github" class="headerlink" title="2，连接github"></a>2，连接github</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;xinwen&quot;</span><br><span class="line">git config --global user.email &quot;hxinwen1218@sina.com&quot;</span><br><span class="line">ssh-keygen -t rsa -C &quot;hxinwen1218@sina.com&quot;    # 之后一路回车，大概率会在/root/.ssh中生成公钥和私钥</span><br></pre></td></tr></table></figure>
<p>登录github，进入Settings-&gt;SSH and GPG keys-&gt;New SSH key，title随意起，将刚才的公钥(id_rsa.pub)复制进去即可。</p>
<h2 id="3，新建github-Pages仓库"><a href="#3，新建github-Pages仓库" class="headerlink" title="3，新建github Pages仓库"></a>3，新建github Pages仓库</h2><p>新建仓库，注意Repository name，请务必为：用户名.github.io</p>
<p>博客地址：<a href="https://用户名.github.io">https://用户名.github.io</a></p>
<h2 id="4，部署hexo到Github-Pages"><a href="#4，部署hexo到Github-Pages" class="headerlink" title="4，部署hexo到Github Pages"></a>4，部署hexo到Github Pages</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 安装 hexo-deployer-git</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line"></span><br><span class="line"># 然后，修改 _config.yml 文件</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:用户名/用户名.github.io.git</span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure>
<p>然后，运行<code>hexo d</code>即可部署到Github Pages.</p>
<h2 id="5，hexo其他信息"><a href="#5，hexo其他信息" class="headerlink" title="5，hexo其他信息"></a>5，hexo其他信息</h2><h3 id="5-1-hexo常用命令"><a href="#5-1-hexo常用命令" class="headerlink" title="5.1 hexo常用命令"></a>5.1 hexo常用命令</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new &quot;postName&quot; #新建文章</span><br><span class="line">hexo new page &quot;pageName&quot; #新建页面</span><br><span class="line">hexo generate #生成静态页面至public目录</span><br><span class="line">hexo server #开启预览访问端口（默认端口4000，&#x27;ctrl + c&#x27;关闭server）</span><br><span class="line">hexo deploy #部署到GitHub</span><br><span class="line">hexo help  # 查看帮助</span><br><span class="line">hexo version  #查看Hexo的版本</span><br></pre></td></tr></table></figure>
<h3 id="5-2-hexo命令缩写"><a href="#5-2-hexo命令缩写" class="headerlink" title="5.2 hexo命令缩写"></a>5.2 hexo命令缩写</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo n == hexo new</span><br><span class="line">hexo g == hexo generate</span><br><span class="line">hexo s == hexo server</span><br><span class="line">hexo d == hexo deploy</span><br></pre></td></tr></table></figure>
<h3 id="5-3-hexo组合命令"><a href="#5-3-hexo组合命令" class="headerlink" title="5.3 hexo组合命令"></a>5.3 hexo组合命令</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo s -g #生成并本地预览</span><br><span class="line">hexo d -g #生成并上传</span><br></pre></td></tr></table></figure>
<h2 id="6，hexo博客文件夹"><a href="#6，hexo博客文件夹" class="headerlink" title="6，hexo博客文件夹"></a>6，hexo博客文件夹</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">_config.yml         #网站的配置信息</span><br><span class="line">package.json        #应用程序的信息</span><br><span class="line">scaffolds           #模板文件夹</span><br><span class="line">source              #存放用户资源，markdown文档</span><br><span class="line">  _drafts</span><br><span class="line">  _posts</span><br><span class="line">themes              #主题文件夹</span><br><span class="line">public              #网站文件</span><br></pre></td></tr></table></figure>
<h2 id="7-参考文章"><a href="#7-参考文章" class="headerlink" title="7 参考文章"></a>7 参考文章</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/60578464#:~:text=%E4%BD%BF%E7%94%A8%20Hexo%2BGitHub%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%85%8D%E8%B4%B9%E5%8D%9A%E5%AE%A2%E6%95%99%E7%A8%8B%EF%BC%88%E5%B0%8F%E7%99%BD%E5%90%91%EF%BC%89%201%20%E5%87%86%E5%A4%87%202%20%E8%BF%9E%E6%8E%A5%20Github....,Hexo%20%E5%88%B0%20GitHub%20Pages%206%20%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89....%207%20%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8">使用 Hexo+GitHub 搭建个人免费博客教程</a>    </li>
<li><a href="https://blog.csdn.net/qq_40540975/article/details/124489981">使用Hexo+github搭建个人博客</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/265077468">hexo博客中添加图片</a></li>
<li><a href="https://blog.csdn.net/tuckEnough/article/details/107383201">next主题美化</a></li>
<li><a href="https://github.com/theme-next/hexo-theme-next">next主题github</a></li>
</ul>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu20.04 install and update nodejs</title>
    <url>/2022/05/10/ubuntuOS/ubuntu20.04-install-and-update-nodejs/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在ubuntu系统中安装和更新nodejs，做什么用呢？在搭建自己的博客系统时用到，因为对ubuntu中常用的开发软件比较熟悉，哈哈~~</p>
<p><strong>喜欢自己搭建的博客，努力码文吧~~</strong><br><span id="more"></span></p>
<h2 id="构建和更新命令"><a href="#构建和更新命令" class="headerlink" title="构建和更新命令"></a>构建和更新命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># install on ubuntu20.04</span><br><span class="line">apt-get install -y nodejs npm</span><br><span class="line"># upgrade</span><br><span class="line">node -v</span><br><span class="line">npm install n -g</span><br><span class="line">n stable</span><br><span class="line">hash -r</span><br><span class="line"># 3, softlink update</span><br><span class="line">cp /usr/local/bin/node /usr/bin/node</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>nodejs &amp; npm</tag>
      </tags>
  </entry>
  <entry>
    <title>jekyll on ubuntu20.04</title>
    <url>/2022/05/10/ubuntuOS/jekyll-on-ubuntu/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>还是在自建博客过程中发现的需求之一，看帖子说现在很多人用jekyll建博客，但是我这边在部署到github的时候出现了一堆问题，还是递归的出现…就是在解决问题时，会冒出另一个~所以果断转战到hexo了，下面是初步尝试环境的搭建~   </p>
<p>说明：在docker中，使用的都是root权限，所以一般用户自行添加sudo来运行命令</p>
<p><strong>还是很弱啊，要加油~</strong><br><span id="more"></span></p>
<h2 id="1，详细步骤"><a href="#1，详细步骤" class="headerlink" title="1，详细步骤"></a>1，详细步骤</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># jekyll依赖</span><br><span class="line">    apt-get install ruby-full build-essential zlib1g-dev</span><br><span class="line"># 环境变量</span><br><span class="line">    echo &#x27;# Install Ruby Gems to ~/gems&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">    echo &#x27;export GEM_HOME=&quot;$HOME/gems&quot;&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">    echo &#x27;export PATH=&quot;$HOME/gems/bin:$PATH&quot;&#x27; &gt;&gt; ~/.bashrc</span><br><span class="line">    source ~/.bashrc</span><br><span class="line"># 移除gem默认源，改成ruby-china源</span><br><span class="line">    gem sources -r https://rubygems.org/ -a https://gems.ruby-china.com/</span><br><span class="line"># 安装jekyll</span><br><span class="line">    gem install jekyll bundler</span><br><span class="line"># 使用Gemfile和Bundle的项目，可以做下面修改，就不用修改Gemfile的source</span><br><span class="line">    bundle config mirror.https://rubygems.org https://gems.ruby-china.com</span><br><span class="line"># 删除Bundle的一个镜像源</span><br><span class="line">    bundle config --delete &#x27;mirror.https://rubygems.org&#x27;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu20.04 change its source</title>
    <url>/2022/05/09/ubuntuOS/ubuntu20.04-change-its-source/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>ubuntu官方源由于众所周知的原因，更新啊、安装软件啊都比较慢，甚至经常就卡住不动了，所以换源还是个很基础的操作滴，以前感觉“阿里云”好用，最近在ubuntu20.04上测试，莫名多了些问题，切换到“清华”源，丝滑般流畅啊~~</p>
<p><strong>相当于基础平A操作了，没有花样，就是好用~</strong></p>
<span id="more"></span>
<h2 id="1，ubuntu换源操作"><a href="#1，ubuntu换源操作" class="headerlink" title="1，ubuntu换源操作"></a>1，ubuntu换源操作</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cp /etc/apt/source.list /etc/apt/source.list.bak</span><br><span class="line">vim /etc/apt/source.list</span><br><span class="line"># 删除原有source，换成下面的源</span><br><span class="line">apt-get update</span><br><span class="line">apt-get upgrade</span><br></pre></td></tr></table></figure>
<h2 id="2，可用源"><a href="#2，可用源" class="headerlink" title="2，可用源"></a>2，可用源</h2><p>可自行选用，亲测可用：清华源<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#清华源</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse</span><br><span class="line">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#添加阿里源</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">#中科大源</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line"> </span><br><span class="line">#163源</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.163.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>ubuntuOS</category>
      </categories>
      <tags>
        <tag>ubuntu source.list</tag>
      </tags>
  </entry>
</search>
