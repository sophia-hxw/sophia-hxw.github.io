<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"github.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="前言title: A Survey on Vision Transformer paper link 继续加油读文章">
<meta property="og:type" content="article">
<meta property="og:title" content="A Survey on Vision Transformer">
<meta property="og:url" content="https://github.com/sophia-hxw/sophia-hxw.github.io/2023/06/28/Transformer/A%20Survey%20on%20Vision%20Transformer/index.html">
<meta property="og:site_name" content="橦言无忌">
<meta property="og:description" content="前言title: A Survey on Vision Transformer paper link 继续加油读文章">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-06-28T11:13:46.000Z">
<meta property="article:modified_time" content="2023-06-28T11:15:27.873Z">
<meta property="article:author" content="xinwen">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://github.com/sophia-hxw/sophia-hxw.github.io/2023/06/28/Transformer/A%20Survey%20on%20Vision%20Transformer/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>A Survey on Vision Transformer | 橦言无忌</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">橦言无忌</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">一个不想改变世界的程序媛</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">100</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">8</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">122</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/sophia-hxw" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://github.com/sophia-hxw/sophia-hxw.github.io/2023/06/28/Transformer/A%20Survey%20on%20Vision%20Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me.jpg">
      <meta itemprop="name" content="xinwen">
      <meta itemprop="description" content="想到哪儿记到哪儿的技术博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="橦言无忌">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A Survey on Vision Transformer
        </h1>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-28 19:13:46 / 修改时间：19:15:27" itemprop="dateCreated datePublished" datetime="2023-06-28T19:13:46+08:00">2023-06-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Transformer/" itemprop="url" rel="index"><span itemprop="name">Transformer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2023/06/28/Transformer/A%20Survey%20on%20Vision%20Transformer/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2023/06/28/Transformer/A%20Survey%20on%20Vision%20Transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>34k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>31 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>title: A Survey on Vision Transformer</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.12556">paper link</a></p>
<p><strong>继续加油读文章</strong><br><span id="more"></span></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Transformer最早应用于自然语言处理领域，是一类主要基于self-attention机制的深度神经网络。由于其强大的表示能力，研究人员正在寻找将 Transformer 应用于计算机视觉任务的方法。在各种视觉基准测试中，基于 Transformer 的模型性能与其他类型的网络（例如卷积神经网络和递归神经网络）相似或更好。鉴于其高性能和对<strong>特定视觉归纳偏差</strong>【？】的需求较少，Transformer 越来越受到计算机视觉社区的关注。在本文中，我们通过将它们分类为不同的任务并分析它们的优缺点来回顾这些视觉Transformer模型。我们探索的主要模型类别包括骨干网络、高级/中级视觉、低级视觉和视频处理，还包括有效的将Transformer推入实际的基于设备的应用程序。此外，我们还简要介绍了计算机视觉中的自注意力机制，因为它是 Transformer 中的基本组件。在本文的最后，我们讨论了未来的挑战并为视觉Transformer提供了几个可以进一步的研究方向。</p>
<h2 id="1，介绍"><a href="#1，介绍" class="headerlink" title="1，介绍"></a>1，介绍</h2><p>深度神经网络 (DNN) 已成为当今人工智能 (AI) 系统的基础设施，不同类型的任务通常涉及不同类型的网络。例如，多层感知器（MLP）或全连接（FC）网络是神经网络的经典类型，它由多个线性层和非线性激活叠加在一起组成，卷积神经网络 (CNN) 引入卷积层和池化层来处理图像等移位不变数据，循环神经网络 (RNN) 利用循环单元来处理顺序数据或时间序列数据，Transformer 是一种新型的神经网络。它主要利用self-attention机制来提取内在特征，显示出在AI应用中广泛使用的巨大潜力。</p>
<p>Transformer 首先应用于自然语言处理 (NLP) 任务，并取得了显着改进。例如 vaswani2017attention 首先提出了基于注意力机制的Transformer用于机器翻译和英语选区解析任务。 bert 引入了一种新的语言表示模型，称为 BERT（Bidirectional Encoder Representations from Transformers 的缩写），它在未标记文本上预训练一个Transformer，因为它是双向的，会同时考虑到每个单词的上下文，当 BERT 发布时，它在 11 个 NLP 任务上获得了最优的性能。 gpt3 使用 1750 亿个参数在 45 TB 的压缩明文数据上预训练了一个名为 GPT-3（Generative Pre-trained Transformer 3 的缩写）的大型基于 Transformer 的模型，它在不同类型的下游自然语言任务上实现了强大的性能，而无需任何微调。这些基于 Transformer 的模型以其强大的表示能力，在 NLP 领域取得了重大突破。</p>
<p>受 Transformer 架构在 NLP 领域取得的重大成功的启发，研究人员最近将 Transformer 应用于计算机视觉 (CV) 任务。在视觉应用中，CNN 被认为是基本组件，但如今 Transformer 表明它是 CNN 的潜在替代品。igpt 训练了一个序列Transformer来自动回归预测像素，在图像分类任务上取得了与 CNN 相当的结果。另一个视觉Transformer模型是 ViT，它将纯Transformer直接应用于图像块序列以对整个图像进行分类。最近由 vit 提出，它在多个图像识别基准上取得了最先进的性能。除了图像分类之外，Transformer 还被用于解决其他各种视觉问题，包括目标检测 、语义分割 、图像处理 ，以及视频理解。由于其卓越的性能，越来越多的研究人员提出基于 Transformer 的模型来改进各种视觉任务。</p>
<p>由于基于 Transformer 的视觉模型数量迅速增加，跟上新进展的速度变得越来越困难，因此，对现有工程进行调研是当务之急，并且对社区有益。在本文中，我们着重于全面概述视觉 Transformer 的最新进展，并讨论潜在可研究方向。为了便于未来对不同主题的研究，我们根据应用场景对 Transformer 模型进行了分类，如表1所列，主要类别包括骨干网、高/中级视觉、低级视觉和视频处理。高级视觉处理对图像中看到的内容进行解释和使用，而中级视觉处理如何将这些信息组织成我们所体验的物体和表面，鉴于在基于 DNN 的视觉系统中，高级和中级视觉之间的差距变得越来越模糊，我们在这里将它们视为一个类别。解决这些高级/中级视觉任务的 Transformer 模型的一些示例包括用于对象检测的 DETR、可变形的 DETR 和用于分割的 Max-DeepLab。低级图像处理主要处理从图像中提取描述（这种描述通常表示为图像本身），典型应用包括超分辨率、图像去噪和风格转换。目前，只有少数作品在低级视觉中使用了 Transformers，因此需要进一步研究。另一类是视频处理，它是计算机视觉和基于图像的任务的重要组成部分。由于视频的顺序属性，Transformer 天生就非常适合用于视频任务，它的表现开始与传统的 CNN 和 RNN 相提并论。在这里，我们调查了与基于 Transformer 的视觉模型相关的工作，以跟踪该领域的进展。图1显示了 vision Transformer 的开发时间表——毫无疑问，未来会有更多的里程碑。</p>
<p>本文的其余部分安排如下，第 2 节讨论了标准 Transformer 的重构和自注意力机制。第 4 节是本文的主要部分，其中我们总结了主干、高级/中级视觉、低级视觉和视频任务的视觉Transformer模型，还简要描述了高效的Transformer方法，因为它们与我们的主题密切相关。在最后一节中，我们给出了结论并讨论了几个研究方向和挑战。由于篇幅限制，我们在补充材料中描述了 NLP 中 Transformer 的方法，因为研究经验可能对视觉任务有益。在补充材料中，我们还回顾了 CV 的自我注意机制作为视觉Transformer模型的补充。在本次调研中，我们主要包括代表性作品（早期的、开创性的、新颖的或启发性的作品），因为 arXiv 上有很多预印本作品，我们无法在有限的页面中将它们全部包括在内。</p>
<h2 id="2，Transformer提出"><a href="#2，Transformer提出" class="headerlink" title="2，Transformer提出"></a>2，Transformer提出</h2><p>Transformer 最早用于自然语言处理（NLP）领域的机器翻译任务， 如图2所示，它由一个编码器和一个解码器以及几个相同架构的Transformer 块组成。 编码器生成输入的编码，而解码器采用所有编码并使用它们合并的上下文信息来生成输出序列。 每个 Transformer 块由多头注意力层、前馈神经网络、跳跃连接和层归一化组成。 下面，我们将详细描述 Transformer 的每个组件。</p>
<h2 id="2-1-自注意力"><a href="#2-1-自注意力" class="headerlink" title="2.1 自注意力"></a>2.1 自注意力</h2><p>在self-attention层，首先将输入向量转化为三个不同的向量：查询向量 $\mathbf q$，关键向量 $\mathbf k$ 和值向量 $\mathbf v$，维度为 $d_q=d_k =d_v= d_{model}=512$。 然后将来自不同输入的向量打包成三个不同的矩阵，即 $\mathbf Q$、$\mathbf K$ 和 $\mathbf V$。 随后，计算不同输入向量之间的注意力函数如下（如图3左所示）：</p>
<ul>
<li><strong>step 1</strong>：计算不同输入向量之间的分数，其中 $\mathbf S=\mathbf Q\cdot \mathbf K^\top$;</li>
<li><strong>step 2</strong>：使用 $\mathbf S_n=\mathbf{S}/{\sqrt{d_k}}$ 归一化梯度稳定性的分数；</li>
<li><strong>step 3</strong>：使用 softmax 函数将分数转换为概率 $\mathbf P=\mathrm{softmax}(\mathbf S_n)$;</li>
<li><strong>step 4</strong>：求得加权值矩阵，其中$\mathbf Z=\mathbf V\cdot \mathbf P$。</li>
</ul>
<p>该过程可以统一为一个函数：</p>
<script type="math/tex; mode=display">\mathrm{Attention}(\mathbf Q,\mathbf K,\mathbf V)=\mathrm{softmax}(\frac{\mathbf Q\cdot\mathbf K^\top}{\sqrt{d_k}})\cdot\mathbf V \tag{1}\label{eq1}</script><p>Eq.\ref{eq1} 背后的逻辑很简单。步骤 1 计算每对不同向量之间的分数，这些分数决定了我们在对当前位置的单词进行编码时给予其他单词的关注程度。第 2 步对分数进行归一化以增强梯度稳定性以改进训练，第 3 步将分数转化为概率。最后，每个值向量乘以概率之和。具有较大概率的向量会受到后面层的额外关注。</p>
<p>解码器模块中的编码器-解码器注意层类似于编码器模块中的自注意层，但有以下不同之处：键矩阵 $K$ 和值矩阵 $V$ 来自编码器模块，查询矩阵 $Q$ 是从上一层推导出来的。</p>
<p>请注意，前面的过程对于每个单词的位置是不变的，这意味着自注意力层缺乏捕获句子中单词位置信息的能力。然而，语言中句子的顺序性质要求我们将位置信息合并到我们的编码中。为了解决这个问题并允许获得单词的最终输入向量，在原始输入嵌入中添加了一个维度为 $d_{model}$ 的位置编码。具体来说，该位置使用以下等式进行编码：</p>
<p>$\mathrm{PE}(pos,2i)=\sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \tag{2}$$</p>
<script type="math/tex; mode=display">\mathrm{PE}(pos,2i+1)=\cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \tag{3}</script><p>其中 $pos$ 表示单词在句子中的位置，$i$ 表示位置编码的当前维度。 这样，位置编码的每个元素都对应一个正弦曲线，它允许 Transformer 模型学习关注相对位置，并在推理过程中外推到更长的序列长度。 除了 vanilla Transformer 中的固定位置编码外，学习位置编码和相对位置编码也被用于各种模型。</p>
<p><strong>多头机制</strong><br>多头注意力是一种可用于提升普通自注意力层性能的机制。请注意，对于给定的参考词，我们通常希望在浏览句子时关注其他几个词，单头自注意力层限制了我们专注于一个或多个特定位置而不会同时影响对其他同等重要位置注意力的能力。这是通过给予注意力层不同的表示子空间来实现的。具体来说，不同的head使用不同的query、key和value矩阵，这些矩阵由于随机初始化可以在训练后将输入向量投影到不同的表示子空间。</p>
<p>为了更详细地阐述这一点，给定一个输入向量和头的数量 $h$，输入向量首先被转换为三个不同的向量组：查询组、键组和值组。在每组中，有 $h$ 个维度为 $d_{q’}=d_{k’}=d_{v’}=d_{model}/h=64$ 的向量。然后将来自不同输入的向量打包成三组不同的矩阵：$\{\mathbf Q_i\}_{i=1}^h$, $\{\mathbf K_i\}_{i=1}^ h$ 和 $\{\mathbf V_i\}_{i=1}^h$。 多头注意力机制的流程如下图所示：</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathrm{MultiHead}(\mathbf Q^{'},\mathbf K^{'},\mathbf V^{'})=\mathrm{Concat}(head_1,\cdots ,head_h)\mathbf W^o,\\
where head_i=\mathrm{Attention}(\mathbf Q_i,\mathbf K_i,\mathbf V_i)
\end{align*}
\tag{4}</script><p>这里，$\mathbf Q’$（以及类似的 $\mathbf K’$ 和 $\mathbf V’$）是 $\{\mathbf Q_i\}_{i=1}^h$ 和 $\mathbf W^o\in\mathbb R^{d_{model}\times d_{model}}$ 是投影权重。</p>
<h3 id="2-2-Transformer其他重要内容"><a href="#2-2-Transformer其他重要内容" class="headerlink" title="2.2 Transformer其他重要内容"></a>2.2 Transformer其他重要内容</h3><p><strong>前馈网络</strong><br>在每个编码器和解码器的自注意层之后应用前馈网络 (FFN),它由两个线性变换层和其中的一个非线性激活函数组成，可以表示为以下函数：</p>
<script type="math/tex; mode=display">\mathrm{FFN}(\mathbf X)=\mathbf W_2\sigma(\mathbf W_1 \mathbf X) \tag{5}</script><p>其中$\mathbf W_1$和$\mathbf W_2$是两个线性变换层的两个参数矩阵，$\sigma$表示非线性激活函数，如GELU，隐藏层的维度是$d_h=2048$。</p>
<p><strong>编解码中的残差连接</strong><br>如图2所示，在编码器和解码器中的每个子层都添加了一个残差连接，这加强了信息流，以实现更高的性能。 在残差连接之后进行层归一化，这些操作的输出可以描述为：</p>
<script type="math/tex; mode=display">\mathrm{LayerNorm}(\mathbf X+\mathrm{Attention}(\mathbf X)) \tag{6}</script><p>这里 $\mathbf X$ 作为self-attention层的输入，query、key和value矩阵 $\mathbf Q、\mathbf K$ 和 $\mathbf V$ 都是从同一个输入矩阵 $\mathbf X$ 推导出来的。 一种预归一化层的变体（Pre-LN）也被广泛使用， 在残差连接内部和多头注意力或 FFN 之前插入层归一化。 对于归一化层，有几种选择，例如批量归一化 bn。 当特征值急剧变化时，批量归一化在应用于 Transformer 时通常表现较差。 其他一些归一化算法 xu2019understanding 等已经被提出来改进Transformer的训练。</p>
<p><strong>解码器的最后一层</strong><br>解码器的最后堆栈层用于将向量转换回单词，这是通过一个线性层和一个 softmax 层来实现的。线性层将向量投影为具有 $d_{word}$ 维度的 logits 向量，其中 $d_{word}$ 是词汇表中的单词数。然后使用 softmax 层将 logits 向量转换为概率。</p>
<p>用于CV任务时，Transformer大多采用原装 Transformer 的encoder模块，这样的Transformer可以被视为一种新型的特征提取器。与仅关注局部特征的 CNN 相比，Transformer 可以捕获远距离特征，这意味着它可以轻松推导出全局信息。与隐藏状态必须按顺序计算的 RNN 相比，Transformer 的效率更高，因为自注意力层和全连接层的输出可以并行计算并且易于加速。由此，我们可以得出结论，进一步研究在计算机视觉和 NLP 中使用 Transformer 将产生有益的结果。</p>
<h2 id="3，视觉Transformer"><a href="#3，视觉Transformer" class="headerlink" title="3，视觉Transformer"></a>3，视觉Transformer</h2><p>在本节中，我们回顾了基于 Transformer 的模型在计算机视觉中的应用，包括图像分类、高级/中级视觉、低级视觉和视频处理。 我们还简要总结了自注意力机制和模型压缩方法在高效 Transformer 中的应用。</p>
<h3 id="3-1-表征学习的backbone"><a href="#3-1-表征学习的backbone" class="headerlink" title="3.1 表征学习的backbone"></a>3.1 表征学习的backbone</h3><p>受 Transformer 在 NLP 领域取得成功的启发，一些研究人员探索了类似的模型是否可以学习图像的有效表示。鉴于与文本相比，图像涉及更多维度、噪声和冗余模态，因此它们被认为更难用于生成建模。</p>
<p>除了 CNN，Transformer 还可以用作图像分类的骨干网络。 Wu 采用 ResNet 作为常用的基线，并使用视觉Transformer替换最后阶段的卷积。具体来说，他们应用卷积层来提取低级特征，然后将这些特征输入视觉Transformer。对于视觉Transformer，他们使用 <strong>tokenizer</strong> 将像素分组为少量 <strong>visual tokens</strong>，每个代表图像中的语义概念。这些 <strong>visual tokens</strong> 直接用于图像分类，而 Transformers 用于对 token 之间的关系建模。如图4所示，工作可以分为单纯使用Transformer做视觉和结合CNN和Transformer。我们在表2和图6中总结了这些模型的结果，以展示主干的发展。除了监督学习，vision Transformer也探索了自监督学习。</p>
<h4 id="3-1-1-纯Transformer"><a href="#3-1-1-纯Transformer" class="headerlink" title="3.1.1 纯Transformer"></a>3.1.1 纯Transformer</h4><p><strong>ViT-Vision Transformer</strong><br>Vision Transformer (ViT)是一种直接应用于图像块序列的纯Transformer，用于图像分类任务，它尽可能遵循 Transformer 的原始设计，图5展示了 ViT 的框架。</p>
<p>为了处理 2D 图像，图像 $X\in \mathbb{R}^{h\times w \times c}$ 被重塑为一系列扁平的 2D 块 $X_p\in \mathbb{R}^{n\times (p^2 \cdot c)}$，其中 $c$ 是通道数。 $(h,w)$ 是原始图像的分辨率，而 $(p,p)$ 是每个图像块的分辨率。因此，Transformer的有效序列长度为 $n=hw/p^2$。由于Transformer在其所有层中使用恒定宽度，因此可训练的线性投影将每个矢量化路径映射到模型维度 $d$，其输出称为块嵌入。</p>
<p>类似于 BERT 的 $[class]$  token ，可学习的嵌入应用于嵌入块序列，该嵌入的状态用作图像表示。在预训练和微调阶段，分类头都附加在相同的尺寸上。此外，将一维位置嵌入添加到块嵌入中以保留位置信息。值得注意的是，ViT 仅使用标准Transformer的编码器（层归一化位置除外），其输出位于 MLP 头之前。在大多数情况下，ViT 在大型数据集上进行预训练，然后针对较小数据的下游任务进行微调。</p>
<p>当在 ImageNet 等中型数据集上训练时，ViT 产生适度的结果，其准确度比同等规模的 ResNet 低几个百分点。因为 Transformer 缺乏 CNN 固有的一些归纳偏差——例如平移等方差和局部性——它们在训练数据量不足时不能很好地泛化。然而，作者发现在大型数据集（1400 万到 3 亿张图像）上训练模型的效果优于归纳偏差。当以足够的规模进行预训练时，Transformers 在数据点较少的任务上取得了优异的成绩。例如，当在 JFT-300M 数据集上进行预训练时，ViT 在多个图像识别基准上的性能接近甚至超过了最先进的性能。具体来说，它在 ImageNet 上达到了 88.36% 的准确率，在包含 19 个任务的 VTAB 套件上达到了 77.16% 的准确率。</p>
<p>DeiT 通过仅在 ImageNet 数据库上进行训练，提出了一种具有竞争力的无卷积Transformer，称为 Data-efficient image Transformer (DeiT)。视觉Transformer DeiT-B 具有与 ViT-B 相同的架构，并使用 8600 万个参数。通过强大的数据增强，DeiT-B 在没有外部数据的情况下在 ImageNet 上实现了 83.1%（单裁剪评估）的 top-1 准确率。此外，作者观察到使用 CNN teacher 比使用 Transformer 具有更好的性能。具体来说，DeiT-B 可以在基于 token 的蒸馏下达到 top-1 准确率 84.40%。</p>
<p><strong>ViT的变体</strong><br>遵循 ViT 的范例，已经提出了一系列 ViT 变体来提高视觉任务的性能，主要方法包括增强局部性、自注意力提升和架构设计。</p>
<p>原始视觉Transformer擅长捕获块之间的远程依赖关系，但忽略了局部特征提取，因为 2D 块被简单线性投影成向量。最近，研究人员开始关注提高对局部信息的建模能力。 TNT 进一步将块划分为多个子块，并引入了一种新颖的 Transformer-in-Transformer 架构，该架构利用内部 Transformer 块来模拟子块和外部 Transformer 块之间的关系，以进行块级信息交换。 Twins 和 CAT 逐层交替执行局部和全局注意力。 Swin Transformers 在一个窗口内进行局部关注，并为跨窗口连接引入了一种移动窗口分区方法。 Shuffle Transformer 进一步利用空间洗牌操作而不是移位窗口分区来允许跨窗口连接。 RegionViT 从图像中生成区域token 和局部token，局部token通过区域token的注意力接收全局信息。除了本地注意力之外，其他一些工作还提出通过本地特征聚合来提升本地信息，例如 T2T。这些工作展示了视觉Transformer中本地信息交换和全局信息交换的好处。</p>
<p>作为Transformer的关键组成部分，self-attention层提供了图像块之间全局交互的能力。改进self-attention层的计算吸引了很多研究者，DeepViT 提出建立跨头通信来重新生成注意力图，以增加不同层的多样性。KVT 引入了 $k$-NN 注意力以利用图像块的局部性并通过仅计算具有前 $k$ 相似 token 的注意力来忽略噪声 token 。Refiner 探索高维空间中的注意力扩展，并应用卷积来增强注意力图的局部模式。<br>XCiT 跨特征通道而不是 token 执行自注意力计算，这允许高效处理高分辨率图像。<strong>自注意力机制的计算复杂度和注意力精度是未来优化的两个关键点。</strong></p>
<p>正如 CNN 领域所证明的那样，网络架构是一个重要因素。 ViT 的原始架构是相同形状 Transformer 块的简单堆叠。<br>视觉Transformer的新架构设计一直是一个有趣的话题。类似金字塔的架构被许多视觉Transformer模型使用，如 wang2021pyramid 等，包括PVT~, HVT, Swin Transformer 和 PiT。还有其他类型的架构，比如two-stream 和U-net，还研究了神经架构搜索 (NAS) 以搜索更好的Transformer架构。目前vision Transformer的网络设计和NAS主要借鉴了CNN的经验。<strong>未来，我们期待在视觉Transformer领域出现特定的、新颖的架构。</strong></p>
<p>除了上述方法外，还有一些其他方向可以进一步改进视觉Transformer，比如，位置编码，归一化策略，跳跃连接和去除注意力等。</p>
<h4 id="3-1-2-带卷积的Transformer"><a href="#3-1-2-带卷积的Transformer" class="headerlink" title="3.1.2 带卷积的Transformer"></a>3.1.2 带卷积的Transformer</h4><p>尽管视觉Transformer能够捕获输入中的远程依赖性，因此成功应用于各种视觉任务，但Transformer与现有 CNN 之间的性能差距仍然存在，一个主要原因可能是缺乏提取本地信息的能力。除了上面提到的增强局部性的 ViT 变体之外，将 Transformer 与卷积相结合可能是将局部性引入传统 Transformer 的更直接的方法。</p>
<p>有很多工作试图通过卷积来增强传统Transformer块或自注意力层，例如，CPVT 提出了一种条件位置编码 (CPE) 方案，该方案以输入 token 的局部邻域为条件，适用于任意输入大小，以利用卷积进行精细级特征编码。 CvT、CeiT、LocalViT 和 CMT 分析了直接从 NLP 借用 Transformer 架构并将卷积与 Transformer 结合在一起时的潜在缺点，具体来说，每个Transformer块中的前馈网络 (FFN) 与促进相邻 token 之间相关性的卷积层相结合。 LeViT 重新审视了大量有关 CNN 文献中的原理，并将其应用于 Transformer，提出了一种用于快速推理图像分类的混合神经网络。 BoTNet 在 ResNet 的最后三个瓶颈块中用全局自注意力替换了空间卷积，并在实例分割和对象检测任务上显着改进了基线，延迟开销最小。</p>
<p>此外，一些研究人员已经证明，基于 Transformer 的模型可能更难以享受良好的数据拟合能力，换句话说，它们对优化器、超参数的选择很敏感，和训练策略。 Visformer 揭示了具有两种不同训练参数下的 Transformer 和 CNN 之间的差距。第一个是 CNN 的标准设置，即训练时间更短，数据增强仅包含随机裁剪和水平翻转。另一个是 DeiT 中使用的训练设置，即训练时间更长，数据增强更强。xiao2021early 改变了 ViT 的早期视觉处理，将其嵌入stem替换为标准卷积stem，并发现这种变化使 ViT 收敛得更快，并且可以使用 AdamW 或 SGD 而不会显着降低精度。除了这两个作品，graham2021levit 和 guo2021cmt还选择在Transformer的顶部加入卷积stem。</p>
<h4 id="3-1-3-自监督表征学习"><a href="#3-1-3-自监督表征学习" class="headerlink" title="3.1.3 自监督表征学习"></a>3.1.3 自监督表征学习</h4><p><strong>基于生成的方法</strong><br>图像的生成式预训练方法已经存在很长时间了，igpt 重新审视了这一类方法，并将其与自监督方法相结合。之后，几项工作 li2021mst 等被提出来扩展视觉 Transformer 的基于生成的自监督学习。</p>
<p>我们简单介绍一下 iGPT 的机制，这种方法包括预训练阶段和微调阶段。在预训练阶段，探索了自回归和 BERT 目标。为了实现像素预测，采用了序列 Transformer 架构而不是语言 token（如 NLP 中使用的）。当与提前停止结合使用时，预训练可以被认为是一种有利的初始化或正则化。在微调阶段，他们向模型添加了一个小的分类头。这有助于优化分类目标并调整所有权重。</p>
<p>通过 $k$ 均值聚类将图像像素转换为序列数据。给定一个由高维数据 $\mathbf x=(x_1,\cdots,x_n)$ 组成的未标记数据集 ${X}$，他们通过最小化负对数似然来训练模型的数据：</p>
<script type="math/tex; mode=display">\mathrm{L}_{AR}=\mathop{\Bbb{E}}_{\bf x\sim\bf X}[-\log p(\bf x)] \tag{7}</script><p>其中 $p(\mathbf x)$ 是图像数据的概率密度，可以建模为：</p>
<script type="math/tex; mode=display">p(\bf x)=\prod^n_{i=1}p(x_{\pi_i}|x_{\pi_1},\cdots,x_{\pi_{i-1}},\theta) \tag{8}</script><p>这里对 $1\leqslant i \leqslant n$ 采用恒等排列$\pi_i=i$，也称为光栅顺序。 Chen 还考虑了 BERT 目标，它对子序列 $M\subset[1,n]$ 进行采样，使得每个索引 $i$ 独立地具有 0.15 的概率出现在 $M$ 中。 $M$ 称为 BERT mask，模型通过最小化以“unmasked”元素 $x_{[1,n]\backslash M}$ 为条件的“masked”元素 $x_M$ 的负对数似然来训练：</p>
<script type="math/tex; mode=display">L_{BERT}=\mathop{\Bbb E}_{\bf x\sim \bf X}\mathop{\Bbb E}_{M}\sum\limits_{i\in M}[-\log p(x_i|x_{[1,n]\backslash M})] \tag{9}</script><p>在预训练阶段，他们选择 $L_{AR}$ 或 $L_{BERT}$，并最小化预训练数据集的损失。</p>
<p>使用了 GPT-2 公式的 Transformer 解码器块，为了确保在训练 AR 目标时进行适当的调节，Chen 等人将标准上三角mask应用于注意对数的 $n\times n$ 矩阵。 使用 BERT 目标时不需要注意逻辑mask，Chen 在将内容嵌入应用于输入序列后将位置归零。 在 Transformer 最后的层之后，他们应用层范数并从输出中学习投影以对每个序列元素的条件分布进行参数化。 在训练 BERT 时，他们只是忽略 unmasked 位置的逻辑。</p>
<p>在微调阶段，他们对整个序列维度的最后归一化层的输出进行平均池化，以提取每个示例的 $d$ 维特征向量。他们学习从c池化特征到类 logits 的投影，并使用该投影来最小化交叉熵损失。实际应用提供了经验证据，表明交叉熵损失和预训练损失（$L_{AR}$ 或 $L_{BERT}$）的联合目标效果更好。</p>
<p>iGPT 和 ViT 是将 Transformer 应用于视觉任务的两项开创性工作。 iGPT 和 ViT-like 模型的区别主要在于 3 个方面：1）iGPT 的输入是通过聚类像素的一系列调色板，而 ViT 将图像统一划分为多个局部块； 2）iGPT的架构是encoder-decode r框架，而ViT只有 Transformer encoder； 3) iGPT 利用自回归自监督损失进行训练，而 ViT 通过监督图像分类任务进行训练。</p>
<p><strong>基于对比学习的方法</strong><br>目前，对比学习是计算机视觉最流行的自我监督学习方式，对比学习已应用于无监督预训练的视觉Transformer。</p>
<p>mocov3 研究了几个基本组件对训练自监督 ViT 的影响，作者观察到，不稳定性是降低准确性的一个主要问题，这些结果确实是部分失败，当训练变得更稳定时，它们可以得到改善。</p>
<p>他们引入了“MoCo v3”框架，这是对 MoCo 的增量改进。具体来说，作者在随机数据增强下对每张图像进行两次裁剪，它们由两个编码器 $f_q$ 和 $f_k$ 编码，输出向量 $\mathbf q$ 和 $\mathbf k$。直觉上，$\mathbf q$ 的行为类似于“查询”，学习的目标是检索相应的“键”。这被表述为最小化对比损失函数，可以写成：</p>
<script type="math/tex; mode=display">\cal L_q=-\log\frac{\exp(\bf q \cdot\bf k^\top/\tau)}{\exp(\bf q\cdot\bf k^\top/\tau)+\sum_{\bf k^-}\exp(\bf q\cdot\bf k^\top/\tau)} \tag{10}</script><p>这里 $\mathbf k^+$ 是 $f_k$ 在与 $\mathbf q$ 相同图像上的输出，称为 $\mathbf q$ 的正样本，集合 {$\mathbf k^-$} 由 $f_k$ 来自其他图像的输出组成，称为 $\mathbf q$ 的负样本，$\tau$ 是 $l_2$ 归一化 $\mathbf q$, $\mathbf k$ 的温度超参数。 MoCo v3 使用在同一批次中自然共存的键并放弃内存队列，他们发现如果批次足够大（例如 4096），内存队列的收益会递减。通过这种简化，对比损失可以以一种简单的方式实现。编码器 $f_q$ 由主干（例如ViT）、投影头和额外的预测头组成，而编码器 $f_k$ 有主干和投影头，但没有预测头。 $f_k$ 由 $f_q$ 的移动平均值更新，不包括预测头。</p>
<p>MoCo v3 表明不稳定性是训练自监督 ViT 的主要问题，因此他们描述了一个简单的技巧，可以提高各种实验情况下的稳定性。他们观察到没有必要训练块投影层，对于标准的 ViT 块大小，块投影矩阵是完备的或超完备的。在这种情况下，随机投影应该足以保留原始块的信息。但是，该技巧缓解了问题，但没有解决问题。如果学习率太大，模型仍然可能不稳定，并且第一层不太可能是不稳定的根本原因。</p>
<h4 id="3-1-4-讨论"><a href="#3-1-4-讨论" class="headerlink" title="3.1.4 讨论"></a>3.1.4 讨论</h4><p>视觉Transformer的所有组件，包括多头自注意力、多层感知器、跳跃连接、层归一化、位置编码和网络拓扑，在视觉识别中起着关键作用。 如上所述，已经提出了许多工作来提高视觉Transformer的有效性和效率。 从图6的结果可以看出，结合CNN和Transformer可以获得更好的性能，表明它们通过局部连接和全局连接相互补充。 对骨干网络的进一步调研可以带来整个视觉社区的改进，至于vision Transformer的自监督表示学习，我们还需要努力追求NLP领域大规模预训练的成功。</p>
<h3 id="3-2-高-中层的视觉"><a href="#3-2-高-中层的视觉" class="headerlink" title="3.2 高/中层的视觉"></a>3.2 高/中层的视觉</h3><p>最近越来越有兴趣使用Transformer来做高/中级的计算机视觉任务，比如物体检测，车道检测，分割和姿态估计，我们将在本节中回顾这些方法。</p>
<h4 id="3-2-1-生成目标检测"><a href="#3-2-1-生成目标检测" class="headerlink" title="3.2.1 生成目标检测"></a>3.2.1 生成目标检测</h4><p>传统的目标检测器主要建立在 CNN 之上，但基于 Transformer 的目标检测由于其优势能力最近获得了极大的关注。</p>
<p>一些目标检测方法尝试使用Transformer的self-attention机制，然后增强现代检测器的特定模块，例如特征融合模块和预测头。 我们在补充材料中对此进行了讨论。基于Transformer的目标检测方法大致分为两类：基于Transformer的批量预测方法和基于Transformer的backbone方法，如图7中所示。 与基于 CNN 的检测器相比，基于 Transformer 的方法在准确性和运行速度方面都表现出了强大的性能。 表3显示了前面提到的各种基于 Transformer 的对象检测器在 COCO 2012 val 集上的检测结果。</p>
<p><strong>基于Transformer的批量预测方法</strong><br>作为基于 Transformer 的检测方法的先驱，Carion 提出的检测 Transformer（DETR）重新设计了目标检测的框架。 DETR 是一种简单且完全端到端的目标检测器，将目标检测任务视为一个直观的批量预测问题，消除了锚点生成和非最大抑制 (NMS) 后处理等传统人工设计组件。如图8所示，DETR 从 CNN 主干开始，从输入图像中提取特征。为了用位置信息补充图像特征，在将特征送到 Transformer 的编码器-解码器之前，将固定位置编码添加到扁平化特征中。解码器使用来自编码器的嵌入以及 $N$ 个学习的位置编码（对象查询），并产生 $N$ 个输出嵌入。这里 $N$ 是预定义参数，通常大于图像中的对象数。简单的前馈网络 (FFN) 用于计算最终预测，其中包括边界框坐标和类标签以指示特定对象类（或指示不存在对象）。与按顺序计算预测的原始Transformer不同，DETR 并行解码 $N$ 个对象。 DETR 采用二分匹配算法来分配预测对象和真实对象。如等式\ref{eq11} 所示，利用 Hungarian 损失来计算所有匹配对象对的损失函数。</p>
<script type="math/tex; mode=display">\cal L_{Hungarian}(y,\hat y)=\sum\limits^N_{i=1}[-\log\hat p_{\sigma(i)}(c_i)+\mathbb 1_{c_i\neq\emptyset}\cal L_{box}(b_i,\hat b_{\hat\sigma}(i)) ] \tag{11}</script><p>其中 $\hat{\sigma}$ 是最优分配，$c_i$ 和 $\hat{p}_{\hat{\sigma}(i)}(c_{i})$ 是目标类标签和预测标签，$b_i$ 和 $\hat{b}_{\hat{\sigma}}(i)$ 分别是真实框和预测框，$y=\{(c_i, b_i)\}$ 和 $\hat y$ 分别是对象的真值和预测值。DETR 在对象检测方面表现出令人印象深刻的性能，在 COCO 基准测试中提供与流行且成熟的 Faster R-CNN 基线相当的准确性和速度。</p>
<p>DETR 是基于 Transformer 的目标检测框架的全新设计，使社区能够开发完全端到端的检测器，然而，vanilla DETR 带来了一些挑战，特别是更长的训练时间和小物体检测性能差。为了应对这些挑战，ddetr 提出了 Deformable DETR，它已成为一种显着提高检测性能的流行方法。可变形注意模块关注参考点周围的一小组关键位置，而不是像 Transformer 中的原始多头注意机制那样查看图像特征图上的所有空间位置。这种方法显着降低了计算复杂度，并在快速收敛方面带来了好处，更重要的是，可变形注意力模块可以很容易地应用于融合多尺度特征。可变形 DETR 的性能优于 DETR，训练成本降低 10 倍，推理速度提高 1.6 倍，并且通过使用迭代边框细化方法和两阶段方案，Deformable DETR 可以进一步提高检测性能。</p>
<p>也有几种方法来处理原始 DETR 收敛慢的问题。例如， sun2020rethinking 调查了为什么DETR模型收敛慢，发现这主要是 Transformer decoder 中的 cross-attention 模块造成的。为了解决这个问题，提出了 DETR 的仅编码器版本，在检测精度和训练收敛性方面取得了相当大的改进。此外，设计了一种新的二分匹配方案以提高训练稳定性和加快收敛速度​​，并提出了两种基于 Transformer 的批量预测模型，即 TSP-FCOS 和 TSP-RCNN，以改进具有特征金字塔的编码器版本 DETR。与原始 DETR 模型相比，这些新模型实现了更好的性能，gao2021fast 提出了空间调制共同注意 (SMCA) 机制，通过将共同注意响应限制在最初估计的边界框附近的位置来加速收敛。通过将所提出的 SMCA 模块集成到 DETR 中，在可比的推理成本下，可以用大约 10$\times$ 更少的训练周期获得类似的 mAP。</p>
<p>鉴于与 DETR 相关的高计算复杂性，zheng2020end 提出了一种自适应聚类Transformer (ACT) 来降低预训练 DETR 的计算成本。 ACT 使用局部敏感性哈希 (LSH) 方法自适应地对查询特征进行聚类，并将注意力输出广播到由所选原型表示的查询。 ACT 用于替换预训练 DETR 模型的自注意力模块，无需任何重新训练。这种方法显着降低了计算成本，同时精度略有下降，通过使用多任务知识蒸馏 (MTKD) 方法可以进一步减少性能下降，该方法利用原始 Transformer 通过几个 epoch 的微调来提取 ACT 模块。yao2021efficient 指出 DETR 中的随机初始化是导致解码器层数多、收敛速度慢的主要原因。为此，他们提出了 Efficient DETR，通过一个额外的区域提议网络将密集先验合并到检测管道中。更好的初始化使他们能够仅使用一个解码器层而不是六层，以通过更紧凑的网络实现具有竞争力的性能。</p>
<p><strong>基于Transformer的检测backbone</strong><br>与 DETR 通过 Transformer 将目标检测重新设计为一组预测任务不同，beal2020toward 提出利用 Transformer 作为常见检测框架的主干，例如 Faster R-CNN。 输入图像被分成几个块并送入视觉 Transformer，其输出嵌入特征根据空间信息重新组织，然后通过检测头获得最终结果。 一个巨大的预训练 Transformer 主干可以为 ViT-FRCNN 带来好处。 还有很多方法可以探索通用的视觉 Transformer 主干设计并将这些主干转移到传统的检测框架，如 RetinaNet 和 Cascade R-CNN。 例如，Swin Transformer 在 ResNet-50 主干上获得了大约 4 box AP 增益，各种检测框架具有相似的 FLOP。</p>
<p><strong>基于Transformer的目标检测预训练</strong><br>受 NLP 中预训练 Transformer 方案的启发，人们提出了几种方法来探索基于 Transformer 的目标检测模型的不同预训练方案。 dai2020detr 提出了目标检测的无监督预训练（UP-DETR），具体来说，提出了一种名为随机查询块检测的新型无监督任务来预训练 DETR 模型。通过这种无监督的预训练方案，UP-DETR 在相对较小的数据集（PASCAL VOC）上显着提高了检测精度，在具有足够训练数据的 COCO 基准上，UP-DETR 仍然优于 DETR，证明了无监督预训练方案的有效性。</p>
<p>fang2021you 探索了如何将在 ImageNet 上预训练的纯 ViT 结构迁移到更具挑战性的目标检测任务，并提出了 YOLOS 检测器。为了应对目标检测任务，所提出的 YOLOS 首先删除 ViT 中的分类 token 并附加可学习的检测 token。此外，二分匹配损失用于对对象进行批量预测。通过这种在 ImageNet 数据集上的简单预训练方案，所提出的 YOLOS 在 COCO 基准测试中显示出具有竞争力的目标检测性能。</p>
<h4 id="3-2-2-分割"><a href="#3-2-2-分割" class="headerlink" title="3.2.2 分割"></a>3.2.2 分割</h4><p>分割是计算机视觉界的一个重要课题，广义上包括全景分割、实例分割和语义分割等。 Vision Transformer 在分割领域也展现出了令人瞩目的潜力。</p>
<p><strong>全景分割 Transformer</strong><br>DETR 可以自然地扩展到全景分割任务，并通过在解码器上附加一个mask头来获得有竞争力的结果。 wang2020max 提出 Max-DeepLab，直接用 mask Transformer 预测全景分割结果，不涉及框检测等子任务。 与 DETR 类似，Max-DeepLab 以端到端的方式简化了全景分割任务，并直接预测一组不重叠的蒙版和相应的标签。 模型训练是使用全景质量 (PQ) 样式损失执行的，但与之前将Transformer堆叠在 CNN 主干之上的方法不同，Max-DeepLab 采用双路径框架，将 CNN 和Transformer结合起来。</p>
<p><strong>实例分割 Transformer</strong><br>VisTR 是一种基于 Transformer 的视频实例分割模型，由 wang2020end 提出，用于根据一系列输入图像生成实例预测结果，提出了一种匹配实例序列的策略，将预测结果对齐到真值。为了获得每个实例的mask序列，VisTR 利用实例序列分割模块从多个帧中累积mask特征，并使用 3D CNN 分割mask序列。hu2021istr 提出了一个实例分割 Transformer (ISTR) 来预测低维mask嵌入，并将它们与 ground truth 相匹配以获得批量损失。 不同于现有自上而下和自下而上的框架逻辑，ISTR 使用循环细化策略进行检测和分割。yang2021association 研究了如何实现更好、更高效的嵌入学习，以解决具有挑战性的多对象场景下的半监督视频对象分割问题。wu2021fully 等论文也讨论了使用Transformer处理分割任务。</p>
<p><strong>语义分割 Transformer</strong><br>zheng2021 提出了一种基于Transformer的语义分割网络（SETR）, SETR 使用类似于 ViT 的编码器作为编码器从输入图像中提取特征，采用多级特征聚合模块来执行逐像素分割。strudel2021segmenter 引入了 Segmenter，它依赖于图像块对应的输出嵌入，并通过逐点线性解码器或带掩码的 Transformer 解码器获得类标签。xie2021segformer 提出了一个简单、高效但功能强大的语义分割框架，它将 Transformer 与轻量级多层感知 (MLP) 解码器统一起来，输出多尺度特征并避免复杂的解码器。</p>
<p><strong>用于医学图像分割的 Transformer</strong><br>cao2021swin 提出了一种用于医学图像分割的类 Unet 纯 Transformer 网络，通过将标记化的图像块送到基于 Transformer 的 U 形编码-解码器架构中，该架构具有局部-全局语义特征的跳跃连接学习。 valanarasu2021medical 探索了基于 Transformer 的解决方案，研究了使用基于 Transformer 的网络架构进行医学图像分割任务的可行性，并提出了一种门坐标轴注意力模型，该模型通过在自注意力模块中引入额外的控制机制来扩展现有架构。Cell-DETR，基于DETR全景分割模型，尝试使用Transformer进行细胞实例分割，它添加了跳跃连接，在分割头中的主干 CNN 和 CNN 解码器之间桥接特征，以增强特征融合，Cell-DETR 实现了从显微镜图像进行细胞实例分割的最先进性能。</p>
<h4 id="3-2-3-姿态估计"><a href="#3-2-3-姿态估计" class="headerlink" title="3.2.3 姿态估计"></a>3.2.3 姿态估计</h4><p>人体姿势和手部姿势估计是引起研究界极大兴趣的基础主题，关节姿态估计类似于结构化预测任务，旨在从输入的 RGB/D 图像中预测关节坐标或网格顶点。 在这里，我们讨论一些方法如huang2020hand等，探索如何利用 Transformer 对人体姿势和手部姿势的全局结构信息进行建模。</p>
<p><strong>用于手势估计的 Transformer</strong><br>huang2020hand 提出了一种基于Transformer的网络，用于从点集进行 3D 手姿势估计。编码器首先利用 PointNet 从输入点云中提取逐点特征，然后采用标准的多头自注意力模块来生成嵌入。为了向解码器公开更多与全局姿势相关的信息，使用 PointNet++ 等特征提取器来提取手部关节特征，然后将其作为位置编码输入解码器。同样，huang2020hot 提出了 HOT-Net（手对象变换网络的简称）用于 3D 手对象姿态估计。与前面使用 Transformer 直接从输入点云预测 3D 手部姿势的方法不同，HOT-Net 使用 ResNet 生成初始 2D 手部对象姿势，然后将其输入 Transformer 以预测 3D 手部对象姿势，因此，谱图卷积网络用于为编码器提取输入嵌入。hampali2021handsformer 提出在给定单色图像的情况下估计两只手的 3D 姿势。具体来说，双手关节的一组潜在 2D 位置的外观和空间编码被输入到一个Transformer中，注意力机制被用来挑选出关节的正确配置并输出双手的 3D 姿势。</p>
<p><strong>用于人体姿势估计的 Transformer</strong><br>lin2020end 提出了一种网格Transformer (METRO)，用于从单个 RGB 图像预测 3D 人体姿势和网格。 METRO 通过 CNN 提取图像特征，然后通过将模板人体网格连接到图像特征来执行位置编码，提出了一种具有渐进降维功能的多层Transformer编码器，以逐渐降低嵌入维数，最终生成人体关节和网格顶点的 3D 坐标。为了鼓励学习人体关节之间的非局部关系，METRO 在训练期间随机屏蔽了一些输入查询。transpose 等基于 Transformer 架构和低级卷积块构建了一个名为 TransPose 的可解释模型，Transformer 中内置的注意力层可以捕获关键点之间的远程空间关系，并解释预测的关键点位置高度依赖的依赖关系。li2021tokenpose 提出了一种基于 Token 表示的人体姿态估计新方法（TokenPose）。每个关键点都被明确地嵌入为一个标记，以同时从图像中学习约束关系和外观线索。mao2021tfpose 提出了一种人体姿势估计框架，以基于回归的方式解决了该任务，他们将姿态估计任务转化为一个序列预测问题，并通过 Transformer 求解，从而绕过了基于热图的姿态估计器的缺点。jiang2021skeletor 提出了一种新颖的基于 Transformer 的网络，它可以以无监督的方式学习姿势和运动的分布，而不是跟踪身体部位并尝试暂时平滑它们。该方法克服了检测中的不准确性并纠正了部分或整个骨架损坏。mazzia2021action 引入了 Action Transformer (AcT) 来利用小时间窗口上的 2D 姿势表示，并提供低延迟解决方案以实现准确有效的实时性能。hao2021test 提议在给定一组人的测试图像的情况下个性化人体姿势估计器，而不使用任何手动注释。该方法在测试期间调整姿势估计器以利用特定于人的信息，并使用 Transformer 模型在自监督关键点和监督关键点之间建立转换。</p>
<h4 id="3-2-4-其他任务"><a href="#3-2-4-其他任务" class="headerlink" title="3.2.4 其他任务"></a>3.2.4 其他任务</h4><p>还有很多不同的高级/中级视觉任务探索了视觉Transformer的使用以获得更好的性能，我们简要回顾以下几项任务。</p>
<p><strong>行人检测</strong><br>由于在遮挡和人群场景中物体分布非常密集，当普通检测网络应用于行人检测任务时，通常需要额外的分析和适应。当直接将 DETR 或 Deformable DETR 应用于行人检测任务时，lin2020detr 揭示了解码器中稀疏统一查询和弱注意力场导致性能下降。为了减轻这些缺点，作者提出了行人端到端检测器（PED），它采用了一种称为密集查询和校正注意域 (DQRF) 的新解码器来支持密集查询并减轻查询的嘈杂或狭窄注意域。他们还提出了 V-Match，它通过充分利用可见注释实现了额外的性能改进。</p>
<p><strong>车道线检测</strong><br>基于PolyLaneNet，liu2020end 提出了一种叫做LSTR的方法，它通过使用 Transformer 网络学习全局上下文来提高曲线车道检测的性能。与 PolyLaneNet 类似，LSTR 将车道检测视为用多项式拟合车道的任务，并使用神经网络来预测多项式的参数，为了捕捉车道和全局上下文的细长结构，LSTR 在架构中引入了一个Transformer网络，这使得能够处理由 CNN 提取的低级特征，此外，LSTR 使用 Hungarian 损失来优化网络参数。正如论文中所展示的，LSTR 优于 PolyLaneNet，使用少 5 倍的参数，精度高 2.82%，FPS 高 3.65 倍。Transformer 网络、CNN 和 Hungarian Loss 的结合最终形成了一个精确、快速和微小的车道检测框架。考虑到整个球道线一般呈拉长形且射程远，liu2021condlanenenet 利用Transformer编码器结构来更有效地提取上下文特征，这种 Transformer 编码器结构大大提高了建议点的检测，它依赖于上下文特征和全局信息，尤其是在骨干网络是一个小模型的情况下。</p>
<p><strong>场景图</strong><br>场景图是一种场景的结构化表示，可以清楚地表达场景中的对象、属性以及对象之间的关系。 为了生成场景图，大多数现有方法首先提取基于图像的对象表示，然后在它们之间进行消息传播。 Graph R-CNN 利用自注意力来整合来自图中相邻节点的上下文信息。 最近，Sharifzadeh2020 在提取的对象嵌入上使用了Transformer，Sharifzadeh2021 提出了一个名为 Texema 的新管道，并采用预训练的 Text-to-Text 传输Transformer （T5）从文本输入中构建场景图，利用它们来改进关系推理模块，T5 模型使 Texema 能够利用文本中的知识。</p>
<p><strong>追踪</strong><br>一些研究人员还探索在基于模板的判别跟踪器中使用 Transformer 编码器-解码器架构，例如 TMT、TrTr 和 TransT。所有这些工作都使用类似 Siamese 的跟踪管道来进行视频对象跟踪，并利用编码器-解码器网络为全局和丰富的上下文相互依赖性替换显式互相关操作。具体来说，Transformer编码器和解码器分别分配给模板分支和搜索分支，此外，Sun 提出了 TransTrack，这是一个在线联合检测和跟踪管道。 它利用查询键机制来跟踪预先存在的对象，并将一组学习对象查询引入管道以检测新出现的对象。</p>
<p><strong>重识别</strong><br>He 提出 TransReID 以研究纯 Transformer 在对象重识别 (ReID) 领域的应用。在将 Transformer 网络引入对象 ReID 时，TransReID 重叠的切片来保留块周围的局部相邻结构，并引入 2D 双线性插值以帮助处理任何给定的输入分辨率。通过 Transformer 模块和损失函数，提出了一个强大的基线来实现与基于 CNN 的框架相当的性能。此外，拼图块模块 (JPM) 旨在促进对象的扰动不变和鲁棒特征表示，并引入辅助信息嵌入 (SIE) 来编码辅助信息。最终框架 TransReID 在人员和车辆 ReID 基准测试中均实现了最先进的性能。Liu2021reid3view 和 Zhang2021stt 都提供了将 Transformer 网络引入基于视频的行人 Re-ID 的解决方案。同样，他们都使用分离的Transformer网络来细化空间和时间特征，然后使用交叉视图Transformer来聚合多视图特征。</p>
<p><strong>点云学习</strong><br>最近还出现了许多探索用于点云学习的Transformer架构的其他作品，如 engel2020point 等。 例如，guo2020pct 提出了一个新颖的框架，用更合适的 offset-attention 模块替换原来的 self-attention 模块，其中包括隐式 Laplace 算子和归一化细化。 此外，zhao2020point 设计了一种名为 Point Transformer 的新型 Transformer 架构。 所提出的自注意力层对点集的排列具有不变性，使其适用于点集处理任务，Point Transformer 对来自 3D 点云的语义分割任务表现出强大的性能。</p>
<h4 id="3-2-5-讨论"><a href="#3-2-5-讨论" class="headerlink" title="3.2.5 讨论"></a>3.2.5 讨论</h4><p>正如前面部分所讨论的，Transformer 在几个高级任务上表现出了强大的性能，包括检测、分割和姿态估计。<br>在将 Transformer 用于高级任务之前需要解决的关键问题涉及<strong>输入嵌入</strong>、<strong>位置编码</strong>和<strong>预测损失</strong>。一些方法提出从不同角度改进 self-attention 模块，例如deformable attention、adaptive clustering 和point Transformer。尽管如此，将 Transformer 用于高级视觉任务的探索仍处于初级阶段，因此进一步的研究可能会证明是有益的。例如，是否有必要在 Transformer 之前使用 CNN 和 PointNet 等特征提取模块以获得更好的性能？ vision Transformer如何像BERT、GPT-3那样在NLP领域充分利用大规模预训练数据集？是否可以预训练单个 Transformer 模型并针对不同的下游任务对其进行微调，而只需几个 epoch 的微调？如何通过结合特定任务的先验知识来设计更强大的架构？之前的几项工作已经对上述主题进行了初步讨论，我们希望进行更多的进一步研究，以探索更强大的高级视觉Transformer。</p>
<h3 id="3-3-低层视觉"><a href="#3-3-低层视觉" class="headerlink" title="3.3 低层视觉"></a>3.3 低层视觉</h3><p>很少有工作将Transformer应用于低级视觉领域，例如图像超分辨率和生成。 这些任务通常将图像作为输出（例如高分辨率或去噪图像），这比分类、分割和检测等输出为标签或框的高级视觉任务更具挑战性。</p>
<h4 id="3-3-1-图像生成"><a href="#3-3-1-图像生成" class="headerlink" title="3.3.1 图像生成"></a>3.3.1 图像生成</h4><p>将 Transformer 模型应用于图像生成任务的一种简单而有效的方法是直接将架构从 CNN 更改为 Transformer，如图9 (a) 所示。jiang2021transgan 提出了TransGAN，它使用Transformer架构构建GAN。由于难以逐像素生成高分辨率图像，因此通过在不同阶段逐渐增加特征图分辨率来利用内存友好型生成器。相应地，设计了一个多尺度鉴别器来处理不同阶段不同大小的输入。引入了各种训练方法，包括网格自注意力、数据增强、相对位置编码和改进的归一化，以稳定训练并提高其性能。在各种基准数据集上的实验证明了基于 Transformer 的 GAN 模型在图像生成任务中的有效性和潜力。 lee2021vitgan 提出了 ViTGAN，它在生成器和鉴别器中引入了几种技术来稳定训练过程和收敛，为自我注意模块引入欧几里德距离，以加强 Transformer 鉴别器的 Lipschitzness，提出了自调制层范数和隐式神经表示来增强生成器的训练。因此，ViTGAN 是第一个证明基于 Transformer 的 GAN 可以达到与最先进的基于 CNN 的 GAN 相当的性能的作品。</p>
<p>parmar2018image 提出了 Image Transformer，迈出了泛化 Transformer 模型的第一步，以自动回归的方式制定图像翻译和生成任务。 Image Transformer 由两部分组成：用于提取图像表示的编码器和用于生成像素的解码器。对于值为 $0-255$ 的每个像素，学习 $256 \times d$ 维嵌入，用于将每个值编码为 $d$ 维向量，该向量作为输入馈入编码器，编码器和解码器采用与 vaswani2017attention 中相同的架构。每个输出像素 $q’$ 是通过计算输入像素 $q$ 和先前生成的像素 $m_1,m_2,…$ 之间的自注意力生成的，位置嵌入为 $p_1,p_2,…$。对于图像条件生成，例如超分辨率和修复，使用编码器-解码器架构，其中编码器的输入是低分辨率或损坏的图像。对于无条件和类条件生成，即图像噪声，只有解码器用于输入噪声向量，由于解码器的输入是先前生成的像素（在生成高分辨率图像时涉及高计算成本），因此提出了局部自注意方案。该方案仅使用最接近的生成像素作为解码器的输入，使 Image Transformer 能够在图像生成和翻译任务上实现与基于 CNN 的模型相当的性能，证明了基于 Transformer 的模型在低级视觉任务上的有效性。</p>
<p>由于Transformer模型难以直接生成高分辨率图像，esser2021taming 提出了Taming Transformer，由两部分组成：VQGAN 和 Transformer。 VQGAN 是 oord2017neural 的变体，它使用鉴别器和感知损失来提高视觉质量。通过 VQGAN，图像可以用一系列上下文丰富的离散向量表示，因此这些向量可以很容易地通过自回归方式由 Transformer 模型预测。 Transformer 模型可以学习远程交互以生成高分辨率图像。因此，所提出的 Taming Transformer 在各种图像合成任务上取得了最先进的结果。</p>
<p>除了图像生成，dalle 还提出了用于文本到图像生成的Transformer 模型，它根据给定的说明合成图像。整个框架包括两个阶段。在第一阶段，使用离散 VAE 来学习视觉码本，在第二阶段，文本通过 BPE-encode 解码，相应的图像通过第一阶段学习的 dVAE 解码。然后使用自回归Transformer来学习编码文本和图像之间的先验，在推理过程中，图像的标记由Transformer预测并由学习的解码器解码。引入 CLIP 模型对生成的样本进行排序。文本到图像生成任务的实验证明了所提出模型的强大能力。请注意，我们的调查主要集中在纯视觉任务上，我们不包括图9中的 DALL$\cdot$E 框架。</p>
<h4 id="3-3-2-图像处理"><a href="#3-3-2-图像处理" class="headerlink" title="3.3.2 图像处理"></a>3.3.2 图像处理</h4><p>最近的一些工作避免使用每个像素作为 Transformer 模型的输入，而是使用块（像素集）作为输入。 例如，yang2020learning 提出了 TTSR，在基于参考的图像超分辨率问题中使用Transformer架构，它旨在将相关纹理从参考图像转移到低分辨率图像，以一张低分辨率图像和参考图像分别作为query $\mathbf Q$和key $\mathbf K$，用下式计算相对量：</p>
<script type="math/tex; mode=display">r_{i,j}=\bigg\langle \frac{\mathbf q_i}{\|\mathbf q_i\|},\frac{\mathbf k_i}{\|\mathbf k_i\|} \bigg\rangle \tag{12}</script><p>提出了一种硬注意模块，根据参考图像选择高分辨率特征$\mathbf V$，从而可以利用相关性来匹配低分辨率图像。 硬注意模块计算如下：</p>
<script type="math/tex; mode=display">h_i=\arg\max_j r_{i,j} \tag{13}</script><p>最相关的参考块是 $\mathbf t_i = \mathbf v_{h_i}$，其中 $\mathbf T$ 中的 $\mathbf t_i$ 是转移的特征， 然后使用软注意模块将 $\mathbf V$ 转移到低分辨率特征，来自高分辨率纹理图像的传输特征和低分辨率特征用于生成低分辨率图像的输出特征，通过利用基于Transformer的架构，TTSR 可以在超分辨率任务中成功地将纹理信息从高分辨率参考图像传输到低分辨率图像。</p>
<p>不同于以往在单一任务上使用Transformer模型的方法，chen2020pre 提出了Image Processing Transformer (IPT)，它通过使用大型预训练数据集充分利用了Transformer的优势，在多个图像处理任务中实现了最先进的性能，包括超分辨率、去噪和去雨。如图10所示，IPT由多个头、一个编码器、一个解码器和多个尾组成。针对不同的图像处理任务引入了多头、多尾结构和任务嵌入，这些特征被分成块，这些块被馈送到编码器-解码器架构中，在此之后，输出被重塑为具有相同大小的特征。鉴于在大数据集上预训练Transformer模型的优势，IPT使用ImageNet数据集进行预训练。具体来说，来自该数据集的图像通过手动添加噪声、雨纹或下采样来降级，以生成损坏的图像，退化图像用作 IPT 的输入，而原始图像用作输出的优化目标，还引入了一种自我监督的方法来增强 IPT 模型的泛化能力。训练模型后，通过使用相应的头、尾和任务嵌入对每个任务进行微调。 IPT 很大程度上实现了图像处理任务的性能提升（例如在图像去噪任务中提升了 2 dB），展示了将基于 Transformer 的模型应用于低级视觉领域的巨大潜力。</p>
<p>除了单一图像生成，wang2020sceneformer 还提出了 SceneFormer 在 3D 室内场景生成中利用 Transformer。通过将场景视为一系列对象，Transformer 解码器可用于预测一系列对象及其位置、类别和大小。这使得 SceneFormer 在用户研究中优于传统的基于 CNN 的方法。</p>
<p>应该注意的是，iGPT 是在类似修复的任务上预训练的，由于 iGPT 主要关注图像分类任务的微调性能，因此我们将这项工作更像是对使用 Transformer 的图像分类任务的尝试，而不是低级视觉任务。</p>
<p>总之，与分类和检测任务不同，图像生成和处理的输出是图像，图11 说明了在低级视觉中使用 Transformer 。在图像处理任务中，图像首先被编码成一系列标记或块，然后 Transformer 编码器使用该序列作为输入，从而使 Transformer 解码器成功生成所需的图像。在图像生成任务中，基于 GAN 的模型直接学习解码器生成块以通过线性投影输出图像，而基于Transformer的模型训练自动编码器学习图像的码本并使用自回归Transformer模型来预测编码的 token 。未来研究的一个有意义的方向是为不同的图像处理任务设计合适的架构。</p>
<h3 id="3-4-视频处理"><a href="#3-4-视频处理" class="headerlink" title="3.4 视频处理"></a>3.4 视频处理</h3><p>Transformer 在基于序列的任务上表现出色，尤其是在 NLP 任务上。 在计算机视觉~（特别是视频任务）中，时空维度信息受到青睐，催生了Transformer在很多视频任务中的应用，比如帧合成 liu2020convTransformer，动作识别 girdhar2019video，以及视频检索 liu2017two。</p>
<h4 id="3-4-1-高层视频处理"><a href="#3-4-1-高层视频处理" class="headerlink" title="3.4.1 高层视频处理"></a>3.4.1 高层视频处理</h4><p><strong>视频动作识别</strong><br>顾名思义，视频人类动作任务涉及识别和定位视频中的人类动作，上下文（例如其他人和物体）在识别人类行为方面起着至关重要的作用。 Rohit 提出了动作Transformer来模拟感兴趣的人与周围环境之间的潜在关系，具体来说，I3D 被用作提取高级特征图的主干，从中间特征图中提取（使用 RoI 池化）的特征被视为查询（Q），而键（K）和值（V）是从中间特征计算的，自我注意机制应用于三个组件，并输出分类和回归预测。 lohit2019temporal 提出了一个可解释的可微分模块，称为时间变换网络，以减少类内方差并增加类间方差。此外，Fayyaz 和 Gall 提出了一种时间Transformer，在弱监督设置下执行动作识别任务。除了人类动作识别，Transformer 还被用于群体活动识别。 Gavrilyuk 提出了一种 actor-Transformer 架构来进行学习和表示，使用 2D 和 3D 网络生成的静态和动态表示作为输入，Transformer的输出是预测的活动。</p>
<p><strong>视频检索</strong><br>基于内容的视频检索的关键是找到视频之间的相似性，shaotemporal 仅利用视频级别的特征图像来克服相关挑战，建议使用Transformer对远程语义依赖性进行建模，他们还引入了有监督的对比学习策略来执行硬负样本挖掘，在基准数据集上使用这种方法的结果证明了它的性能和速度优势。此外，gabeur2020multi 提出了一个多模态Transformer来学习不同的跨模态线索以表示视频。</p>
<p><strong>视频对象检测</strong><br>要检测视频中的对象，需要全局和局部信息，Chen 引入了内存增强的全局-局部聚合~(MEGA) 以捕获更多内容，代表性特征增强了整体性能并解决了无效和不足的问题。此外，yin2020lidar 提出了一种时空Transformer来聚合空间和时间信息，与另一个空间特征编码组件一起，这两个组件在 3D 视频对象检测任务上表现良好。</p>
<p><strong>多任务学习</strong><br>未修剪的视频通常包含许多与目标任务无关的帧，因此，挖掘相关信息并丢弃冗余信息至关重要。为了提取此类信息，Seong 提出了视频多任务Transformer网络，它处理未修剪视频的多任务学习，对于 CoVieW 数据集，任务是场景识别、动作识别和重要性分数预测。 ImageNet 和 Places365 上的两个预训练网络提取场景特征和对象特征。堆叠多任务Transformer以实现特征融合，利用类转换矩阵。</p>
<h4 id="3-4-2-低层视频处理"><a href="#3-4-2-低层视频处理" class="headerlink" title="3.4.2 低层视频处理"></a>3.4.2 低层视频处理</h4><p><strong>帧/视频合成</strong><br>帧合成任务涉及合成两个连续帧之间或帧序列之后的帧，而视频合成任务涉及合成视频。Liu 提出了 ConvTransformer，它由五个部分组成：特征嵌入、位置编码、编码器、查询解码器和合成前馈网络。与基于 LSTM 的作品相比，ConvTransformer 以更可并行化的架构取得了更好的结果。 schatz2020a 提出了另一种基于 Transformer 的方法，它使用循环 Transformer 网络从新颖的视角合成人类行为。</p>
<p><strong>视频修复</strong><br>视频修复任务涉及完成帧内任何缺失的区域，这是具有挑战性的，因为它需要合并空间和时间维度的信息。 Zeng 提出了一个时空Transformer网络，它使用所有输入帧作为输入并并行填充它们，时空对抗损失用于优化 Transformer 网络。</p>
<h4 id="3-4-3-讨论"><a href="#3-4-3-讨论" class="headerlink" title="3.4.3 讨论"></a>3.4.3 讨论</h4><p>与图像相比，视频有一个额外的维度来编码时间信息，利用空间和时间信息有助于更好地理解视频，得益于 Transformer 的关系建模能力，视频处理任务通过同时挖掘空间和时间信息得到了改进。 然而，由于视频数据的高度复杂性和冗余性，如何高效准确地对空间和时间关系进行建模仍然是一个悬而未决的问题。</p>
<h3 id="3-5-多模式任务"><a href="#3-5-多模式任务" class="headerlink" title="3.5 多模式任务"></a>3.5 多模式任务</h3><p>由于 Transformer 在基于文本的 NLP 任务中的成功，许多工作研究热衷于利用其处理多模态任务（例如，视频-文本、图像-文本和音频-文本）的潜力。这方面的一个例子是 VideoBERT，它使用基于 CNN 的模块来预处理视频以获得表示 token 。然后，Transformer编码器在这些标记上进行训练，以学习下游任务（例如视频字幕）的视频文本表示。其他一些示例包括 VisualBERT 和 VL-BERT，它们采用单流统一Transformer来捕获视觉元素和图像文本关系，用于视觉问答 (VQA) 等下游任务和视觉常识推理（VCR）。此外，SpeechBERT 等几项研究探索了使用Transformer编码器对音频和文本对进行编码以处理语音问答 (SQA) 等自动文本任务的可能性。</p>
<p>除了上述开创性的多模态Transformer之外，对比语言-图像预训练 (CLIP) 以自然语言作为监督来学习更有效的图像表示。 CLIP 联合训练文本编码器和图像编码器来预测相应的训练文本-图像对，其文本编码器是一个标准的Transformer，带有掩码的自注意力，用于保留预训练语言模型的初始化能力。对于图像编码器，CLIP 考虑了两种类型的架构，ResNet 和 Vision Transformer，在一个包含从互联网收集的 4 亿对（图像、文本）对的新数据集上进行训练，更具体地，给定一批 $N$（图像，文本）个数据对，CLIP 联合学习文本和图像嵌入，以最大化 $N$ 对匹配嵌入的余弦相似度，同时最小化 $N^{2} - N$ 不正确匹配嵌入。在零样本传输中，CLIP 展示了惊人的零样本分类性能，在不使用任何 ImageNet 训练标签的情况下，在 ImageNet-1K 数据集上实现了 $76.2\%$ top-1 精度。具体来说，在推理时，CLIP 的文本编码器首先计算所有 ImageNet 标签的特征嵌入，然后图像编码器计算所有图像的嵌入。通过计算文本和图像嵌入的余弦相似度，得分最高的文本-图像对应该是图像及其对应的标签。在 30 个各种 CV 基准上的进一步实验表明了 CLIP 的零样本迁移能力和 CLIP 学习到的特征多样性。</p>
<p>CLIP 根据文本描述映射图像，而另一项工作 DALL-E 合成输入文本中描述的类别的新图像。与 GPT-3 类似，DALL-E 是一种多模式Transformer，具有 120 亿个模型参数，在 330 万个文本图像对的数据集上进行自回归训练。更具体地说，为了训练 DALL-E，使用了两阶段训练程序，其中在第 1 阶段，使用离散变分自动编码器将 256$\times$ 256 个 RGB 图像压缩为 32$\times$32 个图像标记，然后在第 2 阶段，一个自回归Transformer被训练来模拟图像和文本标记的联合分布。实验结果表明，DALL-E 可以从头开始生成各种风格的图像，包括逼真图像、卡通和表情符号，或者扩展现有图像，同时仍然匹配文本中的描述。随后，Ding 提出了 CogView，这是一个类似 DALL-E 的带有 VQ-VAE tokenizer 的 Transformer，但支持中文文本输入。他们声称 CogView 优于 DALL-E 和以前的基于 GAN 的方法，而且与 DALL-E 不同的是，CogView 不需要额外的 CLIP 模型来重新排列从 Transformer $i.e.$ DALL-E 中提取的样本。</p>
<p>最近，提出了一种 Unified Transformer (UniT)模型来应对多模态多任务学习，它可以同时处理跨不同领域的多个任务，包括目标检测、自然语言理解和视觉语言推理。具体来说，UniT 有两个 Transformer 编码器分别处理图像和文本输入，然后 Transformer 解码器根据任务模式获取单个或连接的编码器输出，最后，特定于任务的预测头被应用于不同任务的解码器输出。在训练阶段，通过在迭代中随机选择特定任务来联合训练所有任务。实验表明，UniT 使用一组紧凑的模型参数在每项任务上都取得了令人满意的性能。</p>
<p>总之，当前基于 Transformer 的多模态模型展示了其在统一数据和各种模态任务方面的架构优势，这展示了 Transformer 构建通用智能代理以应对大量应用程序的潜力。未来的研究可以在探索多模态 Transformer 的有效训练或可扩展性方面进行。</p>
<h3 id="3-6-Transformer效率"><a href="#3-6-Transformer效率" class="headerlink" title="3.6 Transformer效率"></a>3.6 Transformer效率</h3><p>尽管 Transformer 模型在各种任务中取得了成功，但其对内存和计算资源的高要求阻碍了其在手机等资源受限设备上的实现。 在本节中，我们回顾了为有效实施<strong>压缩</strong>和<strong>加速</strong> Transformer 模型而开展的研究。 这包括包括<strong>网络修剪</strong>、<strong>低秩分解</strong>、<strong>知识蒸馏</strong>、<strong>网络量化</strong>和<strong>紧凑架构设计</strong>。 表4 列出了基于 Transformer 模型压缩的一些代表性作品。</p>
<h4 id="3-6-1-剪枝和压缩"><a href="#3-6-1-剪枝和压缩" class="headerlink" title="3.6.1 剪枝和压缩"></a>3.6.1 剪枝和压缩</h4><p>在基于 Transformer 的预训练模型（例如，BERT）中，并行执行多个注意力操作以独立建模不同标记之间的关系。但是，特定的任务不需要使用所有的头。例如，michel2019sixteen 提供了经验证据表明可以在测试时移除大部分注意力头而不会对性能产生显着影响不同层所需的头数量各不相同——有些层甚至可能只需要一个头。考虑到注意力头上的冗余，定义重要性分数以估计每个 head 对 michel2019sixteen 中最终输出的影响，并且可以删除不重要的 heads 以进行有效部署。 prasanna2020bert 从两个角度分析了预训练 Transformer 模型中的冗余：一般冗余和任务特定冗余。遵循 frankle2018lottery 的彩票假设，prasanna2020bert 分析了 BERT 中的彩票，表明基于 Transformer 的模型中也存在良好的子网络，减少了 FFN 层和注意力头以获得高压缩率。对于将图像分割成多个patch的vit，tang2021patch提出减少patch计算以加速推理，并通过考虑它们的贡献自动发现冗余patch到有效的输出特征。 zhu2021visual 将网络瘦身方法 liu2017learning 扩展到视觉Transformer，以减少 FFN 和注意力模块中线性投影的维度。</p>
<p>除了Transformer模型的宽度，深度（即层数）也可以减少，以加速推理过程。与Transformer模型中不同注意力头可以并行计算的概念不同，不同层必须顺序计算，因为下一层的输入取决于前一层的输出。 fan2019reducing 提出了一种逐层删除策略来规范模型的训练，然后在测试阶段将所有层一起删除。</p>
<p>除了在 Transformer 模型中直接丢弃模块的剪枝方法之外，矩阵分解旨在基于低秩假设用多个小矩阵来逼近大矩阵。 例如 wang2019structured 分解了 Transformer 模型中的标准矩阵乘法，提高了推理效率。</p>
<h4 id="3-6-2-知识蒸馏"><a href="#3-6-2-知识蒸馏" class="headerlink" title="3.6.2 知识蒸馏"></a>3.6.2 知识蒸馏</h4><p>知识蒸馏旨在通过从大型教师网络转移知识来训练学生网络。与教师网络相比，学生网络通常具有更薄更浅的体系结构，更容易部署在资源有限的资源上。神经网络的输出和中间特征也可用于将有效信息从教师传递给学生。专注于 Transformer 模型，mukherjee2020xtremedistil 使用预训练的 BERT 作为教师来指导小模型的训练，利用大量未标记的数据。 wang2020minilm 训练学生网络模仿预训练教师模型中自注意力层的输出。引入值之间的点积作为一种新的知识形式来指导学生。老师的助手 mirzadeh2020improved 在 wang2020minilm 中也被引入，减少了大型预训练Transformer模型和紧凑学生网络之间的差距，从而促进了模仿过程。由于 Transformer 模型中的层类型多种多样（即自注意力层、嵌入层和预测层），tinybert 设计了不同的目标函数来将知识从教师传递给学生。例如，学生模型嵌入层的输出通过 MSE 损失模仿教师的输出。对于vision Transformer，jia2021efficient 提出了细粒度的流形蒸馏方法，通过图像与图像之间的关系挖掘有效知识分割块。</p>
<h4 id="3-6-3-量化"><a href="#3-6-3-量化" class="headerlink" title="3.6.3 量化"></a>3.6.3 量化</h4><p>量化旨在减少表示网络权重或中间特征所需的位数。对一般神经网络的量化方法进行了详细讨论，并实现了与原始网络相当的性能。 最近，人们对如何专门量化 Transformer 模型越来越感兴趣。 例如，shridhar2020end 建议将输入嵌入到二进制高维向量中，然后使用二进制输入表示来训练二进制神经网络。cheong2019Transformers 通过低位（例如，4 位）表示法表示 Transformer 模型中的权重。zhao2020investigation 对各种量化方法进行了实证研究，表明 k-means 量化具有巨大的发展潜力。 针对机器翻译任务，prato2020fully 提出了一种完全量化的Transformer，正如论文所称，这是第一个在翻译质量上没有任何损失的 8 位模型。 此外，liu2021post 探索了一种训练后量化方案，以减少视觉Transformer的内存存储和计算成本。</p>
<h4 id="3-6-4-紧凑结构设计"><a href="#3-6-4-紧凑结构设计" class="headerlink" title="3.6.4 紧凑结构设计"></a>3.6.4 紧凑结构设计</h4><p>除了将预定义的 Transformer 模型压缩成更小的模型外，一些作品还尝试直接设计紧凑的模型。jiang2020convbert 通过提出一个新模块（称为基于跨度的动态卷积）来简化自注意力的计算，该模块结合了全连接层和卷积层。 anonymous2021is 中提出了有趣的``汉堡包’’层，使用矩阵分解来替代原来的self-attention层。与标准的self-attention操作相比，矩阵分解可以更有效地计算，同时清楚地反映不同 token 之间的依赖关系。高效 Transformer 架构的设计也可以通过神经架构搜索(NAS)自动搜索，它会自动搜索如何组合不同的组件。比如su2021vision 搜索的 patch size 线性投影的尺寸和注意模块的头部数量以获得高效的视觉Transformer。 li2021bossnas 探索了一种自我监督的搜索策略，以获得由卷积模块和自注意力模块组成的混合架构。</p>
<p>Transformer 模型中的自注意力操作计算给定序列中不同输入标记的表示之间的点积（图像识别任务中的块vit），其复杂度为 $O(N)$，其中 $N$是序列的长度。最近，有一个目标是将大型方法的复杂性降低到 $O(N)$，以便 Transformer 模型可以扩展到长序列。例如，katharopoulos2020 将自注意力近似为内核特征映射的线性点积，并通过 RNN 揭示标记之间的关系。 zaheer2020big 将每个 token 视为图中的一个顶点，并将两个 token 之间的内积计算定义为一条边。受图论的启发，将各种稀疏图组合起来逼近Transformer模型中的稠密图，可以达到$O(N)$的复杂度。</p>
<p><strong>讨论</strong><br>前面的方法采用不同的方法来尝试识别 Transformer 模型中的冗余~（见图13）。 修剪和分解方法通常需要具有冗余的预定义模型。 具体来说，剪枝侧重于减少 Transformer 模型中组件（例如，层、头）的数量，而分解表示具有多个小矩阵的原始矩阵。 还可以手动（需要足够的专业知识）或自动（例如，通过 NAS）直接设计紧凑型模型。 获得的紧凑模型可以通过量化方法进一步用低位表示，以便在资源有限的设备上有效部署。</p>
<h2 id="4，总结和讨论"><a href="#4，总结和讨论" class="headerlink" title="4，总结和讨论"></a>4，总结和讨论</h2><p>Transformer 因其与 CNN 相比具有竞争力的性能和巨大的潜力而成为计算机视觉领域的热门话题。 为了发现和利用 Transformer，正如本次调查所总结的那样，近年来提出了许多方法。 这些方法在广泛的视觉任务上表现出出色的性能，包括主干、高级/中级视觉、低级视觉和视频处理。 然而，Transformer 用于计算机视觉的潜力尚未得到充分发掘，这意味着仍有一些挑战需要解决。 在本节中，我们将讨论这些挑战并提供对未来前景的见解。</p>
<h3 id="4-1-挑战"><a href="#4-1-挑战" class="headerlink" title="4.1 挑战"></a>4.1 挑战</h3><p>尽管研究人员提出了许多基于 Transformer 的模型来处理计算机视觉任务，但这些工作只是该领域的第一步，还有很大的改进空间。例如，ViT 中的 Transformer 架构遵循 NLP 的标准 Transformer，但专门为 CV 设计的改进版本仍有待探索。此外，有必要将 Transformer 应用于除前面提到的任务之外的更多任务。</p>
<p>用于计算机视觉的 Transformer 的泛化和鲁棒性也具有挑战性。与 CNN 相比，纯 Transformer 缺乏一些归纳偏差，并且严重依赖海量数据集进行大规模训练，因此，数据质量对 Transformer 的泛化性和鲁棒性有重大影响。尽管 ViT 在 CIFAR 和 VTAB 等下游图像分类任务上表现出色，但直接将 ViT 主干应用于对象检测未能取得比 CNN 更好的结果。为了在更通用的视觉任务上更好地泛化预训练的Transformer，还有很长的路要走。从业者关心 Transformer 的健壮性（例如漏洞问题），虽然鲁棒性已经在 zhang2020adversarial 等中进行了研究，但它仍然是一个有待解决的开放性问题。</p>
<p>尽管许多著作已经解释了 Transformer 在 NLP 中的使用，但要清楚地解释为什么 Transformer 在视觉任务上表现良好仍然是一个具有挑战性的课题。归纳偏差，包括翻译等方差和局部性，都带来了 CNN 的成功，但 Transformer 没有任何归纳偏差。目前的文献通常以直观的方式分析效果。例如，Dosovitskiy 声称大规模训练可以超越归纳偏差，位置嵌入被添加到图像块中以保留位置信息，这在计算机视觉任务中很重要。受 Transformer 中大量参数使用的启发，过度参数化可能是视觉 Transformer 可解释性的一个潜在点。</p>
<p>最后但同样重要的是，为 CV 开发<strong>高效</strong>的 Transformer 模型仍然是一个悬而未决的问题，Transformer 模型通常非常庞大且计算量大。例如，基础 ViT 模型需要 180 亿次 FLOP 处理，相比之下，轻量级 CNN 模型 GhostNet 仅需约 6 亿次 FLOP 即可达到类似的性能。尽管已经提出了几种压缩 Transformer 的方法，但它们仍然非常复杂，而这些本来是为 NLP 设计的方法，不一定适合CV。因此，迫切需要高效的 Transformer 模型，以便可以将视觉 Transformer 部署在资源有限的设备上。</p>
<h3 id="4-2-未来展望"><a href="#4-2-未来展望" class="headerlink" title="4.2 未来展望"></a>4.2 未来展望</h3><p>为了推动视觉Transformer的发展，我们为未来的研究提供了几个潜在的方向。</p>
<p>一个方向是 Transformer 在计算机视觉中的有效性和效率。目标是开发高效的视觉Transformer，特别是具有高性能和低资源成本的 Transformer 。性能决定了模型能否应用于实际应用，而资源成本则影响了在设备上的部署。有效性通常与效率相关，因此确定如何在两者之间取得更好的平衡是未来研究的一个有意义的课题。</p>
<p>大多数现有的视觉 Transformer 模型都设计为仅处理单个任务。许多 NLP 模型，例如 GPT-3 已经展示了 Transformer 如何在一个模型中处理多个任务。 IPT 在CV领域也能处理多个低级视觉任务，如超分辨率、图像去噪、去雨等。 Perceiver 和 Perceiver IO 是开创性的模型，可以在多个领域工作，包括图像、音频、多模式、点云，我们相信更多的任务可以只涉及一个模型。在一个Transformer（即大统一模型）中统一所有视觉任务甚至其他任务是一个令人兴奋的话题。</p>
<p>出现了各种类型的神经网络，例如 CNN、RNN 和 Transformer。在CV领域，CNNs曾经是主流选择，但是现在Transformer开始流行了。 CNN 可以捕获归纳偏差，例如翻译等方差和局部性，而 ViT 使用大规模训练来超越归纳偏差。根据目前可用的证据，CNN 在小型数据集上表现良好，而 Transformer 在大型数据集上表现更好。未来的问题是使用 CNN 还是 Transformer。</p>
<p>通过使用大型数据集进行训练，Transformer 可以在 NLP 和 CV 基准测试上实现最先进的性能，神经网络可能需要大数据而不是归纳偏差。最后，我们留给大家一个问题：Transformer 是否可以通过非常简单的计算范式（例如，仅使用全连接层）和海量数据训练来获得令人满意的结果？</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>xinwen
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://github.com/sophia-hxw/sophia-hxw.github.io/2023/06/28/Transformer/A%20Survey%20on%20Vision%20Transformer/" title="A Survey on Vision Transformer">https://github.com/sophia-hxw/sophia-hxw.github.io/2023/06/28/Transformer/A Survey on Vision Transformer/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/transformer/" rel="tag"><i class="fa fa-tag"></i> transformer</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/06/28/ComputerVision/3d-reconstruction-survey/" rel="prev" title="三维重建调研">
      <i class="fa fa-chevron-left"></i> 三维重建调研
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/06/28/Transformer/A%20Survey%20of%20Transformers/" rel="next" title="A Survey of Transformers">
      A Survey of Transformers <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">前言</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1%EF%BC%8C%E4%BB%8B%E7%BB%8D"><span class="nav-text">1，介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2%EF%BC%8CTransformer%E6%8F%90%E5%87%BA"><span class="nav-text">2，Transformer提出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">2.1 自注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Transformer%E5%85%B6%E4%BB%96%E9%87%8D%E8%A6%81%E5%86%85%E5%AE%B9"><span class="nav-text">2.2 Transformer其他重要内容</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3%EF%BC%8C%E8%A7%86%E8%A7%89Transformer"><span class="nav-text">3，视觉Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E7%9A%84backbone"><span class="nav-text">3.1 表征学习的backbone</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-%E7%BA%AFTransformer"><span class="nav-text">3.1.1 纯Transformer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-%E5%B8%A6%E5%8D%B7%E7%A7%AF%E7%9A%84Transformer"><span class="nav-text">3.1.2 带卷积的Transformer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-3-%E8%87%AA%E7%9B%91%E7%9D%A3%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0"><span class="nav-text">3.1.3 自监督表征学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-4-%E8%AE%A8%E8%AE%BA"><span class="nav-text">3.1.4 讨论</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E9%AB%98-%E4%B8%AD%E5%B1%82%E7%9A%84%E8%A7%86%E8%A7%89"><span class="nav-text">3.2 高&#x2F;中层的视觉</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-%E7%94%9F%E6%88%90%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="nav-text">3.2.1 生成目标检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-%E5%88%86%E5%89%B2"><span class="nav-text">3.2.2 分割</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1"><span class="nav-text">3.2.3 姿态估计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-4-%E5%85%B6%E4%BB%96%E4%BB%BB%E5%8A%A1"><span class="nav-text">3.2.4 其他任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-5-%E8%AE%A8%E8%AE%BA"><span class="nav-text">3.2.5 讨论</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E4%BD%8E%E5%B1%82%E8%A7%86%E8%A7%89"><span class="nav-text">3.3 低层视觉</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="nav-text">3.3.1 图像生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86"><span class="nav-text">3.3.2 图像处理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86"><span class="nav-text">3.4 视频处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-1-%E9%AB%98%E5%B1%82%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86"><span class="nav-text">3.4.1 高层视频处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-2-%E4%BD%8E%E5%B1%82%E8%A7%86%E9%A2%91%E5%A4%84%E7%90%86"><span class="nav-text">3.4.2 低层视频处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-3-%E8%AE%A8%E8%AE%BA"><span class="nav-text">3.4.3 讨论</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-%E5%A4%9A%E6%A8%A1%E5%BC%8F%E4%BB%BB%E5%8A%A1"><span class="nav-text">3.5 多模式任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-Transformer%E6%95%88%E7%8E%87"><span class="nav-text">3.6 Transformer效率</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-1-%E5%89%AA%E6%9E%9D%E5%92%8C%E5%8E%8B%E7%BC%A9"><span class="nav-text">3.6.1 剪枝和压缩</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-2-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F"><span class="nav-text">3.6.2 知识蒸馏</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-3-%E9%87%8F%E5%8C%96"><span class="nav-text">3.6.3 量化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-4-%E7%B4%A7%E5%87%91%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="nav-text">3.6.4 紧凑结构设计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4%EF%BC%8C%E6%80%BB%E7%BB%93%E5%92%8C%E8%AE%A8%E8%AE%BA"><span class="nav-text">4，总结和讨论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E6%8C%91%E6%88%98"><span class="nav-text">4.1 挑战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E6%9C%AA%E6%9D%A5%E5%B1%95%E6%9C%9B"><span class="nav-text">4.2 未来展望</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="xinwen"
      src="/images/me.jpg">
  <p class="site-author-name" itemprop="name">xinwen</p>
  <div class="site-description" itemprop="description">想到哪儿记到哪儿的技术博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">122</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">100</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/sophia-hxw" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;sophia-hxw"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/sophia_xw" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;sophia_xw" rel="noopener" target="_blank"><i class="crosshairs fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xinwen618@gmail.com" title="E-Mail → mailto:xinwen618@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2022 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xinwen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">449k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">6:48</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




// 代码折叠
<script src="/js/code-unfold.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'QS91rh0eXkhXnjzhcdHGIRzJ-gzGzoHsz',
      appKey     : 'DD7UTgTkdGwFia0JrJRcs7fs',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
